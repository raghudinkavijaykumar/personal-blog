<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><meta name="generator" content="Gatsby 2.31.1"/><style data-styled="" data-styled-version="5.2.1">.UDybk{text-align:left;margin:1em 0;padding:0.5em;overflow-x:auto;border-radius:3px;font-family:"Consolas",Consolas;font-size:11px;}/*!sc*/
.UDybk .token-line{line-height:1.1em;height:1.1em;}/*!sc*/
data-styled.g1[id="Code__Pre-gy960v-0"]{content:"UDybk,"}/*!sc*/
.llUIua{position:inherit;border:0;border-radius:3px;margin:0.25em;opacity:0.3;}/*!sc*/
.llUIua:hover{opacity:1;}/*!sc*/
data-styled.g2[id="Code__CopyCode-gy960v-1"]{content:"llUIua,"}/*!sc*/
</style><link rel="sitemap" type="application/xml" href="/personal-blog/sitemap.xml"/><style type="text/css">
    .anchor.before {
      position: absolute;
      top: 0;
      left: 0;
      transform: translateX(-100%);
      padding-right: 4px;
    }
    .anchor.after {
      display: inline-block;
      padding-left: 4px;
    }
    h1 .anchor svg,
    h2 .anchor svg,
    h3 .anchor svg,
    h4 .anchor svg,
    h5 .anchor svg,
    h6 .anchor svg {
      visibility: hidden;
    }
    h1:hover .anchor svg,
    h2:hover .anchor svg,
    h3:hover .anchor svg,
    h4:hover .anchor svg,
    h5:hover .anchor svg,
    h6:hover .anchor svg,
    h1 .anchor:focus svg,
    h2 .anchor:focus svg,
    h3 .anchor:focus svg,
    h4 .anchor:focus svg,
    h5 .anchor:focus svg,
    h6 .anchor:focus svg {
      visibility: visible;
    }
  </style><script>
    document.addEventListener("DOMContentLoaded", function(event) {
      var hash = window.decodeURI(location.hash.replace('#', ''))
      if (hash !== '') {
        var element = document.getElementById(hash)
        if (element) {
          var scrollTop = window.pageYOffset || document.documentElement.scrollTop || document.body.scrollTop
          var clientTop = document.documentElement.clientTop || document.body.clientTop || 0
          var offset = element.getBoundingClientRect().top + scrollTop - clientTop
          // Wait for the browser to finish rendering before scrolling.
          setTimeout((function() {
            window.scrollTo(0, offset - 0)
          }), 0)
        }
      }
    })
  </script><link as="script" rel="preload" href="/personal-blog/webpack-runtime-500034a3e408444d0cae.js"/><link as="script" rel="preload" href="/personal-blog/framework-2601ed29d039b1458055.js"/><link as="script" rel="preload" href="/personal-blog/app-eeb4193423903187a951.js"/><link as="script" rel="preload" href="/personal-blog/ce5c1ec6b0c5670e22550a7ef5fd5c2de8a4bdeb-f3cc52c60397f69658d6.js"/><link as="script" rel="preload" href="/personal-blog/component---src-templates-blog-post-template-tsx-a3788de39add0692b44b.js"/><link as="fetch" rel="preload" href="/personal-blog/page-data\kubernetes\page-data.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/personal-blog/page-data/sq/d/4080856488.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/personal-blog/page-data\app-data.json" crossorigin="anonymous"/></head><body><div id="___gatsby"><div style="outline:none" tabindex="-1" id="gatsby-focus-wrapper"><div class="MuiContainer-root MuiContainer-maxWidthLg"><div class="MuiContainer-root MuiContainer-maxWidthLg"><div class="MuiToolbar-root MuiToolbar-regular jss1 MuiToolbar-gutters"><button class="MuiButtonBase-root MuiIconButton-root jss2 MuiIconButton-colorInherit MuiIconButton-edgeStart" tabindex="0" type="button" aria-label="menu"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root" focusable="false" viewBox="0 0 24 24" aria-hidden="true"><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"></path></svg></span></button><h5 class="MuiTypography-root jss3 MuiTypography-h5 MuiTypography-noWrap MuiTypography-alignCenter"><a class="jss4" href="/personal-blog/">Raghu&#x27;s Blog</a></h5><button class="MuiButtonBase-root MuiIconButton-root" tabindex="0" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root" focusable="false" viewBox="0 0 24 24" aria-hidden="true"><path d="M15.5 14h-.79l-.28-.27C15.41 12.59 16 11.11 16 9.5 16 5.91 13.09 3 9.5 3S3 5.91 3 9.5 5.91 16 9.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z"></path></svg></span></button></div><h1>Kubernetes</h1><p>2021 January 19th</p><div style="font-size:30px">Kubernetes</div><h1>Contents</h1><ul><li><a href="#concepts">Concepts</a><ul><li><a href="#overview">Overview</a></li><li><a href="#kubernetes-objects">Kubernetes Objects</a></li><li><a href="#kubernetes-control-plane">Kubernetes Control Plane</a><ul><li><a href="#kubernetes-master">Kubernetes Master</a></li><li><a href="#kubernetes-nodes">Kubernetes Nodes</a><ul><li><a href="#object-metadata">Object Metadata</a></li></ul></li><li><a href="#whats-next">What&#x27;s next</a></li></ul></li><li><a href="#what-is-kubernetes">What is Kubernetes?</a><ul><li><a href="#why-do-i-need-kubernetes-and-what-can-it-do">Why do I need Kubernetes and what can it do?</a></li><li><a href="#how-is-kubernetes-a-platform">How is Kubernetes a platform?</a></li><li><a href="#what-kubernetes-is-not">What Kubernetes is not</a></li><li><a href="#why-containers">Why containers?</a></li><li><a href="#what-doeskubernetesmean-k8s">What does Kubernetes mean? K8s?</a></li><li><a href="#whats-next-1">What&#x27;s next</a></li></ul></li><li><a href="#kubernetes-components">Kubernetes Components</a><ul><li><a href="#master-components">Master Components</a><ul><li><a href="#kube-apiserver">kube-apiserver</a></li><li><a href="#etcd">etcd</a></li><li><a href="#kube-scheduler">kube-scheduler</a></li><li><a href="#kube-controller-manager">kube-controller-manager</a></li><li><a href="#cloud-controller-manager">cloud-controller-manager</a></li></ul></li><li><a href="#node-components">Node Components</a><ul><li><a href="#kubelet">kubelet</a></li><li><a href="#kube-proxy">kube-proxy</a></li><li><a href="#container-runtime">Container Runtime</a></li></ul></li><li><a href="#addons">Addons</a><ul><li><a href="#dns">DNS</a></li><li><a href="#web-ui-dashboard">Web UI (Dashboard)</a></li><li><a href="#container-resource-monitoring">Container Resource Monitoring</a></li><li><a href="#cluster-level-logging">Cluster-level Logging</a></li></ul></li></ul></li><li><a href="#the-kubernetes-api">The Kubernetes API</a><ul><li><a href="#api-changes">API changes</a></li><li><a href="#openapi-and-swagger-definitions">OpenAPI and Swagger definitions</a></li><li><a href="#api-versioning">API versioning</a></li><li><a href="#api-groups">API groups</a></li><li><a href="#enabling-api-groups">Enabling API groups</a></li><li><a href="#enabling-resources-in-the-groups">Enabling resources in the groups</a></li></ul></li><li><a href="#working-with-kubernetes-objects">Working with Kubernetes Objects</a><ul><li><a href="#understanding-kubernetes-objects">Understanding Kubernetes Objects</a><ul><li><a href="#understanding-kubernetes-objects-1">Understanding Kubernetes Objects</a><ul><li><a href="#object-spec-and-status"><strong>Object Spec and Status</strong></a></li><li><a href="#describing-a-kubernetes-object"><strong>Describing a Kubernetes Object</strong></a></li><li><a href="#required-fields"><strong>Required Fields</strong></a></li></ul></li><li><a href="#whats-next-2">What&#x27;s next</a></li></ul></li></ul></li><li><a href="#names">Names</a><ul><li><a href="#names-1">Names</a></li><li><a href="#uids">UIDs</a></li></ul></li><li><a href="#namespaces">Namespaces</a><ul><li><a href="#when-to-use-multiple-namespaces">When to Use Multiple Namespaces</a></li><li><a href="#working-with-namespaces">Working with Namespaces</a><ul><li><a href="#viewing-namespaces">Viewing namespaces</a></li><li><a href="#setting-the-namespace-for-a-request">Setting the namespace for a request</a></li><li><a href="#setting-the-namespace-preference">Setting the namespace preference</a></li></ul></li><li><a href="#namespaces-and-dns">Namespaces and DNS</a></li><li><a href="#not-all-objects-are-in-a-namespace">Not All Objects are in a Namespace</a></li></ul></li><li><a href="#labels-and-selectors">Labels and Selectors</a><ul><li><a href="#motivation">Motivation</a></li><li><a href="#syntax-and-character-set">Syntax and character set</a></li><li><a href="#label-selectors">Label selectors</a><ul><li><a href="#equality-basedrequirement">Equality-based requirement</a></li><li><a href="#set-basedrequirement">Set-based requirement</a></li></ul></li><li><a href="#api">API</a><ul><li><a href="#list-and-watch-filtering">LIST and WATCH filtering</a></li><li><a href="#set-references-in-api-objects">Set references in API objects</a><ul><li><a href="#service-and-replicationcontroller"><strong>Service and ReplicationController</strong></a></li><li><a href="#resources-that-support-set-based-requirements"><strong>Resources that support set-based requirements</strong></a></li><li><a href="#selecting-sets-of-nodes"><strong>Selecting sets of nodes</strong></a></li></ul></li></ul></li></ul></li><li><a href="#annotations">Annotations</a><ul><li><a href="#attaching-metadata-to-objects">Attaching metadata to objects</a></li><li><a href="#whats-next-3">What&#x27;s next</a></li><li><a href="#object-management-using-kubectl">Object Management Using kubectl</a><ul><li><a href="#kubernetes-object-management">Kubernetes Object Management</a><ul><li><a href="#management-techniques"><strong>Management techniques</strong></a></li><li><a href="#imperative-commands"><strong>Imperative commands</strong></a><ul><li><a href="#examples"><strong>Examples</strong></a></li><li><a href="#trade-offs"><strong>Trade-offs</strong></a></li></ul></li><li><a href="#imperative-object-configuration"><strong>Imperative object configuration</strong></a><ul><li><a href="#examples-1"><strong>Examples</strong></a></li><li><a href="#trade-offs-1"><strong>Trade-offs</strong></a></li></ul></li><li><a href="#declarative-object-configuration"><strong>Declarative object configuration</strong></a><ul><li><a href="#examples-2"><strong>Examples</strong></a></li><li><a href="#trade-offs-2"><strong>Trade-offs</strong></a></li></ul></li><li><a href="#whats-next-4"><strong>What&#x27;s next</strong></a></li></ul></li><li><a href="#managing-kubernetes-objects-using-imperative-commands">Managing Kubernetes Objects Using Imperative Commands</a><ul><li><a href="#trade-offs-3"><strong>Trade-offs</strong></a></li><li><a href="#how-to-create-objects"><strong>How to create objects</strong></a></li><li><a href="#how-to-update-objects"><strong>How to update objects</strong></a></li><li><a href="#how-to-delete-objects"><strong>How to delete objects</strong></a></li><li><a href="#how-to-view-an-object"><strong>How to view an object</strong></a></li><li><a href="#usingsetcommands-to-modify-objects-before-creation"><strong>Using </strong>set<strong> commands to modify objects before creation</strong></a></li><li><a href="#using--editto-modify-objects-before-creation"><strong>Using </strong>--edit<strong> to modify objects before creation</strong></a></li><li><a href="#whats-next-5"><strong>What&#x27;s next</strong></a></li></ul></li><li><a href="#imperative-management-of-kubernetes-objects-using-configuration-files">Imperative Management of Kubernetes Objects Using Configuration Files</a><ul><li><a href="#trade-offs-4"><strong>Trade-offs</strong></a></li><li><a href="#how-to-create-objects-1"><strong>How to create objects</strong></a></li><li><a href="#how-to-update-objects-1"><strong>How to update objects</strong></a></li><li><a href="#how-to-delete-objects-1"><strong>How to delete objects</strong></a></li><li><a href="#how-to-view-an-object-1"><strong>How to view an object</strong></a></li><li><a href="#limitations"><strong>Limitations</strong></a></li><li><a href="#creating-and-editing-an-object-from-a-url-without-saving-the-configuration"><strong>Creating and editing an object from a URL without saving the configuration</strong></a></li><li><a href="#migrating-from-imperative-commands-to-imperative-object-configuration"><strong>Migrating from imperative commands to imperative object configuration</strong></a></li><li><a href="#defining-controller-selectors-and-podtemplate-labels"><strong>Defining controller selectors and PodTemplate labels</strong></a></li><li><a href="#whats-next-6"><strong>What&#x27;s next</strong></a></li></ul></li><li><a href="#declarative-management-of-kubernetes-objects-using-configuration-files">Declarative Management of Kubernetes Objects Using Configuration Files</a><ul><li><a href="#trade-offs-5"><strong>Trade-offs</strong></a></li><li><a href="#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="#how-to-create-objects-2"><strong>How to create objects</strong></a></li><li><a href="#how-to-update-objects-2"><strong>How to update objects</strong></a></li><li><a href="#how-to-delete-objects-2"><strong>How to delete objects</strong></a><ul><li><a href="#recommendedkubectl-delete--f-filename"><strong>Recommended: </strong>kubectl delete -f <code>&lt;filename&gt;</code></a></li><li><a href="#alternativekubectl-apply--f-directory---prune--l-yourlabel"><strong>Alternative: </strong>kubectl apply -f <code>&lt;directory/&gt;</code> --prune -l your=label</a></li></ul></li><li><a href="#how-to-view-an-object-2"><strong>How to view an object</strong></a></li><li><a href="#how-apply-calculates-differences-and-merges-changes"><strong>How apply calculates differences and merges changes</strong></a><ul><li><a href="#merge-patch-calculation"><strong>Merge patch calculation</strong></a></li><li><a href="#how-different-types-of-fields-are-merged"><strong>How different types of fields are merged</strong></a></li><li><a href="#merging-changes-to-primitive-fields"><strong>Merging changes to primitive fields</strong></a></li><li><a href="#merging-changes-to-map-fields"><strong>Merging changes to map fields</strong></a></li><li><a href="#merging-changes-for-fields-of-type-list"><strong>Merging changes for fields of type list</strong></a></li></ul></li><li><a href="#default-field-values"><strong>Default field values</strong></a><ul><li><a href="#how-to-clear-server-defaulted-fields-or-fields-set-by-other-writers"><strong>How to clear server-defaulted fields or fields set by other writers</strong></a></li></ul></li><li><a href="#how-to-change-ownership-of-a-field-between-the-configuration-file-and-direct-imperative-writers"><strong>How to change ownership of a field between the configuration file and direct imperative writers</strong></a><ul><li><a href="#changing-the-owner-from-a-direct-imperative-writer-to-a-configuration-file"><strong>Changing the owner from a direct imperative writer to a configuration file</strong></a></li><li><a href="#changing-the-owner-from-a-configuration-file-to-a-direct-imperative-writer"><strong>Changing the owner from a configuration file to a direct imperative writer</strong></a></li></ul></li><li><a href="#changing-management-methods"><strong>Changing management methods</strong></a><ul><li><a href="#migrating-from-imperative-command-management-to-declarative-object-configuration"><strong>Migrating from imperative command management to declarative object configuration</strong></a></li><li><a href="#migrating-from-imperative-object-configuration-to-declarative-object-configuration"><strong>Migrating from imperative object configuration to declarative object configuration</strong></a></li></ul></li><li><a href="#defining-controller-selectors-and-podtemplate-labels-1"><strong>Defining controller selectors and PodTemplate labels</strong></a></li><li><a href="#known-issues"><strong>Known Issues</strong></a></li><li><a href="#whats-next-7"><strong>What&#x27;s next</strong></a></li></ul></li></ul></li></ul></li><li><a href="#kubernetes-architecture">Kubernetes Architecture</a><ul><li><a href="#nodes">Nodes</a><ul><li><a href="#what-is-a-node">What is a node?</a></li><li><a href="#node-status">Node Status</a><ul><li><a href="#addresses"><strong>Addresses</strong></a></li><li><a href="#phase"><strong>Phase</strong></a></li><li><a href="#condition"><strong>Condition</strong></a></li><li><a href="#capacity"><strong>Capacity</strong></a></li><li><a href="#info"><strong>Info</strong></a></li></ul></li><li><a href="#management">Management</a><ul><li><a href="#node-controller"><strong>Node Controller</strong></a></li><li><a href="#self-registration-of-nodes"><strong>Self-Registration of Nodes</strong></a><ul><li><a href="#manual-node-administration"><strong>Manual Node Administration</strong></a></li></ul></li><li><a href="#node-capacity"><strong>Node capacity</strong></a></li></ul></li><li><a href="#api-object">API Object</a></li></ul></li><li><a href="#master-node-communication">Master-Node communication</a><ul><li><a href="#overview-1">Overview</a></li><li><a href="#cluster---master">Cluster -&gt; Master</a></li><li><a href="#master---cluster">Master -&gt; Cluster</a><ul><li><a href="#apiserver---kubelet"><strong>apiserver -&gt; kubelet</strong></a></li><li><a href="#apiserver---nodes-pods-and-services"><strong>apiserver -&gt; nodes, pods, and services</strong></a></li><li><a href="#ssh-tunnels"><strong>SSH Tunnels</strong></a></li></ul></li></ul></li><li><a href="#concepts-underlying-the-cloud-controller-manager">Concepts Underlying the Cloud Controller Manager</a><ul><li><a href="#cloud-controller-manager-1">Cloud Controller Manager</a></li><li><a href="#design">Design</a></li><li><a href="#components-of-the-ccm">Components of the CCM</a></li><li><a href="#functions-of-the-ccm">Functions of the CCM</a><ul><li><a href="#1-kubernetes-controller-manager"><strong>1. Kubernetes controller manager</strong></a><ul><li><a href="#node-controller-1"><strong>Node controller</strong></a></li><li><a href="#route-controller"><strong>Route controller</strong></a></li><li><a href="#service-controller"><strong>Service Controller</strong></a></li><li><a href="#persistentvolumelabels-controller"><strong>PersistentVolumeLabels controller</strong></a></li></ul></li><li><a href="#2-kubelet"><strong>2. Kubelet</strong></a></li><li><a href="#3-kubernetes-api-server"><strong>3. Kubernetes API server</strong></a></li></ul></li><li><a href="#plugin-mechanism">Plugin mechanism</a></li><li><a href="#authorization">Authorization</a><ul><li><a href="#node-controller-2"><strong>Node Controller</strong></a></li><li><a href="#route-controller-1"><strong>Route controller</strong></a></li><li><a href="#service-controller-1"><strong>Service controller</strong></a></li><li><a href="#persistentvolumelabels-controller-1"><strong>PersistentVolumeLabels controller</strong></a></li><li><a href="#others"><strong>Others</strong></a></li></ul></li><li><a href="#vendor-implementations">Vendor Implementations</a></li><li><a href="#cluster-administration">Cluster Administration</a></li></ul></li></ul></li><li><a href="#extending-kubernetes">Extending Kubernetes</a><ul><li><a href="#extending-your-kubernetes-cluster">Extending your Kubernetes Cluster</a><ul><li><a href="#overview-2">Overview</a></li><li><a href="#configuration">Configuration</a></li><li><a href="#extensions">Extensions</a></li><li><a href="#extension-patterns">Extension Patterns</a></li><li><a href="#extension-points">Extension Points</a></li><li><a href="#api-extensions">API Extensions</a><ul><li><a href="#user-defined-types"><strong>User-Defined Types</strong></a></li><li><a href="#combining-new-apis-with-automation"><strong>Combining New APIs with Automation</strong></a></li><li><a href="#changing-built-in-resources"><strong>Changing Built-in Resources</strong></a></li><li><a href="#api-access-extensions"><strong>API Access Extensions</strong></a></li><li><a href="#authentication"><strong>Authentication</strong></a></li><li><a href="#authorization-1"><strong>Authorization</strong></a></li><li><a href="#dynamic-admission-control"><strong>Dynamic Admission Control</strong></a></li></ul></li><li><a href="#infrastructure-extensions">Infrastructure Extensions</a><ul><li><a href="#storage-plugins"><strong>Storage Plugins</strong></a></li><li><a href="#device-plugins"><strong>Device Plugins</strong></a></li><li><a href="#network-plugins"><strong>Network Plugins</strong></a></li><li><a href="#scheduler-extensions"><strong>Scheduler Extensions</strong></a></li></ul></li></ul></li><li><a href="#extending-the-kubernetes-api-with-the-aggregation-layer">Extending the Kubernetes API with the aggregation layer</a><ul><li><a href="#overview-3">Overview</a></li><li><a href="#whats-next-8">What&#x27;s next</a></li></ul></li><li><a href="#custom-resources">Custom Resources</a><ul><li><a href="#custom-resources-1">Custom resources</a><ul><li><a href="#custom-controllers"><strong>Custom controllers</strong></a></li><li><a href="#should-i-add-a-custom-resource-to-my-kubernetes-cluster"><strong>Should I add a custom resource to my Kubernetes Cluster?</strong></a><ul><li><a href="#declarative-apis"><strong>Declarative APIs</strong></a></li></ul></li><li><a href="#should-i-use-a-configmap-or-a-custom-resource"><strong>Should I use a configMap or a custom resource?</strong></a></li></ul></li><li><a href="#adding-custom-resources">Adding custom resources</a></li><li><a href="#customresourcedefinitions">CustomResourceDefinitions</a></li><li><a href="#api-server-aggregation">API server aggregation</a><ul><li><a href="#choosing-a-method-for-adding-custom-resources"><strong>Choosing a method for adding custom resources</strong></a><ul><li><a href="#comparing-ease-of-use"><strong>Comparing ease of use</strong></a></li></ul></li><li><a href="#advanced-features-and-flexibility"><strong>Advanced features and flexibility</strong></a><ul><li><a href="#common-features"><strong>Common Features</strong></a></li></ul></li></ul></li><li><a href="#preparing-to-install-a-custom-resource">Preparing to install a custom resource</a><ul><li><a href="#third-party-code-and-new-points-of-failure"><strong>Third party code and new points of failure</strong></a></li><li><a href="#storage"><strong>Storage</strong></a></li><li><a href="#authentication-authorization-and-auditing"><strong>Authentication, authorization, and auditing</strong></a></li></ul></li><li><a href="#accessing-a-custom-resource">Accessing a custom resource</a></li><li><a href="#whats-next-9">What&#x27;s next</a></li></ul></li><li><a href="#network-plugins-1">Network Plugins</a><ul><li><a href="#installation">Installation</a></li><li><a href="#network-plugin-requirements">Network Plugin Requirements</a><ul><li><a href="#cni"><strong>CNI</strong></a></li><li><a href="#kubenet"><strong>kubenet</strong></a></li><li><a href="#customizing-the-mtu-with-kubenet"><strong>Customizing the MTU (with kubenet)</strong></a></li></ul></li><li><a href="#usage-summary">Usage Summary</a></li></ul></li><li><a href="#device-plugins-1">Device Plugins</a><ul><li><a href="#device-plugin-registration">Device plugin registration</a></li><li><a href="#device-plugin-implementation">Device plugin implementation</a></li><li><a href="#device-plugin-deployment">Device plugin deployment</a></li><li><a href="#examples-3">Examples</a></li></ul></li><li><a href="#service-catalog">Service Catalog</a><ul><li><a href="#example-use-case">Example use case</a></li><li><a href="#architecture">Architecture</a><ul><li><a href="#api-resources"><strong>API Resources</strong></a></li><li><a href="#authentication-1"><strong>Authentication</strong></a></li></ul></li><li><a href="#usage">Usage</a><ul><li><a href="#listing-managed-services-and-service-plans"><strong>Listing managed services and Service Plans</strong></a></li><li><a href="#provisioning-a-new-instance"><strong>Provisioning a new instance</strong></a></li><li><a href="#binding-to-a-managed-service"><strong>Binding to a managed service</strong></a></li><li><a href="#mapping-the-connection-credentials"><strong>Mapping the connection credentials</strong></a><ul><li><a href="#pod-configuration-file"><strong>Pod configuration File</strong></a></li></ul></li></ul></li><li><a href="#whats-next-10">What&#x27;s next</a></li></ul></li><li><a href="#containers">Containers</a><ul><li><a href="#images">Images</a><ul><li><a href="#updating-images"><strong>Updating Images</strong></a></li><li><a href="#using-a-private-registry"><strong>Using a Private Registry</strong></a><ul><li><a href="#using-google-container-registry"><strong>Using Google Container Registry</strong></a></li><li><a href="#using-aws-ec2-container-registry"><strong>Using AWS EC2 Container Registry</strong></a></li><li><a href="#using-azure-container-registry-acr"><strong>Using Azure Container Registry (ACR)</strong></a></li><li><a href="#configuring-nodes-to-authenticate-to-a-private-repository"><strong>Configuring Nodes to Authenticate to a Private Repository</strong></a></li><li><a href="#pre-pulling-images"><strong>Pre-pulling Images</strong></a></li><li><a href="#specifying-imagepullsecrets-on-a-pod"><strong>Specifying ImagePullSecrets on a Pod</strong></a></li><li><a href="#use-cases"><strong>Use Cases</strong></a></li></ul></li></ul></li><li><a href="#container-environment-variables">Container Environment Variables</a><ul><li><a href="#container-environment"><strong>Container environment</strong></a><ul><li><a href="#container-information"><strong>Container information</strong></a></li><li><a href="#cluster-information"><strong>Cluster information</strong></a></li></ul></li><li><a href="#whats-next-11"><strong>What&#x27;s next</strong></a></li></ul></li><li><a href="#container-lifecycle-hooks">Container Lifecycle Hooks</a><ul><li><a href="#overview-4"><strong>Overview</strong></a></li><li><a href="#container-hooks"><strong>Container hooks</strong></a><ul><li><a href="#hook-handler-implementations"><strong>Hook handler implementations</strong></a></li><li><a href="#hook-handler-execution"><strong>Hook handler execution</strong></a></li><li><a href="#hook-delivery-guarantees"><strong>Hook delivery guarantees</strong></a></li><li><a href="#debugging-hook-handlers"><strong>Debugging Hook handlers</strong></a></li></ul></li></ul></li></ul></li></ul></li><li><a href="#workloads">Workloads</a><ul><li><a href="#pods">Pods</a><ul><li><a href="#pod-overview">Pod Overview</a><ul><li><a href="#understanding-pods">Understanding Pods</a><ul><li><a href="#how-pods-manage-multiple-containers"><strong>How Pods manage multiple Containers</strong></a></li></ul></li><li><a href="#working-with-pods"><strong>Working with Pods</strong></a><ul><li><a href="#pods-and-controllers"><strong>Pods and Controllers</strong></a></li></ul></li><li><a href="#pod-templates"><strong>Pod Templates</strong></a></li><li><a href="#whats-next-12"><strong>What&#x27;s next</strong></a></li></ul></li><li><a href="#pods-1">Pods</a><ul><li><a href="#what-is-a-pod"><strong>What is a Pod?</strong></a></li><li><a href="#motivation-for-pods"><strong>Motivation for pods</strong></a><ul><li><a href="#management-1"><strong>Management</strong></a></li><li><a href="#resource-sharing-and-communication"><strong>Resource sharing and communication</strong></a></li></ul></li><li><a href="#uses-of-pods"><strong>Uses of pods</strong></a></li><li><a href="#alternatives-considered"><strong>Alternatives considered</strong></a></li><li><a href="#durability-of-pods-or-lack-thereof"><strong>Durability of pods (or lack thereof)</strong></a></li><li><a href="#termination-of-pods"><strong>Termination of Pods</strong></a><ul><li><a href="#force-deletion-of-pods"><strong>Force deletion of pods</strong></a></li></ul></li><li><a href="#privileged-mode-for-pod-containers"><strong>Privileged mode for pod containers</strong></a></li><li><a href="#api-object-1"><strong>API Object</strong></a></li></ul></li><li><a href="#pod-lifecycle">Pod Lifecycle</a><ul><li><a href="#pod-phase"><strong>Pod phase</strong></a></li><li><a href="#pod-conditions"><strong>Pod conditions</strong></a></li><li><a href="#container-probes"><strong>Container probes</strong></a><ul><li><a href="#when-should-you-use-liveness-or-readiness-probes"><strong>When should you use liveness or readiness probes?</strong></a></li></ul></li><li><a href="#pod-and-container-status"><strong>Pod and Container status</strong></a></li><li><a href="#restart-policy"><strong>Restart policy</strong></a></li><li><a href="#pod-lifetime"><strong>Pod lifetime</strong></a></li><li><a href="#examples-4"><strong>Examples</strong></a><ul><li><a href="#advanced-liveness-probe-example"><strong>Advanced liveness probe example</strong></a></li><li><a href="#example-states"><strong>Example states</strong></a></li></ul></li><li><a href="#whats-next-13"><strong>What&#x27;s next</strong></a></li></ul></li><li><a href="#init-containers">Init Containers</a><ul><li><a href="#understanding-init-containers"><strong>Understanding Init Containers</strong></a><ul><li><a href="#differences-from-regular-containers"><strong>Differences from regular Containers</strong></a></li></ul></li><li><a href="#what-can-init-containers-be-used-for"><strong>What can Init Containers be used for?</strong></a><ul><li><a href="#examples-5"><strong>Examples</strong></a></li><li><a href="#init-containers-in-use"><strong>Init Containers in use</strong></a></li></ul></li><li><a href="#detailed-behavior"><strong>Detailed behavior</strong></a><ul><li><a href="#resources"><strong>Resources</strong></a></li><li><a href="#pod-restart-reasons"><strong>Pod restart reasons</strong></a></li></ul></li><li><a href="#support-and-compatibility"><strong>Support and compatibility</strong></a></li><li><a href="#whats-next-14"><strong>What&#x27;s next</strong></a></li></ul></li><li><a href="#pod-preset">Pod Preset</a><ul><li><a href="#understanding-pod-presets"><strong>Understanding Pod Presets</strong></a></li><li><a href="#how-it-works"><strong>How It Works</strong></a><ul><li><a href="#disable-pod-preset-for-a-specific-pod"><strong>Disable Pod Preset for a Specific Pod</strong></a></li></ul></li><li><a href="#enable-pod-preset"><strong>Enable Pod Preset</strong></a></li><li><a href="#whats-next-15"><strong>What&#x27;s next</strong></a></li></ul></li><li><a href="#disruptions">Disruptions</a><ul><li><a href="#voluntary-and-involuntary-disruptions">Voluntary and Involuntary Disruptions</a></li><li><a href="#dealing-with-disruptions">Dealing with Disruptions</a></li><li><a href="#how-disruption-budgets-work">How Disruption Budgets Work</a></li><li><a href="#pdb-example"><strong>PDB Example</strong></a></li><li><a href="#separating-cluster-owner-and-application-owner-roles"><strong>Separating Cluster Owner and Application Owner Roles</strong></a></li><li><a href="#how-to-perform-disruptive-actions-on-your-cluster"><strong>How to perform Disruptive Actions on your Cluster</strong></a></li></ul></li></ul></li><li><a href="#controllers">Controllers</a><ul><li><a href="#replicaset">ReplicaSet</a><ul><li><a href="#how-to-use-a-replicaset"><strong>How to use a ReplicaSet</strong></a></li><li><a href="#when-to-use-a-replicaset"><strong>When to use a ReplicaSet</strong></a></li><li><a href="#example"><strong>Example</strong></a></li><li><a href="#writing-a-replicaset-spec"><strong>Writing a ReplicaSet Spec</strong></a><ul><li><a href="#pod-template"><strong>Pod Template</strong></a></li><li><a href="#pod-selector"><strong>Pod Selector</strong></a></li><li><a href="#labels-on-a-replicaset"><strong>Labels on a ReplicaSet</strong></a></li><li><a href="#replicas"><strong>Replicas</strong></a></li></ul></li><li><a href="#working-with-replicasets"><strong>Working with ReplicaSets</strong></a><ul><li><a href="#deleting-a-replicaset-and-its-pods"><strong>Deleting a ReplicaSet and its Pods</strong></a></li><li><a href="#deleting-just-a-replicaset"><strong>Deleting just a ReplicaSet</strong></a></li><li><a href="#isolating-pods-from-a-replicaset"><strong>Isolating pods from a ReplicaSet</strong></a></li><li><a href="#scaling-a-replicaset"><strong>Scaling a ReplicaSet</strong></a></li><li><a href="#replicaset-as-an-horizontal-pod-autoscaler-target"><strong>ReplicaSet as an Horizontal Pod Autoscaler Target</strong></a></li></ul></li><li><a href="#alternatives-to-replicaset"><strong>Alternatives to ReplicaSet</strong></a><ul><li><a href="#deployment-recommended"><strong>Deployment (Recommended)</strong></a></li><li><a href="#bare-pods"><strong>Bare Pods</strong></a></li><li><a href="#job"><strong>Job</strong></a></li><li><a href="#daemonset"><strong>DaemonSet</strong></a></li></ul></li></ul></li><li><a href="#replicationcontroller">ReplicationController</a><ul><li><a href="#how-a-replicationcontroller-works"><strong>How a ReplicationController Works</strong></a></li><li><a href="#running-an-example-replicationcontroller"><strong>Running an example ReplicationController</strong></a></li><li><a href="#writing-a-replicationcontroller-spec"><strong>Writing a ReplicationController Spec</strong></a><ul><li><a href="#pod-template-1"><strong>Pod Template</strong></a></li><li><a href="#labels-on-the-replicationcontroller"><strong>Labels on the ReplicationController</strong></a></li><li><a href="#pod-selector-1"><strong>Pod Selector</strong></a></li><li><a href="#multiple-replicas"><strong>Multiple Replicas</strong></a></li></ul></li><li><a href="#working-with-replicationcontrollers"><strong>Working with ReplicationControllers</strong></a><ul><li><a href="#deleting-a-replicationcontroller-and-its-pods"><strong>Deleting a ReplicationController and its Pods</strong></a></li><li><a href="#deleting-just-a-replicationcontroller"><strong>Deleting just a ReplicationController</strong></a></li><li><a href="#isolating-pods-from-a-replicationcontroller"><strong>Isolating pods from a ReplicationController</strong></a></li></ul></li><li><a href="#common-usage-patterns"><strong>Common usage patterns</strong></a><ul><li><a href="#rescheduling"><strong>Rescheduling</strong></a></li><li><a href="#scaling"><strong>Scaling</strong></a></li><li><a href="#rolling-updates"><strong>Rolling updates</strong></a></li><li><a href="#multiple-release-tracks"><strong>Multiple release tracks</strong></a></li><li><a href="#using-replicationcontrollers-with-services"><strong>Using ReplicationControllers with Services</strong></a></li></ul></li><li><a href="#writing-programs-for-replication"><strong>Writing programs for Replication</strong></a></li><li><a href="#responsibilities-of-the-replicationcontroller"><strong>Responsibilities of the ReplicationController</strong></a></li><li><a href="#api-object-2"><strong>API Object</strong></a></li><li><a href="#alternatives-to-replicationcontroller"><strong>Alternatives to ReplicationController</strong></a><ul><li><a href="#replicaset-1"><strong>ReplicaSet</strong></a></li><li><a href="#deployment-recommended-1"><strong>Deployment (Recommended)</strong></a></li><li><a href="#bare-pods-1"><strong>Bare Pods</strong></a></li><li><a href="#job-1"><strong>Job</strong></a></li><li><a href="#daemonset-1"><strong>DaemonSet</strong></a></li></ul></li><li><a href="#for-more-information"><strong>For more information</strong></a></li></ul></li><li><a href="#deployments">Deployments</a><ul><li><a href="#use-case"><strong>Use Case</strong></a></li><li><a href="#creating-a-deployment"><strong>Creating a Deployment</strong></a><ul><li><a href="#pod-template-hash-label"><strong>Pod-template-hash label</strong></a></li></ul></li><li><a href="#updating-a-deployment"><strong>Updating a Deployment</strong></a><ul><li><a href="#rollover-aka-multiple-updates-in-flight"><strong>Rollover (aka multiple updates in-flight)</strong></a></li><li><a href="#label-selector-updates"><strong>Label selector updates</strong></a></li></ul></li><li><a href="#rolling-back-a-deployment"><strong>Rolling Back a Deployment</strong></a><ul><li><a href="#checking-rollout-history-of-a-deployment"><strong>Checking Rollout History of a Deployment</strong></a></li><li><a href="#rolling-back-to-a-previous-revision"><strong>Rolling Back to a Previous Revision</strong></a></li></ul></li><li><a href="#scaling-a-deployment"><strong>Scaling a Deployment</strong></a><ul><li><a href="#proportional-scaling"><strong>Proportional scaling</strong></a></li></ul></li><li><a href="#pausing-and-resuming-a-deployment"><strong>Pausing and Resuming a Deployment</strong></a></li><li><a href="#deployment-status"><strong>Deployment status</strong></a><ul><li><a href="#progressing-deployment"><strong>Progressing Deployment</strong></a></li><li><a href="#complete-deployment"><strong>Complete Deployment</strong></a></li><li><a href="#failed-deployment"><strong>Failed Deployment</strong></a></li><li><a href="#operating-on-a-failed-deployment"><strong>Operating on a failed deployment</strong></a></li></ul></li><li><a href="#clean-up-policy"><strong>Clean up Policy</strong></a></li><li><a href="#use-cases-1"><strong>Use Cases</strong></a><ul><li><a href="#canary-deployment"><strong>Canary Deployment</strong></a></li></ul></li><li><a href="#writing-a-deployment-spec"><strong>Writing a Deployment Spec</strong></a><ul><li><a href="#pod-template-2"><strong>Pod Template</strong></a></li><li><a href="#replicas-1"><strong>Replicas</strong></a></li><li><a href="#selector"><strong>Selector</strong></a></li><li><a href="#strategy"><strong>Strategy</strong></a></li><li><a href="#progress-deadline-seconds"><strong>Progress Deadline Seconds</strong></a></li><li><a href="#min-ready-seconds"><strong>Min Ready Seconds</strong></a></li><li><a href="#rollback-to"><strong>Rollback To</strong></a></li><li><a href="#revision-history-limit"><strong>Revision History Limit</strong></a></li><li><a href="#paused"><strong>Paused</strong></a></li></ul></li><li><a href="#alternative-to-deployments"><strong>Alternative to Deployments</strong></a><ul><li><a href="#kubectl-rolling-update"><strong>kubectl rolling update</strong></a></li></ul></li></ul></li><li><a href="#statefulsets">StatefulSets</a><ul><li><a href="#using-statefulsets"><strong>Using StatefulSets</strong></a></li><li><a href="#limitations-1"><strong>Limitations</strong></a></li><li><a href="#components"><strong>Components</strong></a></li><li><a href="#pod-selector-2"><strong>Pod Selector</strong></a></li><li><a href="#pod-identity"><strong>Pod Identity</strong></a><ul><li><a href="#ordinal-index"><strong>Ordinal Index</strong></a></li><li><a href="#stable-network-id"><strong>Stable Network ID</strong></a></li><li><a href="#stable-storage"><strong>Stable Storage</strong></a></li><li><a href="#pod-name-label"><strong>Pod Name Label</strong></a></li></ul></li><li><a href="#deployment-and-scaling-guarantees"><strong>Deployment and Scaling Guarantees</strong></a><ul><li><a href="#pod-management-policies"><strong>Pod Management Policies</strong></a></li></ul></li><li><a href="#update-strategies"><strong>Update Strategies</strong></a><ul><li><a href="#on-delete"><strong>On Delete</strong></a></li><li><a href="#rolling-updates-1"><strong>Rolling Updates</strong></a></li></ul></li><li><a href="#whats-next-16"><strong>What&#x27;s next</strong></a></li></ul></li><li><a href="#daemonset-2">DaemonSet</a><ul><li><a href="#what-is-a-daemonset"><strong>What is a DaemonSet?</strong></a></li><li><a href="#writing-a-daemonset-spec"><strong>Writing a DaemonSet Spec</strong></a><ul><li><a href="#create-a-daemonset"><strong>Create a DaemonSet</strong></a></li><li><a href="#required-fields-1"><strong>Required Fields</strong></a></li><li><a href="#pod-template-3"><strong>Pod Template</strong></a></li><li><a href="#pod-selector-3"><strong>Pod Selector</strong></a></li><li><a href="#running-pods-on-only-some-nodes"><strong>Running Pods on Only Some Nodes</strong></a></li></ul></li><li><a href="#how-daemon-pods-are-scheduled"><strong>How Daemon Pods are Scheduled</strong></a></li><li><a href="#communicating-with-daemon-pods"><strong>Communicating with Daemon Pods</strong></a></li><li><a href="#updating-a-daemonset"><strong>Updating a DaemonSet</strong></a></li><li><a href="#alternatives-to-daemonset"><strong>Alternatives to DaemonSet</strong></a><ul><li><a href="#init-scripts"><strong>Init Scripts</strong></a></li><li><a href="#bare-pods-2"><strong>Bare Pods</strong></a></li><li><a href="#static-pods"><strong>Static Pods</strong></a></li><li><a href="#deployments-1"><strong>Deployments</strong></a></li></ul></li></ul></li><li><a href="#garbage-collection">Garbage Collection</a><ul><li><a href="#owners-and-dependents"><strong>Owners and dependents</strong></a></li><li><a href="#controlling-how-the-garbage-collector-deletes-dependents"><strong>Controlling how the garbage collector deletes dependents</strong></a><ul><li><a href="#foreground-cascading-deletion"><strong>Foreground cascading deletion</strong></a></li><li><a href="#background-cascading-deletion"><strong>Background cascading deletion</strong></a></li><li><a href="#setting-the-cascading-deletion-policy"><strong>Setting the cascading deletion policy</strong></a></li><li><a href="#additional-note-on-deployments"><strong>Additional note on Deployments</strong></a></li></ul></li><li><a href="#known-issues-1"><strong>Known issues</strong></a></li><li><a href="#whats-next-17"><strong>What&#x27;s next</strong></a></li></ul></li><li><a href="#jobs---run-to-completion">Jobs - Run to Completion</a><ul><li><a href="#what-is-a-job"><strong>What is a Job?</strong></a></li><li><a href="#running-an-example-job"><strong>Running an example Job</strong></a></li><li><a href="#writing-a-job-spec"><strong>Writing a Job Spec</strong></a><ul><li><a href="#pod-template-4"><strong>Pod Template</strong></a></li><li><a href="#pod-selector-4"><strong>Pod Selector</strong></a></li><li><a href="#parallel-jobs"><strong>Parallel Jobs</strong></a></li></ul></li><li><a href="#handling-pod-and-container-failures"><strong>Handling Pod and Container Failures</strong></a><ul><li><a href="#pod-backoff-failure-policy"><strong>Pod Backoff failure policy</strong></a></li></ul></li><li><a href="#job-termination-and-cleanup"><strong>Job Termination and Cleanup</strong></a></li><li><a href="#job-patterns"><strong>Job Patterns</strong></a></li><li><a href="#advanced-usage"><strong>Advanced Usage</strong></a><ul><li><a href="#specifying-your-own-pod-selector"><strong>Specifying your own pod selector</strong></a></li></ul></li><li><a href="#alternatives"><strong>Alternatives</strong></a><ul><li><a href="#bare-pods-3"><strong>Bare Pods</strong></a></li><li><a href="#replication-controller"><strong>Replication Controller</strong></a></li><li><a href="#single-job-starts-controller-pod"><strong>Single Job starts Controller Pod</strong></a></li></ul></li><li><a href="#cron-jobs"><strong>Cron Jobs</strong></a></li></ul></li><li><a href="#cronjob">CronJob</a><ul><li><a href="#what-is-a-cron-job"><strong>What is a cron job?</strong></a><ul><li><a href="#prerequisites"><strong>Prerequisites</strong></a></li></ul></li><li><a href="#creating-a-cron-job"><strong>Creating a Cron Job</strong></a></li><li><a href="#deleting-a-cron-job"><strong>Deleting a Cron Job</strong></a></li><li><a href="#cron-job-limitations"><strong>Cron Job Limitations</strong></a></li><li><a href="#writing-a-cron-job-spec"><strong>Writing a Cron Job Spec</strong></a><ul><li><a href="#schedule"><strong>Schedule</strong></a></li><li><a href="#job-template"><strong>Job Template</strong></a></li><li><a href="#starting-deadline-seconds"><strong>Starting Deadline Seconds</strong></a></li><li><a href="#concurrency-policy"><strong>Concurrency Policy</strong></a></li><li><a href="#suspend"><strong>Suspend</strong></a></li><li><a href="#jobs-history-limits"><strong>Jobs History Limits</strong></a></li></ul></li></ul></li></ul></li></ul></li><li><a href="#configuration-1">Configuration</a><ul><li><a href="#configuration-best-practices">Configuration Best Practices</a><ul><li><a href="#general-configuration-tips">General Configuration Tips</a></li><li><a href="#naked-pods-vs-replicasets-deployments-and-jobs">&quot;Naked&quot; Pods vs ReplicaSets, Deployments, and Jobs</a></li><li><a href="#services">Services</a></li><li><a href="#using-labels">Using Labels</a></li><li><a href="#container-images">Container Images</a></li><li><a href="#using-kubectl">Using kubectl</a></li></ul></li><li><a href="#managing-compute-resources-for-containers">Managing Compute Resources for Containers</a><ul><li><a href="#resource-types">Resource types</a></li><li><a href="#resource-requests-and-limits-of-pod-and-container">Resource requests and limits of Pod and Container</a></li><li><a href="#meaning-of-cpu">Meaning of CPU</a></li><li><a href="#meaning-of-memory">Meaning of memory</a></li><li><a href="#how-pods-with-resource-requests-are-scheduled">How Pods with resource requests are scheduled</a></li><li><a href="#how-pods-with-resource-limits-are-run">How Pods with resource limits are run</a></li><li><a href="#monitoring-compute-resource-usage">Monitoring compute resource usage</a></li><li><a href="#troubleshooting">Troubleshooting</a><ul><li><a href="#my-pods-are-pending-with-event-message-failedscheduling"><strong>My Pods are pending with event message failedScheduling</strong></a></li><li><a href="#my-container-is-terminated"><strong>My Container is terminated</strong></a></li></ul></li><li><a href="#local-ephemeral-storage">Local ephemeral storage</a><ul><li><a href="#requests-and-limits-setting-for-local-ephemeral-storage"><strong>Requests and limits setting for local ephemeral storage</strong></a></li><li><a href="#how-pods-with-ephemeral-storage-requests-are-scheduled"><strong>How Pods with ephemeral-storage requests are scheduled</strong></a></li><li><a href="#how-pods-with-ephemeral-storage-limits-run"><strong>How Pods with ephemeral-storage limits run</strong></a></li></ul></li><li><a href="#extended-resources">Extended Resources</a><ul><li><a href="#managing-extended-resources"><strong>Managing extended resources</strong></a><ul><li><a href="#node-level-extended-resources"><strong>Node-level extended resources</strong></a></li><li><a href="#cluster-level-extended-resources"><strong>Cluster-level extended resources</strong></a></li></ul></li><li><a href="#consuming-extended-resources"><strong>Consuming extended resources</strong></a></li></ul></li><li><a href="#planned-improvements">Planned Improvements</a></li><li><a href="#whats-next-18">What&#x27;s next</a></li></ul></li><li><a href="#assigning-pods-to-nodes">Assigning Pods to Nodes</a><ul><li><a href="#nodeselector">nodeSelector</a><ul><li><a href="#step-zero-prerequisites"><strong>Step Zero: Prerequisites</strong></a></li><li><a href="#step-one-attach-label-to-the-node"><strong>Step One: Attach label to the node</strong></a></li><li><a href="#step-two-add-a-nodeselector-field-to-your-pod-configuration"><strong>Step Two: Add a nodeSelector field to your pod configuration</strong></a></li></ul></li><li><a href="#interlude-built-in-node-labels">Interlude: built-in node labels</a></li><li><a href="#affinity-and-anti-affinity">Affinity and anti-affinity</a><ul><li><a href="#node-affinity-beta-feature"><strong>Node affinity (beta feature)</strong></a></li><li><a href="#inter-pod-affinity-and-anti-affinity-beta-feature"><strong>Inter-pod affinity and anti-affinity (beta feature)</strong></a><ul><li><a href="#an-example-of-a-pod-that-uses-pod-affinity"><strong>An example of a pod that uses pod affinity:</strong></a></li><li><a href="#more-practical-use-cases"><strong>More Practical Use-cases</strong></a></li></ul></li></ul></li></ul></li><li><a href="#taints-and-tolerations">Taints and Tolerations</a><ul><li><a href="#concepts-1">Concepts</a></li><li><a href="#example-use-cases">Example Use Cases</a></li><li><a href="#taint-based-evictions">Taint based Evictions</a></li><li><a href="#taint-nodes-by-condition">Taint Nodes by Condition</a></li></ul></li><li><a href="#secrets">Secrets</a><ul><li><a href="#overview-of-secrets">Overview of Secrets</a><ul><li><a href="#built-in-secrets"><strong>Built-in Secrets</strong></a><ul><li><a href="#service-accounts-automatically-create-and-attach-secrets-with-api-credentials"><strong>Service Accounts Automatically Create and Attach Secrets with API Credentials</strong></a></li></ul></li><li><a href="#creating-your-own-secrets"><strong>Creating your own Secrets</strong></a><ul><li><a href="#creating-a-secret-using-kubectl-create-secret"><strong>Creating a Secret Using kubectl create secret</strong></a></li><li><a href="#creating-a-secret-manually"><strong>Creating a Secret Manually</strong></a></li><li><a href="#decoding-a-secret"><strong>Decoding a Secret</strong></a></li></ul></li><li><a href="#using-secrets"><strong>Using Secrets</strong></a><ul><li><a href="#using-secrets-as-files-from-a-pod"><strong>Using Secrets as Files from a Pod</strong></a></li><li><a href="#using-secrets-as-environment-variables"><strong>Using Secrets as Environment Variables</strong></a></li><li><a href="#using-imagepullsecrets"><strong>Using imagePullSecrets</strong></a></li></ul></li><li><a href="#arranging-for-imagepullsecrets-to-be-automatically-attached"><strong>Arranging for imagePullSecrets to be Automatically Attached</strong></a></li><li><a href="#automatic-mounting-of-manually-created-secrets"><strong>Automatic Mounting of Manually Created Secrets</strong></a></li></ul></li><li><a href="#details">Details</a><ul><li><a href="#restrictions"><strong>Restrictions</strong></a></li><li><a href="#secret-and-pod-lifetime-interaction"><strong>Secret and Pod Lifetime interaction</strong></a></li></ul></li><li><a href="#use-cases-2">Use cases</a><ul><li><a href="#use-case-pod-with-ssh-keys"><strong>Use-Case: Pod with ssh keys</strong></a></li><li><a href="#use-case-pods-with-prod--test-credentials"><strong>Use-Case: Pods with prod / test credentials</strong></a></li><li><a href="#use-case-dotfiles-in-secret-volume"><strong>Use-case: Dotfiles in secret volume</strong></a></li><li><a href="#use-case-secret-visible-to-one-container-in-a-pod"><strong>Use-case: Secret visible to one container in a pod</strong></a></li></ul></li><li><a href="#best-practices">Best practices</a><ul><li><a href="#clients-that-use-the-secrets-api"><strong>Clients that use the secrets API</strong></a></li></ul></li><li><a href="#security-properties">Security Properties</a><ul><li><a href="#protections"><strong>Protections</strong></a></li><li><a href="#risks"><strong>Risks</strong></a></li></ul></li></ul></li><li><a href="#organizing-cluster-access-using-kubeconfig-files">Organizing Cluster Access Using kubeconfig Files</a><ul><li><a href="#supporting-multiple-clusters-users-and-authentication-mechanisms">Supporting multiple clusters, users, and authentication mechanisms</a></li><li><a href="#context">Context</a></li><li><a href="#the-kubeconfig-environment-variable">The KUBECONFIG environment variable</a></li><li><a href="#merging-kubeconfig-files">Merging kubeconfig files</a></li><li><a href="#file-references">File references</a></li><li><a href="#whats-next-19">What&#x27;s next</a></li></ul></li><li><a href="#pod-priority-and-preemption">Pod Priority and Preemption</a><ul><li><a href="#how-to-use-priority-and-preemption">How to use priority and preemption</a></li><li><a href="#enabling-priority-and-preemption">Enabling priority and preemption</a></li><li><a href="#priorityclass">PriorityClass</a><ul><li><a href="#example-priorityclass"><strong>Example PriorityClass</strong></a></li></ul></li><li><a href="#pod-priority">Pod priority</a><ul><li><a href="#effect-of-pod-priority-on-scheduling-order"><strong>Effect of Pod priority on scheduling order</strong></a></li></ul></li><li><a href="#preemption">Preemption</a><ul><li><a href="#user-exposed-information"><strong>User exposed information</strong></a></li><li><a href="#limitations-of-preemption"><strong>Limitations of preemption</strong></a><ul><li><a href="#graceful-termination-of-preemption-victims"><strong>Graceful termination of preemption victims</strong></a></li><li><a href="#poddisruptionbudget-is-supported-but-not-guaranteed"><strong>PodDisruptionBudget is supported, but not guaranteed!</strong></a></li><li><a href="#inter-pod-affinity-on-lower-priority-pods"><strong>Inter-Pod affinity on lower-priority Pods</strong></a></li><li><a href="#cross-node-preemption"><strong>Cross node preemption</strong></a></li></ul></li></ul></li></ul></li></ul></li><li><a href="#services-load-balancing-and-networking">Services, Load Balancing, and Networking</a><ul><li><a href="#services-1">Services</a><ul><li><a href="#defining-a-service">Defining a service</a><ul><li><a href="#services-without-selectors"><strong>Services without selectors</strong></a></li></ul></li><li><a href="#virtual-ips-and-service-proxies">Virtual IPs and service proxies</a><ul><li><a href="#proxy-mode-userspace"><strong>Proxy-mode: userspace</strong></a></li><li><a href="#proxy-mode-iptables"><strong>Proxy-mode: iptables</strong></a></li><li><a href="#proxy-mode-ipvs"><strong>Proxy-mode: ipvs</strong></a></li></ul></li><li><a href="#multi-port-services">Multi-Port Services</a></li><li><a href="#choosing-your-own-ip-address">Choosing your own IP address</a><ul><li><a href="#why-not-use-round-robin-dns"><strong>Why not use round-robin DNS?</strong></a></li></ul></li><li><a href="#discovering-services">Discovering services</a><ul><li><a href="#environment-variables"><strong>Environment variables</strong></a></li><li><a href="#dns-1"><strong>DNS</strong></a></li></ul></li><li><a href="#headless-services">Headless services</a><ul><li><a href="#with-selectors"><strong>With selectors</strong></a></li><li><a href="#without-selectors"><strong>Without selectors</strong></a></li></ul></li><li><a href="#publishing-services---service-types">Publishing services - service types</a><ul><li><a href="#type-nodeport"><strong>Type NodePort</strong></a></li><li><a href="#type-loadbalancer"><strong>Type LoadBalancer</strong></a><ul><li><a href="#internal-load-balancer"><strong>Internal load balancer</strong></a></li><li><a href="#ssl-support-on-aws"><strong>SSL support on AWS</strong></a></li><li><a href="#proxy-protocol-support-on-aws"><strong>PROXY protocol support on AWS</strong></a></li><li><a href="#elb-access-logs-on-aws"><strong>ELB Access Logs on AWS</strong></a></li><li><a href="#connection-draining-on-aws"><strong>Connection Draining on AWS</strong></a></li><li><a href="#other-elb-annotations"><strong>Other ELB annotations</strong></a></li><li><a href="#network-load-balancer-support-on-aws-alpha"><strong>Network Load Balancer support on AWS [alpha]</strong></a></li></ul></li><li><a href="#external-ips"><strong>External IPs</strong></a></li></ul></li><li><a href="#shortcomings">Shortcomings</a></li><li><a href="#future-work">Future work</a></li><li><a href="#the-gory-details-of-virtual-ips">The gory details of virtual IPs</a><ul><li><a href="#avoiding-collisions"><strong>Avoiding collisions</strong></a></li><li><a href="#ips-and-vips"><strong>IPs and VIPs</strong></a><ul><li><a href="#userspace"><strong>Userspace</strong></a></li><li><a href="#iptables"><strong>Iptables</strong></a></li><li><a href="#ipvs"><strong>Ipvs</strong></a></li></ul></li></ul></li><li><a href="#api-object-3">API Object</a></li><li><a href="#for-more-information-1">For More Information</a></li></ul></li><li><a href="#dns-for-services-and-pods">DNS for Services and Pods</a><ul><li><a href="#introduction">Introduction</a><ul><li><a href="#what-things-get-dns-names"><strong>What things get DNS names?</strong></a></li></ul></li><li><a href="#services-2">Services</a><ul><li><a href="#a-records"><strong>A records</strong></a></li><li><a href="#srv-records"><strong>SRV records</strong></a></li></ul></li><li><a href="#pods-2">Pods</a><ul><li><a href="#a-records-1"><strong>A Records</strong></a></li><li><a href="#pods-hostname-and-subdomain-fields"><strong>Pod&#x27;s hostname and subdomain fields</strong></a></li><li><a href="#pods-dns-policy"><strong>Pod&#x27;s DNS Policy</strong></a></li><li><a href="#pods-dns-config"><strong>Pod&#x27;s DNS Config</strong></a></li></ul></li><li><a href="#whats-next-20">What&#x27;s next</a></li></ul></li><li><a href="#connecting-applications-with-services">Connecting Applications with Services</a><ul><li><a href="#the-kubernetes-model-for-connecting-containers">The Kubernetes model for connecting containers</a></li><li><a href="#exposing-pods-to-the-cluster">Exposing pods to the cluster</a></li><li><a href="#creating-a-service">Creating a Service</a></li><li><a href="#accessing-the-service">Accessing the Service</a><ul><li><a href="#environment-variables-1"><strong>Environment Variables</strong></a></li><li><a href="#dns-2"><strong>DNS</strong></a></li></ul></li><li><a href="#securing-the-service">Securing the Service</a></li><li><a href="#exposing-the-service">Exposing the Service</a></li><li><a href="#further-reading">Further reading</a></li></ul></li><li><a href="#ingress">Ingress</a><ul><li><a href="#what-is-ingress">What is Ingress?</a></li><li><a href="#prerequisites-1">Prerequisites</a></li><li><a href="#the-ingress-resource">The Ingress Resource</a></li><li><a href="#ingress-controllers">Ingress controllers</a></li><li><a href="#before-you-begin-1">Before you begin</a></li><li><a href="#types-of-ingress">Types of Ingress</a><ul><li><a href="#single-service-ingress"><strong>Single Service Ingress</strong></a></li><li><a href="#simple-fanout"><strong>Simple fanout</strong></a></li><li><a href="#name-based-virtual-hosting"><strong>Name based virtual hosting</strong></a></li><li><a href="#tls"><strong>TLS</strong></a></li><li><a href="#loadbalancing"><strong>Loadbalancing</strong></a></li></ul></li><li><a href="#updating-an-ingress">Updating an Ingress</a></li><li><a href="#failing-across-availability-zones">Failing across availability zones</a></li><li><a href="#future-work-1">Future Work</a></li><li><a href="#alternatives-1">Alternatives</a></li></ul></li><li><a href="#network-policies">Network Policies</a><ul><li><a href="#prerequisites-2">Prerequisites</a></li><li><a href="#isolated-and-non-isolated-pods">Isolated and Non-isolated Pods</a></li><li><a href="#thenetworkpolicyresource">The NetworkPolicy Resource</a></li><li><a href="#default-policies">Default policies</a><ul><li><a href="#default-deny-all-ingress-traffic"><strong>Default deny all ingress traffic</strong></a></li><li><a href="#default-allow-all-ingress-traffic"><strong>Default allow all ingress traffic</strong></a></li><li><a href="#default-deny-all-egress-traffic"><strong>Default deny all egress traffic</strong></a></li><li><a href="#default-allow-all-egress-traffic"><strong>Default allow all egress traffic</strong></a></li><li><a href="#default-deny-all-ingress-and-all-egress-traffic"><strong>Default deny all ingress and all egress traffic</strong></a></li></ul></li><li><a href="#whats-next-21">What&#x27;s next?</a></li></ul></li><li><a href="#adding-entries-to-pod-etchosts-with-hostaliases">Adding entries to Pod /etc/hosts with HostAliases</a><ul><li><a href="#default-hosts-file-content">Default Hosts File Content</a></li><li><a href="#adding-additional-entries-with-hostaliases">Adding Additional Entries with HostAliases</a></li><li><a href="#limitations-2">Limitations</a></li><li><a href="#why-does-kubelet-manage-the-hosts-file">Why Does Kubelet Manage the Hosts File?</a></li></ul></li></ul></li><li><a href="#storage-1">Storage</a><ul><li><a href="#volumes">Volumes</a><ul><li><a href="#background">Background</a></li><li><a href="#types-of-volumes">Types of Volumes</a><ul><li><a href="#awselasticblockstore"><strong>awsElasticBlockStore</strong></a><ul><li><a href="#creating-an-ebs-volume"><strong>Creating an EBS volume</strong></a></li><li><a href="#aws-ebs-example-configuration"><strong>AWS EBS Example configuration</strong></a></li></ul></li><li><a href="#azuredisk"><strong>azureDisk</strong></a></li><li><a href="#azurefile"><strong>azureFile</strong></a></li><li><a href="#cephfs"><strong>cephfs</strong></a></li><li><a href="#configmap"><strong>configMap</strong></a></li><li><a href="#downwardapi"><strong>downwardAPI</strong></a></li><li><a href="#emptydir"><strong>emptyDir</strong></a><ul><li><a href="#example-pod"><strong>Example pod</strong></a></li></ul></li><li><a href="#fc-fibre-channel"><strong>fc (fibre channel)</strong></a></li><li><a href="#flocker"><strong>flocker</strong></a></li><li><a href="#gcepersistentdisk"><strong>gcePersistentDisk</strong></a><ul><li><a href="#creating-a-pd"><strong>Creating a PD</strong></a></li><li><a href="#example-pod-1"><strong>Example pod</strong></a></li></ul></li><li><a href="#gitrepo"><strong>gitRepo</strong></a></li><li><a href="#glusterfs"><strong>glusterfs</strong></a></li><li><a href="#hostpath"><strong>hostPath</strong></a><ul><li><a href="#example-pod-2"><strong>Example pod</strong></a></li></ul></li><li><a href="#iscsi"><strong>iscsi</strong></a></li><li><a href="#local"><strong>local</strong></a></li><li><a href="#nfs"><strong>nfs</strong></a></li><li><a href="#persistentvolumeclaim"><strong>persistentVolumeClaim</strong></a></li><li><a href="#projected"><strong>projected</strong></a><ul><li><a href="#example-pod-with-a-secret-a-downward-api-and-a-configmap"><strong>Example pod with a secret, a downward API, and a configmap.</strong></a></li><li><a href="#example-pod-with-multiple-secrets-with-a-non-default-permission-mode-set"><strong>Example pod with multiple secrets with a non-default permission mode set.</strong></a></li></ul></li><li><a href="#portworxvolume"><strong>portworxVolume</strong></a></li><li><a href="#quobyte"><strong>quobyte</strong></a></li><li><a href="#rbd"><strong>rbd</strong></a></li><li><a href="#scaleio"><strong>scaleIO</strong></a></li><li><a href="#secret"><strong>secret</strong></a></li><li><a href="#storageos"><strong>storageOS</strong></a></li><li><a href="#vspherevolume"><strong>vsphereVolume</strong></a><ul><li><a href="#creating-a-vmdk-volume"><strong>Creating a VMDK volume</strong></a></li><li><a href="#vsphere-vmdk-example-configuration"><strong>vSphere VMDK Example configuration</strong></a></li></ul></li></ul></li><li><a href="#using-subpath">Using subPath</a></li><li><a href="#resources-1">Resources</a></li><li><a href="#out-of-tree-volume-plugins">Out-of-Tree Volume Plugins</a><ul><li><a href="#csi"><strong>CSI</strong></a></li><li><a href="#flexvolume"><strong>FlexVolume</strong></a></li></ul></li><li><a href="#mount-propagation">Mount propagation</a><ul><li><a href="#configuration-2"><strong>Configuration</strong></a></li></ul></li><li><a href="#whats-next-22">What&#x27;s next</a></li></ul></li><li><a href="#persistent-volumes">Persistent Volumes</a><ul><li><a href="#introduction-1">Introduction</a></li><li><a href="#lifecycle-of-a-volume-and-claim">Lifecycle of a volume and claim</a><ul><li><a href="#provisioning"><strong>Provisioning</strong></a><ul><li><a href="#static"><strong>Static</strong></a></li><li><a href="#dynamic"><strong>Dynamic</strong></a></li></ul></li><li><a href="#binding"><strong>Binding</strong></a></li><li><a href="#using"><strong>Using</strong></a></li><li><a href="#storage-object-in-use-protection"><strong>Storage Object in Use Protection</strong></a></li><li><a href="#reclaiming"><strong>Reclaiming</strong></a><ul><li><a href="#retain"><strong>Retain</strong></a></li><li><a href="#delete"><strong>Delete</strong></a></li><li><a href="#recycle"><strong>Recycle</strong></a></li></ul></li><li><a href="#expanding-persistent-volumes-claims"><strong>Expanding Persistent Volumes Claims</strong></a></li></ul></li><li><a href="#types-of-persistent-volumes">Types of Persistent Volumes</a></li><li><a href="#persistent-volumes-1">Persistent Volumes</a><ul><li><a href="#capacity-1"><strong>Capacity</strong></a></li><li><a href="#volume-mode"><strong>Volume Mode</strong></a></li><li><a href="#access-modes"><strong>Access Modes</strong></a></li><li><a href="#class"><strong>Class</strong></a></li><li><a href="#reclaim-policy"><strong>Reclaim Policy</strong></a></li><li><a href="#mount-options"><strong>Mount Options</strong></a></li><li><a href="#phase-1"><strong>Phase</strong></a></li></ul></li><li><a href="#persistentvolumeclaims">PersistentVolumeClaims</a><ul><li><a href="#access-modes-1"><strong>Access Modes</strong></a></li><li><a href="#volume-modes"><strong>Volume Modes</strong></a></li><li><a href="#resources-2"><strong>Resources</strong></a></li><li><a href="#selector-1"><strong>Selector</strong></a></li><li><a href="#class-1"><strong>Class</strong></a></li></ul></li><li><a href="#claims-as-volumes">Claims As Volumes</a><ul><li><a href="#a-note-on-namespaces"><strong>A Note on Namespaces</strong></a></li></ul></li><li><a href="#raw-block-volume-support">Raw Block Volume Support</a><ul><li><a href="#persistent-volumes-using-a-raw-block-volume"><strong>Persistent Volumes using a Raw Block Volume</strong></a></li><li><a href="#persistent-volume-claim-requesting-a-raw-block-volume"><strong>Persistent Volume Claim requesting a Raw Block Volume</strong></a></li><li><a href="#pod-specification-adding-raw-block-device-path-in-container"><strong>Pod specification adding Raw Block Device path in container</strong></a></li><li><a href="#binding-block-volumes"><strong>Binding Block Volumes</strong></a></li></ul></li><li><a href="#writing-portable-configuration">Writing Portable Configuration</a></li></ul></li><li><a href="#storage-classes">Storage Classes</a><ul><li><a href="#introduction-2">Introduction</a></li><li><a href="#the-storageclass-resource">The StorageClass Resource</a><ul><li><a href="#provisioner"><strong>Provisioner</strong></a></li><li><a href="#reclaim-policy-1"><strong>Reclaim Policy</strong></a></li><li><a href="#mount-options-1"><strong>Mount Options</strong></a></li></ul></li><li><a href="#parameters">Parameters</a><ul><li><a href="#aws"><strong>AWS</strong></a></li><li><a href="#gce"><strong>GCE</strong></a></li><li><a href="#glusterfs-1"><strong>Glusterfs</strong></a></li><li><a href="#openstack-cinder"><strong>OpenStack Cinder</strong></a></li><li><a href="#vsphere"><strong>vSphere</strong></a></li><li><a href="#ceph-rbd"><strong>Ceph RBD</strong></a></li><li><a href="#quobyte-1"><strong>Quobyte</strong></a></li><li><a href="#azure-disk"><strong>Azure Disk</strong></a><ul><li><a href="#azure-unmanaged-disk-storage-class"><strong>Azure Unmanaged Disk Storage Class</strong></a></li><li><a href="#new-azure-disk-storage-class-starting-from-v172"><strong>New Azure Disk Storage Class (starting from v1.7.2)</strong></a></li></ul></li><li><a href="#azure-file"><strong>Azure File</strong></a></li><li><a href="#portworx-volume"><strong>Portworx Volume</strong></a></li><li><a href="#scaleio-1"><strong>ScaleIO</strong></a></li><li><a href="#storageos-1"><strong>StorageOS</strong></a></li><li><a href="#local-1"><strong>Local</strong></a></li></ul></li></ul></li><li><a href="#dynamic-volume-provisioning">Dynamic Volume Provisioning</a><ul><li><a href="#background-1">Background</a></li><li><a href="#enabling-dynamic-provisioning">Enabling Dynamic Provisioning</a></li><li><a href="#using-dynamic-provisioning">Using Dynamic Provisioning</a></li><li><a href="#defaulting-behavior">Defaulting Behavior</a></li></ul></li></ul></li><li><a href="#cluster-administration-1">Cluster Administration</a><ul><li><a href="#cluster-administration-overview">Cluster Administration Overview</a><ul><li><a href="#planning-a-cluster">Planning a cluster</a></li><li><a href="#managing-a-cluster">Managing a cluster</a></li><li><a href="#securing-a-cluster">Securing a cluster</a><ul><li><a href="#securing-the-kubelet"><strong>Securing the kubelet</strong></a></li></ul></li><li><a href="#optional-cluster-services">Optional Cluster Services</a></li></ul></li><li><a href="#certificates">Certificates</a><ul><li><a href="#creating-certificates">Creating Certificates</a><ul><li><a href="#easyrsa"><strong>easyrsa</strong></a></li><li><a href="#openssl"><strong>openssl</strong></a></li><li><a href="#cfssl"><strong>cfssl</strong></a></li></ul></li><li><a href="#distributing-self-signed-ca-certificate">Distributing Self-Signed CA Certificate</a></li><li><a href="#certificates-api">Certificates API</a></li></ul></li><li><a href="#cloud-providers">Cloud Providers</a><ul><li><a href="#aws-1">AWS</a><ul><li><a href="#load-balancers"><strong>Load Balancers</strong></a></li></ul></li><li><a href="#openstack">OpenStack</a><ul><li><a href="#cloudconf"><strong>cloud.conf</strong></a><ul><li><a href="#typical-configuration"><strong>Typical configuration</strong></a></li></ul></li></ul></li></ul></li><li><a href="#managing-resources">Managing Resources</a><ul><li><a href="#organizing-resource-configurations">Organizing resource configurations</a></li><li><a href="#bulk-operations-in-kubectl">Bulk operations in kubectl</a></li><li><a href="#using-labels-effectively">Using labels effectively</a></li><li><a href="#canary-deployments">Canary deployments</a></li><li><a href="#updating-labels">Updating labels</a></li><li><a href="#updating-annotations">Updating annotations</a></li><li><a href="#scaling-your-application">Scaling your application</a></li><li><a href="#in-place-updates-of-resources">In-place updates of resources</a><ul><li><a href="#kubectl-apply"><strong>kubectl apply</strong></a></li><li><a href="#kubectl-edit"><strong>kubectl edit</strong></a></li><li><a href="#kubectl-patch"><strong>kubectl patch</strong></a></li></ul></li><li><a href="#disruptive-updates">Disruptive updates</a></li><li><a href="#updating-your-application-without-a-service-outage">Updating your application without a service outage</a></li><li><a href="#whats-next-23">What&#x27;s next?</a></li></ul></li><li><a href="#cluster-networking">Cluster Networking</a><ul><li><a href="#summary">Summary</a></li><li><a href="#docker-model">Docker model</a></li><li><a href="#kubernetes-model">Kubernetes model</a></li><li><a href="#how-to-achieve-this">How to achieve this</a><ul><li><a href="#aci"><strong>ACI</strong></a></li><li><a href="#big-cloud-fabric-from-big-switch-networks"><strong>Big Cloud Fabric from Big Switch Networks</strong></a></li><li><a href="#cilium"><strong>Cilium</strong></a></li><li><a href="#contiv"><strong>Contiv</strong></a></li><li><a href="#contrail"><strong>Contrail</strong></a></li><li><a href="#flannel"><strong>Flannel</strong></a></li><li><a href="#google-compute-engine-gce"><strong>Google Compute Engine (GCE)</strong></a></li><li><a href="#kube-router"><strong>Kube-router</strong></a></li><li><a href="#l2-networks-and-linux-bridging"><strong>L2 networks and linux bridging</strong></a></li><li><a href="#multus-a-multi-network-plugin"><strong>Multus (a Multi Network plugin)</strong></a></li><li><a href="#nsx-t"><strong>NSX-T</strong></a></li><li><a href="#nuage-networks-vcs-virtualized-cloud-services"><strong>Nuage Networks VCS (Virtualized Cloud Services)</strong></a></li><li><a href="#openvswitch"><strong>OpenVSwitch</strong></a></li><li><a href="#ovn-open-virtual-networking"><strong>OVN (Open Virtual Networking)</strong></a></li><li><a href="#project-calico"><strong>Project Calico</strong></a></li><li><a href="#romana"><strong>Romana</strong></a></li><li><a href="#weave-net-from-weaveworks"><strong>Weave Net from Weaveworks</strong></a></li><li><a href="#cni-genie-from-huawei"><strong>CNI-Genie from Huawei</strong></a></li></ul></li><li><a href="#other-reading">Other reading</a></li></ul></li><li><a href="#network-plugins-2">Network Plugins</a><ul><li><a href="#installation-1">Installation</a></li><li><a href="#network-plugin-requirements-1">Network Plugin Requirements</a><ul><li><a href="#cni-1"><strong>CNI</strong></a></li><li><a href="#kubenet-1"><strong>kubenet</strong></a></li><li><a href="#customizing-the-mtu-with-kubenet-1"><strong>Customizing the MTU (with kubenet)</strong></a></li></ul></li><li><a href="#usage-summary-1">Usage Summary</a></li></ul></li><li><a href="#logging-architecture">Logging Architecture</a><ul><li><a href="#basic-logging-in-kubernetes">Basic logging in Kubernetes</a></li><li><a href="#logging-at-the-node-level">Logging at the node level</a><ul><li><a href="#system-component-logs"><strong>System component logs</strong></a></li></ul></li><li><a href="#cluster-level-logging-architectures">Cluster-level logging architectures</a><ul><li><a href="#using-a-node-logging-agent"><strong>Using a node logging agent</strong></a></li><li><a href="#using-a-sidecar-container-with-the-logging-agent"><strong>Using a sidecar container with the logging agent</strong></a><ul><li><a href="#streaming-sidecar-container"><strong>Streaming sidecar container</strong></a></li><li><a href="#sidecar-container-with-a-logging-agent"><strong>Sidecar container with a logging agent</strong></a></li></ul></li><li><a href="#exposing-logs-directly-from-the-application"><strong>Exposing logs directly from the application</strong></a></li></ul></li></ul></li><li><a href="#configuring-kubelet-garbage-collection">Configuring kubelet Garbage Collection</a><ul><li><a href="#image-collection"><strong>Image Collection</strong></a></li><li><a href="#container-collection"><strong>Container Collection</strong></a></li><li><a href="#user-configuration"><strong>User Configuration</strong></a></li><li><a href="#deprecation"><strong>Deprecation</strong></a></li></ul></li><li><a href="#federation">Federation</a><ul><li><a href="#why-federation">Why federation</a><ul><li><a href="#caveats"><strong>Caveats</strong></a></li><li><a href="#hybrid-cloud-capabilities"><strong>Hybrid cloud capabilities</strong></a></li></ul></li><li><a href="#setting-up-federation">Setting up federation</a></li><li><a href="#api-resources-1">API resources</a></li><li><a href="#cascading-deletion">Cascading deletion</a></li><li><a href="#scope-of-a-single-cluster">Scope of a single cluster</a></li><li><a href="#selecting-the-right-number-of-clusters">Selecting the right number of clusters</a></li><li><a href="#whats-next-24">What&#x27;s next</a></li></ul></li><li><a href="#proxies-in-kubernetes">Proxies in Kubernetes</a><ul><li><a href="#proxies">Proxies</a></li><li><a href="#requesting-redirects">Requesting redirects</a></li></ul></li><li><a href="#controller-manager-metrics">Controller manager metrics</a><ul><li><a href="#what-are-controller-manager-metrics">What are controller manager metrics</a></li><li><a href="#configuration-3">Configuration</a></li></ul></li><li><a href="#device-plugins-2">Device Plugins</a><ul><li><a href="#device-plugin-registration-1">Device plugin registration</a></li><li><a href="#device-plugin-implementation-1">Device plugin implementation</a></li><li><a href="#device-plugin-deployment-1">Device plugin deployment</a></li><li><a href="#examples-6">Examples</a></li></ul></li><li><a href="#resource-quotas">Resource Quotas</a><ul><li><a href="#enabling-resource-quota">Enabling Resource Quota</a></li><li><a href="#compute-resource-quota">Compute Resource Quota</a></li><li><a href="#storage-resource-quota">Storage Resource Quota</a></li><li><a href="#object-count-quota">Object Count Quota</a></li><li><a href="#quota-scopes">Quota Scopes</a></li><li><a href="#requests-vs-limits">Requests vs Limits</a></li><li><a href="#viewing-and-setting-quotas">Viewing and Setting Quotas</a></li><li><a href="#quota-and-cluster-capacity">Quota and Cluster Capacity</a></li><li><a href="#example-1">Example</a></li><li><a href="#read-more">Read More</a></li></ul></li><li><a href="#pod-security-policies">Pod Security Policies</a><ul><li><a href="#what-is-a-pod-security-policy">What is a Pod Security Policy?</a></li><li><a href="#enabling-pod-security-policies">Enabling Pod Security Policies</a></li><li><a href="#authorizing-policies">Authorizing Policies</a><ul><li><a href="#via-rbac"><strong>Via RBAC</strong></a></li><li><a href="#troubleshooting-1"><strong>Troubleshooting</strong></a></li></ul></li><li><a href="#policy-order">Policy Order</a></li><li><a href="#example-2">Example</a><ul><li><a href="#set-up"><strong>Set up</strong></a></li><li><a href="#create-a-policy-and-a-pod"><strong>Create a policy and a pod</strong></a></li><li><a href="#run-another-pod"><strong>Run another pod</strong></a></li><li><a href="#clean-up"><strong>Clean up</strong></a></li><li><a href="#example-policies"><strong>Example Policies</strong></a></li></ul></li><li><a href="#policy-reference">Policy Reference</a><ul><li><a href="#privileged"><strong>Privileged</strong></a></li><li><a href="#host-namespaces"><strong>Host namespaces</strong></a></li><li><a href="#volumes-and-file-systems"><strong>Volumes and file systems</strong></a></li><li><a href="#flexvolume-drivers"><strong>FlexVolume drivers</strong></a></li><li><a href="#users-and-groups"><strong>Users and groups</strong></a></li><li><a href="#privilege-escalation"><strong>Privilege Escalation</strong></a></li><li><a href="#capabilities"><strong>Capabilities</strong></a></li><li><a href="#selinux"><strong>SELinux</strong></a></li><li><a href="#apparmor"><strong>AppArmor</strong></a></li><li><a href="#seccomp"><strong>Seccomp</strong></a></li><li><a href="#sysctl"><strong>Sysctl</strong></a></li></ul></li></ul></li></ul></li></ul></li><li><a href="#tutorials">Tutorials</a><ul><li><a href="#kubernetes-basics">Kubernetes Basics</a><ul><li><a href="#what-can-kubernetes-do-for-you">What can Kubernetes do for you?</a></li></ul></li><li><a href="#create-a-cluster">Create a Cluster</a><ul><li><a href="#using-minikube-to-create-a-cluster">Using Minikube to Create a Cluster</a><ul><li><a href="#objectives">Objectives</a></li><li><a href="#kubernetes-clusters">Kubernetes Clusters</a><ul><li><a href="#summary-1">Summary</a></li></ul></li><li><a href="#cluster-diagram">Cluster Diagram</a></li></ul></li></ul></li><li><a href="#deploy-an-app">Deploy an App</a><ul><li><a href="#using-kubectl-to-create-a-deployment">Using kubectl to Create a Deployment</a><ul><li><a href="#objectives-1">Objectives</a></li><li><a href="#kubernetes-deployments">Kubernetes Deployments</a></li><li><a href="#summary-2">Summary:</a></li><li><a href="#deploying-your-first-app-on-kubernetes">Deploying your first app on Kubernetes</a></li></ul></li></ul></li><li><a href="#explore-your-app">Explore your App</a><ul><li><a href="#viewing-pods-and-nodes">Viewing Pods and Nodes</a><ul><li><a href="#objectives-2">Objectives</a></li><li><a href="#kubernetes-pods">Kubernetes Pods</a></li><li><a href="#summary-3">Summary</a></li><li><a href="#pods-overview">Pods overview</a></li><li><a href="#nodes-1">Nodes</a></li><li><a href="#node-overview">Node overview</a></li><li><a href="#troubleshooting-with-kubectl">Troubleshooting with kubectl</a></li></ul></li></ul></li><li><a href="#expose-your-app">Expose Your App</a><ul><li><a href="#using-a-service-to-expose-your-app">Using a Service to Expose Your App</a><ul><li><a href="#objectives-3">Objectives</a></li><li><a href="#overview-of-kubernetes-services">Overview of Kubernetes Services</a></li><li><a href="#summary-4">Summary</a></li><li><a href="#services-and-labels">Services and Labels</a></li></ul></li></ul></li><li><a href="#scale-your-app">Scale your App</a><ul><li><a href="#running-multiple-instances-of-your-app">Running Multiple Instances of Your App</a><ul><li><a href="#objectives-4">Objectives</a></li><li><a href="#scaling-an-application">Scaling an application</a></li><li><a href="#summary-5">Summary</a></li><li><a href="#scaling-overview">Scaling overview</a></li></ul></li></ul></li><li><a href="#update-your-app">Update your App</a><ul><li><a href="#performing-a-rolling-update">Performing a Rolling Update</a><ul><li><a href="#objectives-5">Objectives</a></li><li><a href="#updating-an-application">Updating an application</a></li><li><a href="#summary-6">Summary</a></li><li><a href="#rolling-updates-overview">Rolling updates overview</a></li></ul></li></ul></li><li><a href="#overview-of-kubernetes-online-training">Overview of Kubernetes Online Training</a></li><li><a href="#hello-minikube">Hello Minikube</a><ul><li><a href="#objectives-6">Objectives</a></li><li><a href="#before-you-begin-2">Before you begin</a></li><li><a href="#create-a-minikube-cluster">Create a Minikube cluster</a></li><li><a href="#create-your-nodejs-application">Create your Node.js application</a></li><li><a href="#create-a-docker-container-image">Create a Docker container image</a></li><li><a href="#create-a-deployment">Create a Deployment</a></li><li><a href="#create-a-service">Create a Service</a></li><li><a href="#update-your-app-1">Update your app</a></li><li><a href="#enable-addons">Enable addons</a></li><li><a href="#clean-up-1">Clean up</a></li></ul></li><li><a href="#kubernetes-101">Kubernetes 101</a><ul><li><a href="#kubectl-cli-and-pods">Kubectl CLI and Pods</a></li><li><a href="#kubectl-cli">Kubectl CLI</a></li><li><a href="#pods-3">Pods</a><ul><li><a href="#pod-definition">Pod Definition</a><ul><li><a href="#pod-management"><strong>Pod Management</strong></a></li></ul></li><li><a href="#volumes-1">Volumes</a><ul><li><a href="#volume-types">Volume Types</a></li></ul></li><li><a href="#multiple-containers">Multiple Containers</a></li></ul></li></ul></li><li><a href="#kubernetes-201">Kubernetes 201</a><ul><li><a href="#labels-deployments-services-and-health-checking">Labels, Deployments, Services and Health Checking</a></li><li><a href="#labels">Labels</a></li><li><a href="#deployments-2">Deployments</a><ul><li><a href="#deployment-management">Deployment Management</a></li></ul></li><li><a href="#services-3">Services</a><ul><li><a href="#service-management">Service Management</a></li></ul></li><li><a href="#health-checking">Health Checking</a><ul><li><a href="#process-health-checking">Process Health Checking</a></li><li><a href="#application-health-checking">Application Health Checking</a></li></ul></li></ul></li><li><a href="#configuration-4">Configuration</a><ul><li><a href="#configuring-redis-using-a-configmap">Configuring Redis using a ConfigMap</a><ul><li><a href="#objectives-7">Objectives</a></li><li><a href="#before-you-begin-3">Before you begin</a></li><li><a href="#real-world-example-configuring-redis-using-a-configmap">Real World Example: Configuring Redis using a ConfigMap</a></li></ul></li><li><a href="#stateless-applications">StateLess Applications</a><ul><li><a href="#run-a-stateless-application-using-a-deployment">Run a Stateless Application Using a Deployment</a><ul><li><a href="#objectives-8">Objectives</a></li><li><a href="#before-you-begin-4">Before you begin</a></li><li><a href="#creating-and-exploring-an-nginx-deployment">Creating and exploring an nginx deployment</a></li><li><a href="#updating-the-deployment">Updating the deployment</a></li><li><a href="#scaling-the-application-by-increasing-the-replica-count">Scaling the application by increasing the replica count</a></li><li><a href="#deleting-a-deployment">Deleting a deployment</a></li><li><a href="#replication-controllers----the-old-way">Replication Controllers -- the Old Way</a></li></ul></li><li><a href="#example-deploying-php-guestbook-application-with-redis">Example: Deploying PHP Guestbook application with Redis</a><ul><li><a href="#objectives-9"><strong>Objectives</strong></a></li><li><a href="#before-you-begin-5"><strong>Before you begin</strong></a></li><li><a href="#start-up-the-redis-master"><strong>Start up the Redis Master</strong></a><ul><li><a href="#creating-the-redis-master-deployment">Creating the Redis Master Deployment</a></li><li><a href="#creating-the-redis-master-service">Creating the Redis Master Service</a></li></ul></li><li><a href="#start-up-the-redis-slaves"><strong>Start up the Redis Slaves</strong></a><ul><li><a href="#creating-the-redis-slave-deployment">Creating the Redis Slave Deployment</a></li><li><a href="#creating-the-redis-slave-service">Creating the Redis Slave Service</a></li></ul></li><li><a href="#set-up-and-expose-the-guestbook-frontend"><strong>Set up and Expose the Guestbook Frontend</strong></a><ul><li><a href="#creating-the-guestbook-frontend-deployment">Creating the Guestbook Frontend Deployment</a></li><li><a href="#creating-the-frontend-service">Creating the Frontend Service</a></li><li><a href="#viewing-the-frontend-service-vianodeport">Viewing the Frontend Service via NodePort</a></li><li><a href="#viewing-the-frontend-service-vialoadbalancer">Viewing the Frontend Service via LoadBalancer</a></li></ul></li><li><a href="#scale-the-web-frontend"><strong>Scale the Web Frontend</strong></a></li><li><a href="#cleaning-up"><strong>Cleaning up</strong></a></li><li><a href="#whats-next-25"><strong>What&#x27;s next</strong></a></li></ul></li><li><a href="#use-a-service-to-access-an-application-in-a-cluster">Use a Service to Access an Application in a Cluster</a><ul><li><a href="#objectives-10"><strong>Objectives</strong></a></li><li><a href="#before-you-begin-6"><strong>Before you begin</strong></a></li><li><a href="#creating-a-service-for-an-application-running-in-two-pods"><strong>Creating a service for an application running in two pods</strong></a></li><li><a href="#using-a-service-configuration-file"><strong>Using a service configuration file</strong></a></li><li><a href="#cleaning-up-1"><strong>Cleaning up</strong></a></li><li><a href="#whats-next-26"><strong>What&#x27;s next</strong></a></li></ul></li><li><a href="#exposing-an-external-ip-address-to-access-an-application-in-a-cluster">Exposing an External IP Address to Access an Application in a Cluster</a><ul><li><a href="#objectives-11"><strong>Objectives</strong></a></li><li><a href="#before-you-begin-7"><strong>Before you begin</strong></a></li><li><a href="#creating-a-service-for-an-application-running-in-five-pods"><strong>Creating a service for an application running in five pods</strong></a></li><li><a href="#cleaning-up-2"><strong>Cleaning up</strong></a></li><li><a href="#whats-next-27"><strong>What&#x27;s next</strong></a></li></ul></li></ul></li></ul></li><li><a href="#stateful-applications">Stateful Applications</a><ul><li><a href="#run-a-single-instance-stateful-application">Run a Single-Instance Stateful Application</a><ul><li><a href="#objectives-12">Objectives</a></li><li><a href="#before-you-begin-8">Before you begin</a></li><li><a href="#deploy-mysql">Deploy MySQL</a></li><li><a href="#accessing-the-mysql-instance">Accessing the MySQL instance</a></li><li><a href="#updating">Updating</a></li><li><a href="#deleting-a-deployment-1">Deleting a deployment</a></li></ul></li><li><a href="#run-a-replicated-stateful-application">Run a Replicated Stateful Application</a><ul><li><a href="#objectives-13">Objectives</a></li><li><a href="#before-you-begin-9">Before you begin</a></li><li><a href="#deploy-mysql-1">Deploy MySQL</a><ul><li><a href="#configmap-1">ConfigMap</a></li><li><a href="#services-4">Services</a></li><li><a href="#statefulset">StatefulSet</a></li></ul></li><li><a href="#understanding-stateful-pod-initialization">Understanding stateful Pod initialization</a><ul><li><a href="#generating-configuration">Generating configuration</a></li><li><a href="#cloning-existing-data">Cloning existing data</a></li><li><a href="#starting-replication">Starting replication</a></li></ul></li><li><a href="#sending-client-traffic">Sending client traffic</a></li><li><a href="#simulating-pod-and-node-downtime">Simulating Pod and Node downtime</a><ul><li><a href="#break-the-readiness-probe"><strong>Break the Readiness Probe</strong></a></li><li><a href="#delete-pods">Delete Pods</a></li><li><a href="#drain-a-node">Drain a Node</a></li></ul></li><li><a href="#scaling-the-number-of-slaves">Scaling the number of slaves</a></li><li><a href="#cleaning-up-3">Cleaning up</a></li></ul></li><li><a href="#example-deploying-wordpress-and-mysql-with-persistent-volumes">Example: Deploying WordPress and MySQL with Persistent Volumes</a><ul><li><a href="#objectives-14">Objectives</a></li><li><a href="#before-you-begin-10">Before you begin</a></li><li><a href="#create-persistentvolumeclaims-and-persistentvolumes">Create PersistentVolumeClaims and PersistentVolumes</a></li><li><a href="#create-a-secret-for-mysql-password">Create a Secret for MySQL Password</a></li><li><a href="#deploy-mysql-2">Deploy MySQL</a></li><li><a href="#deploy-wordpress">Deploy WordPress</a></li><li><a href="#cleaning-up-4">Cleaning up</a></li><li><a href="#whats-next-28">What&#x27;s next</a></li></ul></li><li><a href="#example-deploying-cassandra-with-stateful-sets">Example: Deploying Cassandra with Stateful Sets</a><ul><li><a href="#objectives-15">Objectives</a></li><li><a href="#before-you-begin-11">Before you begin</a><ul><li><a href="#additional-minikube-setup-instructions"><strong>Additional Minikube Setup Instructions</strong></a></li></ul></li><li><a href="#creating-a-cassandra-headless-service">Creating a Cassandra Headless Service</a><ul><li><a href="#validating-optional"><strong>Validating (optional)</strong></a></li></ul></li><li><a href="#using-a-statefulset-to-create-a-cassandra-ring">Using a StatefulSet to Create a Cassandra Ring</a></li><li><a href="#validating-the-cassandra-statefulset">Validating The Cassandra StatefulSet</a></li><li><a href="#modifying-the-cassandra-statefulset">Modifying the Cassandra StatefulSet</a></li><li><a href="#cleaning-up-5">Cleaning up</a></li><li><a href="#whats-next-29">What&#x27;s next</a></li></ul></li><li><a href="#running-zookeeper-a-cp-distributed-system">Running ZooKeeper, A CP Distributed System</a><ul><li><a href="#objectives-16">Objectives</a></li><li><a href="#before-you-begin-12">Before you begin</a><ul><li><a href="#zookeeper-basics"><strong>ZooKeeper Basics</strong></a></li></ul></li><li><a href="#creating-a-zookeeper-ensemble">Creating a ZooKeeper Ensemble</a><ul><li><a href="#facilitating-leader-election"><strong>Facilitating Leader Election</strong></a></li><li><a href="#achieving-consensus"><strong>Achieving Consensus</strong></a></li><li><a href="#sanity-testing-the-ensemble"><strong>Sanity Testing the Ensemble</strong></a></li><li><a href="#providing-durable-storage"><strong>Providing Durable Storage</strong></a></li></ul></li><li><a href="#ensuring-consistent-configuration">Ensuring Consistent Configuration</a><ul><li><a href="#configuring-logging"><strong>Configuring Logging</strong></a></li><li><a href="#configuring-a-non-privileged-user"><strong>Configuring a Non-Privileged User</strong></a></li></ul></li><li><a href="#managing-the-zookeeper-process">Managing the ZooKeeper Process</a><ul><li><a href="#updating-the-ensemble"><strong>Updating the Ensemble</strong></a></li><li><a href="#handling-process-failure"><strong>Handling Process Failure</strong></a></li><li><a href="#testing-for-liveness"><strong>Testing for Liveness</strong></a></li><li><a href="#testing-for-readiness"><strong>Testing for Readiness</strong></a></li></ul></li><li><a href="#tolerating-node-failure">Tolerating Node Failure</a></li><li><a href="#surviving-maintenance">Surviving Maintenance</a></li><li><a href="#cleaning-up-6">Cleaning up</a></li></ul></li></ul></li><li><a href="#cluster">Cluster</a><ul><li><a href="#apparmor-1">AppArmor</a><ul><li><a href="#objectives-17">Objectives</a></li><li><a href="#before-you-begin-13">Before you begin</a></li><li><a href="#securing-a-pod">Securing a Pod</a></li><li><a href="#example-3">Example</a></li><li><a href="#administration">Administration</a><ul><li><a href="#setting-up-nodes-with-profiles"><strong>Setting up nodes with profiles</strong></a></li><li><a href="#restricting-profiles-with-the-podsecuritypolicy"><strong>Restricting profiles with the PodSecurityPolicy</strong></a></li><li><a href="#disabling-apparmor"><strong>Disabling AppArmor</strong></a></li><li><a href="#upgrading-to-kubernetes-v14-with-apparmor"><strong>Upgrading to Kubernetes v1.4 with AppArmor</strong></a></li><li><a href="#upgrade-path-to-general-availability"><strong>Upgrade path to General Availability</strong></a></li></ul></li><li><a href="#authoring-profiles">Authoring Profiles</a></li><li><a href="#api-reference">API Reference</a><ul><li><a href="#pod-annotation"><strong>Pod Annotation</strong></a></li><li><a href="#profile-reference"><strong>Profile Reference</strong></a></li><li><a href="#podsecuritypolicy-annotations"><strong>PodSecurityPolicy Annotations</strong></a></li></ul></li><li><a href="#whats-next-30">What&#x27;s next</a></li></ul></li></ul></li><li><a href="#services-5">Services</a><ul><li><a href="#using-source-ip">Using Source IP</a><ul><li><a href="#objectives-18">Objectives</a></li><li><a href="#before-you-begin-14">Before you begin</a></li><li><a href="#terminology">Terminology</a></li><li><a href="#prerequisites-3">Prerequisites</a></li><li><a href="#source-ip-for-services-with-typeclusterip">Source IP for Services with Type=ClusterIP</a></li><li><a href="#source-ip-for-services-with-typenodeport">Source IP for Services with Type=NodePort</a></li><li><a href="#source-ip-for-services-with-typeloadbalancer">Source IP for Services with Type=LoadBalancer</a></li><li><a href="#cleaning-up-7">Cleaning up</a></li><li><a href="#whats-next-31">What&#x27;s next</a></li></ul></li></ul></li></ul></li><li><a href="#tasks">Tasks</a><ul><li><a href="#tasks-1">Tasks</a><div class="MuiContainer-root MuiContainer-maxWidthLg"><pre class="Code__Pre-gy960v-0 UDybk prism-code language-undefined" style="color:#9CDCFE;background-color:#1E1E1E"><div class="MuiGrid-root MuiGrid-container MuiGrid-justify-xs-flex-end"><button class="Code__CopyCode-gy960v-1 llUIua">Copy</button></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">- [**Web UI (Dashboard)**](#web-ui-dashboard-1)</span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">- [**Using the kubectl Command-line**](#using-the-kubectl-command-line)</span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">- [**Configuring Pods and Containers**](#configuring-pods-and-containers)</span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">- [**Running Applications**](#running-applications)</span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">- [**Running Jobs**](#running-jobs)</span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">- [**Accessing Applications in a Cluster**](#accessing-applications-in-a-cluster)</span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">- [**Monitoring, Logging, and Debugging**](#monitoring-logging-and-debugging)</span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">- [**Accessing the Kubernetes API**](#accessing-the-kubernetes-api)</span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">- [**Using TLS**](#using-tls)</span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">- [**Administering a Cluster**](#administering-a-cluster)</span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">- [**Administering Federation**](#administering-federation)</span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">- [**Managing Stateful Applications**](#managing-stateful-applications)</span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">- [Cluster Daemons](#cluster-daemons)</span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">- [Managing GPUs](#managing-gpus)</span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">- [Managing HugePages](#managing-hugepages)</span></div></pre></div><ul><li><a href="#whats-next-32">What&#x27;s next</a></li></ul></li><li><a href="#install-tools">Install Tools</a><ul><li><a href="#install-and-set-up-kubectl">Install and Set Up kubectl</a><ul><li><a href="#install-kubectl">Install kubectl</a></li><li><a href="#before-you-begin-15">Before you begin</a><ul><li><a href="#install-kubectl-binary-via-curl"><strong>Install kubectl binary via curl</strong></a></li><li><a href="#download-as-part-of-the-google-cloud-sdk"><strong>Download as part of the Google Cloud SDK</strong></a></li><li><a href="#install-with-snap-on-ubuntu"><strong>Install with snap on Ubuntu</strong></a></li><li><a href="#install-with-homebrew-on-macos"><strong>Install with Homebrew on macOS</strong></a></li><li><a href="#install-with-powershell-from-psgallery"><strong>Install with Powershell from PSGallery</strong></a></li><li><a href="#install-with-chocolatey-on-windows"><strong>Install with Chocolatey on Windows</strong></a></li></ul></li><li><a href="#configure-kubectl">Configure kubectl</a></li><li><a href="#check-the-kubectl-configuration">Check the kubectl configuration</a></li><li><a href="#enabling-shell-autocompletion">Enabling shell autocompletion</a><ul><li><a href="#on-linux-using-bash"><strong>On Linux, using bash</strong></a></li><li><a href="#on-macos-using-bash"><strong>On macOS, using bash</strong></a></li><li><a href="#using-zsh"><strong>Using Zsh</strong></a></li></ul></li><li><a href="#whats-next-33">What&#x27;s next</a></li></ul></li><li><a href="#install-minikube">Install Minikube</a><ul><li><a href="#before-you-begin-16">Before you begin</a></li><li><a href="#install-a-hypervisor">Install a Hypervisor</a></li><li><a href="#install-kubectl-1">Install kubectl</a></li><li><a href="#install-minikube-1">Install Minikube</a></li><li><a href="#whats-next-34">What&#x27;s next</a></li></ul></li><li><a href="#installing-kubeadm">Installing kubeadm</a><ul><li><a href="#before-you-begin-17">Before you begin</a></li><li><a href="#verify-the-mac-address-and-product_uuid-are-unique-for-every-node">Verify the MAC address and product_uuid are unique for every node</a></li><li><a href="#check-network-adapters">Check network adapters</a></li><li><a href="#check-required-ports">Check required ports</a><ul><li><a href="#master-nodes"><strong>Master node(s)</strong></a></li><li><a href="#worker-nodes"><strong>Worker node(s)</strong></a></li></ul></li><li><a href="#installing-docker">Installing Docker</a></li><li><a href="#installing-kubeadm-kubelet-and-kubectl">Installing kubeadm, kubelet and kubectl</a></li><li><a href="#configure-cgroup-driver-used-by-kubelet-on-master-node">Configure cgroup driver used by kubelet on Master Node</a></li><li><a href="#troubleshooting-2">Troubleshooting</a></li><li><a href="#whats-next-35">What&#x27;s next</a></li></ul></li></ul></li><li><a href="#configure-pods-and-containers">Configure Pods and Containers</a><ul><li><a href="#assign-memory-resources-to-containers-and-pods">Assign Memory Resources to Containers and Pods</a><ul><li><a href="#before-you-begin-18">Before you begin</a></li><li><a href="#create-a-namespace">Create a namespace</a></li><li><a href="#specify-a-memory-request-and-a-memory-limit">Specify a memory request and a memory limit</a></li><li><a href="#exceed-a-containers-memory-limit">Exceed a Container&#x27;s memory limit</a></li><li><a href="#specify-a-memory-request-that-is-too-big-for-your-nodes">Specify a memory request that is too big for your Nodes</a></li><li><a href="#memory-units">Memory units</a></li><li><a href="#if-you-dont-specify-a-memory-limit">If you don&#x27;t specify a memory limit</a></li><li><a href="#motivation-for-memory-requests-and-limits">Motivation for memory requests and limits</a></li><li><a href="#clean-up-2">Clean up</a></li><li><a href="#whats-next-36">What&#x27;s next</a><ul><li><a href="#for-app-developers"><strong>For app developers</strong></a></li><li><a href="#for-cluster-administrators"><strong>For cluster administrators</strong></a></li></ul></li></ul></li><li><a href="#assign-cpu-resources-to-containers-and-pods">Assign CPU Resources to Containers and Pods</a><ul><li><a href="#before-you-begin-19">Before you begin</a></li><li><a href="#create-a-namespace-1">Create a namespace</a></li><li><a href="#specify-a-cpu-request-and-a-cpu-limit">Specify a CPU request and a CPU limit</a></li><li><a href="#cpu-units">CPU units</a></li><li><a href="#specify-a-cpu-request-that-is-too-big-for-your-nodes">Specify a CPU request that is too big for your Nodes</a></li><li><a href="#if-you-dont-specify-a-cpu-limit">If you don&#x27;t specify a CPU limit</a></li><li><a href="#motivation-for-cpu-requests-and-limits">Motivation for CPU requests and limits</a></li><li><a href="#clean-up-3">Clean up</a></li><li><a href="#whats-next-37">What&#x27;s next</a><ul><li><a href="#for-app-developers-1"><strong>For app developers</strong></a></li><li><a href="#for-cluster-administrators-1"><strong>For cluster administrators</strong></a></li></ul></li></ul></li><li><a href="#configure-quality-of-service-for-pods">Configure Quality of Service for Pods</a><ul><li><a href="#before-you-begin-20">Before you begin</a></li><li><a href="#qos-classes">QoS classes</a></li><li><a href="#create-a-namespace-2">Create a namespace</a></li><li><a href="#create-a-pod-that-gets-assigned-a-qos-class-of-guaranteed">Create a Pod that gets assigned a QoS class of Guaranteed</a></li><li><a href="#create-a-pod-that-gets-assigned-a-qos-class-of-burstable">Create a Pod that gets assigned a QoS class of Burstable</a></li><li><a href="#create-a-pod-that-gets-assigned-a-qos-class-of-besteffort">Create a Pod that gets assigned a QoS class of BestEffort</a></li><li><a href="#create-a-pod-that-has-two-containers">Create a Pod that has two Containers</a></li><li><a href="#clean-up-4">Clean up</a></li><li><a href="#whats-next-38">What&#x27;s next</a><ul><li><a href="#for-app-developers-2"><strong>For app developers</strong></a></li><li><a href="#for-cluster-administrators-2"><strong>For cluster administrators</strong></a></li></ul></li></ul></li><li><a href="#assign-extended-resources-to-a-container">Assign Extended Resources to a Container</a><ul><li><a href="#before-you-begin-21">Before you begin</a></li><li><a href="#assign-an-extended-resource-to-a-pod">Assign an extended resource to a Pod</a></li><li><a href="#attempt-to-create-a-second-pod">Attempt to create a second Pod</a></li><li><a href="#clean-up-5">Clean up</a></li><li><a href="#whats-next-39">What&#x27;s next</a><ul><li><a href="#for-application-developers"><strong>For application developers</strong></a></li><li><a href="#for-cluster-administrators-3"><strong>For cluster administrators</strong></a></li></ul></li></ul></li><li><a href="#configure-a-pod-to-use-a-volume-for-storage">Configure a Pod to Use a Volume for Storage</a><ul><li><a href="#before-you-begin-22">Before you begin</a></li><li><a href="#configure-a-volume-for-a-pod">Configure a volume for a Pod</a></li><li><a href="#whats-next-40">What&#x27;s next</a></li></ul></li><li><a href="#configure-a-pod-to-use-a-persistentvolume-for-storage">Configure a Pod to Use a PersistentVolume for Storage</a><ul><li><a href="#before-you-begin-23">Before you begin</a></li><li><a href="#create-an-indexhtml-file-on-your-node">Create an index.html file on your Node</a></li><li><a href="#create-a-persistentvolume">Create a PersistentVolume</a></li><li><a href="#create-a-persistentvolumeclaim">Create a PersistentVolumeClaim</a></li><li><a href="#create-a-pod">Create a Pod</a></li><li><a href="#access-control">Access control</a></li><li><a href="#whats-next-41">What&#x27;s next</a><ul><li><a href="#reference"><strong>Reference</strong></a></li></ul></li></ul></li><li><a href="#configure-a-pod-to-use-a-projected-volume-for-storage">Configure a Pod to Use a Projected Volume for Storage</a><ul><li><a href="#before-you-begin-24">Before you begin</a></li><li><a href="#configure-a-projected-volume-for-a-pod">Configure a projected volume for a pod</a></li><li><a href="#whats-next-42">What&#x27;s next</a></li></ul></li><li><a href="#configure-a-security-context-for-a-pod-or-container">Configure a Security Context for a Pod or Container</a><ul><li><a href="#before-you-begin-25">Before you begin</a></li><li><a href="#set-the-security-context-for-a-pod">Set the security context for a Pod</a></li><li><a href="#set-the-security-context-for-a-container">Set the security context for a Container</a></li><li><a href="#set-capabilities-for-a-container">Set capabilities for a Container</a></li><li><a href="#assign-selinux-labels-to-a-container">Assign SELinux labels to a Container</a></li><li><a href="#discussion">Discussion</a></li><li><a href="#whats-next-43">What&#x27;s next</a></li></ul></li><li><a href="#configure-service-accounts-for-pods">Configure Service Accounts for Pods</a><ul><li><a href="#use-the-default-service-account-to-access-the-api-server">Use the Default Service Account to access the API server.</a></li><li><a href="#use-multiple-service-accounts">Use Multiple Service Accounts.</a></li><li><a href="#manually-create-a-service-account-api-token">Manually create a service account API token.</a></li><li><a href="#add-imagepullsecrets-to-a-service-account">Add ImagePullSecrets to a service account</a></li></ul></li><li><a href="#pull-an-image-from-a-private-registry">Pull an Image from a Private Registry</a><ul><li><a href="#before-you-begin-26">Before you begin</a></li><li><a href="#log-in-to-docker">Log in to Docker</a></li><li><a href="#create-a-secret-in-the-cluster-that-holds-your-authorization-token">Create a Secret in the cluster that holds your authorization token</a></li><li><a href="#inspecting-the-secretregcred">Inspecting the Secret regcred</a></li><li><a href="#create-a-pod-that-uses-your-secret">Create a Pod that uses your Secret</a></li><li><a href="#whats-next-44">What&#x27;s next</a></li></ul></li><li><a href="#configure-liveness-and-readiness-probes">Configure Liveness and Readiness Probes</a><ul><li><a href="#before-you-begin-27">Before you begin</a></li><li><a href="#define-a-liveness-command">Define a liveness command</a></li><li><a href="#define-a-liveness-http-request">Define a liveness HTTP request</a></li><li><a href="#define-a-tcp-liveness-probe">Define a TCP liveness probe</a></li><li><a href="#use-a-named-port">Use a named port</a></li><li><a href="#define-readiness-probes">Define readiness probes</a></li><li><a href="#configure-probes">Configure Probes</a></li><li><a href="#whats-next-45">What&#x27;s next</a><ul><li><a href="#reference-1"><strong>Reference</strong></a></li></ul></li></ul></li><li><a href="#assign-pods-to-nodes">Assign Pods to Nodes</a><ul><li><a href="#before-you-begin-28">Before you begin</a></li><li><a href="#add-a-label-to-a-node">Add a label to a node</a></li><li><a href="#create-a-pod-that-gets-scheduled-to-your-chosen-node">Create a pod that gets scheduled to your chosen node</a></li><li><a href="#whats-next-46">What&#x27;s next</a></li></ul></li><li><a href="#configure-pod-initialization">Configure Pod Initialization</a><ul><li><a href="#before-you-begin-29">Before you begin</a></li><li><a href="#create-a-pod-that-has-an-init-container">Create a Pod that has an Init Container</a></li><li><a href="#whats-next-47">What&#x27;s next</a></li></ul></li><li><a href="#attach-handlers-to-container-lifecycle-events">Attach Handlers to Container Lifecycle Events</a><ul><li><a href="#before-you-begin-30">Before you begin</a></li><li><a href="#define-poststart-and-prestop-handlers">Define postStart and preStop handlers</a></li><li><a href="#discussion-1">Discussion</a></li><li><a href="#whats-next-48">What&#x27;s next</a><ul><li><a href="#reference-2"><strong>Reference</strong></a></li></ul></li></ul></li><li><a href="#configure-a-pod-to-use-a-configmap">Configure a Pod to Use a ConfigMap</a><ul><li><a href="#before-you-begin-31">Before you begin</a></li><li><a href="#create-a-configmap">Create a ConfigMap</a><ul><li><a href="#create-configmaps-from-directories"><strong>Create ConfigMaps from directories</strong></a></li><li><a href="#create-configmaps-from-files"><strong>Create ConfigMaps from files</strong></a><ul><li><a href="#define-the-key-to-use-when-creating-a-configmap-from-a-file"><strong>Define the key to use when creating a ConfigMap from a file</strong></a></li></ul></li><li><a href="#create-configmaps-from-literal-values"><strong>Create ConfigMaps from literal values</strong></a></li></ul></li><li><a href="#define-pod-environment-variables-using-configmap-data">Define Pod environment variables using ConfigMap data</a><ul><li><a href="#define-a-pod-environment-variable-with-data-from-a-single-configmap"><strong>Define a Pod environment variable with data from a single ConfigMap</strong></a></li><li><a href="#define-pod-environment-variables-with-data-from-multiple-configmaps"><strong>Define Pod environment variables with data from multiple ConfigMaps</strong></a></li></ul></li><li><a href="#configure-all-key-value-pairs-in-a-configmap-as-pod-environment-variables">Configure all key-value pairs in a ConfigMap as Pod environment variables</a></li><li><a href="#use-configmap-defined-environment-variables-in-pod-commands">Use ConfigMap-defined environment variables in Pod commands</a></li><li><a href="#add-configmap-data-to-a-volume">Add ConfigMap data to a Volume</a><ul><li><a href="#populate-a-volume-with-data-stored-in-a-configmap"><strong>Populate a Volume with data stored in a ConfigMap</strong></a></li><li><a href="#add-configmap-data-to-a-specific-path-in-the-volume"><strong>Add ConfigMap data to a specific path in the Volume</strong></a></li><li><a href="#project-keys-to-specific-paths-and-file-permissions"><strong>Project keys to specific paths and file permissions</strong></a></li><li><a href="#mounted-configmaps-are-updated-automatically"><strong>Mounted ConfigMaps are updated automatically</strong></a></li></ul></li><li><a href="#understanding-configmaps-and-pods">Understanding ConfigMaps and Pods</a><ul><li><a href="#restrictions-1"><strong>Restrictions</strong></a></li></ul></li><li><a href="#whats-next-49">What&#x27;s next</a></li></ul></li><li><a href="#share-process-namespace-between-containers-in-a-pod">Share Process Namespace between Containers in a Pod</a><ul><li><a href="#before-you-begin-32">Before you begin</a></li><li><a href="#configure-a-pod">Configure a Pod</a></li><li><a href="#understanding-process-namespace-sharing">Understanding Process Namespace Sharing</a></li></ul></li><li><a href="#translate-a-docker-compose-file-to-kubernetes-resources">Translate a Docker Compose File to Kubernetes Resources</a><ul><li><a href="#kubernetes--compose--kompose">Kubernetes + Compose = Kompose</a><ul><li><a href="#installation-2"><strong>Installation</strong></a></li><li><a href="#user-guide"><strong>User Guide</strong></a><ul><li><a href="#kompose-convert">kompose convert</a></li><li><a href="#kompose-up">kompose up</a></li><li><a href="#kompose-down">kompose down</a></li><li><a href="#build-and-push-docker-images"><strong>Build and Push Docker Images</strong></a></li><li><a href="#alternative-conversions"><strong>Alternative Conversions</strong></a></li><li><a href="#labels-1"><strong>Labels</strong></a></li><li><a href="#restart"><strong>Restart</strong></a></li><li><a href="#docker-compose-versions">Docker Compose Versions</a></li></ul></li></ul></li></ul></li></ul></li><li><a href="#inject-data-into-applications">Inject Data Into Applications</a><ul><li><a href="#define-a-command-and-arguments-for-a-container">Define a Command and Arguments for a Container</a><ul><li><a href="#before-you-begin-33">Before you begin</a></li><li><a href="#define-a-command-and-arguments-when-you-create-a-pod">Define a command and arguments when you create a Pod</a></li><li><a href="#use-environment-variables-to-define-arguments">Use environment variables to define arguments</a></li><li><a href="#run-a-command-in-a-shell">Run a command in a shell</a></li><li><a href="#notes">Notes</a></li><li><a href="#whats-next-50">What&#x27;s next</a></li></ul></li><li><a href="#define-environment-variables-for-a-container">Define Environment Variables for a Container</a><ul><li><a href="#before-you-begin-34">Before you begin</a></li><li><a href="#define-an-environment-variable-for-a-container">Define an environment variable for a container</a></li><li><a href="#whats-next-51">What&#x27;s next</a></li></ul></li></ul></li><li><a href="#expose-pod-information-to-containers-through-environment-variables">Expose Pod Information to Containers Through Environment Variables</a><ul><li><a href="#before-you-begin-35">Before you begin</a></li><li><a href="#the-downward-api">The Downward API</a></li><li><a href="#use-pod-fields-as-values-for-environment-variables">Use Pod fields as values for environment variables</a></li><li><a href="#use-container-fields-as-values-for-environment-variables">Use Container fields as values for environment variables</a></li><li><a href="#whats-next-52">What&#x27;s next</a></li><li><a href="#expose-pod-information-to-containers-through-files">Expose Pod Information to Containers Through Files</a><ul><li><a href="#before-you-begin-36">Before you begin</a></li><li><a href="#the-downward-api-1">The Downward API</a></li><li><a href="#store-pod-fields">Store Pod fields</a></li><li><a href="#store-container-fields">Store Container fields</a></li><li><a href="#capabilities-of-the-downward-api">Capabilities of the Downward API</a></li><li><a href="#project-keys-to-specific-paths-and-file-permissions-1">Project keys to specific paths and file permissions</a></li><li><a href="#motivation-for-the-downward-api">Motivation for the Downward API</a></li><li><a href="#whats-next-53">What&#x27;s next</a></li></ul></li><li><a href="#distribute-credentials-securely-using-secrets">Distribute Credentials Securely Using Secrets</a><ul><li><a href="#before-you-begin-37">Before you begin</a></li><li><a href="#convert-your-secret-data-to-a-base-64-representation">Convert your secret data to a base-64 representation</a></li><li><a href="#create-a-secret">Create a Secret</a></li><li><a href="#create-a-pod-that-has-access-to-the-secret-data-through-a-volume">Create a Pod that has access to the secret data through a Volume</a></li><li><a href="#create-a-pod-that-has-access-to-the-secret-data-through-environment-variables">Create a Pod that has access to the secret data through environment variables</a></li><li><a href="#whats-next-54">What&#x27;s next</a><ul><li><a href="#reference-3"><strong>Reference</strong></a></li></ul></li></ul></li><li><a href="#inject-information-into-pods-using-a-podpreset">Inject Information into Pods Using a PodPreset</a><ul><li><a href="#create-a-pod-preset">Create a Pod Preset</a><ul><li><a href="#simple-pod-spec-example"><strong>Simple Pod Spec Example</strong></a></li><li><a href="#pod-spec-withconfigmapexample"><strong>Pod Spec with </strong>ConfigMap<strong> Example</strong></a></li><li><a href="#replicaset-with-pod-spec-example"><strong>ReplicaSet with Pod Spec Example</strong></a></li><li><a href="#multiple-podpreset-example"><strong>Multiple PodPreset Example</strong></a></li><li><a href="#conflict-example"><strong>Conflict Example</strong></a></li></ul></li><li><a href="#deleting-a-pod-preset">Deleting a Pod Preset</a></li></ul></li></ul></li><li><a href="#run-applications">Run Applications</a><ul><li><a href="#run-a-stateless-application-using-a-deployment-1">Run a Stateless Application Using a Deployment</a><ul><li><a href="#objectives-19">Objectives</a></li><li><a href="#before-you-begin-38">Before you begin</a></li><li><a href="#creating-and-exploring-an-nginx-deployment-1">Creating and exploring an nginx deployment</a></li><li><a href="#updating-the-deployment-1">Updating the deployment</a></li><li><a href="#scaling-the-application-by-increasing-the-replica-count-1">Scaling the application by increasing the replica count</a></li><li><a href="#deleting-a-deployment-2">Deleting a deployment</a></li><li><a href="#replicationcontrollers----the-old-way">ReplicationControllers -- the Old Way</a></li><li><a href="#whats-next-55">What&#x27;s next</a></li></ul></li><li><a href="#run-a-single-instance-stateful-application-1">Run a Single-Instance Stateful Application</a><ul><li><a href="#objectives-20">Objectives</a></li><li><a href="#before-you-begin-39">Before you begin</a></li><li><a href="#deploy-mysql-3">Deploy MySQL</a></li><li><a href="#accessing-the-mysql-instance-1">Accessing the MySQL instance</a></li><li><a href="#updating-1">Updating</a></li><li><a href="#deleting-a-deployment-3">Deleting a deployment</a></li><li><a href="#whats-next-56">What&#x27;s next</a></li></ul></li><li><a href="#run-a-replicated-stateful-application-1">Run a Replicated Stateful Application</a><ul><li><a href="#objectives-21">Objectives</a></li><li><a href="#before-you-begin-40">Before you begin</a></li><li><a href="#deploy-mysql-4">Deploy MySQL</a><ul><li><a href="#configmap-2"><strong>ConfigMap</strong></a></li><li><a href="#services-6"><strong>Services</strong></a></li><li><a href="#statefulset-1"><strong>StatefulSet</strong></a></li></ul></li><li><a href="#understanding-stateful-pod-initialization-1">Understanding stateful Pod initialization</a><ul><li><a href="#generating-configuration-1"><strong>Generating configuration</strong></a></li><li><a href="#cloning-existing-data-1"><strong>Cloning existing data</strong></a></li><li><a href="#starting-replication-1"><strong>Starting replication</strong></a></li></ul></li><li><a href="#sending-client-traffic-1">Sending client traffic</a></li><li><a href="#simulating-pod-and-node-downtime-1">Simulating Pod and Node downtime</a><ul><li><a href="#break-the-readiness-probe-1"><strong>Break the Readiness Probe</strong></a></li><li><a href="#delete-pods-1"><strong>Delete Pods</strong></a></li><li><a href="#drain-a-node-1"><strong>Drain a Node</strong></a></li></ul></li><li><a href="#scaling-the-number-of-slaves-1">Scaling the number of slaves</a></li><li><a href="#cleaning-up-8">Cleaning up</a></li><li><a href="#whats-next-57">What&#x27;s next</a></li></ul></li><li><a href="#update-api-objects-in-place-using-kubectl-patch">Update API Objects in Place Using kubectl patch</a><ul><li><a href="#before-you-begin-41">Before you begin</a></li><li><a href="#use-a-strategic-merge-patch-to-update-a-deployment">Use a strategic merge patch to update a Deployment</a><ul><li><a href="#notes-on-the-strategic-merge-patch"><strong>Notes on the strategic merge patch</strong></a></li></ul></li><li><a href="#use-a-json-merge-patch-to-update-a-deployment">Use a JSON merge patch to update a Deployment</a></li><li><a href="#alternate-forms-of-the-kubectl-patch-command">Alternate forms of the kubectl patch command</a></li><li><a href="#summary-7">Summary</a></li><li><a href="#whats-next-58">What&#x27;s next</a></li></ul></li><li><a href="#scale-a-statefulset">Scale a StatefulSet</a><ul><li><a href="#before-you-begin-42">Before you begin</a></li><li><a href="#usekubectlto-scale-statefulsets">Use kubectl to scale StatefulSets</a><ul><li><a href="#kubectl-scale">kubectl scale</a></li><li><a href="#alternativekubectl-applykubectl-editkubectl-patch"><strong>Alternative: </strong>kubectl apply<strong> / </strong>kubectl edit<strong> / </strong>kubectl patch</a></li></ul></li><li><a href="#troubleshooting-3">Troubleshooting</a><ul><li><a href="#scaling-down-doesnt-work-right"><strong>Scaling down doesn&#x27;t work right</strong></a></li></ul></li><li><a href="#whats-next-59">What&#x27;s next</a></li></ul></li><li><a href="#delete-a-statefulset">Delete a StatefulSet</a><ul><li><a href="#before-you-begin-43">Before you begin</a></li><li><a href="#deleting-a-statefulset">Deleting a StatefulSet</a><ul><li><a href="#persistent-volumes-2"><strong>Persistent Volumes</strong></a></li><li><a href="#complete-deletion-of-a-statefulset"><strong>Complete deletion of a StatefulSet</strong></a></li><li><a href="#force-deletion-of-statefulset-pods"><strong>Force deletion of StatefulSet pods</strong></a></li></ul></li><li><a href="#whats-next-60">What&#x27;s next</a></li></ul></li><li><a href="#force-delete-statefulset-pods">Force Delete StatefulSet Pods</a><ul><li><a href="#before-you-begin-44">Before you begin</a></li><li><a href="#statefulset-considerations">StatefulSet considerations</a></li><li><a href="#delete-pods-2">Delete Pods</a><ul><li><a href="#force-deletion"><strong>Force Deletion</strong></a></li></ul></li><li><a href="#whats-next-61">What&#x27;s next</a></li></ul></li><li><a href="#perform-rolling-update-using-a-replication-controller">Perform Rolling Update Using a Replication Controller</a><ul><li><a href="#overview-5">Overview</a></li><li><a href="#passing-a-configuration-file">Passing a configuration file</a><ul><li><a href="#examples-7"><strong>Examples</strong></a></li></ul></li><li><a href="#updating-the-container-image">Updating the container image</a><ul><li><a href="#examples-8"><strong>Examples</strong></a></li></ul></li><li><a href="#required-and-optional-fields">Required and optional fields</a></li><li><a href="#walkthrough">Walkthrough</a></li><li><a href="#troubleshooting-4">Troubleshooting</a></li></ul></li><li><a href="#horizontal-pod-autoscaler">Horizontal Pod Autoscaler</a><ul><li><a href="#what-is-the-horizontal-pod-autoscaler">What is the Horizontal Pod Autoscaler?</a></li><li><a href="#how-does-the-horizontal-pod-autoscaler-work">How does the Horizontal Pod Autoscaler work?</a></li><li><a href="#api-object-4">API Object</a></li><li><a href="#support-for-horizontal-pod-autoscaler-in-kubectl">Support for Horizontal Pod Autoscaler in kubectl</a></li><li><a href="#autoscaling-during-rolling-update">Autoscaling during rolling update</a></li><li><a href="#support-for-cooldowndelay">Support for cooldown/delay</a></li><li><a href="#support-for-multiple-metrics">Support for multiple metrics</a></li><li><a href="#support-for-custom-metrics">Support for custom metrics</a><ul><li><a href="#requirements"><strong>Requirements</strong></a></li></ul></li><li><a href="#further-reading-1">Further reading</a></li></ul></li><li><a href="#horizontal-pod-autoscaler-walkthrough">Horizontal Pod Autoscaler Walkthrough</a><ul><li><a href="#prerequisites-4">Prerequisites</a></li><li><a href="#step-one-run--expose-php-apache-server">Step One: Run &amp; expose php-apache server</a></li><li><a href="#step-two-create-horizontal-pod-autoscaler">Step Two: Create Horizontal Pod Autoscaler</a></li><li><a href="#step-three-increase-load">Step Three: Increase load</a></li><li><a href="#step-four-stop-load">Step Four: Stop load</a></li><li><a href="#autoscaling-on-multiple-metrics-and-custom-metrics">Autoscaling on multiple metrics and custom metrics</a><ul><li><a href="#autoscaling-on-metrics-not-related-to-kubernetes-objects"><strong>Autoscaling on metrics not related to Kubernetes objects</strong></a></li></ul></li><li><a href="#appendix-horizontal-pod-autoscaler-status-conditions">Appendix: Horizontal Pod Autoscaler Status Conditions</a></li><li><a href="#appendix-other-possible-scenarios">Appendix: Other possible scenarios</a><ul><li><a href="#creating-the-autoscaler-declaratively"><strong>Creating the autoscaler declaratively</strong></a></li></ul></li></ul></li><li><a href="#specifying-a-disruption-budget-for-your-application">Specifying a Disruption Budget for your Application</a><ul><li><a href="#before-you-begin-45">Before you begin</a></li><li><a href="#protecting-an-application-with-a-poddisruptionbudget">Protecting an Application with a PodDisruptionBudget</a></li><li><a href="#identify-an-application-to-protect">Identify an Application to Protect</a></li><li><a href="#think-about-how-your-application-reacts-to-disruptions">Think about how your application reacts to disruptions</a></li><li><a href="#specifying-a-poddisruptionbudget">Specifying a PodDisruptionBudget</a></li><li><a href="#create-the-pdb-object">Create the PDB object</a></li><li><a href="#check-the-status-of-the-pdb">Check the status of the PDB</a></li><li><a href="#arbitrary-controllers-and-selectors">Arbitrary Controllers and Selectors</a></li></ul></li></ul></li><li><a href="#run-jobs">Run Jobs</a><ul><li><a href="#parallel-processing-using-expansions">Parallel Processing using Expansions</a><ul><li><a href="#example-multiple-job-objects-from-template-expansion">Example: Multiple Job Objects from Template Expansion</a><ul><li><a href="#basic-template-expansion"><strong>Basic Template Expansion</strong></a></li><li><a href="#multiple-template-parameters"><strong>Multiple Template Parameters</strong></a></li><li><a href="#alternatives-2"><strong>Alternatives</strong></a></li></ul></li></ul></li><li><a href="#coarse-parallel-processing-using-a-work-queue">Coarse Parallel Processing Using a Work Queue</a><ul><li><a href="#example-job-with-work-queue-with-pod-per-work-item">Example: Job with Work Queue with Pod Per Work Item</a><ul><li><a href="#starting-a-message-queue-service"><strong>Starting a message queue service</strong></a></li><li><a href="#testing-the-message-queue-service"><strong>Testing the message queue service</strong></a></li><li><a href="#filling-the-queue-with-tasks"><strong>Filling the Queue with tasks</strong></a></li><li><a href="#create-an-image"><strong>Create an Image</strong></a></li><li><a href="#defining-a-job"><strong>Defining a Job</strong></a></li><li><a href="#running-the-job"><strong>Running the Job</strong></a></li><li><a href="#alternatives-3"><strong>Alternatives</strong></a></li><li><a href="#caveats-1"><strong>Caveats</strong></a></li></ul></li></ul></li><li><a href="#fine-parallel-processing-using-a-work-queue">Fine Parallel Processing Using a Work Queue</a><ul><li><a href="#example-job-with-work-queue-with-multiple-work-items-per-pod">Example: Job with Work Queue with Multiple Work Items Per Pod</a><ul><li><a href="#starting-redis"><strong>Starting Redis</strong></a></li><li><a href="#filling-the-queue-with-tasks-1"><strong>Filling the Queue with tasks</strong></a></li><li><a href="#create-an-image-1"><strong>Create an Image</strong></a><ul><li><a href="#push-the-image"><strong>Push the image</strong></a></li></ul></li><li><a href="#defining-a-job-1"><strong>Defining a Job</strong></a></li><li><a href="#running-the-job-1"><strong>Running the Job</strong></a></li><li><a href="#alternatives-4"><strong>Alternatives</strong></a></li></ul></li></ul></li></ul></li><li><a href="#access-applications-in-a-cluster">Access Applications in a Cluster</a><ul><li><a href="#web-ui-dashboard-2">Web UI (Dashboard)</a><ul><li><a href="#deploying-the-dashboard-ui">Deploying the Dashboard UI</a></li><li><a href="#accessing-the-dashboard-ui">Accessing the Dashboard UI</a><ul><li><a href="#command-line-proxy"><strong>Command line proxy</strong></a></li><li><a href="#master-server"><strong>Master server</strong></a></li></ul></li><li><a href="#welcome-view">Welcome view</a></li><li><a href="#deploying-containerized-applications">Deploying containerized applications</a><ul><li><a href="#specifying-application-details"><strong>Specifying application details</strong></a></li><li><a href="#uploading-a-yaml-or-json-file"><strong>Uploading a YAML or JSON file</strong></a></li></ul></li><li><a href="#using-dashboard">Using Dashboard</a><ul><li><a href="#navigation"><strong>Navigation</strong></a><ul><li><a href="#admin"><strong>Admin</strong></a></li><li><a href="#workloads-1"><strong>Workloads</strong></a></li><li><a href="#services-and-discovery"><strong>Services and discovery</strong></a></li><li><a href="#storage-2"><strong>Storage</strong></a></li><li><a href="#config"><strong>Config</strong></a></li><li><a href="#logs-viewer"><strong>Logs viewer</strong></a></li></ul></li></ul></li><li><a href="#more-information">More information</a></li></ul></li><li><a href="#accessing-clusters">Accessing Clusters</a><ul><li><a href="#accessing-the-cluster-api">Accessing the cluster API</a><ul><li><a href="#accessing-for-the-first-time-with-kubectl"><strong>Accessing for the first time with kubectl</strong></a></li><li><a href="#directly-accessing-the-rest-api"><strong>Directly accessing the REST API</strong></a><ul><li><a href="#using-kubectl-proxy"><strong>Using kubectl proxy</strong></a></li><li><a href="#without-kubectl-proxy-before-v13x"><strong>Without kubectl proxy (before v1.3.x)</strong></a></li><li><a href="#without-kubectl-proxy-post-v13x"><strong>Without kubectl proxy (post v1.3.x)</strong></a></li></ul></li><li><a href="#programmatic-access-to-the-api"><strong>Programmatic access to the API</strong></a><ul><li><a href="#go-client"><strong>Go client</strong></a></li><li><a href="#python-client"><strong>Python client</strong></a></li><li><a href="#other-languages"><strong>Other languages</strong></a></li></ul></li><li><a href="#accessing-the-api-from-a-pod"><strong>Accessing the API from a Pod</strong></a></li></ul></li><li><a href="#accessing-services-running-on-the-cluster">Accessing services running on the cluster</a><ul><li><a href="#ways-to-connect"><strong>Ways to connect</strong></a></li><li><a href="#discovering-builtin-services"><strong>Discovering builtin services</strong></a><ul><li><a href="#manually-constructing-apiserver-proxy-urls"><strong>Manually constructing apiserver proxy URLs</strong></a></li><li><a href="#using-web-browsers-to-access-services-running-on-the-cluster"><strong>Using web browsers to access services running on the cluster</strong></a></li></ul></li></ul></li><li><a href="#requesting-redirects-1">Requesting redirects</a></li><li><a href="#so-many-proxies">So Many Proxies</a></li></ul></li><li><a href="#configure-access-to-multiple-clusters">Configure Access to Multiple Clusters</a><ul><li><a href="#before-you-begin-46">Before you begin</a></li><li><a href="#define-clusters-users-and-contexts">Define clusters, users, and contexts</a></li><li><a href="#create-a-second-configuration-file">Create a second configuration file</a></li><li><a href="#set-the-kubeconfig-environment-variable">Set the KUBECONFIG environment variable</a></li><li><a href="#explore-the-homekube-directory">Explore the $HOME/.kube directory</a></li><li><a href="#append-homekubeconfig-to-your-kubeconfig-environment-variable">Append $HOME/.kube/config to your KUBECONFIG environment variable</a></li><li><a href="#clean-up-6">Clean up</a></li><li><a href="#whats-next-62">What&#x27;s next</a></li></ul></li><li><a href="#use-port-forwarding-to-access-applications-in-a-cluster">Use Port Forwarding to Access Applications in a Cluster</a><ul><li><a href="#before-you-begin-47">Before you begin</a></li><li><a href="#creating-redis-deployment-and-service">Creating Redis deployment and service</a></li><li><a href="#forward-a-local-port-to-a-port-on-the-pod">Forward a local port to a port on the pod</a></li><li><a href="#discussion-2">Discussion</a></li><li><a href="#whats-next-63">What&#x27;s next</a></li></ul></li><li><a href="#provide-load-balanced-access-to-an-application-in-a-cluster">Provide Load-Balanced Access to an Application in a Cluster</a><ul><li><a href="#objectives-22">Objectives</a></li><li><a href="#before-you-begin-48">Before you begin</a></li><li><a href="#creating-a-service-for-an-application-running-in-two-pods-1">Creating a Service for an application running in two pods</a></li><li><a href="#using-a-service-configuration-file-1">Using a service configuration file</a></li><li><a href="#whats-next-64">What&#x27;s next</a></li></ul></li><li><a href="#use-a-service-to-access-an-application-in-a-cluster-1">Use a Service to Access an Application in a Cluster</a><ul><li><a href="#objectives-23">Objectives</a></li><li><a href="#before-you-begin-49">Before you begin</a></li><li><a href="#creating-a-service-for-an-application-running-in-two-pods-2">Creating a service for an application running in two pods</a></li><li><a href="#using-a-service-configuration-file-2">Using a service configuration file</a></li><li><a href="#cleaning-up-9">Cleaning up</a></li><li><a href="#whats-next-65">What&#x27;s next</a></li></ul></li><li><a href="#connect-a-front-end-to-a-back-end-using-a-service">Connect a Front End to a Back End Using a Service</a><ul><li><a href="#objectives-24">Objectives</a></li><li><a href="#before-you-begin-50">Before you begin</a><ul><li><a href="#creating-the-backend-using-a-deployment"><strong>Creating the backend using a Deployment</strong></a></li><li><a href="#creating-the-backend-service-object"><strong>Creating the backend Service object</strong></a></li><li><a href="#creating-the-frontend"><strong>Creating the frontend</strong></a></li><li><a href="#interact-with-the-frontend-service"><strong>Interact with the frontend Service</strong></a></li><li><a href="#send-traffic-through-the-frontend"><strong>Send traffic through the frontend</strong></a></li></ul></li><li><a href="#whats-next-66">What&#x27;s next</a></li></ul></li><li><a href="#create-an-external-load-balancer">Create an External Load Balancer</a><ul><li><a href="#before-you-begin-51">Before you begin</a></li><li><a href="#configuration-file">Configuration file</a></li><li><a href="#using-kubectl-1">Using kubectl</a></li><li><a href="#finding-your-ip-address">Finding your IP address</a></li><li><a href="#preserving-the-client-source-ip">Preserving the client source IP</a><ul><li><a href="#feature-availability"><strong>Feature availability</strong></a></li></ul></li><li><a href="#external-load-balancer-providers">External Load Balancer Providers</a></li><li><a href="#caveats-and-limitations-when-preserving-source-ips">Caveats and Limitations when preserving source IPs</a></li></ul></li><li><a href="#configure-your-cloud-providers-firewalls">Configure Your Cloud Provider\&#x27;s Firewalls</a><ul><li><a href="#restrict-access-for-loadbalancer-service"><strong>Restrict Access For LoadBalancer Service</strong></a></li><li><a href="#google-compute-engine"><strong>Google Compute Engine</strong></a></li><li><a href="#other-cloud-providers"><strong>Other cloud providers</strong></a></li></ul></li><li><a href="#list-all-container-images-running-in-a-cluster">List All Container Images Running in a Cluster</a><ul><li><a href="#before-you-begin-52">Before you begin</a></li><li><a href="#list-all-containers-in-all-namespaces">List all Containers in all namespaces</a></li><li><a href="#list-containers-by-pod">List Containers by Pod</a></li><li><a href="#list-containers-filtering-by-pod-label">List Containers filtering by Pod label</a></li><li><a href="#list-containers-filtering-by-pod-namespace">List Containers filtering by Pod namespace</a></li><li><a href="#list-containers-using-a-go-template-instead-of-jsonpath">List Containers using a go-template instead of jsonpath</a></li><li><a href="#whats-next-67">What&#x27;s next</a><ul><li><a href="#reference-4"><strong>Reference</strong></a></li></ul></li></ul></li><li><a href="#communicate-between-containers-in-the-same-pod-using-a-shared-volume">Communicate Between Containers in the Same Pod Using a Shared Volume</a><ul><li><a href="#before-you-begin-53">Before you begin</a></li><li><a href="#creating-a-pod-that-runs-two-containers">Creating a Pod that runs two Containers</a></li><li><a href="#discussion-3">Discussion</a></li><li><a href="#whats-next-68">What&#x27;s next</a></li></ul></li><li><a href="#kubernetes-dns-example">Kubernetes DNS example</a><ul><li><a href="#step-zero-prerequisites-1">Step Zero: Prerequisites</a></li><li><a href="#step-one-create-two-namespaces">Step One: Create two namespaces</a></li><li><a href="#step-two-create-backend-replication-controller-in-each-namespace">Step Two: Create backend replication controller in each namespace</a></li><li><a href="#step-three-create-backend-service">Step Three: Create backend service</a></li><li><a href="#step-four-create-client-pod-in-one-namespace">Step Four: Create client pod in one namespace</a><ul><li><a href="#note-about-default-namespace">Note about default namespace</a></li></ul></li><li><a href="#tl-dr">tl; dr;</a></li></ul></li></ul></li><li><a href="#monitor-log-and-debug">Monitor, Log, and Debug</a><ul><li><a href="#core-metrics-pipeline">Core metrics pipeline</a><ul><li><a href="#the-metrics-api">The Metrics API</a></li><li><a href="#metrics-server">Metrics Server</a></li></ul></li><li><a href="#tools-for-monitoring-compute-storage-and-network-resources">Tools for Monitoring Compute, Storage, and Network Resources</a><ul><li><a href="#overview-6">Overview</a><ul><li><a href="#cadvisor"><strong>cAdvisor</strong></a></li><li><a href="#kubelet-1"><strong>Kubelet</strong></a></li></ul></li><li><a href="#storage-backends">Storage Backends</a><ul><li><a href="#influxdb-and-grafana"><strong>InfluxDB and Grafana</strong></a></li><li><a href="#google-cloud-monitoring"><strong>Google Cloud Monitoring</strong></a></li></ul></li><li><a href="#try-it-out">Try it out!</a></li></ul></li><li><a href="#get-a-shell-to-a-running-container">Get a Shell to a Running Container</a><ul><li><a href="#before-you-begin-54">Before you begin</a></li><li><a href="#getting-a-shell-to-a-container">Getting a shell to a Container</a></li><li><a href="#writing-the-root-page-for-nginx">Writing the root page for nginx</a></li><li><a href="#running-individual-commands-in-a-container">Running individual commands in a Container</a></li><li><a href="#opening-a-shell-when-a-pod-has-more-than-one-container">Opening a shell when a Pod has more than one Container</a></li><li><a href="#whats-next-69">What&#x27;s next</a></li></ul></li><li><a href="#monitor-node-health">Monitor Node Health</a><ul><li><a href="#node-problem-detector">Node Problem Detector</a></li><li><a href="#limitations-3">Limitations</a></li><li><a href="#enabledisable-in-gce-cluster">Enable/Disable in GCE cluster</a></li><li><a href="#use-in-other-environment">Use in Other Environment</a><ul><li><a href="#kubectl"><strong>Kubectl</strong></a></li><li><a href="#addon-pod"><strong>Addon Pod</strong></a></li></ul></li><li><a href="#overwrite-the-configuration">Overwrite the Configuration</a></li><li><a href="#kernel-monitor">Kernel Monitor</a><ul><li><a href="#add-new-nodeconditions"><strong>Add New NodeConditions</strong></a></li><li><a href="#detect-new-problems"><strong>Detect New Problems</strong></a></li><li><a href="#change-log-path"><strong>Change Log Path</strong></a></li><li><a href="#support-other-log-format"><strong>Support Other Log Format</strong></a></li></ul></li><li><a href="#caveats-2">Caveats</a></li></ul></li><li><a href="#logging-using-stackdriver">Logging Using Stackdriver</a><ul><li><a href="#deploying">Deploying</a><ul><li><a href="#deploying-to-a-new-cluster"><strong>Deploying to a new cluster</strong></a><ul><li><a href="#google-kubernetes-engine"><strong>Google Kubernetes Engine</strong></a></li><li><a href="#other-platforms"><strong>Other platforms</strong></a></li></ul></li><li><a href="#deploying-to-an-existing-cluster"><strong>Deploying to an existing cluster</strong></a></li></ul></li><li><a href="#verifying-your-logging-agent-deployment">Verifying your Logging Agent Deployment</a></li><li><a href="#viewing-logs">Viewing logs</a><ul><li><a href="#exporting-logs"><strong>Exporting logs</strong></a></li></ul></li><li><a href="#configuring-stackdriver-logging-agents">Configuring Stackdriver Logging Agents</a><ul><li><a href="#prerequisites-5"><strong>Prerequisites</strong></a></li><li><a href="#changingdaemonsetparameters"><strong>Changing </strong>DaemonSet<strong> parameters</strong></a></li><li><a href="#changing-fluentd-parameters"><strong>Changing fluentd parameters</strong></a></li><li><a href="#adding-fluentd-plugins"><strong>Adding fluentd plugins</strong></a></li></ul></li></ul></li><li><a href="#events-in-stackdriver">Events in Stackdriver</a><ul><li><a href="#deployment">Deployment</a><ul><li><a href="#google-kubernetes-engine-1"><strong>Google Kubernetes Engine</strong></a></li><li><a href="#deploying-to-the-existing-cluster"><strong>Deploying to the Existing Cluster</strong></a></li></ul></li><li><a href="#user-guide-1">User Guide</a></li></ul></li><li><a href="#logging-using-elasticsearch-and-kibana">Logging Using Elasticsearch and Kibana</a></li><li><a href="#determine-the-reason-for-pod-failure">Determine the Reason for Pod Failure</a><ul><li><a href="#before-you-begin-55">Before you begin</a></li><li><a href="#writing-and-reading-a-termination-message">Writing and reading a termination message</a></li><li><a href="#customizing-the-termination-message">Customizing the termination message</a></li><li><a href="#whats-next-70">What&#x27;s next</a></li></ul></li><li><a href="#debug-init-containers">Debug Init Containers</a><ul><li><a href="#before-you-begin-56">Before you begin</a></li><li><a href="#checking-the-status-of-init-containers">Checking the status of Init Containers</a></li><li><a href="#getting-details-about-init-containers">Getting details about Init Containers</a></li><li><a href="#accessing-logs-from-init-containers">Accessing logs from Init Containers</a></li><li><a href="#understanding-pod-status">Understanding Pod status</a></li></ul></li><li><a href="#debug-pods-and-replication-controllers">Debug Pods and Replication Controllers</a><ul><li><a href="#debugging-pods">Debugging pods</a><ul><li><a href="#my-pod-stays-pending"><strong>My pod stays pending</strong></a><ul><li><a href="#insufficient-resources"><strong>Insufficient resources</strong></a></li><li><a href="#using-hostport"><strong>Using hostPort</strong></a></li></ul></li><li><a href="#my-pod-stays-waiting"><strong>My pod stays waiting</strong></a></li><li><a href="#my-pod-is-crashing-or-otherwise-unhealthy"><strong>My pod is crashing or otherwise unhealthy</strong></a></li></ul></li><li><a href="#debugging-replication-controllers">Debugging Replication Controllers</a></li></ul></li><li><a href="#debug-services">Debug Services</a><ul><li><a href="#conventions">Conventions</a></li><li><a href="#running-commands-in-a-pod">Running commands in a Pod</a></li><li><a href="#setup">Setup</a></li><li><a href="#does-the-service-exist">Does the Service exist?</a></li><li><a href="#does-the-service-work-by-dns">Does the Service work by DNS?</a><ul><li><a href="#does-any-service-exist-in-dns"><strong>Does any Service exist in DNS?</strong></a></li></ul></li><li><a href="#does-the-service-work-by-ip">Does the Service work by IP?</a></li><li><a href="#is-the-service-correct">Is the Service correct?</a></li><li><a href="#does-the-service-have-any-endpoints">Does the Service have any Endpoints?</a></li><li><a href="#are-the-pods-working">Are the Pods working?</a></li><li><a href="#is-the-kube-proxy-working">Is the kube-proxy working?</a><ul><li><a href="#is-kube-proxy-running"><strong>Is kube-proxy running?</strong></a></li><li><a href="#is-kube-proxy-writing-iptables-rules"><strong>Is kube-proxy writing iptables rules?</strong></a><ul><li><a href="#userspace-1"><strong>Userspace</strong></a></li><li><a href="#iptables-1"><strong>Iptables</strong></a></li></ul></li><li><a href="#is-kube-proxy-proxying"><strong>Is kube-proxy proxying?</strong></a></li><li><a href="#a-pod-cannot-reach-itself-via-service-ip"><strong>A Pod cannot reach itself via Service IP</strong></a></li></ul></li><li><a href="#seek-help">Seek help</a></li><li><a href="#more-information-1">More information</a></li></ul></li><li><a href="#troubleshoot-clusters">Troubleshoot Clusters</a><ul><li><a href="#listing-your-cluster">Listing your cluster</a></li><li><a href="#looking-at-logs">Looking at logs</a><ul><li><a href="#master"><strong>Master</strong></a></li><li><a href="#worker-nodes-1"><strong>Worker Nodes</strong></a></li></ul></li><li><a href="#a-general-overview-of-cluster-failure-modes">A general overview of cluster failure modes</a></li></ul></li><li><a href="#troubleshoot-applications">Troubleshoot Applications</a><ul><li><a href="#diagnosing-the-problem">Diagnosing the problem</a><ul><li><a href="#debugging-pods-1"><strong>Debugging Pods</strong></a><ul><li><a href="#my-pod-stays-pending-1"><strong>My pod stays pending</strong></a></li><li><a href="#my-pod-stays-waiting-1"><strong>My pod stays waiting</strong></a></li><li><a href="#my-pod-is-crashing-or-otherwise-unhealthy-1"><strong>My pod is crashing or otherwise unhealthy</strong></a></li><li><a href="#my-pod-is-running-but-not-doing-what-i-told-it-to-do"><strong>My pod is running but not doing what I told it to do</strong></a></li></ul></li><li><a href="#debugging-replication-controllers-1"><strong>Debugging Replication Controllers</strong></a></li><li><a href="#debugging-services"><strong>Debugging Services</strong></a><ul><li><a href="#my-service-is-missing-endpoints"><strong>My service is missing endpoints</strong></a></li><li><a href="#network-traffic-is-not-forwarded"><strong>Network traffic is not forwarded</strong></a></li><li><a href="#more-information-2"><strong>More information</strong></a></li></ul></li></ul></li></ul></li><li><a href="#debug-a-statefulset">Debug a StatefulSet</a><ul><li><a href="#before-you-begin-57">Before you begin</a></li><li><a href="#debugging-a-statefulset">Debugging a StatefulSet</a></li><li><a href="#whats-next-71">What&#x27;s next</a></li></ul></li><li><a href="#application-introspection-and-debugging">Application Introspection and Debugging</a><ul><li><a href="#usingkubectl-describe-podto-fetch-details-about-pods">Using kubectl describe pod to fetch details about pods</a></li><li><a href="#example-debugging-pending-pods">Example: debugging Pending Pods</a></li><li><a href="#example-debugging-a-downunreachable-node">Example: debugging a down/unreachable node</a></li><li><a href="#whats-next-72">What&#x27;s next?</a></li></ul></li><li><a href="#auditing">Auditing</a><ul><li><a href="#audit-policy">Audit Policy</a></li><li><a href="#audit-backends">Audit backends</a><ul><li><a href="#log-backend"><strong>Log backend</strong></a></li><li><a href="#webhook-backend"><strong>Webhook backend</strong></a></li><li><a href="#batching"><strong>Batching</strong></a><ul><li><a href="#parameter-tuning"><strong>Parameter tuning</strong></a></li></ul></li></ul></li><li><a href="#multi-cluster-setup">Multi-cluster setup</a></li><li><a href="#log-collector-examples">Log Collector Examples</a><ul><li><a href="#use-fluentd-to-collect-and-distribute-audit-events-from-log-file"><strong>Use fluentd to collect and distribute audit events from log file</strong></a></li><li><a href="#use-logstash-to-collect-and-distribute-audit-events-from-webhook-backend"><strong>Use logstash to collect and distribute audit events from webhook backend</strong></a></li></ul></li><li><a href="#legacy-audit">Legacy Audit</a><ul><li><a href="#configuration-5"><strong>Configuration</strong></a></li></ul></li></ul></li><li><a href="#developing-and-debugging-services-locally">Developing and debugging services locally</a><ul><li><a href="#before-you-begin-58">Before you begin</a></li><li><a href="#getting-a-shell-on-a-remote-cluster">Getting a shell on a remote cluster</a></li><li><a href="#developing-or-debugging-an-existing-service">Developing or debugging an existing service</a></li><li><a href="#whats-next-73">What&#x27;s next</a></li></ul></li><li><a href="#explorer">Explorer</a></li></ul></li><li><a href="#extend-kubernetes">Extend Kubernetes</a><ul><li><a href="#use-an-http-proxy-to-access-the-kubernetes-api">Use an HTTP Proxy to Access the Kubernetes API</a><ul><li><a href="#before-you-begin-59">Before you begin</a></li><li><a href="#using-kubectl-to-start-a-proxy-server">Using kubectl to start a proxy server</a></li><li><a href="#exploring-the-kubernetes-api">Exploring the Kubernetes API</a></li><li><a href="#whats-next-74">What&#x27;s next</a></li></ul></li><li><a href="#extend-the-kubernetes-api-with-customresourcedefinitions">Extend the Kubernetes API with CustomResourceDefinitions</a><ul><li><a href="#before-you-begin-60">Before you begin</a></li><li><a href="#create-a-customresourcedefinition">Create a CustomResourceDefinition</a></li><li><a href="#create-custom-objects">Create custom objects</a></li><li><a href="#delete-a-customresourcedefinition">Delete a CustomResourceDefinition</a></li><li><a href="#advanced-topics">Advanced topics</a><ul><li><a href="#finalizers"><strong>Finalizers</strong></a></li><li><a href="#validation"><strong>Validation</strong></a></li><li><a href="#subresources"><strong>Subresources</strong></a><ul><li><a href="#status-subresource"><strong>Status subresource</strong></a></li><li><a href="#scale-subresource"><strong>Scale subresource</strong></a></li></ul></li><li><a href="#categories"><strong>Categories</strong></a></li></ul></li><li><a href="#whats-next-75">What&#x27;s next</a></li></ul></li><li><a href="#extend-the-kubernetes-api-with-thirdpartyresources">Extend the Kubernetes API with ThirdPartyResources</a><ul><li><a href="#what-is-thirdpartyresource">What is ThirdPartyResource?</a></li><li><a href="#structure-of-a-thirdpartyresource">Structure of a ThirdPartyResource</a></li><li><a href="#creating-a-thirdpartyresource">Creating a ThirdPartyResource</a></li><li><a href="#creating-custom-objects">Creating Custom Objects</a></li><li><a href="#whats-next-76">What&#x27;s next</a></li></ul></li><li><a href="#migrate-a-thirdpartyresource-to-customresourcedefinition">Migrate a ThirdPartyResource to CustomResourceDefinition</a><ul><li><a href="#before-you-begin-61">Before you begin</a></li><li><a href="#migrate-tpr-data">Migrate TPR data</a></li><li><a href="#whats-next-77">What&#x27;s next</a></li></ul></li><li><a href="#configure-the-aggregation-layer">Configure the aggregation layer</a><ul><li><a href="#before-you-begin-62">Before you begin</a></li><li><a href="#enable-apiserver-flags">Enable apiserver flags</a></li><li><a href="#whats-next-78">What&#x27;s next</a></li></ul></li><li><a href="#setup-an-extension-api-server">Setup an extension API server</a><ul><li><a href="#before-you-begin-63">Before you begin</a></li><li><a href="#setup-an-extension-api-server-to-work-with-the-aggregation-layer">Setup an extension api-server to work with the aggregation layer</a></li><li><a href="#whats-next-79">What&#x27;s next</a></li></ul></li><li><a href="#install-service-catalog-using-helm">Install Service Catalog using Helm</a><ul><li><a href="#before-you-begin-64">Before you begin</a></li><li><a href="#add-the-service-catalog-helm-repository">Add the service-catalog Helm repository</a></li><li><a href="#enable-rbac">Enable RBAC</a></li><li><a href="#install-service-catalog-in-your-kubernetes-cluster">Install Service Catalog in your Kubernetes cluster</a></li><li><a href="#whats-next-80">What&#x27;s next</a></li></ul></li><li><a href="#install-service-catalog-using-sc">Install Service Catalog using SC</a><ul><li><a href="#before-you-begin-65">Before you begin</a></li><li><a href="#installscin-your-local-environment">Install sc in your local environment</a></li><li><a href="#install-service-catalog-in-your-kubernetes-cluster-1">Install Service Catalog in your Kubernetes cluster</a></li><li><a href="#uninstall-service-catalog">Uninstall Service Catalog</a></li><li><a href="#whats-next-81">What&#x27;s next</a></li></ul></li></ul></li><li><a href="#tls-1">TLS</a><ul><li><a href="#manage-tls-certificates-in-a-cluster">Manage TLS Certificates in a Cluster</a><ul><li><a href="#overview-7">Overview</a></li><li><a href="#trusting-tls-in-a-cluster">Trusting TLS in a Cluster</a></li><li><a href="#requesting-a-certificate">Requesting a Certificate</a><ul><li><a href="#step-0-download-and-install-cfssl"><strong>Step 0. Download and install CFSSL</strong></a></li><li><a href="#step-1-create-a-certificate-signing-request"><strong>Step 1. Create a Certificate Signing Request</strong></a></li><li><a href="#step-2-create-a-certificate-signing-request-object-to-send-to-the-kubernetes-api"><strong>Step 2. Create a Certificate Signing Request object to send to the Kubernetes API</strong></a></li><li><a href="#step-3-get-the-certificate-signing-request-approved"><strong>Step 3. Get the Certificate Signing Request Approved</strong></a></li><li><a href="#step-4-download-the-certificate-and-use-it"><strong>Step 4. Download the Certificate and Use It</strong></a></li></ul></li><li><a href="#approving-certificate-signing-requests">Approving Certificate Signing Requests</a></li><li><a href="#a-word-ofwarningon-the-approval-permission">A Word of *<strong>*Warning**</strong> on the Approval Permission</a></li><li><a href="#a-note-to-cluster-administrators">A Note to Cluster Administrators</a></li></ul></li><li><a href="#certificate-rotation">Certificate Rotation</a><ul><li><a href="#before-you-begin-66">Before you begin</a></li></ul></li><li><a href="#overview-8">Overview</a></li><li><a href="#enabling-client-certificate-rotation">Enabling client certificate rotation</a></li><li><a href="#understanding-the-certificate-rotation-configuration">Understanding the certificate rotation configuration</a></li></ul></li><li><a href="#administer-a-cluster">Administer a Cluster</a></li><li><a href="#federation---run-an-app-on-multiple-clusters">Federation - Run an App on Multiple Clusters</a></li><li><a href="#manage-cluster-daemons">Manage Cluster Daemons</a><ul><li><a href="#perform-a-rolling-update-on-a-daemonset">Perform a Rolling Update on a DaemonSet</a><ul><li><a href="#before-you-begin-67">Before you begin</a></li><li><a href="#daemonset-update-strategy">DaemonSet Update Strategy</a></li><li><a href="#caveat-updating-daemonset-created-from-kubernetes-version-15-or-before">Caveat: Updating DaemonSet created from Kubernetes version 1.5 or before</a></li><li><a href="#performing-a-rolling-update-1">Performing a Rolling Update</a><ul><li><a href="#step-1-checking-daemonsetrollingupdateupdate-strategy"><strong>Step 1: Checking DaemonSet </strong>RollingUpdate<strong> update strategy</strong></a></li><li><a href="#step-2-creating-a-daemonset-withrollingupdateupdate-strategy"><strong>Step 2: Creating a DaemonSet with </strong>RollingUpdate<strong> update strategy</strong></a></li><li><a href="#step-3-updating-a-daemonset-template"><strong>Step 3: Updating a DaemonSet template</strong></a><ul><li><a href="#declarative-commands"><strong>Declarative commands</strong></a></li><li><a href="#imperative-commands-1"><strong>Imperative commands</strong></a></li></ul></li><li><a href="#step-4-watching-the-rolling-update-status"><strong>Step 4: Watching the rolling update status</strong></a></li></ul></li><li><a href="#troubleshooting-5">Troubleshooting</a><ul><li><a href="#daemonset-rolling-update-is-stuck"><strong>DaemonSet rolling update is stuck</strong></a><ul><li><a href="#some-nodes-run-out-of-resources"><strong>Some nodes run out of resources</strong></a></li><li><a href="#broken-rollout"><strong>Broken rollout</strong></a></li><li><a href="#clock-skew"><strong>Clock skew</strong></a></li></ul></li></ul></li><li><a href="#whats-next-82">What&#x27;s next</a></li></ul></li><li><a href="#performing-a-rollback-on-a-daemonset">Performing a Rollback on a DaemonSet</a><ul><li><a href="#before-you-begin-68">Before you begin</a></li><li><a href="#performing-a-rollback-on-a-daemonset-1">Performing a Rollback on a DaemonSet</a><ul><li><a href="#step-1-find-the-daemonset-revision-you-want-to-roll-back-to"><strong>Step 1: Find the DaemonSet revision you want to roll back to</strong></a></li><li><a href="#step-2-roll-back-to-a-specific-revision"><strong>Step 2: Roll back to a specific revision</strong></a></li><li><a href="#step-3-watch-the-progress-of-the-daemonset-rollback"><strong>Step 3: Watch the progress of the DaemonSet rollback</strong></a></li></ul></li><li><a href="#understanding-daemonset-revisions">Understanding DaemonSet Revisions</a></li><li><a href="#troubleshooting-6">Troubleshooting</a></li></ul></li></ul></li><li><a href="#schedule-gpus">Schedule GPUs</a><ul><li><a href="#v18-onwards">v1.8 onwards</a><ul><li><a href="#deploying-nvidia-gpu-device-plugin">Deploying NVIDIA GPU device plugin</a><ul><li><a href="#official-nvidia-gpu-device-plugin"><strong>Official NVIDIA GPU device plugin</strong></a></li><li><a href="#nvidia-gpu-device-plugin-used-by-gkegce"><strong>NVIDIA GPU device plugin used by GKE/GCE</strong></a></li></ul></li></ul></li><li><a href="#clusters-containing-different-types-of-nvidia-gpus">Clusters containing different types of NVIDIA GPUs</a></li><li><a href="#v16-and-v17">v1.6 and v1.7</a></li><li><a href="#future">Future</a></li></ul></li><li><a href="#manage-hugepages">Manage HugePages</a><ul><li><a href="#before-you-begin-69">Before you begin</a></li><li><a href="#api-1">API</a></li><li><a href="#future-1">Future</a></li></ul></li><li><a href="#extend-kubectl-with-plugins">Extend kubectl with plugins</a><ul><li><a href="#before-you-begin-70">Before you begin</a></li><li><a href="#installing-kubectl-plugins">Installing kubectl plugins</a><ul><li><a href="#plugin-loader">Plugin loader</a><ul><li><a href="#search-order"><strong>Search order</strong></a></li></ul></li></ul></li><li><a href="#writing-kubectl-plugins">Writing kubectl plugins</a><ul><li><a href="#the-pluginyaml-descriptor">The plugin.yaml descriptor</a></li><li><a href="#recommended-directory-structure">Recommended directory structure</a></li><li><a href="#accessing-runtime-attributes">Accessing runtime attributes</a></li></ul></li><li><a href="#whats-next-83">What&#x27;s next</a></li></ul></li></ul></li><li><a href="#kubectl-cheat-sheet">kubectl Cheat Sheet</a><ul><li><a href="#kubectl-autocomplete">Kubectl Autocomplete</a></li><li><a href="#kubectl-context-and-configuration">Kubectl Context and Configuration</a></li><li><a href="#creating-objects">Creating Objects</a></li><li><a href="#viewing-finding-resources">Viewing, Finding Resources</a></li><li><a href="#updating-resources">Updating Resources</a></li><li><a href="#patching-resources">Patching Resources</a></li><li><a href="#editing-resources">Editing Resources</a></li><li><a href="#scaling-resources">Scaling Resources</a></li><li><a href="#deleting-resources">Deleting Resources</a></li><li><a href="#interacting-with-running-pods">Interacting with running Pods</a></li><li><a href="#interacting-with-nodes-and-cluster">Interacting with Nodes and Cluster</a></li><li><a href="#resource-types-1">Resource types</a><div class="MuiContainer-root MuiContainer-maxWidthLg"><pre class="Code__Pre-gy960v-0 UDybk prism-code language-undefined" style="color:#9CDCFE;background-color:#1E1E1E"><div class="MuiGrid-root MuiGrid-container MuiGrid-justify-xs-flex-end"><button class="Code__CopyCode-gy960v-1 llUIua">Copy</button></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">- [Formatting output](#formatting-output)</span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">- [Kubectl output verbosity and debugging](#kubectl-output-verbosity-and-debugging)</span></div></pre></div>Important Link</li></ul></li></ul><p>Code Links
<a href="https://github.com/wardviaene/kubernetes-course">https://github.com/wardviaene/kubernetes-course</a></p><h1>Concepts</h1><p>The Concepts section helps you learn about the parts of the Kubernetes system and the abstractions Kubernetes uses to represent your cluster, and helps you obtain a deeper understanding of how Kubernetes works.</p><h2>Overview</h2><p>To work with Kubernetes, you use Kubernetes API objects to describe your cluster&#x27;s desired state: what applications or other workloads you want to run, what container images they use, the number of replicas, what network and disk resources you want to make available, and more. You set your desired state by creating objects using the Kubernetes API, typically via the command-line interface, <strong>kubectl</strong>. You can also use the Kubernetes API directly to interact with the cluster and set or modify your desired state.</p><p>Once you&#x27;ve set your desired state, the Kubernetes Control Plane works to make the cluster&#x27;s current state match the desired state. To do so, Kubernetes performs a variety of tasks automatically--such as starting or restarting containers, scaling the number of replicas of a given application, and more. The Kubernetes Control Plane consists of a collection of processes running on your cluster:</p><ul><li>The <strong>Kubernetes Master</strong> is a collection of three processes that run on a single node in your cluster, which is designated as the master node. Those processes are: <a href="https://kubernetes.io/docs/admin/kube-apiserver/">kube-apiserver</a>, <a href="https://kubernetes.io/docs/admin/kube-controller-manager/">kube-controller-manager</a> and <a href="https://kubernetes.io/docs/admin/kube-scheduler/">kube-scheduler</a>.</li><li>Each individual non-master node in your cluster runs two processes:</li><li><a href="https://kubernetes.io/docs/admin/kubelet/"><strong>kubelet</strong></a>, which communicates with the Kubernetes Master.</li><li><a href="https://kubernetes.io/docs/admin/kube-proxy/"><strong>kube-proxy</strong></a>, a network proxy which reflects Kubernetes networking services on each node.</li></ul><h2>Kubernetes Objects</h2><p>Kubernetes contains several abstractions that represent the state of your system: deployed containerized applications and workloads, their associated network and disk resources, and other information about what your cluster is doing. These abstractions are represented by objects in the Kubernetes API; see the <a href="https://kubernetes.io/docs/concepts/abstractions/overview/">Kubernetes Objects overview</a> for more details.</p><p>The basic Kubernetes objects include:</p><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/">Pod</a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/">Service</a></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/">Volume</a></li><li><a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/">Namespace</a></li></ul><p>In addition, Kubernetes contains several higher-level abstractions called Controllers. Controllers build upon the basic objects and provide additional functionality and convenience features. They include:</p><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/">ReplicaSet</a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployment</a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">StatefulSet</a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/">DaemonSet</a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/">Job</a></li></ul><h2>Kubernetes Control Plane</h2><p>The various parts of the Kubernetes Control Plane, such as the Kubernetes Master and kubelet processes, govern how Kubernetes communicates with your cluster. The Control Plane maintains a record of all of the Kubernetes Objects in the system and runs continuous control loops to manage those objects&#x27; state. At any given time, the Control Plane&#x27;s control loops will respond to changes in the cluster and work to make the actual state of all the objects in the system match the desired state that you provided.</p><p>For example, when you use the Kubernetes API to create a Deployment object, you provide a new desired state for the system. The Kubernetes Control Plane records that object creation and carries out your instructions by starting the required applications and scheduling them to cluster nodes--thus making the cluster&#x27;s actual state match the desired state.</p><h3>Kubernetes Master</h3><p>The Kubernetes master is responsible for maintaining the desired state for your cluster. When you interact with Kubernetes, such as by using the <strong>kubectl</strong> command-line interface, you&#x27;re communicating with your cluster&#x27;s Kubernetes master.</p><p>The &quot;master&quot; refers to a collection of processes managing the cluster state. Typically, these processes are all run on a single node in the cluster, and this node is also referred to as the master. The master can also be replicated for availability and redundancy.</p><h3>Kubernetes Nodes</h3><p>The nodes in a cluster are the machines (VMs, physical servers, etc) that run your applications and cloud workflows. The Kubernetes master controls each node; you&#x27;ll rarely interact with nodes directly.</p><h4>Object Metadata</h4><ul><li><a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/">Annotations</a></li></ul><h3>What&#x27;s next</h3><p>If you would like to write a concept page, see <a href="https://kubernetes.io/docs/home/contribute/page-templates/">Using Page Templates</a> for information about the concept page type and the concept template.</p><h2>What is Kubernetes?</h2><p>Kubernetes is a portable, extensible open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.</p><p>Google open-sourced the Kubernetes project in 2014. Kubernetes builds upon a <a href="https://research.google.com/pubs/pub43438.html">decade and a half of experience that Google has with running production workloads at scale</a>, combined with best-of-breed ideas and practices from the community.</p><h3>Why do I need Kubernetes and what can it do?</h3><p>Kubernetes has several features. It can be thought of as:</p><ul><li>a container platform</li><li>a microservices platform</li><li>a portable cloud platform and a lot more.</li></ul><p>Kubernetes provides a <strong>container-centric</strong> management environment. It orchestrates computing, networking, and storage infrastructure on behalf of user workloads. This provides much of the simplicity of Platform as a Service (PaaS) with the flexibility of Infrastructure as a Service (IaaS), and enables portability across infrastructure providers.</p><h3>How is Kubernetes a platform?</h3><p>Even though Kubernetes provides a lot of functionality, there are always new scenarios that would benefit from new features. Application-specific workflows can be streamlined to accelerate developer velocity. Ad hoc orchestration that is acceptable initially often requires robust automation at scale. This is why Kubernetes was also designed to serve as a platform for building an ecosystem of components and tools to make it easier to deploy, scale, and manage applications.</p><p><a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/">Labels</a> empower users to organize their resources however they please. <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/">Annotations</a> enable users to decorate resources with custom information to facilitate their workflows and provide an easy way for management tools to checkpoint state.</p><p>Additionally, the <a href="https://kubernetes.io/docs/concepts/overview/components/">Kubernetes control plane</a> is built upon the same <a href="https://kubernetes.io/docs/reference/api-overview/">APIs</a> that are available to developers and users. Users can write their own controllers, such as <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/scheduler.md">schedulers</a>, with <a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/">their own APIs</a>that can be targeted by a general-purpose <a href="https://kubernetes.io/docs/user-guide/kubectl-overview/">command-line tool</a>.</p><p>This <a href="https://git.k8s.io/community/contributors/design-proposals/architecture/architecture.md">design</a> has enabled a number of other systems to build atop Kubernetes.</p><h3>What Kubernetes is not</h3><p>Kubernetes is not a traditional, all-inclusive PaaS (Platform as a Service) system. Since Kubernetes operates at the container level rather than at the hardware level, it provides some generally applicable features common to PaaS offerings, such as deployment, scaling, load balancing, logging, and monitoring. However, Kubernetes is not monolithic, and these default solutions are optional and pluggable. Kubernetes provides the building blocks for building developer platforms, but preserves user choice and flexibility where it is important.</p><p>Kubernetes:</p><ul><li>Does not limit the types of applications supported. Kubernetes aims to support an extremely diverse variety of workloads, including stateless, stateful, and data-processing workloads. If an application can run in a container, it should run great on Kubernetes.</li><li>Does not deploy source code and does not build your application. Continuous Integration, Delivery, and Deployment (CI/CD) workflows are determined by organization cultures and preferences as well as technical requirements.</li><li>Does not provide application-level services, such as middleware (e.g., message buses), data-processing frameworks (for example, Spark), databases (e.g., mysql), caches, nor cluster storage systems (e.g., Ceph) as built-in services. Such components can run on Kubernetes, and/or can be accessed by applications running on Kubernetes through portable mechanisms, such as the Open Service Broker.</li><li>Does not dictate logging, monitoring, or alerting solutions. It provides some integrations as proof of concept, and mechanisms to collect and export metrics.</li><li>Does not provide nor mandate a configuration language/system (e.g., <a href="https://github.com/google/jsonnet">jsonnet</a>). It provides a declarative API that may be targeted by arbitrary forms of declarative specifications.</li><li>Does not provide nor adopt any comprehensive machine configuration, maintenance, management, or self-healing systems.</li></ul><p>Additionally, Kubernetes is not a mere orchestration system. In fact, it eliminates the need for orchestration. The technical definition of orchestration is execution of a defined workflow: first do A, then B, then C. In contrast, Kubernetes is comprised of a set of independent, composable control processes that continuously drive the current state towards the provided desired state. It shouldn&#x27;t matter how you get from A to C. Centralized control is also not required. This results in a system that is easier to use and more powerful, robust, resilient, and extensible.</p><h3>Why containers?</h3><p>Looking for reasons why you should be using containers?</p><p>The Old Way to deploy applications was to install the applications on a host using the operating-system package manager. This had the disadvantage of entangling the applications&#x27; executables, configuration, libraries, and lifecycles with each other and with the host OS. One could build immutable virtual-machine images in order to achieve predictable rollouts and rollbacks, but VMs are heavyweight and non-portable.</p><p>The New Way is to deploy containers based on operating-system-level virtualization rather than hardware virtualization. These containers are isolated from each other and from the host: they have their own filesystems, they can&#x27;t see each others&#x27; processes, and their computational resource usage can be bounded. They are easier to build than VMs, and because they are decoupled from the underlying infrastructure and from the host filesystem, they are portable across clouds and OS distributions.</p><p>Because containers are small and fast, one application can be packed in each container image. This one-to-one application-to-image relationship unlocks the full benefits of containers. With containers, immutable container images can be created at build/release time rather than deployment time, since each application doesn&#x27;t need to be composed with the rest of the application stack, nor married to the production infrastructure environment. Generating container images at build/release time enables a consistent environment to be carried from development into production. Similarly, containers are vastly more transparent than VMs, which facilitates monitoring and management. This is especially true when the containers&#x27; process lifecycles are managed by the infrastructure rather than hidden by a process supervisor inside the container. Finally, with a single application per container, managing the containers becomes tantamount to managing deployment of the application.</p><p>Summary of container benefits:</p><ul><li><strong>Agile application creation and deployment</strong>: Increased ease and efficiency of container image creation compared to VM image use.</li><li><strong>Continuous development, integration, and deployment</strong>: Provides for reliable and frequent container image build and deployment with quick and easy rollbacks (due to image immutability).</li><li><strong>Dev and Ops separation of concerns</strong>: Create application container images at build/release time rather than deployment time, thereby decoupling applications from infrastructure.</li><li><strong>Observability</strong> Not only surfaces OS-level information and metrics, but also application health and other signals.</li><li><strong>Environmental consistency across development, testing, and production</strong>: Runs the same on a laptop as it does in the cloud.</li><li><strong>Cloud and OS distribution portability</strong>: Runs on Ubuntu, RHEL, CoreOS, on-prem, Google Kubernetes Engine, and anywhere else.</li><li><strong>Application-centric management</strong>: Raises the level of abstraction from running an OS on virtual hardware to run an application on an OS using logical resources.</li><li><strong>Loosely coupled, distributed, elastic, liberated </strong><a href="https://martinfowler.com/articles/microservices.html"><strong>micro-services</strong></a>: Applications are broken into smaller, independent pieces and can be deployed and managed dynamically -- not a fat monolithic stack running on one big single-purpose machine.</li><li><strong>Resource isolation</strong>: Predictable application performance.</li><li><strong>Resource utilization</strong>: High efficiency and density.</li></ul><h3>What does Kubernetes mean? K8s?</h3><p>The name <strong>Kubernetes</strong> originates from Greek, meaning helmsman or pilot, and is the root of governor and <a href="http://www.etymonline.com/index.php?term=cybernetics">cybernetic</a>. K8s is an abbreviation derived by replacing the 8 letters &quot;ubernete&quot; with &quot;8&quot;.</p><h3>What&#x27;s next</h3><ul><li>Ready to <a href="https://kubernetes.io/docs/setup/">Get Started</a>?</li><li>For more details, see the <a href="https://kubernetes.io/docs/home/">Kubernetes Documentation</a>.</li></ul><h2>Kubernetes Components</h2><p>This document outlines the various binary components needed to deliver a functioning Kubernetes cluster.</p><h3>Master Components</h3><p>Master components provide the cluster&#x27;s control plane. Master components make global decisions about the cluster (for example, scheduling), and detecting and responding to cluster events (starting up a new pod when a replication controller&#x27;s &#x27;replicas&#x27; field is unsatisfied).</p><p>Master components can be run on any machine in the cluster. However, for simplicity, set up scripts typically start all master components on the same machine, and do not run user containers on this machine. See <a href="https://kubernetes.io/docs/admin/high-availability/">Building High-Availability Clusters</a> for an example multi-master-VM setup.</p><h4>kube-apiserver</h4><p>Component on the master that exposes the Kubernetes API. It is the front-end for the Kubernetes control plane.</p><p>It is designed to scale horizontally -- that is, it scales by deploying more instances. See <a href="https://kubernetes.io/docs/admin/high-availability/">Building High-Availability Clusters</a>.</p><h4>etcd</h4><p>Consistent and highly-available key value store used as Kubernetes&#x27; backing store for all cluster data.</p><p>Always have a backup plan for etcd&#x27;s data for your Kubernetes cluster. For in-depth information on etcd, see <a href="https://github.com/coreos/etcd/blob/master/Documentation/docs.md">etcd documentation</a>.</p><h4>kube-scheduler</h4><p>Component on the master that watches newly created pods that have no node assigned, and selects a node for them to run on.</p><p>Factors considered for scheduling decisions include individual and collective resource requirements, hardware/software/policy constraints, affinity and anti-affinity specifications, data locality, inter-workload interference and deadlines.</p><h4>kube-controller-manager</h4><p>Component on the master that runs <a href="https://kubernetes.io/docs/admin/kube-controller-manager/">controllers</a>.</p><p>Logically, each <a href="https://kubernetes.io/docs/admin/kube-controller-manager/">controller</a> is a separate process, but to reduce complexity, they are all compiled into a single binary and run in a single process.</p><p>These controllers include:</p><ul><li>Node Controller: Responsible for noticing and responding when nodes go down.</li><li>Replication Controller: Responsible for maintaining the correct number of pods for every replication controller object in the system.</li><li>Endpoints Controller: Populates the Endpoints object (that is, joins Services &amp; Pods).</li><li>Service Account &amp; Token Controllers: Create default accounts and API access tokens for new namespaces.</li></ul><h4>cloud-controller-manager</h4><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/">cloud-controller-manager</a> runs controllers that interact with the underlying cloud providers. The cloud-controller-manager binary is an alpha feature introduced in Kubernetes release 1.6.</p><p>cloud-controller-manager runs cloud-provider-specific controller loops only. You must disable these controller loops in the kube-controller-manager. You can disable the controller loops by setting the <strong>--cloud-provider</strong> flag to <strong>external</strong> when starting the kube-controller-manager.</p><p>cloud-controller-manager allows cloud vendors code and the Kubernetes core to evolve independent of each other. In prior releases, the core Kubernetes code was dependent upon cloud-provider-specific code for functionality. In future releases, code specific to cloud vendors should be maintained by the cloud vendor themselves, and linked to cloud-controller-manager while running Kubernetes.</p><p>The following controllers have cloud provider dependencies:</p><ul><li>Node Controller: For checking the cloud provider to determine if a node has been deleted in the cloud after it stops responding</li><li>Route Controller: For setting up routes in the underlying cloud infrastructure</li><li>Service Controller: For creating, updating and deleting cloud provider load balancers</li><li>Volume Controller: For creating, attaching, and mounting volumes, and interacting with the cloud provider to orchestrate volumes</li></ul><h3>Node Components</h3><p>Node components run on every node, maintaining running pods and providing the Kubernetes runtime environment.</p><h4>kubelet</h4><p>An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.</p><p>The kubelet takes a set of PodSpecs that are provided through various mechanisms and ensures that the containers described in those PodSpecs are running and healthy. The kubelet doesn&#x27;t manage containers which were not created by Kubernetes.</p><h4>kube-proxy</h4><p><a href="https://kubernetes.io/docs/admin/kube-proxy/">kube-proxy</a> enables the Kubernetes service abstraction by maintaining network rules on the host and performing connection forwarding.</p><h4>Container Runtime</h4><p>The container runtime is the software that is responsible for running containers. Kubernetes supports several runtimes: <a href="http://www.docker.com/">Docker</a>, <a href="https://coreos.com/rkt/">rkt</a>, <a href="https://github.com/opencontainers/runc">runc</a> and any OCI <a href="https://github.com/opencontainers/runtime-spec">runtime-spec</a> implementation.</p><h3>Addons</h3><p>Addons are pods and services that implement cluster features. The pods may be managed by Deployments, ReplicationControllers, and so on. Namespaced addon objects are created in the <strong>kube-system</strong> namespace.</p><p>Selected addons are described below, for an extended list of available addons please see <a href="https://kubernetes.io/docs/concepts/cluster-administration/addons/">Addons</a>.</p><h4>DNS</h4><p>While the other addons are not strictly required, all Kubernetes clusters should have <a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/">cluster DNS</a>, as many examples rely on it.</p><p>Cluster DNS is a DNS server, in addition to the other DNS server(s) in your environment, which serves DNS records for Kubernetes services.</p><p>Containers started by Kubernetes automatically include this DNS server in their DNS searches.</p><h4>Web UI (Dashboard)</h4><p><a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/">Dashboard</a> is a general purpose, web-based UI for Kubernetes clusters. It allows users to manage and troubleshoot applications running in the cluster, as well as the cluster itself.</p><h4>Container Resource Monitoring</h4><p><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/resource-usage-monitoring/">Container Resource Monitoring</a> records generic time-series metrics about containers in a central database, and provides a UI for browsing that data.</p><h4>Cluster-level Logging</h4><p>A <a href="https://kubernetes.io/docs/concepts/cluster-administration/logging/">Cluster-level logging</a> mechanism is responsible for saving container logs to a central log store with search/browsing interface.</p><h2>The Kubernetes API</h2><p>Overall API conventions are described in the <a href="https://git.k8s.io/community/contributors/devel/api-conventions.md">API conventions doc</a>.</p><p>API endpoints, resource types and samples are described in <a href="https://kubernetes.io/docs/reference">API Reference</a>.</p><p>Remote access to the API is discussed in the <a href="https://kubernetes.io/docs/admin/accessing-the-api">access doc</a>.</p><p>The Kubernetes API also serves as the foundation for the declarative configuration schema for the system. The <a href="https://kubernetes.io/docs/user-guide/kubectl/">kubectl</a> command-line tool can be used to create, update, delete, and get API objects.</p><p>Kubernetes also stores its serialized state (currently in <a href="https://coreos.com/docs/distributed-configuration/getting-started-with-etcd/">etcd</a>) in terms of the API resources.</p><p>Kubernetes itself is decomposed into multiple components, which interact through its API.</p><h3>API changes</h3><p>In our experience, any system that is successful needs to grow and change as new use cases emerge or existing ones change. Therefore, we expect the Kubernetes API to continuously change and grow. However, we intend to not break compatibility with existing clients, for an extended period of time. In general, new API resources and new resource fields can be expected to be added frequently. Elimination of resources or fields will require following the <a href="https://kubernetes.io/docs/reference/deprecation-policy/">API deprecation policy</a>.</p><p>What constitutes a compatible change and how to change the API are detailed by the <a href="https://git.k8s.io/community/contributors/devel/api_changes.md">API change document</a>.</p><h3>OpenAPI and Swagger definitions</h3><p>Complete API details are documented using <a href="http://swagger.io/">Swagger v1.2</a> and <a href="https://www.openapis.org/">OpenAPI</a>. The Kubernetes apiserver (aka &quot;master&quot;) exposes an API that can be used to retrieve the Swagger v1.2 Kubernetes API spec located at <strong>/swaggerapi</strong>.</p><p>Starting with Kubernetes 1.4, OpenAPI spec is also available at <a href="https://git.k8s.io/kubernetes/api/openapi-spec/swagger.json"><strong>/swagger.json</strong></a>. While we are transitioning from Swagger v1.2 to OpenAPI (aka Swagger v2.0), some of the tools such as kubectl and swagger-ui are still using v1.2 spec. OpenAPI spec is in Beta as of Kubernetes 1.5.</p><p>Kubernetes implements an alternative Protobuf based serialization format for the API that is primarily intended for intra-cluster communication, documented in the <a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/protobuf.md">design proposal</a> and the IDL files for each schema are located in the Go packages that define the API objects.</p><h3>API versioning</h3><p>To make it easier to eliminate fields or restructure resource representations, Kubernetes supports multiple API versions, each at a different API path, such as <strong>/api/v1</strong> or <strong>/apis/extensions/v1beta1</strong>.</p><p>We chose to version at the API level rather than at the resource or field level to ensure that the API presents a clear, consistent view of system resources and behavior, and to enable controlling access to end-of-lifed and/or experimental APIs. The JSON and Protobuf serialization schemas follow the same guidelines for schema changes - all descriptions below cover both formats.</p><p>Note that API versioning and Software versioning are only indirectly related. The <a href="https://git.k8s.io/community/contributors/design-proposals/release/versioning.md">API and release versioning proposal</a> describes the relationship between API versioning and software versioning.</p><p>Different API versions imply different levels of stability and support. The criteria for each level are described in more detail in the <a href="https://git.k8s.io/community/contributors/devel/api_changes.md#alpha-beta-and-stable-versions">API Changes documentation</a>. They are summarized here:</p><ul><li>Alpha level:<ul><li>The version names contain <strong>alpha</strong> (e.g. <strong>v1alpha1</strong>).</li><li>May be buggy. Enabling the feature may expose bugs. Disabled by default.</li><li>Support for feature may be dropped at any time without notice.</li><li>The API may change in incompatible ways in a later software release without notice.</li><li>Recommended for use only in short-lived testing clusters, due to increased risk of bugs and lack of long-term support.</li></ul></li><li>Beta level:<ul><li>The version names contain <strong>beta</strong> (e.g. <strong>v2beta3</strong>).</li><li>Code is well tested. Enabling the feature is considered safe. Enabled by default.</li><li>Support for the overall feature will not be dropped, though details may change.</li><li>The schema and/or semantics of objects may change in incompatible ways in a subsequent beta or stable release. When this happens, we will provide instructions for migrating to the next version. This may require deleting, editing, and re-creating API objects. The editing process may require some thought. This may require downtime for applications that rely on the feature.</li><li>Recommended for only non-business-critical uses because of potential for incompatible changes in subsequent releases. If you have multiple clusters which can be upgraded independently, you may be able to relax this restriction.</li><li><strong>Please do try our beta features and give feedback on them! Once they exit beta, it may not be practical for us to make more changes.</strong></li></ul></li><li>Stable level:<ul><li>The version name is <strong>vX</strong> where <strong>X</strong> is an integer.</li><li>Stable versions of features will appear in released software for many subsequent versions.</li></ul></li></ul><h3>API groups</h3><p>To make it easier to extend the Kubernetes API, we implemented <a href="https://git.k8s.io/community/contributors/design-proposals/api-machinery/api-group.md">API groups</a>. The API group is specified in a REST path and in the <strong>apiVersion</strong> field of a serialized object.</p><p>Currently there are several API groups in use:</p><ol><li>The core group, often referred to as the legacy group, is at the REST path <strong>/api/v1</strong> and uses <strong>apiVersion: v1</strong>.</li><li>The named groups are at REST path <strong>/apis/$GROUP_NAME/$VERSION</strong>, and use <strong>apiVersion: $GROUP_NAME/$VERSION</strong> (e.g. <strong>apiVersion: batch/v1</strong>). Full list of supported API groups can be seen in <a href="https://kubernetes.io/docs/reference/">Kubernetes API reference</a>.</li></ol><p>There are two supported paths to extending the API with <a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/">custom resources</a>:</p><ol><li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/">CustomResourceDefinition</a> is for users with very basic CRUD needs.</li><li>Coming soon: users needing the full set of Kubernetes API semantics can implement their own apiserver and use the <a href="https://git.k8s.io/community/contributors/design-proposals/api-machinery/aggregated-api-servers.md">aggregator</a> to make it seamless for clients.</li></ol><h3>Enabling API groups</h3><p>Certain resources and API groups are enabled by default. They can be enabled or disabled by setting <strong>--runtime-config</strong> on apiserver. <strong>--runtime-config</strong> accepts comma separated values. For ex: to disable batch/v1, set <strong>--runtime-config=batch/v1=false</strong>, to enable batch/v2alpha1, set <strong>--runtime-config=batch/v2alpha1</strong>. The flag accepts comma separated set of key=value pairs describing runtime configuration of the apiserver.</p><p>IMPORTANT: Enabling or disabling groups or resources requires restarting apiserver and controller-manager to pick up the <strong>--runtime-config</strong> changes.</p><h3>Enabling resources in the groups</h3><p>DaemonSets, Deployments, HorizontalPodAutoscalers, Ingress, Jobs and ReplicaSets are enabled by default. Other extensions resources can be enabled by setting <strong>--runtime-config</strong> on apiserver. <strong>--runtime-config</strong> accepts comma separated values. For example: to disable deployments and ingress, set <strong>--runtime-config=extensions/v1beta1/deployments=false,extensions/v1beta1/ingress=false</strong></p><h2>Working with Kubernetes Objects</h2><h3>Understanding Kubernetes Objects</h3><p>This page explains how Kubernetes objects are represented in the Kubernetes API, and how you can express them in <strong>.yaml</strong> format.</p><ul><li><a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/#understanding-kubernetes-objects"><strong>Understanding Kubernetes Objects</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/#object-spec-and-status"><strong>Object Spec and Status</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/#describing-a-kubernetes-object"><strong>Describing a Kubernetes Object</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/#required-fields"><strong>Required Fields</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Understanding Kubernetes Objects</h4><p>Kubernetes Objects are persistent entities in the Kubernetes system. Kubernetes uses these entities to represent the state of your cluster. Specifically, they can describe:</p><ul><li>What containerized applications are running (and on which nodes)</li><li>The resources available to those applications</li><li>The policies around how those applications behave, such as restart policies, upgrades, and fault-tolerance</li></ul><p>A Kubernetes object is a &quot;record of intent&quot;--once you create the object, the Kubernetes system will constantly work to ensure that object exists. By creating an object, you&#x27;re effectively telling the Kubernetes system what you want your cluster&#x27;s workload to look like; this is your cluster&#x27;s <strong>desired state</strong>.</p><p>To work with Kubernetes objects--whether to create, modify, or delete them--you&#x27;ll need to use the <a href="https://kubernetes.io/docs/concepts/overview/kubernetes-api/">Kubernetes API</a>. When you use the <strong>kubectl</strong> command-line interface, for example, the CLI makes the necessary Kubernetes API calls for you. You can also use the Kubernetes API directly in your own programs using one of the <a href="https://kubernetes.io/docs/reference/client-libraries/">Client Libraries</a>.</p><h5><strong>Object Spec and Status</strong></h5><p>Every Kubernetes object includes two nested object fields that govern the object&#x27;s configuration: the object spec and the object status. The spec, which you must provide, describes your desired state for the object--the characteristics that you want the object to have. The status describes the actual stateof the object, and is supplied and updated by the Kubernetes system. At any given time, the Kubernetes Control Plane actively manages an object&#x27;s actual state to match the desired state you supplied.</p><p>For example, a Kubernetes Deployment is an object that can represent an application running on your cluster. When you create the Deployment, you might set the Deployment spec to specify that you want three replicas of the application to be running. The Kubernetes system reads the Deployment spec and starts three instances of your desired application--updating the status to match your spec. If any of those instances should fail (a status change), the Kubernetes system responds to the difference between spec and status by making a correction--in this case, starting a replacement instance.</p><p>For more information on the object spec, status, and metadata, see the <a href="https://git.k8s.io/community/contributors/devel/api-conventions.md">Kubernetes API Conventions</a>.</p><h5><strong>Describing a Kubernetes Object</strong></h5><p>When you create an object in Kubernetes, you must provide the object spec that describes its desired state, as well as some basic information about the object (such as a name). When you use the Kubernetes API to create the object (either directly or via <strong>kubectl</strong>), that API request must include that information as JSON in the request body. <strong>Most often, you provide the information to kubectl in a .yaml file.</strong> <strong>kubectl</strong> converts the information to JSON when making the API request.</p><p>Here&#x27;s an example <strong>.yaml</strong> file that shows the required fields and object spec for a Kubernetes Deployment:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>nginx-deployme                                                     |
| nt.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/website/mast">https://raw.githubusercontent.com/kubernetes/website/mast</a> |
| er/docs/concepts/overview/working-with-objects/nginx-deployment.yaml) |
+=======================================================================+
| <strong>apiVersion: apps/v1 <em># for versions before 1.9.0 use               |
| apps/v1beta2</em></strong>                                                       |
|                                                                       |
| <strong>kind: Deployment</strong>                                                  |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: nginx-deployment</strong>                                            |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>replicas: 3</strong>                                                       |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>matchLabels:</strong>                                                      |
|                                                                       |
| <strong>app: nginx</strong>                                                        |
|                                                                       |
| <strong>template:</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: nginx</strong>                                                        |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: nginx</strong>                                                     |
|                                                                       |
| <strong>image: nginx:1.7.9</strong>                                                |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- containerPort: 80</strong>                                               |
+-----------------------------------------------------------------------+</p><p>One way to create a Deployment using a <strong>.yaml</strong> file like the one above is to use the <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#create"><strong>kubectl create</strong></a> command in the <strong>kubectl</strong> command-line interface, passing the <strong>.yaml</strong> file as an argument. Here&#x27;s an example:</p><p><strong>$ kubectl create -f <a href="https://k8s.io/docs/user-guide/nginx-deployment.yaml">https://k8s.io/docs/user-guide/nginx-deployment.yaml</a> --record</strong></p><p>The output is similar to this:</p><p><strong>deployment &quot;nginx-deployment&quot; created</strong></p><h5><strong>Required Fields</strong></h5><p>In the <strong>.yaml</strong> file for the Kubernetes object you want to create, you&#x27;ll need to set values for the following fields:</p><ul><li><strong>apiVersion</strong> - Which version of the Kubernetes API you&#x27;re using to create this object</li><li><strong>kind</strong> - What kind of object you want to create</li><li><strong>metadata</strong> - Data that helps uniquely identify the object, including a <strong>name</strong> string, UID, and optional <strong>namespace</strong></li></ul><p>You&#x27;ll also need to provide the object <strong>spec</strong> field. The precise format of the object <strong>spec</strong> is different for every Kubernetes object, and contains nested fields specific to that object. The <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/">Kubernetes API Reference</a> can help you find the spec format for all of the objects you can create using Kubernetes. For example, the <strong>spec</strong> format for a <strong>Pod</strong> object can be found <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#podspec-v1-core">here</a>, and the <strong>spec</strong> format for a <strong>Deployment</strong> object can be found <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#deploymentspec-v1-apps">here</a>.</p><h4>What&#x27;s next</h4><ul><li>Learn about the most important basic Kubernetes objects, such as <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/">Pod</a>.</li></ul><h2>Names</h2><p>All objects in the Kubernetes REST API are unambiguously identified by a Name and a UID.</p><p>For non-unique user-provided attributes, Kubernetes provides <a href="https://kubernetes.io/docs/user-guide/labels">labels</a> and <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/">annotations</a>.</p><p>See the <a href="https://git.k8s.io/community/contributors/design-proposals/architecture/identifiers.md">identifiers design doc</a> for the precise syntax rules for Names and UIDs.</p><h3>Names</h3><p>A client-provided string that refers to an object in a resource URL, such as <strong>/api/v1/pods/some-name</strong>.</p><p>Only one object of a given kind can have a given name at a time. However, if you delete the object, you can make a new object with the same name.</p><p>By convention, the names of Kubernetes resources should be up to maximum length of 253 characters and consist of lower case alphanumeric characters, <strong>-</strong>, and <strong>.</strong>, but certain resources have more specific restrictions.</p><h3>UIDs</h3><p>A Kubernetes systems-generated string to uniquely identify objects.</p><p>Every object created over the whole lifetime of a Kubernetes cluster has a distinct UID. It is intended to distinguish between historical occurrences of similar entities.</p><h2>Namespaces</h2><p>Kubernetes supports multiple virtual clusters backed by the same physical cluster. These virtual clusters are called namespaces.</p><h3>When to Use Multiple Namespaces</h3><p>Namespaces are intended for use in environments with many users spread across multiple teams, or projects. For clusters with a few to tens of users, you should not need to create or think about namespaces at all. Start using namespaces when you need the features they provide.</p><p>Namespaces provide a scope for names. Names of resources need to be unique within a namespace, but not across namespaces.</p><p>Namespaces are a way to divide cluster resources between multiple users (via <a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/">resource quota</a>).</p><p>In future versions of Kubernetes, objects in the same namespace will have the same access control policies by default.</p><p>It is not necessary to use multiple namespaces just to separate slightly different resources, such as different versions of the same software: use <a href="https://kubernetes.io/docs/user-guide/labels">labels</a> to distinguish resources within the same namespace.</p><h3>Working with Namespaces</h3><p>Creation and deletion of namespaces are described in the <a href="https://kubernetes.io/docs/admin/namespaces">Admin Guide documentation for namespaces</a>.</p><h4>Viewing namespaces</h4><p>You can list the current namespaces in a cluster using:</p><p><strong>$ kubectl get namespaces</strong></p><p><strong>NAME STATUS AGE</strong></p><p><strong>default Active 1d</strong></p><p><strong>kube-system Active 1d</strong></p><p><strong>kube-public Active 1d</strong></p><p>Kubernetes starts with three initial namespaces:</p><ul><li><strong>default</strong> The default namespace for objects with no other namespace</li><li><strong>kube-system</strong> The namespace for objects created by the Kubernetes system</li><li><strong>kube-public</strong> The namespace is created automatically and readable by all users (including those not authenticated). This namespace is mostly reserved for cluster usage, in case that some resources should be visible and readable publicly throughout the whole cluster. The public aspect of this namespace is only a convention, not a requirement.</li></ul><h4>Setting the namespace for a request</h4><p>To temporarily set the namespace for a request, use the <strong>--namespace</strong> flag.</p><p>For example:</p><p><strong>$ kubectl --namespace=<code>&lt;insert-namespace-name-here&gt;</code> run nginx --image=nginx</strong></p><p><strong>$ kubectl --namespace=<code>&lt;insert-namespace-name-here&gt;</code> get pods</strong></p><h4>Setting the namespace preference</h4><p>You can permanently save the namespace for all subsequent kubectl commands in that context.</p><div class="MuiContainer-root MuiContainer-maxWidthLg"><pre class="Code__Pre-gy960v-0 UDybk prism-code language-bash" style="color:#9CDCFE;background-color:#1E1E1E"><div class="MuiGrid-root MuiGrid-container MuiGrid-justify-xs-flex-end"><button class="Code__CopyCode-gy960v-1 llUIua">Copy</button></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">**$ kubectl config set-context </span><span class="token variable" style="color:rgb(156, 220, 254)">$(</span><span class="token variable" style="color:rgb(156, 220, 254)">kubectl config current-context</span><span class="token variable" style="color:rgb(156, 220, 254)">)</span><span class="token plain"> --namespace</span><span class="token operator" style="color:rgb(212, 212, 212)">=</span><span class="token variable" style="color:rgb(156, 220, 254)">`</span><span class="token variable operator" style="color:rgb(212, 212, 212)">&lt;</span><span class="token variable" style="color:rgb(156, 220, 254)">insert-namespace-name-here</span><span class="token variable operator" style="color:rgb(212, 212, 212)">&gt;</span><span class="token variable" style="color:rgb(156, 220, 254)">`</span><span class="token plain">**</span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain"></span><span class="token comment" style="color:rgb(106, 153, 85)"># Validate it</span><span class="token plain"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">**$ kubectl config view </span><span class="token operator" style="color:rgb(212, 212, 212)">|</span><span class="token plain"> </span><span class="token function" style="color:rgb(220, 220, 170)">grep</span><span class="token plain"> namespace:**</span></div></pre></div><h3>Namespaces and DNS</h3><p>When you create a <a href="https://kubernetes.io/docs/user-guide/services">Service</a>, it creates a corresponding <a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/">DNS entry</a>. This entry is of the form <strong><code>&lt;service-name&gt;.&lt;namespace-name&gt;.svc.cluster.local, which means that if a container just uses &lt;service-name&gt;</code></strong>, it will resolve to the service which is local to a namespace. This is useful for using the same configuration across multiple namespaces such as Development, Staging and Production. If you want to reach across namespaces, you need to use the fully qualified domain name (FQDN).</p><h3>Not All Objects are in a Namespace</h3><p>Most Kubernetes resources (e.g. pods, services, replication controllers, and others) are in some namespaces. However namespace resources are not themselves in a namespace. And low-level resources, such as <a href="https://kubernetes.io/docs/admin/node">nodes</a> and persistentVolumes, are not in any namespace.</p><h2>Labels and Selectors</h2><p>Labels are key/value pairs that are attached to objects, such as pods. Labels are intended to be used to specify identifying attributes of objects that are meaningful and relevant to users, but do not directly imply semantics to the core system. Labels can be used to organize and to select subsets of objects. Labels can be attached to objects at creation time and subsequently added and modified at any time. Each object can have a set of key/value labels defined. Each Key must be unique for a given object.</p><p><strong>&quot;labels&quot;: {</strong></p><p><strong>&quot;key1&quot; : &quot;value1&quot;,</strong></p><p><strong>&quot;key2&quot; : &quot;value2&quot;</strong></p><p><strong>}</strong></p><p>We&#x27;ll eventually index and reverse-index labels for efficient queries and watches, use them to sort and group in UIs and CLIs, etc. We don&#x27;t want to pollute labels with non-identifying, especially large and/or structured, data. Non-identifying information should be recorded using <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/">annotations</a>.</p><ul><li><a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#motivation"><strong>Motivation</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set"><strong>Syntax and character set</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors"><strong>Label selectors</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#equality-based-requirement"><strong>Equality-based requirement</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#set-based-requirement"><strong>Set-based requirement</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#api"><strong>API</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#list-and-watch-filtering"><strong>LIST and WATCH filtering</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#set-references-in-api-objects"><strong>Set references in API objects</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#service-and-replicationcontroller"><strong>Service and ReplicationController</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#resources-that-support-set-based-requirements"><strong>Resources that support set-based requirements</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#selecting-sets-of-nodes"><strong>Selecting sets of nodes</strong></a></li></ul></li></ul></li></ul><h3>Motivation</h3><p>Labels enable users to map their own organizational structures onto system objects in a loosely coupled fashion, without requiring clients to store these mappings.</p><p>Service deployments and batch processing pipelines are often multi-dimensional entities (e.g., multiple partitions or deployments, multiple release tracks, multiple tiers, multiple micro-services per tier). Management often requires cross-cutting operations, which breaks encapsulation of strictly hierarchical representations, especially rigid hierarchies determined by the infrastructure rather than by users.</p><p>Example labels:</p><ul><li><strong>&quot;release&quot; : &quot;stable&quot;</strong>, <strong>&quot;release&quot; : &quot;canary&quot;</strong></li><li><strong>&quot;environment&quot; : &quot;dev&quot;</strong>, <strong>&quot;environment&quot; : &quot;qa&quot;</strong>, <strong>&quot;environment&quot; : &quot;production&quot;</strong></li><li><strong>&quot;tier&quot; : &quot;frontend&quot;</strong>, <strong>&quot;tier&quot; : &quot;backend&quot;</strong>, <strong>&quot;tier&quot; : &quot;cache&quot;</strong></li><li><strong>&quot;partition&quot; : &quot;customerA&quot;</strong>, <strong>&quot;partition&quot; : &quot;customerB&quot;</strong></li><li><strong>&quot;track&quot; : &quot;daily&quot;</strong>, <strong>&quot;track&quot; : &quot;weekly&quot;</strong></li></ul><p>These are just examples of commonly used labels; you are free to develop your own conventions. Keep in mind that label Key must be unique for a given object.</p><h3>Syntax and character set</h3><p>Labels are key/value pairs. Valid label keys have two segments: an optional prefix and name, separated by a slash (<strong>/</strong>). The name segment is required and must be 63 characters or less, beginning and ending with an alphanumeric character (<strong>[a-z0-9A-Z]</strong>) with dashes (<strong>-</strong>), underscores (<strong>_</strong>), dots (<strong>.</strong>), and alphanumerics between. The prefix is optional. If specified, the prefix must be a DNS subdomain: a series of DNS labels separated by dots (<strong>.</strong>), not longer than 253 characters in total, followed by a slash (<strong>/</strong>). If the prefix is omitted, the label Key is presumed to be private to the user. Automated system components (e.g. <strong>kube-scheduler</strong>, <strong>kube-controller-manager</strong>, <strong>kube-apiserver</strong>, <strong>kubectl</strong>, or other third-party automation) which add labels to end-user objects must specify a prefix. The <strong>kubernetes.io/</strong> prefix is reserved for Kubernetes core components.</p><p>Valid label values must be 63 characters or less and must be empty or begin and end with an alphanumeric character (<strong>[a-z0-9A-Z]</strong>) with dashes (<strong>-</strong>), underscores (<strong>_</strong>), dots (<strong>.</strong>), and alphanumerics between.</p><h3>Label selectors</h3><p>Unlike <a href="https://kubernetes.io/docs/user-guide/identifiers">names and UIDs</a>, labels do not provide uniqueness. In general, we expect many objects to carry the same label(s).</p><p>Via a label selector, the client/user can identify a set of objects. The label selector is the core grouping primitive in Kubernetes.</p><p>The API currently supports two types of selectors: equality-based and set-based. A label selector can be made of multiple requirements which are comma-separated. In the case of multiple requirements, all must be satisfied so the comma separator acts as a logical AND (<strong>&amp;&amp;</strong>) operator.</p><p>An empty label selector (that is, one with zero requirements) selects every object in the collection.</p><p>A null label selector (which is only possible for optional selector fields) selects no objects.</p><p><strong>Note</strong>: the label selectors of two controllers must not overlap within a namespace, otherwise they will fight with each other.</p><h4>Equality-based requirement</h4><p>Equality- or inequality-based requirements allow filtering by label keys and values. Matching objects must satisfy all of the specified label constraints, though they may have additional labels as well. Three kinds of operators are admitted <strong>=</strong>,<strong>==</strong>,<strong>!=</strong>. The first two represent equality (and are simply synonyms), while the latter represents inequality. For example:</p><p><strong>environment = production</strong></p><p><strong>tier != frontend</strong></p><p>The former selects all resources with key equal to <strong>environment</strong> and value equal to <strong>production</strong>. The latter selects all resources with key equal to <strong>tier</strong> and value distinct from <strong>frontend</strong>, and all resources with no labels with the <strong>tier</strong> key. One could filter for resources in <strong>production</strong> excluding <strong>frontend</strong> using the comma operator: <strong>environment=production,tier!=frontend</strong></p><p>One usage scenario for equality-based label requirement is for Pods to specify node selection criteria. For example, the sample Pod below selects nodes with the label &quot;<strong>accelerator=nvidia-tesla-p100</strong>&quot;.</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: cuda-test</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: cuda-test</strong></p><p><strong>image: &quot;k8s.gcr.io/cuda-vector-add:v0.1&quot;</strong></p><p><strong>resources:</strong></p><p><strong>limits:</strong></p><p><strong>nvidia.com/gpu: 1</strong></p><p><strong>nodeSelector:</strong></p><p><strong>accelerator: nvidia-tesla-p100</strong></p><h4>Set-based requirement</h4><p>Set-based label requirements allow filtering keys according to a set of values. Three kinds of operators are supported: <strong>in</strong>,<strong>notin</strong> and <strong>exists</strong> (only the key identifier). For example:</p><p><strong>environment in (production, qa)</strong></p><p><strong>tier notin (frontend, backend)</strong></p><p><strong>partition</strong></p><p><strong>!partition</strong></p><p>The first example selects all resources with key equal to <strong>environment</strong> and value equal to <strong>production</strong> or <strong>qa</strong>. The second example selects all resources with key equal to <strong>tier</strong> and values other than <strong>frontend</strong> and <strong>backend</strong>, and all resources with no labels with the <strong>tier</strong> key. The third example selects all resources including a label with key <strong>partition</strong>; no values are checked. The fourth example selects all resources without a label with key <strong>partition</strong>; no values are checked. Similarly the comma separator acts as an AND operator. So filtering resources with a <strong>partition</strong>key (no matter the value) and with <strong>environment</strong> different than  <strong>qa</strong> can be achieved using <strong>partition,environment notin (qa)</strong>. The set-based label selector is a general form of equality since <strong>environment=production</strong> is equivalent to <strong>environment in (production)</strong>; similarly for <strong>!=</strong>and <strong>notin</strong>.</p><p>Set-based requirements can be mixed with equality-based requirements. For example: <strong>partition in (customerA, customerB),environment!=qa</strong>.</p><h3>API</h3><h4>LIST and WATCH filtering</h4><p>LIST and WATCH operations may specify label selectors to filter the sets of objects returned using a query parameter. Both requirements are permitted (presented here as they would appear in a URL query string):</p><ul><li>equality-based requirements: <strong>?labelSelector=environment%3Dproduction,tier%3Dfrontend</strong></li><li>set-based requirements: <strong>?labelSelector=environment+in+%28production%2Cqa%29%2Ctier+in+%28frontend%29</strong></li></ul><p>Both label selector styles can be used to list or watch resources via a REST client. For example, targeting <strong>apiserver</strong> with <strong>kubectl</strong> and using equality-based one may write:</p><p><strong>$ kubectl get pods -l environment=production,tier=frontend</strong></p><p>or using set-based requirements:</p><p><strong>$ kubectl get pods -l \&#x27;environment in (production),tier in (frontend)\&#x27;</strong></p><p>As already mentioned set-based requirements are more expressive.  For instance, they can implement the OR operator on values:</p><p><strong>$ kubectl get pods -l \&#x27;environment in (production, qa)\&#x27;</strong></p><p>or restricting negative matching via exists operator:</p><p><strong>$ kubectl get pods -l \&#x27;environment,environment notin (frontend)\&#x27;</strong></p><h4>Set references in API objects</h4><p>Some Kubernetes objects, such as <a href="https://kubernetes.io/docs/user-guide/services"><strong>services</strong></a> and <a href="https://kubernetes.io/docs/user-guide/replication-controller"><strong>replicationcontrollers</strong></a>, also use label selectors to specify sets of other resources, such as <a href="https://kubernetes.io/docs/user-guide/pods">pods</a>.</p><h5><strong>Service and ReplicationController</strong></h5><p>The set of pods that a <strong>service</strong> targets is defined with a label selector. Similarly, the population of pods that a <strong>replicationcontroller</strong> should manage is also defined with a label selector.</p><p>Labels selectors for both objects are defined in <strong>json</strong> or <strong>yaml</strong> files using maps, and only equality-based requirement selectors are supported:</p><p><strong>&quot;selector&quot;: {</strong></p><p><strong>&quot;component&quot; : &quot;redis&quot;,</strong></p><p><strong>}</strong></p><p>or</p><p><strong>selector:</strong></p><p><strong>component: redis</strong></p><p>this selector (respectively in <strong>json</strong> or <strong>yaml</strong> format) is equivalent to <strong>component=redis</strong> or <strong>component in (redis)</strong>.</p><h5><strong>Resources that support set-based requirements</strong></h5><p>Newer resources, such as <a href="https://kubernetes.io/docs/concepts/jobs/run-to-completion-finite-workloads/"><strong>Job</strong></a>, <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/"><strong>Deployment</strong></a>, <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/"><strong>Replica Set</strong></a>, and <a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/"><strong>Daemon Set</strong></a>, support set-basedrequirements as well.</p><p><strong>selector:</strong></p><p><strong>matchLabels:</strong></p><p><strong>component: redis</strong></p><p><strong>matchExpressions:</strong></p><p><strong>- {key: tier, operator: In, values: <!-- -->[cache]<!-- -->}</strong></p><p><strong>- {key: environment, operator: NotIn, values: <!-- -->[dev]<!-- -->}</strong></p><p><strong>matchLabels</strong> is a map of <strong>{key,value}</strong> pairs. A single <strong>{key,value}</strong> in the <strong>matchLabels</strong> map is equivalent to an element of <strong>matchExpressions</strong>, whose <strong>key</strong> field is &quot;key&quot;, the <strong>operator</strong> is &quot;In&quot;, and the <strong>values</strong> array contains only &quot;value&quot;. <strong>matchExpressions</strong> is a list of pod selector requirements. Valid operators include In, NotIn, Exists, and DoesNotExist. The values set must be non-empty in the case of In and NotIn. All of the requirements, from both <strong>matchLabels</strong> and <strong>matchExpressions</strong> are ANDed together -- they must all be satisfied in order to match.</p><h5><strong>Selecting sets of nodes</strong></h5><p>One use case for selecting over labels is to constrain the set of nodes onto which a pod can schedule. See the documentation on <a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/">node selection</a> for more information.</p><h2>Annotations</h2><p>You can use Kubernetes annotations to attach arbitrary non-identifying metadata to objects. Clients such as tools and libraries can retrieve this metadata.</p><ul><li><a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/#attaching-metadata-to-objects"><strong>Attaching metadata to objects</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h3>Attaching metadata to objects</h3><p>You can use either labels or annotations to attach metadata to Kubernetes objects. Labels can be used to select objects and to find collections of objects that satisfy certain conditions. In contrast, annotations are not used to identify and select objects. The metadata in an annotation can be small or large, structured or unstructured, and can include characters not permitted by labels.</p><p>Annotations, like labels, are key/value maps:</p><p><strong>&quot;annotations&quot;: {</strong></p><p><strong>&quot;key1&quot; : &quot;value1&quot;,</strong></p><p><strong>&quot;key2&quot; : &quot;value2&quot;</strong></p><p><strong>}</strong></p><p>Here are some examples of information that could be recorded in annotations:</p><ul><li>Fields managed by a declarative configuration layer. Attaching these fields as annotations distinguishes them from default values set by clients or servers, and from auto-generated fields and fields set by auto-sizing or auto-scaling systems.</li><li>Build, release, or image information like timestamps, release IDs, git branch, PR numbers, image hashes, and registry address.</li><li>Pointers to logging, monitoring, analytics, or audit repositories.</li><li>Client library or tool information that can be used for debugging purposes: for example, name, version, and build information.</li><li>User or tool/system provenance information, such as URLs of related objects from other ecosystem components.</li><li>Lightweight rollout tool metadata: for example, config or checkpoints.</li><li>Phone or pager numbers of persons responsible, or directory entries that specify where that information can be found, such as a team web site.</li></ul><p>Instead of using annotations, you could store this type of information in an external database or directory, but that would make it much harder to produce shared client libraries and tools for deployment, management, introspection, and the like.</p><h3>What&#x27;s next</h3><h3>Object Management Using kubectl</h3><h4>Kubernetes Object Management</h4><p>The <strong>kubectl</strong> command-line tool supports several different ways to create and manage Kubernetes objects. This document provides an overview of the different approaches.</p><ul><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/overview/#management-techniques"><strong>Management techniques</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/overview/#imperative-commands"><strong>Imperative commands</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/overview/#examples"><strong>Examples</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/overview/#trade-offs"><strong>Trade-offs</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/overview/#imperative-object-configuration"><strong>Imperative object configuration</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/overview/#examples-1"><strong>Examples</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/overview/#trade-offs-1"><strong>Trade-offs</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/overview/#declarative-object-configuration"><strong>Declarative object configuration</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/overview/#examples-2"><strong>Examples</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/overview/#trade-offs-2"><strong>Trade-offs</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/overview/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h5><strong>Management techniques</strong></h5><p><strong>Warning:</strong> A Kubernetes object should be managed using only one technique. Mixing and matching techniques for the same object results in undefined behavior.</p><p>  Management technique               Operates on            Recommended environment   Supported writers   Learning curve</p><hr/><p>  Imperative commands                Live objects           Development projects      1+                  Lowest
Imperative object configuration    Individual files       Production projects       1                   Moderate
Declarative object configuration   Directories of files   Production projects       1+                  Highest</p><h5><strong>Imperative commands</strong></h5><p>When using imperative commands, a user operates directly on live objects in a cluster. The user provides operations to the <strong>kubectl</strong> command as arguments or flags.</p><p>This is the simplest way to get started or to run a one-off task in a cluster. Because this technique operates directly on live objects, it provides no history of previous configurations.</p><h6><strong>Examples</strong></h6><p>Run an instance of the nginx container by creating a Deployment object:</p><p><strong>kubectl run nginx --image nginx</strong></p><p>Do the same thing using a different syntax:</p><p><strong>kubectl create deployment nginx --image nginx</strong></p><h6><strong>Trade-offs</strong></h6><p>Advantages compared to object configuration:</p><ul><li>Commands are simple, easy to learn and easy to remember.</li><li>Commands require only a single step to make changes to the cluster.</li></ul><p>Disadvantages compared to object configuration:</p><ul><li>Commands do not integrate with change review processes.</li><li>Commands do not provide an audit trail associated with changes.</li><li>Commands do not provide a source of records except for what is live.</li><li>Commands do not provide a template for creating new objects.</li></ul><h5><strong>Imperative object configuration</strong></h5><p>In imperative object configuration, the kubectl command specifies the operation (create, replace, etc.), optional flags and at least one file name. The file specified must contain a full definition of the object in YAML or JSON format.</p><p>See the <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/">API reference</a> for more details on object definitions.</p><p><strong>Warning:</strong> The imperative <strong>replace</strong> command replaces the existing spec with the newly provided one, dropping all changes to the object missing from the configuration file. This approach should not be used with resource types whose specs are updated independently of the configuration file. Services of type <strong>LoadBalancer</strong>, for example, have their <strong>externalIPs</strong> field updated independently from the configuration by the cluster.</p><h6><strong>Examples</strong></h6><p>Create the objects defined in a configuration file:</p><p><strong>kubectl create -f nginx.yaml</strong></p><p>Delete the objects defined in two configuration files:</p><p><strong>kubectl delete -f nginx.yaml -f redis.yaml</strong></p><p>Update the objects defined in a configuration file by overwriting the live configuration:</p><p><strong>kubectl replace -f nginx.yaml</strong></p><h6><strong>Trade-offs</strong></h6><p>Advantages compared to imperative commands:</p><ul><li>Object configuration can be stored in a source control system such as Git.</li><li>Object configuration can integrate with processes such as reviewing changes before push and audit trails.</li><li>Object configuration provides a template for creating new objects.</li></ul><p>Disadvantages compared to imperative commands:</p><ul><li>Object configuration requires basic understanding of the object schema.</li><li>Object configuration requires the additional step of writing a YAML file.</li></ul><p>Advantages compared to declarative object configuration:</p><ul><li>Imperative object configuration behavior is simpler and easier to understand.</li><li>As of Kubernetes version 1.5, imperative object configuration is more mature.</li></ul><p>Disadvantages compared to declarative object configuration:</p><ul><li>Imperative object configuration works best on files, not directories.</li><li>Updates to live objects must be reflected in configuration files, or they will be lost during the next replacement.</li></ul><h5><strong>Declarative object configuration</strong></h5><p>When using declarative object configuration, a user operates on object configuration files stored locally, however the user does not define the operations to be taken on the files. Create, update, and delete operations are automatically detected per-object by <strong>kubectl</strong>. This enables working on directories, where different operations might be needed for different objects.</p><p><strong>Note:</strong> Declarative object configuration retains changes made by other writers, even if the changes are not merged back to the object configuration file. This is possible by using the <strong>patch</strong> API operation to write only observed differences, instead of using the <strong>replace</strong> API operation to replace the entire object configuration.</p><h6><strong>Examples</strong></h6><p>Process all object configuration files in the <strong>configs</strong> directory, and create or patch the live objects:</p><p><strong>kubectl apply -f configs/</strong></p><p>Recursively process directories:</p><p><strong>kubectl apply -R -f configs/</strong></p><h6><strong>Trade-offs</strong></h6><p>Advantages compared to imperative object configuration:</p><ul><li>Changes made directly to live objects are retained, even if they are not merged back into the configuration files.</li><li>Declarative object configuration has better support for operating on directories and automatically detecting operation types (create, patch, delete) per-object.</li></ul><p>Disadvantages compared to imperative object configuration:</p><ul><li>Declarative object configuration is harder to debug and understand results when they are unexpected.</li><li>Partial updates using diffs create complex merge and patch operations.</li></ul><h5><strong>What&#x27;s next</strong></h5><ul><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/imperative-command/">Managing Kubernetes Objects Using Imperative Commands</a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/imperative-config/">Managing Kubernetes Objects Using Object Configuration (Imperative)</a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/">Managing Kubernetes Objects Using Object Configuration (Declarative)</a></li><li><a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/">Kubectl Command Reference</a></li><li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/">Kubernetes API Reference</a></li></ul><h4>Managing Kubernetes Objects Using Imperative Commands</h4><p>Kubernetes objects can quickly be created, updated, and deleted directly using imperative commands built into the <strong>kubectl</strong> command-line tool. This document explains how those commands are organized and how to use them to manage live objects.</p><ul><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/imperative-command/#trade-offs"><strong>Trade-offs</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/imperative-command/#how-to-create-objects"><strong>How to create objects</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/imperative-command/#how-to-update-objects"><strong>How to update objects</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/imperative-command/#how-to-delete-objects"><strong>How to delete objects</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/imperative-command/#how-to-view-an-object"><strong>How to view an object</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/imperative-command/#using-set-commands-to-modify-objects-before-creation"><strong>Using set commands to modify objects before creation</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/imperative-command/#using---edit-to-modify-objects-before-creation"><strong>Using --edit to modify objects before creation</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/imperative-command/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h5><strong>Trade-offs</strong></h5><p>The <strong>kubectl</strong> tool supports three kinds of object management:</p><ul><li>Imperative commands</li><li>Imperative object configuration</li><li>Declarative object configuration</li></ul><p>See <a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/overview/">Kubernetes Object Management</a> for a discussion of the advantages and disadvantage of each kind of object management.</p><h5><strong>How to create objects</strong></h5><p>The <strong>kubectl</strong> tool supports verb-driven commands for creating some of the most common object types. The commands are named to be recognizable to users unfamiliar with the Kubernetes object types.</p><ul><li><strong>run</strong>: Create a new Deployment object to run Containers in one or more Pods.</li><li><strong>expose</strong>: Create a new Service object to load balance traffic across Pods.</li><li><strong>autoscale</strong>: Create a new Autoscaler object to automatically horizontally scale a controller, such as a Deployment.</li></ul><p>The <strong>kubectl</strong> tool also supports creation commands driven by object type. These commands support more object types and are more explicit about their intent, but require users to know the type of objects they intend to create.</p><ul><li><strong>create <code>&lt;objecttype&gt; [&lt;subtype&gt;] &lt;instancename&gt;</code></strong></li></ul><p>Some objects types have subtypes that you can specify in the <strong>create</strong> command. For example, the Service object has several subtypes including ClusterIP, LoadBalancer, and NodePort. Here&#x27;s an example that creates a Service with subtype NodePort:</p><p><strong>kubectl create service nodeport <code>&lt;myservicename&gt;</code></strong></p><p>In the preceding example, the <strong>create service nodeport</strong> command is called a subcommand of the <strong>create service</strong> command.</p><p>You can use the <strong>-h</strong> flag to find the arguments and flags supported by a subcommand:</p><p><strong>kubectl create service nodeport -h</strong></p><h5><strong>How to update objects</strong></h5><p>The <strong>kubectl</strong> command supports verb-driven commands for some common update operations. These commands are named to enable users unfamiliar with Kubernetes objects to perform updates without knowing the specific fields that must be set:</p><ul><li><strong>scale</strong>: Horizontally scale a controller to add or remove Pods by updating the replica count of the controller.</li><li><strong>annotate</strong>: Add or remove an annotation from an object.</li><li><strong>label</strong>: Add or remove a label from an object.</li></ul><p>The <strong>kubectl</strong> command also supports update commands driven by an aspect of the object. Setting this aspect may set different fields for different object types:</p><ul><li><strong>set</strong> : Set an aspect of an object.</li></ul><p><strong>Note</strong>: In Kubernetes version 1.5, not every verb-driven command has an associated aspect-driven command.</p><p>The <strong>kubectl</strong> tool supports these additional ways to update a live object directly, however they require a better understanding of the Kubernetes object schema.</p><ul><li><strong>edit</strong>: Directly edit the raw configuration of a live object by opening its configuration in an editor.</li><li><strong>patch</strong>: Directly modify specific fields of a live object by using a patch string. For more details on patch strings, see the patch section in <a href="https://git.k8s.io/community/contributors/devel/api-conventions.md#patch-operations">API Conventions</a>.</li></ul><h5><strong>How to delete objects</strong></h5><p>You can use the <strong>delete</strong> command to delete an object from a cluster:</p><ul><li><strong>delete <code>&lt;type&gt;/&lt;name&gt;</code></strong></li></ul><p><strong>Note</strong>: You can use <strong>kubectl delete</strong> for both imperative commands and imperative object configuration. The difference is in the arguments passed to the command. To use <strong>kubectl delete</strong>as an imperative command, pass the object to be deleted as an argument. Here&#x27;s an example that passes a Deployment object named nginx:</p><p><strong>kubectl delete deployment/nginx</strong></p><h5><strong>How to view an object</strong></h5><p>There are several commands for printing information about an object:</p><ul><li><strong>get</strong>: Prints basic information about matching objects. Use <strong>get -h</strong> to see a list of options.</li><li><strong>describe</strong>: Prints aggregated detailed information about matching objects.</li><li><strong>logs</strong>: Prints the stdout and stderr for a container running in a Pod.</li></ul><h5><strong>Using </strong>set<strong> commands to modify objects before creation</strong></h5><p>There are some object fields that don&#x27;t have a flag you can use in a <strong>create</strong> command. In some of those cases, you can use a combination of <strong>set</strong> and <strong>create</strong> to specify a value for the field before object creation. This is done by piping the output of the <strong>create</strong> command to the <strong>set</strong> command, and then back to the <strong>create</strong> command. Here&#x27;s an example:</p><p><strong>kubectl create service clusterip my-svc --clusterip=&quot;None&quot; -o yaml --dry-run | kubectl set selector --local -f - \&#x27;environment=qa\&#x27; -o yaml | kubectl create -f -</strong></p><ol><li>The <strong>kubectl create service -o yaml --dry-run</strong> command creates the configuration for the Service, but prints it to stdout as YAML instead of sending it to the Kubernetes API server.</li><li>The <strong>kubectl set --local -f - -o yaml</strong> command reads the configuration from stdin, and writes the updated configuration to stdout as YAML.</li><li>The <strong>kubectl create -f -</strong> command creates the object using the configuration provided via stdin.</li></ol><h5><strong>Using </strong>--edit<strong> to modify objects before creation</strong></h5><p>You can use <strong>kubectl create --edit</strong> to make arbitrary changes to an object before it is created. Here&#x27;s an example:</p><p><strong>kubectl create service clusterip my-svc --clusterip=&quot;None&quot; -o yaml --dry-run &gt; /tmp/srv.yaml</strong></p><p><strong>kubectl create --edit -f /tmp/srv.yaml</strong></p><ol><li>The <strong>kubectl create service</strong> command creates the configuration for the Service and saves it to <strong>/tmp/srv.yaml</strong>.</li><li>The <strong>kubectl create --edit</strong> command opens the configuration file for editing before it creates the object.</li></ol><h5><strong>What&#x27;s next</strong></h5><ul><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/imperative-config/">Managing Kubernetes Objects Using Object Configuration (Imperative)</a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/">Managing Kubernetes Objects Using Object Configuration (Declarative)</a></li><li><a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/">Kubectl Command Reference</a></li><li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/">Kubernetes API Reference</a></li></ul><h4>Imperative Management of Kubernetes Objects Using Configuration Files</h4><p>Kubernetes objects can be created, updated, and deleted by using the <strong>kubectl</strong> command-line tool along with an object configuration file written in YAML or JSON. This document explains how to define and manage objects using configuration files.</p><ul><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/imperative-config/#trade-offs"><strong>Trade-offs</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/imperative-config/#how-to-create-objects"><strong>How to create objects</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/imperative-config/#how-to-update-objects"><strong>How to update objects</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/imperative-config/#how-to-delete-objects"><strong>How to delete objects</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/imperative-config/#how-to-view-an-object"><strong>How to view an object</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/imperative-config/#limitations"><strong>Limitations</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/imperative-config/#creating-and-editing-an-object-from-a-url-without-saving-the-configuration"><strong>Creating and editing an object from a URL without saving the configuration</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/imperative-config/#migrating-from-imperative-commands-to-imperative-object-configuration"><strong>Migrating from imperative commands to imperative object configuration</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/imperative-config/#defining-controller-selectors-and-podtemplate-labels"><strong>Defining controller selectors and PodTemplate labels</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/imperative-config/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h5><strong>Trade-offs</strong></h5><p>The <strong>kubectl</strong> tool supports three kinds of object management:</p><ul><li>Imperative commands</li><li>Imperative object configuration</li><li>Declarative object configuration</li></ul><p>See <a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/overview/">Kubernetes Object Management</a> for a discussion of the advantages and disadvantage of each kind of object management.</p><h5><strong>How to create objects</strong></h5><p>You can use <strong>kubectl create -f</strong> to create an object from a configuration file. Refer to the <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/">kubernetes API reference</a> for details.</p><ul><li><strong>kubectl create -f <code>&lt;filename|url&gt;</code></strong></li></ul><h5><strong>How to update objects</strong></h5><p><strong>Warning:</strong> Updating objects with the <strong>replace</strong> command drops all parts of the spec not specified in the configuration file. This should not be used with objects whose specs are partially managed by the cluster, such as Services of type <strong>LoadBalancer</strong>, where the <strong>externalIPs</strong> field is managed independently from the configuration file. Independently managed fields must be copied to the configuration file to prevent <strong>replace</strong> from dropping them.</p><p>You can use <strong>kubectl replace -f</strong> to update a live object according to a configuration file.</p><ul><li><strong>kubectl replace -f <code>&lt;filename|url&gt;</code></strong></li></ul><h5><strong>How to delete objects</strong></h5><p>You can use <strong>kubectl delete -f</strong> to delete an object that is described in a configuration file.</p><ul><li><strong>kubectl delete -f <code>&lt;filename|url&gt;</code></strong></li></ul><h5><strong>How to view an object</strong></h5><p>You can use <strong>kubectl get -f</strong> to view information about an object that is described in a configuration file.</p><ul><li><strong>kubectl get -f <code>&lt;filename|url&gt;</code> -o yaml</strong></li></ul><p>The <strong>-o yaml</strong> flag specifies that the full object configuration is printed. Use <strong>kubectl get -h</strong> to see a list of options.</p><h5><strong>Limitations</strong></h5><p>The <strong>create</strong>, <strong>replace</strong>, and <strong>delete</strong> commands work well when each object&#x27;s configuration is fully defined and recorded in its configuration file. However when a live object is updated, and the updates are not merged into its configuration file, the updates will be lost the next time a <strong>replace</strong> is executed. This can happen if a controller, such as a HorizontalPodAutoscaler, makes updates directly to a live object. Here&#x27;s an example:</p><ol><li>You create an object from a configuration file.</li><li>Another source updates the object by changing some field.</li><li>You replace the object from the configuration file. Changes made by the other source in step 2 are lost.</li></ol><p>If you need to support multiple writers to the same object, you can use <strong>kubectl apply</strong> to manage the object.</p><h5><strong>Creating and editing an object from a URL without saving the configuration</strong></h5><p>Suppose you have the URL of an object configuration file. You can use <strong>kubectl create --edit</strong> to make changes to the configuration before the object is created. This is particularly useful for tutorials and tasks that point to a configuration file that could be modified by the reader.</p><p><strong>kubectl create -f <code>&lt;url&gt;</code> --edit</strong></p><h5><strong>Migrating from imperative commands to imperative object configuration</strong></h5><p>Migrating from imperative commands to imperative object configuration involves several manual steps.</p><ol><li>Export the live object to a local object configuration file:</li><li><strong>kubectl get <code>&lt;kind&gt;/&lt;name&gt; -o yaml --export &gt; &lt;kind&gt;\_&lt;name&gt;</code>.yaml</strong></li><li>Manually remove the status field from the object configuration file.</li><li>For subsequent object management, use <strong>replace</strong> exclusively.</li><li><strong>kubectl replace -f <code>&lt;kind&gt;\_&lt;name&gt;</code>.yaml</strong></li></ol><h5><strong>Defining controller selectors and PodTemplate labels</strong></h5><p><strong>Warning</strong>: Updating selectors on controllers is strongly discouraged.</p><p>The recommended approach is to define a single, immutable PodTemplate label used only by the controller selector with no other semantic meaning.</p><p>Example label:</p><p><strong>selector:</strong></p><p><strong>matchLabels:</strong></p><p><strong>controller-selector: &quot;extensions/v1beta1/deployment/nginx&quot;</strong></p><p><strong>template:</strong></p><p><strong>metadata:</strong></p><p><strong>labels:</strong></p><p><strong>controller-selector: &quot;extensions/v1beta1/deployment/nginx&quot;</strong></p><h5><strong>What&#x27;s next</strong></h5><ul><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/imperative-command/">Managing Kubernetes Objects Using Imperative Commands</a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/">Managing Kubernetes Objects Using Object Configuration (Declarative)</a></li><li><a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/">Kubectl Command Reference</a></li><li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/">Kubernetes API Reference</a></li></ul><h4>Declarative Management of Kubernetes Objects Using Configuration Files</h4><p>Kubernetes objects can be created, updated, and deleted by storing multiple object configuration files in a directory and using <strong>kubectl apply</strong> to recursively create and update those objects as needed. This method retains writes made to live objects without merging the changes back into the object configuration files.</p><ul><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/#trade-offs"><strong>Trade-offs</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/#how-to-create-objects"><strong>How to create objects</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/#how-to-update-objects"><strong>How to update objects</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/#how-to-delete-objects"><strong>How to delete objects</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/#recommended-kubectl-delete--f-filename"><strong>Recommended: kubectl delete -f <code>&lt;filename&gt;</code></strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/#alternative-kubectl-apply--f-directory---prune--l-yourlabel"><strong>Alternative: kubectl apply -f <code>&lt;directory/&gt;</code> --prune -l your=label</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/#how-to-view-an-object"><strong>How to view an object</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/#how-apply-calculates-differences-and-merges-changes"><strong>How apply calculates differences and merges changes</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/#merge-patch-calculation"><strong>Merge patch calculation</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/#how-different-types-of-fields-are-merged"><strong>How different types of fields are merged</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/#merging-changes-to-primitive-fields"><strong>Merging changes to primitive fields</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/#merging-changes-to-map-fields"><strong>Merging changes to map fields</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/#merging-changes-for-fields-of-type-list"><strong>Merging changes for fields of type list</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/#replace-the-list"><strong>Replace the list</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/#merge-individual-elements-of-a-list-of-complex-elements"><strong>Merge individual elements of a list of complex elements:</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/#merge-a-list-of-primitive-elements"><strong>Merge a list of primitive elements</strong></a></li></ul></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/#default-field-values"><strong>Default field values</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/#how-to-clear-server-defaulted-fields-or-fields-set-by-other-writers"><strong>How to clear server-defaulted fields or fields set by other writers</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/#how-to-change-ownership-of-a-field-between-the-configuration-file-and-direct-imperative-writers"><strong>How to change ownership of a field between the configuration file and direct imperative writers</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/#changing-the-owner-from-a-direct-imperative-writer-to-a-configuration-file"><strong>Changing the owner from a direct imperative writer to a configuration file</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/#changing-the-owner-from-a-configuration-file-to-a-direct-imperative-writer"><strong>Changing the owner from a configuration file to a direct imperative writer</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/#changing-management-methods"><strong>Changing management methods</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/#migrating-from-imperative-command-management-to-declarative-object-configuration"><strong>Migrating from imperative command management to declarative object configuration</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/#migrating-from-imperative-object-configuration-to-declarative-object-configuration"><strong>Migrating from imperative object configuration to declarative object configuration</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/#defining-controller-selectors-and-podtemplate-labels"><strong>Defining controller selectors and PodTemplate labels</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/#known-issues"><strong>Known Issues</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h5><strong>Trade-offs</strong></h5><p>The <strong>kubectl</strong> tool supports three kinds of object management:</p><ul><li>Imperative commands</li><li>Imperative object configuration</li><li>Declarative object configuration</li></ul><p>See <a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/overview/">Kubernetes Object Management</a> for a discussion of the advantages and disadvantage of each kind of object management.</p><h5><strong>Before you begin</strong></h5><p>Declarative object configuration requires a firm understanding of the Kubernetes object definitions and configuration. Read and complete the following documents if you have not already:</p><ul><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/imperative-command/">Managing Kubernetes Objects Using Imperative Commands</a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/imperative-config/">Imperative Management of Kubernetes Objects Using Configuration Files</a></li></ul><p>Following are definitions for terms used in this document:</p><ul><li>object configuration file / configuration file: A file that defines the configuration for a Kubernetes object. This topic shows how to pass configuration files to <strong>kubectl apply</strong>. Configuration files are typically stored in source control, such as Git.</li><li>live object configuration / live configuration: The live configuration values of an object, as observed by the Kubernetes cluster. These are kept in the Kubernetes cluster storage, typically etcd.</li><li>declarative configuration writer / declarative writer: A person or software component that makes updates to a live object. The live writers referred to in this topic make changes to object configuration files and run <strong>kubectl apply</strong> to write the changes.</li></ul><h5><strong>How to create objects</strong></h5><p>Use <strong>kubectl apply</strong> to create all objects, except those that already exist, defined by configuration files in a specified directory:</p><p><strong>kubectl apply -f <code>&lt;directory&gt;</code>/</strong></p><p>This sets the <strong>kubectl.kubernetes.io/last-applied-configuration: \&#x27;{<!-- -->.<!-- -->..}\&#x27;</strong> annotation on each object. The annotation contains the contents of the object configuration file that was used to create the object.</p><p><strong>Note</strong>: Add the <strong>-R</strong> flag to recursively process directories.</p><p>Here&#x27;s an example of an object configuration file:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>simple_deployment.yam                                              |
| l</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/website/master/doc">https://raw.githubusercontent.com/kubernetes/website/master/doc</a> |
| s/concepts/overview/object-management-kubectl/simple_deployment.yaml) |
+=======================================================================+
| <strong>apiVersion: apps/v1</strong>                                               |
|                                                                       |
| <strong>kind: Deployment</strong>                                                  |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: nginx-deployment</strong>                                            |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>matchLabels:</strong>                                                      |
|                                                                       |
| <strong>app: nginx</strong>                                                        |
|                                                                       |
| <strong>minReadySeconds: 5</strong>                                                |
|                                                                       |
| <strong>template:</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: nginx</strong>                                                        |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: nginx</strong>                                                     |
|                                                                       |
| <strong>image: nginx:1.7.9</strong>                                                |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- containerPort: 80</strong>                                               |
+-----------------------------------------------------------------------+</p><p>Create the object using <strong>kubectl apply</strong>:</p><p><strong>kubectl apply -f <a href="https://k8s.io/docs/concepts/overview/object-management-kubectl/simple_deployment.yaml">https://k8s.io/docs/concepts/overview/object-management-kubectl/simple_deployment.yaml</a></strong></p><p>Print the live configuration using <strong>kubectl get</strong>:</p><p><strong>kubectl get -f <a href="https://k8s.io/docs/concepts/overview/object-management-kubectl/simple_deployment.yaml">https://k8s.io/docs/concepts/overview/object-management-kubectl/simple_deployment.yaml</a> -o yaml</strong></p><p>The output shows that the <strong>kubectl.kubernetes.io/last-applied-configuration</strong> annotation was written to the live configuration, and it matches the configuration file:</p><p><strong>kind: Deployment</strong></p><p><strong>metadata:</strong></p><p><strong>annotations:</strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong><em># This is the json representation of simple_deployment.yaml</em></strong></p><p><strong><em># It was written by kubectl apply when the object was created</em></strong></p><p><strong>kubectl.kubernetes.io/last-applied-configuration: |</strong></p><p><strong>{&quot;apiVersion&quot;:&quot;apps/v1&quot;,&quot;kind&quot;:&quot;Deployment&quot;,</strong></p><p><strong>&quot;metadata&quot;:{&quot;annotations&quot;:{},&quot;name&quot;:&quot;nginx-deployment&quot;,&quot;namespace&quot;:&quot;default&quot;},</strong></p><p><strong>&quot;spec&quot;:{&quot;minReadySeconds&quot;:5,&quot;selector&quot;:{&quot;matchLabels&quot;:{&quot;app&quot;:nginx}},&quot;template&quot;:{&quot;metadata&quot;:{&quot;labels&quot;:{&quot;app&quot;:&quot;nginx&quot;}},</strong></p><p><strong>&quot;spec&quot;:{&quot;containers&quot;:[{&quot;image&quot;:&quot;nginx:1.7.9&quot;,&quot;name&quot;:&quot;nginx&quot;,</strong></p><p><strong>&quot;ports&quot;:<!-- -->[{&quot;containerPort&quot;:80}]<!-- -->}]}}}}</strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong>spec:</strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong>minReadySeconds: 5</strong></p><p><strong>selector:</strong></p><p><strong>matchLabels:</strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong>app: nginx</strong></p><p><strong>template:</strong></p><p><strong>metadata:</strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong>labels:</strong></p><p><strong>app: nginx</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- image: nginx:1.7.9</strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong>name: nginx</strong></p><p><strong>ports:</strong></p><p><strong>- containerPort: 80</strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><h5><strong>How to update objects</strong></h5><p>You can also use <strong>kubectl apply</strong> to update all objects defined in a directory, even if those objects already exist. This approach accomplishes the following:</p><ol><li>Sets fields that appear in the configuration file in the live configuration.</li><li>Clears fields removed from the configuration file in the live configuration.</li></ol><p><strong>kubectl apply -f <code>&lt;directory&gt;</code>/</strong></p><p><strong>Note</strong>: Add the <strong>-R</strong> flag to recursively process directories.</p><p>Here&#x27;s an example configuration file:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>simple_deployment.yam                                              |
| l</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/website/master/doc">https://raw.githubusercontent.com/kubernetes/website/master/doc</a> |
| s/concepts/overview/object-management-kubectl/simple_deployment.yaml) |
+=======================================================================+
| <strong>apiVersion: apps/v1</strong>                                               |
|                                                                       |
| <strong>kind: Deployment</strong>                                                  |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: nginx-deployment</strong>                                            |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>matchLabels:</strong>                                                      |
|                                                                       |
| <strong>app: nginx</strong>                                                        |
|                                                                       |
| <strong>minReadySeconds: 5</strong>                                                |
|                                                                       |
| <strong>template:</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: nginx</strong>                                                        |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: nginx</strong>                                                     |
|                                                                       |
| <strong>image: nginx:1.7.9</strong>                                                |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- containerPort: 80</strong>                                               |
+-----------------------------------------------------------------------+</p><p>Create the object using <strong>kubectl apply</strong>:</p><p><strong>kubectl apply -f <a href="https://k8s.io/docs/concepts/overview/object-management-kubectl/simple_deployment.yaml">https://k8s.io/docs/concepts/overview/object-management-kubectl/simple_deployment.yaml</a></strong></p><p><strong>Note:</strong> For purposes of illustration, the preceding command refers to a single configuration file instead of a directory.</p><p>Print the live configuration using <strong>kubectl get</strong>:</p><p><strong>kubectl get -f <a href="https://k8s.io/docs/concepts/overview/object-management-kubectl/simple_deployment.yaml">https://k8s.io/docs/concepts/overview/object-management-kubectl/simple_deployment.yaml</a> -o yaml</strong></p><p>The output shows that the <strong>kubectl.kubernetes.io/last-applied-configuration</strong> annotation was written to the live configuration, and it matches the configuration file:</p><p><strong>kind: Deployment</strong></p><p><strong>metadata:</strong></p><p><strong>annotations:</strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong><em># This is the json representation of simple_deployment.yaml</em></strong></p><p><strong><em># It was written by kubectl apply when the object was created</em></strong></p><p><strong>kubectl.kubernetes.io/last-applied-configuration: |</strong></p><p><strong>{&quot;apiVersion&quot;:&quot;apps/v1&quot;,&quot;kind&quot;:&quot;Deployment&quot;,</strong></p><p><strong>&quot;metadata&quot;:{&quot;annotations&quot;:{},&quot;name&quot;:&quot;nginx-deployment&quot;,&quot;namespace&quot;:&quot;default&quot;},</strong></p><p><strong>&quot;spec&quot;:{&quot;minReadySeconds&quot;:5,&quot;selector&quot;:{&quot;matchLabels&quot;:{&quot;app&quot;:nginx}},&quot;template&quot;:{&quot;metadata&quot;:{&quot;labels&quot;:{&quot;app&quot;:&quot;nginx&quot;}},</strong></p><p><strong>&quot;spec&quot;:{&quot;containers&quot;:[{&quot;image&quot;:&quot;nginx:1.7.9&quot;,&quot;name&quot;:&quot;nginx&quot;,</strong></p><p><strong>&quot;ports&quot;:<!-- -->[{&quot;containerPort&quot;:80}]<!-- -->}]}}}}</strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong>spec:</strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong>minReadySeconds: 5</strong></p><p><strong>selector:</strong></p><p><strong>matchLabels:</strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong>app: nginx</strong></p><p><strong>template:</strong></p><p><strong>metadata:</strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong>labels:</strong></p><p><strong>app: nginx</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- image: nginx:1.7.9</strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong>name: nginx</strong></p><p><strong>ports:</strong></p><p><strong>- containerPort: 80</strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p>Directly update the <strong>replicas</strong> field in the live configuration by using <strong>kubectl scale</strong>. This does not use <strong>kubectl apply</strong>:</p><p><strong>kubectl scale deployment/nginx-deployment --replicas=2</strong></p><p>Print the live configuration using <strong>kubectl get</strong>:</p><p><strong>kubectl get -f <a href="https://k8s.io/docs/concepts/overview/object-management-kubectl/simple_deployment.yaml">https://k8s.io/docs/concepts/overview/object-management-kubectl/simple_deployment.yaml</a> -o yaml</strong></p><p>The output shows that the <strong>replicas</strong> field has been set to 2, and the <strong>last-applied-configuration</strong> annotation does not contain a <strong>replicas</strong> field:</p><p><strong>apiVersion: apps/v1</strong></p><p><strong>kind: Deployment</strong></p><p><strong>metadata:</strong></p><p><strong>annotations:</strong></p><p><strong># <!-- -->.<!-- -->..</strong></p><p><strong># note that the annotation does not contain replicas</strong></p><p><strong># because it was not updated through apply</strong></p><p><strong>kubectl.kubernetes.io/last-applied-configuration: |</strong></p><p><strong>{&quot;apiVersion&quot;:&quot;apps/v1&quot;,&quot;kind&quot;:&quot;Deployment&quot;,</strong></p><p><strong>&quot;metadata&quot;:{&quot;annotations&quot;:{},&quot;name&quot;:&quot;nginx-deployment&quot;,&quot;namespace&quot;:&quot;default&quot;},</strong></p><p><strong>&quot;spec&quot;:{&quot;minReadySeconds&quot;:5,&quot;selector&quot;:{&quot;matchLabels&quot;:{&quot;app&quot;:nginx}},&quot;template&quot;:{&quot;metadata&quot;:{&quot;labels&quot;:{&quot;app&quot;:&quot;nginx&quot;}},</strong></p><p><strong>&quot;spec&quot;:{&quot;containers&quot;:[{&quot;image&quot;:&quot;nginx:1.7.9&quot;,&quot;name&quot;:&quot;nginx&quot;,</strong></p><p><strong>&quot;ports&quot;:<!-- -->[{&quot;containerPort&quot;:80}]<!-- -->}]}}}}</strong></p><p><strong># <!-- -->.<!-- -->..</strong></p><p><strong>spec:</strong></p><p><strong>replicas: 2 # written by scale</strong></p><p><strong># <!-- -->.<!-- -->..</strong></p><p><strong>minReadySeconds: 5</strong></p><p><strong>selector:</strong></p><p><strong>matchLabels:</strong></p><p><strong># <!-- -->.<!-- -->..</strong></p><p><strong>app: nginx</strong></p><p><strong>template:</strong></p><p><strong>metadata:</strong></p><p><strong># <!-- -->.<!-- -->..</strong></p><p><strong>labels:</strong></p><p><strong>app: nginx</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- image: nginx:1.7.9</strong></p><p><strong># <!-- -->.<!-- -->..</strong></p><p><strong>name: nginx</strong></p><p><strong>ports:</strong></p><p><strong>- containerPort: 80</strong></p><p><strong># <!-- -->.<!-- -->..</strong></p><p>Update the <strong>simple_deployment.yaml</strong> configuration file to change the image from <strong>nginx:1.7.9</strong> to <strong>nginx:1.11.9</strong>, and delete the <strong>minReadySeconds</strong> field:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>update_deployment.yam                                              |
| l</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/website/master/doc">https://raw.githubusercontent.com/kubernetes/website/master/doc</a> |
| s/concepts/overview/object-management-kubectl/update_deployment.yaml) |
+=======================================================================+
| <strong>apiVersion: apps/v1</strong>                                               |
|                                                                       |
| <strong>kind: Deployment</strong>                                                  |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: nginx-deployment</strong>                                            |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>matchLabels:</strong>                                                      |
|                                                                       |
| <strong>app: nginx</strong>                                                        |
|                                                                       |
| <strong>template:</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: nginx</strong>                                                        |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: nginx</strong>                                                     |
|                                                                       |
| <strong>image: nginx:1.11.9 <em># update the image</em></strong>                         |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- containerPort: 80</strong>                                               |
+-----------------------------------------------------------------------+</p><p>Apply the changes made to the configuration file:</p><p><strong>kubectl apply -f <a href="https://k8s.io/docs/concepts/overview/object-management-kubectl/update_deployment.yaml">https://k8s.io/docs/concepts/overview/object-management-kubectl/update_deployment.yaml</a></strong></p><p>Print the live configuration using <strong>kubectl get</strong>:</p><p><strong>kubectl get -f <a href="https://k8s.io/docs/concepts/overview/object-management-kubectl/simple_deployment.yaml">https://k8s.io/docs/concepts/overview/object-management-kubectl/simple_deployment.yaml</a> -o yaml</strong></p><p>The output shows the following changes to the live configuration:</p><ul><li>The <strong>replicas</strong> field retains the value of 2 set by <strong>kubectl scale</strong>. This is possible because it is omitted from the configuration file.</li><li>The <strong>image</strong> field has been updated to <strong>nginx:1.11.9</strong> from <strong>nginx:1.7.9</strong>.</li><li>The <strong>last-applied-configuration</strong> annotation has been updated with the new image.</li><li>The <strong>minReadySeconds</strong> field has been cleared.</li><li>The <strong>last-applied-configuration</strong> annotation no longer contains the <strong>minReadySeconds</strong> field.</li></ul><p><strong>apiVersion: apps/v1</strong></p><p><strong>kind: Deployment</strong></p><p><strong>metadata:</strong></p><p><strong>annotations:</strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong><em># The annotation contains the updated image to nginx 1.11.9,</em></strong></p><p><strong><em># but does not contain the updated replicas to 2</em></strong></p><p><strong>kubectl.kubernetes.io/last-applied-configuration: |</strong></p><p><strong>{&quot;apiVersion&quot;:&quot;apps/v1&quot;,&quot;kind&quot;:&quot;Deployment&quot;,</strong></p><p><strong>&quot;metadata&quot;:{&quot;annotations&quot;:{},&quot;name&quot;:&quot;nginx-deployment&quot;,&quot;namespace&quot;:&quot;default&quot;},</strong></p><p><strong>&quot;spec&quot;:{&quot;selector&quot;:{&quot;matchLabels&quot;:{&quot;app&quot;:nginx}},&quot;template&quot;:{&quot;metadata&quot;:{&quot;labels&quot;:{&quot;app&quot;:&quot;nginx&quot;}},</strong></p><p><strong>&quot;spec&quot;:{&quot;containers&quot;:[{&quot;image&quot;:&quot;nginx:1.11.9&quot;,&quot;name&quot;:&quot;nginx&quot;,</strong></p><p><strong>&quot;ports&quot;:<!-- -->[{&quot;containerPort&quot;:80}]<!-- -->}]}}}}</strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong>spec:</strong></p><p><strong>replicas: 2 <em># Set by <code>kubectl scale</code>. Ignored by <code>kubectl apply</code>.</em></strong></p><p><strong><em># minReadySeconds cleared by <code>kubectl apply</code></em></strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong>selector:</strong></p><p><strong>matchLabels:</strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong>app: nginx</strong></p><p><strong>template:</strong></p><p><strong>metadata:</strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong>labels:</strong></p><p><strong>app: nginx</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- image: nginx:1.11.9 <em># Set by <code>kubectl apply</code></em></strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong>name: nginx</strong></p><p><strong>ports:</strong></p><p><strong>- containerPort: 80</strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong>Warning</strong>: Mixing <strong>kubectl apply</strong> with the imperative object configuration commands <strong>create</strong> and <strong>replace</strong> is not supported. This is because <strong>create</strong> and <strong>replace</strong> do not retain the <strong>kubectl.kubernetes.io/last-applied-configuration</strong> that <strong>kubectl apply</strong> uses to compute updates.</p><h5><strong>How to delete objects</strong></h5><p>There are two approaches to delete objects managed by <strong>kubectl apply</strong>.</p><h6><strong>Recommended: </strong>kubectl delete -f <code>&lt;filename&gt;</code></h6><p>Manually deleting objects using the imperative command is the recommended approach, as it is more explicit about what is being deleted, and less likely to result in the user deleting something unintentionally:</p><p><strong>kubectl delete -f <code>&lt;filename&gt;</code></strong></p><h6><strong>Alternative: </strong>kubectl apply -f <code>&lt;directory/&gt;</code> --prune -l your=label</h6><p>Only use this if you know what you are doing.</p><p><strong>Warning:</strong> <strong>kubectl apply --prune</strong> is in alpha, and backwards incompatible changes might be introduced in subsequent releases.</p><p><strong>Warning</strong>: You must be careful when using this command, so that you do not delete objects unintentionally.</p><p>As an alternative to <strong>kubectl delete</strong>, you can use <strong>kubectl apply</strong> to identify objects to be deleted after their configuration files have been removed from the directory. Apply with <strong>--prune</strong> queries the API server for all objects matching a set of labels, and attempts to match the returned live object configurations against the object configuration files. If an object matches the query, and it does not have a configuration file in the directory, and it does not have a <strong>last-applied-configuration</strong>annotation, it is deleted.</p><p><strong>kubectl apply -f <code>&lt;directory/&gt; --prune -l &lt;labels&gt;</code></strong></p><p><strong>Important:</strong> Apply with prune should only be run against the root directory containing the object configuration files. Running against sub-directories can cause objects to be unintentionally deleted if they are returned by the label selector query specified with <strong>-l <code>&lt;labels&gt;</code></strong> and do not appear in the subdirectory.</p><h5><strong>How to view an object</strong></h5><p>You can use <strong>kubectl get</strong> with <strong>-o yaml</strong> to view the configuration of a live object:</p><p><strong>kubectl get -f <code>&lt;filename|url&gt;</code> -o yaml</strong></p><h5><strong>How apply calculates differences and merges changes</strong></h5><p><strong>Definition:</strong> A patch is an update operation that is scoped to specific fields of an object instead of the entire object. This enables updating only a specific set of fields on an object without reading the object first.</p><p>When <strong>kubectl apply</strong> updates the live configuration for an object, it does so by sending a patch request to the API server. The patch defines updates scoped to specific fields of the live object configuration. The <strong>kubectl apply</strong> command calculates this patch request using the configuration file, the live configuration, and the <strong>last-applied-configuration</strong> annotation stored in the live configuration.</p><h6><strong>Merge patch calculation</strong></h6><p>The <strong>kubectl apply</strong> command writes the contents of the configuration file to the <strong>kubectl.kubernetes.io/last-applied-configuration</strong> annotation. This is used to identify fields that have been removed from the configuration file and need to be cleared from the live configuration. Here are the steps used to calculate which fields should be deleted or set:</p><ol><li>Calculate the fields to delete. These are the fields present in <strong>last-applied-configuration</strong> and missing from the configuration file.</li><li>Calculate the fields to add or set. These are the fields present in the configuration file whose values don&#x27;t match the live configuration.</li></ol><p>Here&#x27;s an example. Suppose this is the configuration file for a Deployment object:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>update_deployment.yam                                              |
| l</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/website/master/doc">https://raw.githubusercontent.com/kubernetes/website/master/doc</a> |
| s/concepts/overview/object-management-kubectl/update_deployment.yaml) |
+=======================================================================+
| <strong>apiVersion: apps/v1</strong>                                               |
|                                                                       |
| <strong>kind: Deployment</strong>                                                  |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: nginx-deployment</strong>                                            |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>matchLabels:</strong>                                                      |
|                                                                       |
| <strong>app: nginx</strong>                                                        |
|                                                                       |
| <strong>template:</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: nginx</strong>                                                        |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: nginx</strong>                                                     |
|                                                                       |
| <strong>image: nginx:1.11.9 <em># update the image</em></strong>                         |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- containerPort: 80</strong>                                               |
+-----------------------------------------------------------------------+</p><p>Also, suppose this is the live configuration for the same Deployment object:</p><p><strong>apiVersion: apps/v1</strong></p><p><strong>kind: Deployment</strong></p><p><strong>metadata:</strong></p><p><strong>annotations:</strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong><em># note that the annotation does not contain replicas</em></strong></p><p><strong><em># because it was not updated through apply</em></strong></p><p><strong>kubectl.kubernetes.io/last-applied-configuration: |</strong></p><p><strong>{&quot;apiVersion&quot;:&quot;apps/v1&quot;,&quot;kind&quot;:&quot;Deployment&quot;,</strong></p><p><strong>&quot;metadata&quot;:{&quot;annotations&quot;:{},&quot;name&quot;:&quot;nginx-deployment&quot;,&quot;namespace&quot;:&quot;default&quot;},</strong></p><p><strong>&quot;spec&quot;:{&quot;minReadySeconds&quot;:5,&quot;selector&quot;:{&quot;matchLabels&quot;:{&quot;app&quot;:nginx}},&quot;template&quot;:{&quot;metadata&quot;:{&quot;labels&quot;:{&quot;app&quot;:&quot;nginx&quot;}},</strong></p><p><strong>&quot;spec&quot;:{&quot;containers&quot;:[{&quot;image&quot;:&quot;nginx:1.7.9&quot;,&quot;name&quot;:&quot;nginx&quot;,</strong></p><p><strong>&quot;ports&quot;:<!-- -->[{&quot;containerPort&quot;:80}]<!-- -->}]}}}}</strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong>spec:</strong></p><p><strong>replicas: 2 <em># written by scale</em></strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong>minReadySeconds: 5</strong></p><p><strong>selector:</strong></p><p><strong>matchLabels:</strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong>app: nginx</strong></p><p><strong>template:</strong></p><p><strong>metadata:</strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong>labels:</strong></p><p><strong>app: nginx</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- image: nginx:1.7.9</strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong>name: nginx</strong></p><p><strong>ports:</strong></p><p><strong>- containerPort: 80</strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p>Here are the merge calculations that would be performed by <strong>kubectl apply</strong>:</p><ol><li>Calculate the fields to delete by reading values from <strong>last-applied-configuration</strong> and comparing them to values in the configuration file. In this example, <strong>minReadySeconds</strong> appears in the <strong>last-applied-configuration</strong> annotation, but does not appear in the configuration file.<strong>Action:</strong> Clear <strong>minReadySeconds</strong> from the live configuration.</li><li>Calculate the fields to set by reading values from the configuration file and comparing them to values in the live configuration. In this example, the value of <strong>image</strong> in the configuration file does not match the value in the live configuration. <strong>Action:</strong> Set the value of <strong>image</strong> in the live configuration.</li><li>Set the <strong>last-applied-configuration</strong> annotation to match the value of the configuration file.</li><li>Merge the results from 1, 2, 3 into a single patch request to the API server.</li></ol><p>Here is the live configuration that is the result of the merge:</p><p><strong>apiVersion: apps/v1</strong></p><p><strong>kind: Deployment</strong></p><p><strong>metadata:</strong></p><p><strong>annotations:</strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong><em># The annotation contains the updated image to nginx 1.11.9,</em></strong></p><p><strong><em># but does not contain the updated replicas to 2</em></strong></p><p><strong>kubectl.kubernetes.io/last-applied-configuration: |</strong></p><p><strong>{&quot;apiVersion&quot;:&quot;apps/v1&quot;,&quot;kind&quot;:&quot;Deployment&quot;,</strong></p><p><strong>&quot;metadata&quot;:{&quot;annotations&quot;:{},&quot;name&quot;:&quot;nginx-deployment&quot;,&quot;namespace&quot;:&quot;default&quot;},</strong></p><p><strong>&quot;spec&quot;:{&quot;selector&quot;:{&quot;matchLabels&quot;:{&quot;app&quot;:nginx}},&quot;template&quot;:{&quot;metadata&quot;:{&quot;labels&quot;:{&quot;app&quot;:&quot;nginx&quot;}},</strong></p><p><strong>&quot;spec&quot;:{&quot;containers&quot;:[{&quot;image&quot;:&quot;nginx:1.11.9&quot;,&quot;name&quot;:&quot;nginx&quot;,</strong></p><p><strong>&quot;ports&quot;:<!-- -->[{&quot;containerPort&quot;:80}]<!-- -->}]}}}}</strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong>spec:</strong></p><p><strong>selector:</strong></p><p><strong>matchLabels:</strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong>app: nginx</strong></p><p><strong>replicas: 2 <em># Set by <code>kubectl scale</code>. Ignored by <code>kubectl apply</code>.</em></strong></p><p><strong><em># minReadySeconds cleared by <code>kubectl apply</code></em></strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong>template:</strong></p><p><strong>metadata:</strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong>labels:</strong></p><p><strong>app: nginx</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- image: nginx:1.11.9 <em># Set by <code>kubectl apply</code></em></strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong>name: nginx</strong></p><p><strong>ports:</strong></p><p><strong>- containerPort: 80</strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><h6><strong>How different types of fields are merged</strong></h6><p>How a particular field in a configuration file is merged with the live configuration depends on the type of the field. There are several types of fields:</p><ul><li>primitive: A field of type string, integer, or boolean. For example, <strong>image</strong> and <strong>replicas</strong> are primitive fields. <strong>Action:</strong> Replace.</li><li>map, also called object: A field of type map or a complex type that contains subfields. For example, <strong>labels</strong>, <strong>annotations</strong>,<strong>spec</strong> and <strong>metadata</strong> are all maps. <strong>Action:</strong> Merge elements or subfields.</li><li>list: A field containing a list of items that can be either primitive types or maps. For example, <strong>containers</strong>, <strong>ports</strong>, and <strong>args</strong> are lists. <strong>Action:</strong> Varies.</li></ul><p>When <strong>kubectl apply</strong> updates a map or list field, it typically does not replace the entire field, but instead updates the individual subelements. For instance, when merging the <strong>spec</strong> on a Deployment, the entire <strong>spec</strong> is not replaced. Instead the subfields of <strong>spec</strong>, such as <strong>replicas</strong>, are compared and merged.</p><h6><strong>Merging changes to primitive fields</strong></h6><p>Primitive fields are replaced or cleared.</p><p><strong>Note:</strong> &#x27;-&#x27; is used for &quot;not applicable&quot; because the value is not used.</p><p>  Field in object configuration file   Field in live object configuration   Field in last-applied-configuration   Action</p><hr/><p>  Yes                                  Yes                                  -                                    Set live to configuration file value.
Yes                                  No                                   -                                    Set live to local configuration.
No                                   -                                   Yes                                   Clear from live configuration.
No                                   -                                   No                                    Do nothing. Keep live value.</p><h6><strong>Merging changes to map fields</strong></h6><p>Fields that represent maps are merged by comparing each of the subfields or elements of the map:</p><p><strong>Note:</strong> &#x27;-&#x27; is used for &quot;not applicable&quot; because the value is not used.</p><p>  Key in object configuration file   Key in live object configuration   Field in last-applied-configuration   Action</p><hr/><p>  Yes                                Yes                                -                                    Compare sub fields values.
Yes                                No                                 -                                    Set live to local configuration.
No                                 -                                 Yes                                   Delete from live configuration.
No                                 -                                 No                                    Do nothing. Keep live value.</p><h6><strong>Merging changes for fields of type list</strong></h6><p>Merging changes to a list uses one of three strategies:</p><ul><li>Replace the list.</li><li>Merge individual elements in a list of complex elements.</li><li>Merge a list of primitive elements.</li></ul><p>The choice of strategy is made on a per-field basis.</p><p><strong>Replace the list</strong></p><p>Treat the list the same as a primitive field. Replace or delete the entire list. This preserves ordering.</p><p><strong>Example:</strong> Use <strong>kubectl apply</strong> to update the <strong>args</strong> field of a Container in a Pod. This sets the value of <strong>args</strong> in the live configuration to the value in the configuration file. Any <strong>args</strong> elements that had previously been added to the live configuration are lost. The order of the <strong>args</strong> elements defined in the configuration file is retained in the live configuration.</p><p><strong><em># last-applied-configuration value</em></strong></p><p><strong>args: <!-- -->[&quot;a, b&quot;]</strong></p><p><strong><em># configuration file value</em></strong></p><p><strong>args: <!-- -->[&quot;a&quot;, &quot;c&quot;]</strong></p><p><strong><em># live configuration</em></strong></p><p><strong>args: <!-- -->[&quot;a&quot;, &quot;b&quot;, &quot;d&quot;]</strong></p><p><strong><em># result after merge</em></strong></p><p><strong>args: <!-- -->[&quot;a&quot;, &quot;c&quot;]</strong></p><p><strong>Explanation:</strong> The merge used the configuration file value as the new list value.</p><p><strong>Merge individual elements of a list of complex elements:</strong></p><p>Treat the list as a map, and treat a specific field of each element as a key. Add, delete, or update individual elements. This does not preserve ordering.</p><p>This merge strategy uses a special tag on each field called a <strong>patchMergeKey</strong>. The <strong>patchMergeKey</strong>is defined for each field in the Kubernetes source code: <a href="https://git.k8s.io/api/core/v1/types.go#L2565">types.go</a> When merging a list of maps, the field specified as the <strong>patchMergeKey</strong> for a given element is used like a map key for that element.</p><p><strong>Example:</strong> Use <strong>kubectl apply</strong> to update the <strong>containers</strong> field of a PodSpec. This merges the list as though it was a map where each element is keyed by <strong>name</strong>.</p><p><strong><em># last-applied-configuration value</em></strong></p><p><strong>containers:</strong></p><p><strong>- name: nginx</strong></p><p><strong>image: nginx:1.10</strong></p><p><strong>- name: nginx-helper-a <em># key: nginx-helper-a; will be deleted in result</em></strong></p><p><strong>image: helper:1.3</strong></p><p><strong>- name: nginx-helper-b <em># key: nginx-helper-b; will be retained</em></strong></p><p><strong>image: helper:1.3</strong></p><p><strong><em># configuration file value</em></strong></p><p><strong>containers:</strong></p><p><strong>- name: nginx</strong></p><p><strong>image: nginx:1.10</strong></p><p><strong>- name: nginx-helper-b</strong></p><p><strong>image: helper:1.3</strong></p><p><strong>- name: nginx-helper-c <em># key: nginx-helper-c; will be added in result</em></strong></p><p><strong>image: helper:1.3</strong></p><p><strong><em># live configuration</em></strong></p><p><strong>containers:</strong></p><p><strong>- name: nginx</strong></p><p><strong>image: nginx:1.10</strong></p><p><strong>- name: nginx-helper-a</strong></p><p><strong>image: helper:1.3</strong></p><p><strong>- name: nginx-helper-b</strong></p><p><strong>image: helper:1.3</strong></p><p><strong>args: <!-- -->[&quot;run&quot;]<!-- --> <em># Field will be retained</em></strong></p><p><strong>- name: nginx-helper-d <em># key: nginx-helper-d; will be retained</em></strong></p><p><strong>image: helper:1.3</strong></p><p><strong><em># result after merge</em></strong></p><p><strong>containers:</strong></p><p><strong>- name: nginx</strong></p><p><strong>image: nginx:1.10</strong></p><p><strong><em># Element nginx-helper-a was deleted</em></strong></p><p><strong>- name: nginx-helper-b</strong></p><p><strong>image: helper:1.3</strong></p><p><strong>args: <!-- -->[&quot;run&quot;]<!-- --> <em># Field was retained</em></strong></p><p><strong>- name: nginx-helper-c <em># Element was added</em></strong></p><p><strong>image: helper:1.3</strong></p><p><strong>- name: nginx-helper-d <em># Element was ignored</em></strong></p><p><strong>image: helper:1.3</strong></p><p><strong>Explanation:</strong></p><ul><li>The container named &quot;nginx-helper-a&quot; was deleted because no container named &quot;nginx-helper-a&quot; appeared in the configuration file.</li><li>The container named &quot;nginx-helper-b&quot; retained the changes to <strong>args</strong> in the live configuration. <strong>kubectl apply</strong> was able to identify that &quot;nginx-helper-b&quot; in the live configuration was the same &quot;nginx-helper-b&quot; as in the configuration file, even though their fields had different values (no <strong>args</strong>in the configuration file). This is because the <strong>patchMergeKey</strong> field value (name) was identical in both.</li><li>The container named &quot;nginx-helper-c&quot; was added because no container with that name appeared in the live configuration, but one with that name appeared in the configuration file.</li><li>The container named &quot;nginx-helper-d&quot; was retained because no element with that name appeared in the last-applied-configuration.</li></ul><p><strong>Merge a list of primitive elements</strong></p><p>As of Kubernetes 1.5, merging lists of primitive elements is not supported.</p><p><strong>Note:</strong> Which of the above strategies is chosen for a given field is controlled by the <strong>patchStrategy</strong>tag in <a href="https://git.k8s.io/api/core/v1/types.go#L2565">types.go</a> If no <strong>patchStrategy</strong> is specified for a field of type list, then the list is replaced.</p><h5><strong>Default field values</strong></h5><p>The API server sets certain fields to default values in the live configuration if they are not specified when the object is created.</p><p>Here&#x27;s a configuration file for a Deployment. The file does not specify <strong>strategy</strong> or <strong>selector</strong>:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>simple_deployment.yam                                              |
| l</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/website/master/doc">https://raw.githubusercontent.com/kubernetes/website/master/doc</a> |
| s/concepts/overview/object-management-kubectl/simple_deployment.yaml) |
+=======================================================================+
| <strong>apiVersion: apps/v1</strong>                                               |
|                                                                       |
| <strong>kind: Deployment</strong>                                                  |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: nginx-deployment</strong>                                            |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>matchLabels:</strong>                                                      |
|                                                                       |
| <strong>app: nginx</strong>                                                        |
|                                                                       |
| <strong>minReadySeconds: 5</strong>                                                |
|                                                                       |
| <strong>template:</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: nginx</strong>                                                        |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: nginx</strong>                                                     |
|                                                                       |
| <strong>image: nginx:1.7.9</strong>                                                |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- containerPort: 80</strong>                                               |
+-----------------------------------------------------------------------+</p><p>Create the object using <strong>kubectl apply</strong>:</p><p><strong>kubectl apply -f <a href="https://k8s.io/docs/concepts/overview/object-management-kubectl/simple_deployment.yaml">https://k8s.io/docs/concepts/overview/object-management-kubectl/simple_deployment.yaml</a></strong></p><p>Print the live configuration using <strong>kubectl get</strong>:</p><p><strong>kubectl get -f <a href="https://k8s.io/docs/concepts/overview/object-management-kubectl/simple_deployment.yaml">https://k8s.io/docs/concepts/overview/object-management-kubectl/simple_deployment.yaml</a> -o yaml</strong></p><p>The output shows that the API server set several fields to default values in the live configuration. These fields were not specified in the configuration file.</p><p><strong>apiVersion: apps/v1</strong></p><p><strong>kind: Deployment</strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong>spec:</strong></p><p><strong>selector:</strong></p><p><strong>matchLabels:</strong></p><p><strong>app: nginx</strong></p><p><strong>minReadySeconds: 5</strong></p><p><strong>replicas: 1 <em># defaulted by apiserver</em></strong></p><p><strong>selector:</strong></p><p><strong>matchLabels: <em># defaulted by apiserver - derived from template.metadata.labels</em></strong></p><p><strong>app: nginx</strong></p><p><strong>strategy:</strong></p><p><strong>rollingUpdate: <em># defaulted by apiserver - derived from strategy.type</em></strong></p><p><strong>maxSurge: 1</strong></p><p><strong>maxUnavailable: 1</strong></p><p><strong>type: RollingUpdate <em># defaulted apiserver</em></strong></p><p><strong>template:</strong></p><p><strong>metadata:</strong></p><p><strong>creationTimestamp: null</strong></p><p><strong>labels:</strong></p><p><strong>app: nginx</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- image: nginx:1.7.9</strong></p><p><strong>imagePullPolicy: IfNotPresent <em># defaulted by apiserver</em></strong></p><p><strong>name: nginx</strong></p><p><strong>ports:</strong></p><p><strong>- containerPort: 80</strong></p><p><strong>protocol: TCP <em># defaulted by apiserver</em></strong></p><p><strong>resources: {} <em># defaulted by apiserver</em></strong></p><p><strong>terminationMessagePath: /dev/termination-log <em># defaulted by apiserver</em></strong></p><p><strong>dnsPolicy: ClusterFirst <em># defaulted by apiserver</em></strong></p><p><strong>restartPolicy: Always <em># defaulted by apiserver</em></strong></p><p><strong>securityContext: {} <em># defaulted by apiserver</em></strong></p><p><strong>terminationGracePeriodSeconds: 30 <em># defaulted by apiserver</em></strong></p><p><strong><em># <!-- -->.<!-- -->..</em></strong></p><p><strong>Note:</strong> Some of the fields&#x27; default values have been derived from the values of other fields that were specified in the configuration file, such as the <strong>selector</strong> field.</p><p>In a patch request, defaulted fields are not re-defaulted unless they are explicitly cleared as part of a patch request. This can cause unexpected behavior for fields that are defaulted based on the values of other fields. When the other fields are later changed, the values defaulted from them will not be updated unless they are explicitly cleared.</p><p>For this reason, it is recommended that certain fields defaulted by the server are explicitly defined in the configuration file, even if the desired values match the server defaults. This makes it easier to recognize conflicting values that will not be re-defaulted by the server.</p><p><strong>Example:</strong></p><p><strong><em># last-applied-configuration</em></strong></p><p><strong>spec:</strong></p><p><strong>template:</strong></p><p><strong>metadata:</strong></p><p><strong>labels:</strong></p><p><strong>app: nginx</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: nginx</strong></p><p><strong>image: nginx:1.7.9</strong></p><p><strong>ports:</strong></p><p><strong>- containerPort: 80</strong></p><p><strong><em># configuration file</em></strong></p><p><strong>spec:</strong></p><p><strong>strategy:</strong></p><p><strong>type: Recreate <em># updated value</em></strong></p><p><strong>template:</strong></p><p><strong>metadata:</strong></p><p><strong>labels:</strong></p><p><strong>app: nginx</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: nginx</strong></p><p><strong>image: nginx:1.7.9</strong></p><p><strong>ports:</strong></p><p><strong>- containerPort: 80</strong></p><p><strong><em># live configuration</em></strong></p><p><strong>spec:</strong></p><p><strong>strategy:</strong></p><p><strong>type: RollingUpdate <em># defaulted value</em></strong></p><p><strong>rollingUpdate: <em># defaulted value derived from type</em></strong></p><p><strong>maxSurge : 1</strong></p><p><strong>maxUnavailable: 1</strong></p><p><strong>template:</strong></p><p><strong>metadata:</strong></p><p><strong>labels:</strong></p><p><strong>app: nginx</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: nginx</strong></p><p><strong>image: nginx:1.7.9</strong></p><p><strong>ports:</strong></p><p><strong>- containerPort: 80</strong></p><p><strong><em># result after merge - ERROR!</em></strong></p><p><strong>spec:</strong></p><p><strong>strategy:</strong></p><p><strong>type: Recreate <em># updated value: incompatible with rollingUpdate</em></strong></p><p><strong>rollingUpdate: <em># defaulted value: incompatible with &quot;type: Recreate&quot;</em></strong></p><p><strong>maxSurge : 1</strong></p><p><strong>maxUnavailable: 1</strong></p><p><strong>template:</strong></p><p><strong>metadata:</strong></p><p><strong>labels:</strong></p><p><strong>app: nginx</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: nginx</strong></p><p><strong>image: nginx:1.7.9</strong></p><p><strong>ports:</strong></p><p><strong>- containerPort: 80</strong></p><p><strong>Explanation:</strong></p><ol><li>The user creates a Deployment without defining <strong>strategy.type</strong>.</li><li>The server defaults <strong>strategy.type</strong> to <strong>RollingUpdate</strong> and defaults the <strong>strategy.rollingUpdate</strong> values.</li><li>The user changes <strong>strategy.type</strong> to <strong>Recreate</strong>. The <strong>strategy.rollingUpdate</strong> values remain at their defaulted values, though the server expects them to be cleared. If the <strong>strategy.rollingUpdate</strong> values had been defined initially in the configuration file, it would have been more clear that they needed to be deleted.</li><li>Apply fails because <strong>strategy.rollingUpdate</strong> is not cleared. The <strong>strategy.rollingupdate</strong>field cannot be defined with a <strong>strategy.type</strong> of <strong>Recreate</strong>.</li></ol><p>Recommendation: These fields should be explicitly defined in the object configuration file:</p><ul><li>Selectors and PodTemplate labels on workloads, such as Deployment, StatefulSet, Job, DaemonSet, ReplicaSet, and ReplicationController</li><li>Deployment rollout strategy</li></ul><h6><strong>How to clear server-defaulted fields or fields set by other writers</strong></h6><p>As of Kubernetes 1.5, fields that do not appear in the configuration file cannot be cleared by a merge operation. Here are some workarounds:</p><p>Option 1: Remove the field by directly modifying the live object.</p><p><strong>Note:</strong> As of Kubernetes 1.5, <strong>kubectl edit</strong> does not work with <strong>kubectl apply</strong>. Using these together will cause unexpected behavior.</p><p>Option 2: Remove the field through the configuration file.</p><ol><li>Add the field to the configuration file to match the live object.</li><li>Apply the configuration file; this updates the annotation to include the field.</li><li>Delete the field from the configuration file.</li><li>Apply the configuration file; this deletes the field from the live object and annotation.</li></ol><h5><strong>How to change ownership of a field between the configuration file and direct imperative writers</strong></h5><p>These are the only methods you should use to change an individual object field:</p><ul><li>Use <strong>kubectl apply</strong>.</li><li>Write directly to the live configuration without modifying the configuration file: for example, use <strong>kubectl scale</strong>.</li></ul><h6><strong>Changing the owner from a direct imperative writer to a configuration file</strong></h6><p>Add the field to the configuration file. For the field, discontinue direct updates to the live configuration that do not go through <strong>kubectl apply</strong>.</p><h6><strong>Changing the owner from a configuration file to a direct imperative writer</strong></h6><p>As of Kubernetes 1.5, changing ownership of a field from a configuration file to an imperative writer requires manual steps:</p><ul><li>Remove the field from the configuration file.</li><li>Remove the field from the <strong>kubectl.kubernetes.io/last-applied-configuration</strong> annotation on the live object.</li></ul><h5><strong>Changing management methods</strong></h5><p>Kubernetes objects should be managed using only one method at a time. Switching from one method to another is possible, but is a manual process.</p><p><strong>Exception:</strong> It is OK to use imperative deletion with declarative management.</p><h6><strong>Migrating from imperative command management to declarative object configuration</strong></h6><p>Migrating from imperative command management to declarative object configuration involves several manual steps:</p><ol><li>Export the live object to a local configuration file:</li><li><strong>kubectl get <code>&lt;kind&gt;/&lt;name&gt; -o yaml --export &gt; &lt;kind&gt;\_&lt;name&gt;</code>.yaml</strong></li><li>Manually remove the <strong>status</strong> field from the configuration file.</li></ol><p><strong>Note:</strong> This step is optional, as <strong>kubectl apply</strong> does not update the status field even if it is present in the configuration file.</p><ol><li>Set the <strong>kubectl.kubernetes.io/last-applied-configuration</strong> annotation on the object:</li><li><strong>kubectl replace --save-config -f <code>&lt;kind&gt;\_&lt;name&gt;</code>.yaml</strong></li><li>Change processes to use <strong>kubectl apply</strong> for managing the object exclusively.</li></ol><h6><strong>Migrating from imperative object configuration to declarative object configuration</strong></h6><ol><li>Set the <strong>kubectl.kubernetes.io/last-applied-configuration</strong> annotation on the object:</li><li><strong>kubectl replace --save-config -f <code>&lt;kind&gt;\_&lt;name&gt;</code>.yaml</strong></li><li>Change processes to use <strong>kubectl apply</strong> for managing the object exclusively.</li></ol><h5><strong>Defining controller selectors and PodTemplate labels</strong></h5><p><strong>Warning</strong>: Updating selectors on controllers is strongly discouraged.</p><p>The recommended approach is to define a single, immutable PodTemplate label used only by the controller selector with no other semantic meaning.</p><p><strong>Example:</strong></p><p><strong>selector:</strong></p><p><strong>matchLabels:</strong></p><p><strong>controller-selector: &quot;extensions/v1beta1/deployment/nginx&quot;</strong></p><p><strong>template:</strong></p><p><strong>metadata:</strong></p><p><strong>labels:</strong></p><p><strong>controller-selector: &quot;extensions/v1beta1/deployment/nginx&quot;</strong></p><h5><strong>Known Issues</strong></h5><ul><li>Prior to Kubernetes 1.6, <strong>kubectl apply</strong> did not support operating on objects stored in a <a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/">custom resource</a>. For these cluster versions, you should instead use <a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/imperative-config/">imperative object configuration</a>.</li></ul><h5><strong>What&#x27;s next</strong></h5><ul><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/imperative-command/">Managing Kubernetes Objects Using Imperative Commands</a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/imperative-config/">Imperative Management of Kubernetes Objects Using Configuration Files</a></li><li><a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/">Kubectl Command Reference</a></li><li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/">Kubernetes API Reference</a></li></ul><h2>Kubernetes Architecture</h2><h3>Nodes</h3><ul><li><a href="https://kubernetes.io/docs/concepts/architecture/nodes/#what-is-a-node"><strong>What is a node?</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/architecture/nodes/#node-status"><strong>Node Status</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/architecture/nodes/#addresses"><strong>Addresses</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/architecture/nodes/#phase"><strong>Phase</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/architecture/nodes/#condition"><strong>Condition</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/architecture/nodes/#capacity"><strong>Capacity</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/architecture/nodes/#info"><strong>Info</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/architecture/nodes/#management"><strong>Management</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/architecture/nodes/#node-controller"><strong>Node Controller</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/architecture/nodes/#self-registration-of-nodes"><strong>Self-Registration of Nodes</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/architecture/nodes/#manual-node-administration"><strong>Manual Node Administration</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/architecture/nodes/#node-capacity"><strong>Node capacity</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/architecture/nodes/#api-object"><strong>API Object</strong></a></li></ul><h4>What is a node?</h4><p>A <strong>node</strong> is a worker machine in Kubernetes, previously known as a <strong>minion</strong>. A node may be a VM or physical machine, depending on the cluster. Each node has the services necessary to run <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/">pods</a> and is managed by the master components. The services on a node include Docker, kubelet and kube-proxy. See <a href="https://git.k8s.io/community/contributors/design-proposals/architecture/architecture.md#the-kubernetes-node">The Kubernetes Node</a> section in the architecture design doc for more details.</p><h4>Node Status</h4><p>A node&#x27;s status contains the following information:</p><ul><li><a href="https://kubernetes.io/docs/concepts/architecture/nodes/#addresses">Addresses</a></li><li> <strong>deprecated</strong></li><li><a href="https://kubernetes.io/docs/concepts/architecture/nodes/#condition">Condition</a></li><li><a href="https://kubernetes.io/docs/concepts/architecture/nodes/#capacity">Capacity</a></li><li><a href="https://kubernetes.io/docs/concepts/architecture/nodes/#info">Info</a></li></ul><p>Each section is described in detail below.</p><h5><strong>Addresses</strong></h5><p>The usage of these fields varies depending on your cloud provider or bare metal configuration.</p><ul><li>HostName: The hostname as reported by the node&#x27;s kernel. Can be overridden via the kubelet <strong>--hostname-override</strong> parameter.</li><li>ExternalIP: Typically the IP address of the node that is externally routable (available from outside the cluster).</li><li>InternalIP: Typically the IP address of the node that is routable only within the cluster.</li></ul><h5><strong>Phase</strong></h5><p>Deprecated: node phase is no longer used.</p><h5><strong>Condition</strong></h5><p>The <strong>conditions</strong> field describes the status of all <strong>Running</strong> nodes.</p><p>  Node Condition           Description</p><hr/><p>  <strong>OutOfDisk</strong>            <strong>True</strong> if there is insufficient free space on the node for adding new pods, otherwise <strong>False</strong>
<strong>Ready</strong>                <strong>True</strong> if the node is healthy and ready to accept pods, <strong>False</strong> if the node is not healthy and is not accepting pods, and <strong>Unknown</strong> if the node controller has not heard from the node in the last 40 seconds
<strong>MemoryPressure</strong>       <strong>True</strong> if pressure exists on the node memory -- that is, if the node memory is low; otherwise <strong>False</strong>
<strong>DiskPressure</strong>         <strong>True</strong> if pressure exists on the disk size -- that is, if the disk capacity is low; otherwise <strong>False</strong>
<strong>NetworkUnavailable</strong>   <strong>True</strong> if the network for the node is not correctly configured, otherwise <strong>False</strong>
<strong>ConfigOK</strong>             <strong>True</strong> if the kubelet is correctly configured, otherwise <strong>False</strong></p><p>The node condition is represented as a JSON object. For example, the following response describes a healthy node.</p><p><strong>&quot;conditions&quot;: [</strong></p><p><strong>{</strong></p><p><strong>&quot;type&quot;: &quot;Ready&quot;,</strong></p><p><strong>&quot;status&quot;: &quot;True&quot;</strong></p><p><strong>}</strong></p><p><strong>]</strong></p><p>If the Status of the Ready condition is &quot;Unknown&quot; or &quot;False&quot; for longer than the <strong>pod-eviction-timeout</strong>, an argument is passed to the <a href="https://kubernetes.io/docs/admin/kube-controller-manager/">kube-controller-manager</a> and all of the Pods on the node are scheduled for deletion by the Node Controller. The default eviction timeout duration is <strong>five minutes</strong>. In some cases when the node is unreachable, the apiserver is unable to communicate with the kubelet on it. The decision to delete the pods cannot be communicated to the kubelet until it re-establishes communication with the apiserver. In the meantime, the pods which are scheduled for deletion may continue to run on the partitioned node.</p><p>In versions of Kubernetes prior to 1.5, the node controller would <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/#force-deletion-of-pods">force delete</a> these unreachable pods from the apiserver. However, in 1.5 and higher, the node controller does not force delete pods until it is confirmed that they have stopped running in the cluster. One can see these pods which may be running on an unreachable node as being in the &quot;Terminating&quot; or &quot;Unknown&quot; states. In cases where Kubernetes cannot deduce from the underlying infrastructure if a node has permanently left a cluster, the cluster administrator may need to delete the node object by hand. Deleting the node object from Kubernetes causes all the Pod objects running on it to be deleted from the apiserver, freeing up their names.</p><p>Version 1.8 introduced an alpha feature that automatically creates <a href="https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/">taints</a> that represent conditions. To enable this behavior, pass an additional feature gate flag <strong>--feature-gates=<!-- -->.<!-- -->..,TaintNodesByCondition=true</strong> to the API server, controller manager, and scheduler. When <strong>TaintNodesByCondition</strong> is enabled, the scheduler ignores conditions when considering a Node; instead it looks at the Node&#x27;s taints and a Pod&#x27;s tolerations.</p><p>Now users can choose between the old scheduling model and a new, more flexible scheduling model. A Pod that does not have any tolerations gets scheduled according to the old model. But a Pod that tolerates the taints of a particular Node can be scheduled on that Node.</p><p>Note that because of small delay, usually less than one second, between time when condition is observed and a taint is created, it&#x27;s possible that enabling this feature will slightly increase number of Pods that are successfully scheduled but rejected by the kubelet.</p><h5><strong>Capacity</strong></h5><p>Describes the resources available on the node: CPU, memory and the maximum number of pods that can be scheduled onto the node.</p><h5><strong>Info</strong></h5><p>General information about the node, such as kernel version, Kubernetes version (kubelet and kube-proxy version), Docker version (if used), OS name. The information is gathered by Kubelet from the node.</p><h4>Management</h4><p>Unlike <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/">pods</a> and <a href="https://kubernetes.io/docs/concepts/services-networking/service/">services</a>, a node is not inherently created by Kubernetes: it is created externally by cloud providers like Google Compute Engine, or exists in your pool of physical or virtual machines. What this means is that when Kubernetes creates a node, it is really just creating an object that represents the node. After creation, Kubernetes will check whether the node is valid or not. For example, if you try to create a node from the following content:</p><p><strong>{</strong></p><p><strong>&quot;kind&quot;: &quot;Node&quot;,</strong></p><p><strong>&quot;apiVersion&quot;: &quot;v1&quot;,</strong></p><p><strong>&quot;metadata&quot;: {</strong></p><p><strong>&quot;name&quot;: &quot;10.240.79.157&quot;,</strong></p><p><strong>&quot;labels&quot;: {</strong></p><p><strong>&quot;name&quot;: &quot;my-first-k8s-node&quot;</strong></p><p><strong>}</strong></p><p><strong>}</strong></p><p><strong>}</strong></p><p>Kubernetes will create a node object internally (the representation), and validate the node by health checking based on the <strong>metadata.name</strong> field (we assume <strong>metadata.name</strong> can be resolved). If the node is valid, i.e. all necessary services are running, it is eligible to run a pod; otherwise, it will be ignored for any cluster activity until it becomes valid. Note that Kubernetes will keep the object for the invalid node unless it is explicitly deleted by the client, and it will keep checking to see if it becomes valid.</p><p>Currently, there are three components that interact with the Kubernetes node interface: node controller, kubelet, and kubectl.</p><h5><strong>Node Controller</strong></h5><p>The node controller is a Kubernetes master component which manages various aspects of nodes.</p><p>The node controller has multiple roles in a node&#x27;s life. The first is assigning a CIDR block to the node when it is registered (if CIDR assignment is turned on).</p><p>The second is keeping the node controller&#x27;s internal list of nodes up to date with the cloud provider&#x27;s list of available machines. When running in a cloud environment, whenever a node is unhealthy, the node controller asks the cloud provider if the VM for that node is still available. If not, the node controller deletes the node from its list of nodes.</p><p>The third is monitoring the nodes&#x27; health. The node controller is responsible for updating the NodeReady condition of NodeStatus to ConditionUnknown when a node becomes unreachable (i.e. the node controller stops receiving heartbeats for some reason, e.g. due to the node being down), and then later evicting all the pods from the node (using graceful termination) if the node continues to be unreachable. (The default timeouts are 40s to start reporting ConditionUnknown and 5m after that to start evicting pods.) The node controller checks the state of each node every <strong>--node-monitor-period</strong> seconds.</p><p>In Kubernetes 1.4, we updated the logic of the node controller to better handle cases when a large number of nodes have problems with reaching the master (e.g. because the master has networking problem). Starting with 1.4, the node controller will look at the state of all nodes in the cluster when making a decision about pod eviction.</p><p>In most cases, node controller limits the eviction rate to <strong>--node-eviction-rate</strong> (default 0.1) per second, meaning it won&#x27;t evict pods from more than 1 node per 10 seconds.</p><p>The node eviction behavior changes when a node in a given availability zone becomes unhealthy. The node controller checks what percentage of nodes in the zone are unhealthy (NodeReady condition is ConditionUnknown or ConditionFalse) at the same time. If the fraction of unhealthy nodes is at least <strong>--unhealthy-zone-threshold</strong> (default 0.55) then the eviction rate is reduced: if the cluster is small (i.e. has less than or equal to <strong>--large-cluster-size-threshold</strong> nodes - default 50) then evictions are stopped, otherwise the eviction rate is reduced to <strong>--secondary-node-eviction-rate</strong> (default 0.01) per second. The reason these policies are implemented per availability zone is because one availability zone might become partitioned from the master while the others remain connected. If your cluster does not span multiple cloud provider availability zones, then there is only one availability zone (the whole cluster).</p><p>A key reason for spreading your nodes across availability zones is so that the workload can be shifted to healthy zones when one entire zone goes down. Therefore, if all nodes in a zone are unhealthy then node controller evicts at the normal rate <strong>--node-eviction-rate</strong>. The corner case is when all zones are completely unhealthy (i.e. there are no healthy nodes in the cluster). In such case, the node controller assumes that there&#x27;s some problem with master connectivity and stops all evictions until some connectivity is restored.</p><p>Starting in Kubernetes 1.6, the NodeController is also responsible for evicting pods that are running on nodes with <strong>NoExecute</strong> taints, when the pods do not tolerate the taints. Additionally, as an alpha feature that is disabled by default, the NodeController is responsible for adding taints corresponding to node problems like node unreachable or not ready. See <a href="https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/">this documentation</a> for details about <strong>NoExecute</strong> taints and the alpha feature.</p><p>Starting in version 1.8, the node controller can be made responsible for creating taints that represent Node conditions. This is an alpha feature of version 1.8.</p><h5><strong>Self-Registration of Nodes</strong></h5><p>When the kubelet flag <strong>--register-node</strong> is true (the default), the kubelet will attempt to register itself with the API server. This is the preferred pattern, used by most distros.</p><p>For self-registration, the kubelet is started with the following options:</p><ul><li><strong>--kubeconfig</strong> - Path to credentials to authenticate itself to the apiserver.</li><li><strong>--cloud-provider</strong> - How to talk to a cloud provider to read metadata about itself.</li><li><strong>--register-node</strong> - Automatically register with the API server.</li><li><strong>--register-with-taints</strong> - Register the node with the given list of taints (comma separated <strong><code>&lt;key&gt;=&lt;value&gt;:&lt;effect&gt;</code></strong>). No-op if <strong>register-node</strong> is false.</li><li><strong>--node-ip</strong> - IP address of the node.</li><li><strong>--node-labels</strong> - Labels to add when registering the node in the cluster.</li><li><strong>--node-status-update-frequency</strong> - Specifies how often kubelet posts node status to master.</li></ul><p>Currently, any kubelet is authorized to create/modify any node resource, but in practice it only creates/modifies its own. (In the future, we plan to only allow a kubelet to modify its own node resource.)</p><h6><strong>Manual Node Administration</strong></h6><p>A cluster administrator can create and modify node objects.</p><p>If the administrator wishes to create node objects manually, set the kubelet flag <strong>--register-node=false</strong>.</p><p>The administrator can modify node resources (regardless of the setting of <strong>--register-node</strong>). Modifications include setting labels on the node and marking it unschedulable.</p><p>Labels on nodes can be used in conjunction with node selectors on pods to control scheduling, e.g. to constrain a pod to only be eligible to run on a subset of the nodes.</p><p>Marking a node as unschedulable will prevent new pods from being scheduled to that node, but will not affect any existing pods on the node. This is useful as a preparatory step before a node reboot, etc. For example, to mark a node unschedulable, run this command:</p><p><strong>kubectl cordon $NODENAME</strong></p><p>Note that pods which are created by a DaemonSet controller bypass the Kubernetes scheduler, and do not respect the unschedulable attribute on a node. The assumption is that daemons belong on the machine even if it is being drained of applications in preparation for a reboot.</p><h5><strong>Node capacity</strong></h5><p>The capacity of the node (number of cpus and amount of memory) is part of the node object. Normally, nodes register themselves and report their capacity when creating the node object. If you are doing <a href="https://kubernetes.io/docs/concepts/architecture/nodes/#manual-node-administration">manual node administration</a>, then you need to set node capacity when adding a node.</p><p>The Kubernetes scheduler ensures that there are enough resources for all the pods on a node. It checks that the sum of the requests of containers on the node is no greater than the node capacity. It includes all containers started by the kubelet, but not containers started directly by Docker nor processes not in containers.</p><p>If you want to explicitly reserve resources for non-pod processes, you can create a placeholder pod. Use the following template:</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: resource-reserver</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: sleep-forever</strong></p><p><strong>image: k8s.gcr.io/pause:0.8.0</strong></p><p><strong>resources:</strong></p><p><strong>requests:</strong></p><p><strong>cpu: 100m</strong></p><p><strong>memory: 100Mi</strong></p><p>Set the <strong>cpu</strong> and <strong>memory</strong> values to the amount of resources you want to reserve. Place the file in the manifest directory (<strong>--config=DIR</strong> flag of kubelet). Do this on each kubelet where you want to reserve resources.</p><h4>API Object</h4><p>Node is a top-level resource in the Kubernetes REST API. More details about the API object can be found at: <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#node-v1-core">Node API object</a>.</p><h3>Master-Node communication</h3><ul><li><a href="https://kubernetes.io/docs/concepts/architecture/master-node-communication/#overview"><strong>Overview</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/architecture/master-node-communication/#cluster---master"><strong>Cluster -&gt; Master</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/architecture/master-node-communication/#master---cluster"><strong>Master -&gt; Cluster</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/architecture/master-node-communication/#apiserver---kubelet"><strong>apiserver -&gt; kubelet</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/architecture/master-node-communication/#apiserver---nodes-pods-and-services"><strong>apiserver -&gt; nodes, pods, and services</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/architecture/master-node-communication/#ssh-tunnels"><strong>SSH Tunnels</strong></a></li></ul></li></ul><h4>Overview</h4><p>This document catalogs the communication paths between the master (really the apiserver) and the Kubernetes cluster. The intent is to allow users to customize their installation to harden the network configuration such that the cluster can be run on an untrusted network (or on fully public IPs on a cloud provider).</p><h4>Cluster -&gt; Master</h4><p>All communication paths from the cluster to the master terminate at the apiserver (none of the other master components are designed to expose remote services). In a typical deployment, the apiserver is configured to listen for remote connections on a secure HTTPS port (443) with one or more forms of client <a href="https://kubernetes.io/docs/admin/authentication/">authentication</a> enabled. One or more forms of <a href="https://kubernetes.io/docs/admin/authorization/">authorization</a> should be enabled, especially if <a href="https://kubernetes.io/docs/admin/authentication/#anonymous-requests">anonymous requests</a> or <a href="https://kubernetes.io/docs/admin/authentication/#service-account-tokens">service account tokens</a> are allowed.</p><p>Nodes should be provisioned with the public root certificate for the cluster such that they can connect securely to the apiserver along with valid client credentials. For example, on a default GCE deployment, the client credentials provided to the kubelet are in the form of a client certificate. See<a href="https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/">kubelet TLS bootstrapping</a> for automated provisioning of kubelet client certificates.</p><p>Pods that wish to connect to the apiserver can do so securely by leveraging a service account so that Kubernetes will automatically inject the public root certificate and a valid bearer token into the pod when it is instantiated. The <strong>kubernetes</strong> service (in all namespaces) is configured with a virtual IP address that is redirected (via kube-proxy) to the HTTPS endpoint on the apiserver.</p><p>The master components also communicate with the cluster apiserver over the secure port.</p><p>As a result, the default operating mode for connections from the cluster (nodes and pods running on the nodes) to the master is secured by default and can run over untrusted and/or public networks.</p><h4>Master -&gt; Cluster</h4><p>There are two primary communication paths from the master (apiserver) to the cluster. The first is from the apiserver to the kubelet process which runs on each node in the cluster. The second is from the apiserver to any node, pod, or service through the apiserver&#x27;s proxy functionality.</p><h5><strong>apiserver -&gt; kubelet</strong></h5><p>The connections from the apiserver to the kubelet are used for:</p><ul><li>Fetching logs for pods.</li><li>Attaching (through kubectl) to running pods.</li><li>Providing the kubelet&#x27;s port-forwarding functionality.</li></ul><p>These connections terminate at the kubelet&#x27;s HTTPS endpoint. By default, the apiserver does not verify the kubelet&#x27;s serving certificate, which makes the connection subject to man-in-the-middle attacks, and <strong>unsafe</strong> to run over untrusted and/or public networks.</p><p>To verify this connection, use the <strong>--kubelet-certificate-authority</strong> flag to provide the apiserver with a root certificate bundle to use to verify the kubelet&#x27;s serving certificate.</p><p>If that is not possible, use <a href="https://kubernetes.io/docs/concepts/architecture/master-node-communication/#ssh-tunnels">SSH tunneling</a> between the apiserver and kubelet if required to avoid connecting over an untrusted or public network.</p><p>Finally, <a href="https://kubernetes.io/docs/admin/kubelet-authentication-authorization/">Kubelet authentication and/or authorization</a> should be enabled to secure the kubelet API.</p><h5>apiserver -&gt; nodes, pods, and services</h5><p>The connections from the apiserver to a node, pod, or service default to plain HTTP connections and are therefore neither authenticated nor encrypted. They can be run over a secure HTTPS connection by prefixing <strong>https:</strong> to the node, pod, or service name in the API URL, but they will not validate the certificate provided by the HTTPS endpoint nor provide client credentials so while the connection will be encrypted, it will not provide any guarantees of integrity. These connections <strong>are not currently safe</strong> to run over untrusted and/or public networks.</p><h5><strong>SSH Tunnels</strong></h5><p><a href="https://cloud.google.com/kubernetes-engine/">Google Kubernetes Engine</a> uses SSH tunnels to protect the Master -&gt; Cluster communication paths. In this configuration, the apiserver initiates an SSH tunnel to each node in the cluster (connecting to the ssh server listening on port 22) and passes all traffic destined for a kubelet, node, pod, or service through the tunnel. This tunnel ensures that the traffic is not exposed outside of the private GCE network in which the cluster is running.</p><h3>Concepts Underlying the Cloud Controller Manager</h3><h4>Cloud Controller Manager</h4><p>The cloud controller manager (CCM) concept (not to be confused with the binary) was originally created to allow cloud specific vendor code and the Kubernetes core to evolve independent of one another. The cloud controller manager runs alongside other master components such as the Kubernetes controller manager, the API server, and scheduler. It can also be started as a Kubernetes addon, in which case it runs on top of Kubernetes.</p><p>The cloud controller manager&#x27;s design is based on a plugin mechanism that allows new cloud providers to integrate with Kubernetes easily by using plugins. There are plans in place for on-boarding new cloud providers on Kubernetes and for migrating cloud providers from the old model to the new CCM model.</p><p>This document discusses the concepts behind the cloud controller manager and gives details about its associated functions.</p><p>Here&#x27;s the architecture of a Kubernetes cluster without the cloud controller manager:</p><h4>Design</h4><p>In the preceding diagram, Kubernetes and the cloud provider are integrated through several different components:</p><ul><li>Kubelet</li><li>Kubernetes controller manager</li><li>Kubernetes API server</li></ul><p>The CCM consolidates all of the cloud-dependent logic from the preceding three components to create a single point of integration with the cloud. The new architecture with the CCM looks like this:</p><h4>Components of the CCM</h4><p>The CCM breaks away some of the functionality of Kubernetes controller manager (KCM) and runs it as a separate process. Specifically, it breaks away those controllers in the KCM that are cloud dependent. The KCM has the following cloud dependent controller loops:</p><ul><li>Node controller</li><li>Volume controller</li><li>Route controller</li><li>Service controller</li></ul><p>In version 1.9, the CCM runs the following controllers from the preceding list:</p><ul><li>Node controller</li><li>Route controller</li><li>Service controller</li></ul><p>Additionally, it runs another controller called the PersistentVolumeLabels controller. This controller is responsible for setting the zone and region labels on PersistentVolumes created in GCP and AWS clouds.</p><p><strong>Note:</strong> Volume controller was deliberately chosen to not be a part of CCM. Due to the complexity involved and due to the existing efforts to abstract away vendor specific volume logic, it was decided that volume controller will not be moved to CCM.</p><p>The original plan to support volumes using CCM was to use Flex volumes to support pluggable volumes. However, a competing effort known as CSI is being planned to replace Flex.</p><p>Considering these dynamics, we decided to have an intermediate stop gap measure until CSI becomes ready.</p><h4>Functions of the CCM</h4><p>The CCM inherits its functions from components of Kubernetes that are dependent on a cloud provider. This section is structured based on those components.</p><h5><strong>1. Kubernetes controller manager</strong></h5><p>The majority of the CCM&#x27;s functions are derived from the KCM. As mentioned in the previous section, the CCM runs the following control loops:</p><ul><li>Node controller</li><li>Route controller</li><li>Service controller</li><li>PersistentVolumeLabels controller</li></ul><h6><strong>Node controller</strong></h6><p>The Node controller is responsible for initializing a node by obtaining information about the nodes running in the cluster from the cloud provider. The node controller performs the following functions:</p><ol><li>Initialize a node with cloud specific zone/region labels.</li><li>Initialize a node with cloud specific instance details, for example, type and size.</li><li>Obtain the node&#x27;s network addresses and hostname.</li><li>In case a node becomes unresponsive, check the cloud to see if the node has been deleted from the cloud. If the node has been deleted from the cloud, delete the Kubernetes Node object.</li></ol><h6><strong>Route controller</strong></h6><p>The Route controller is responsible for configuring routes in the cloud appropriately so that containers on different nodes in the Kubernetes cluster can communicate with each other. The route controller is only applicable for Google Compute Engine clusters.</p><h6><strong>Service Controller</strong></h6><p>The Service controller is responsible for listening to service create, update, and delete events. Based on the current state of the services in Kubernetes, it configures cloud load balancers (such as ELB or Google LB) to reflect the state of the services in Kubernetes. Additionally, it ensures that service backends for cloud load balancers are up to date.</p><h6><strong>PersistentVolumeLabels controller</strong></h6><p>The PersistentVolumeLabels controller applies labels on AWS EBS/GCE PD volumes when they are created. This removes the need for users to manually set the labels on these volumes.</p><p>These labels are essential for the scheduling of pods as these volumes are constrained to work only within the region/zone that they are in. Any Pod using these volumes needs to be scheduled in the same region/zone.</p><p>The PersistentVolumeLabels controller was created specifically for the CCM; that is, it did not exist before the CCM was created. This was done to move the PV labelling logic in the Kubernetes API server (it was an admission controller) to the CCM. It does not run on the KCM.</p><h5><strong>2. Kubelet</strong></h5><p>The Node controller contains the cloud-dependent functionality of the kubelet. Prior to the introduction of the CCM, the kubelet was responsible for initializing a node with cloud-specific details such as IP addresses, region/zone labels and instance type information. The introduction of the CCM has moved this initialization operation from the kubelet into the CCM.</p><p>In this new model, the kubelet initializes a node without cloud-specific information. However, it adds a taint to the newly created node that makes the node unschedulable until the CCM initializes the node with cloud-specific information. It then removes this taint.</p><h5><strong>3. Kubernetes API server</strong></h5><p>The PersistentVolumeLabels controller moves the cloud-dependent functionality of the Kubernetes API server to the CCM as described in the preceding sections.</p><h4>Plugin mechanism</h4><p>The cloud controller manager uses Go interfaces to allow implementations from any cloud to be plugged in. Specifically, it uses the CloudProvider Interface defined <a href="https://github.com/kubernetes/kubernetes/blob/master/pkg/cloudprovider/cloud.go">here</a>.</p><p>The implementation of the four shared controllers highlighted above, and some scaffolding along with the shared cloudprovider interface, will stay in the Kubernetes core. Implementations specific to cloud providers will be built outside of the core and implement interfaces defined in the core.</p><p>For more information about developing plugins, see <a href="https://kubernetes.io/docs/tasks/administer-cluster/developing-cloud-controller-manager/">Developing Cloud Controller Manager</a>.</p><h4>Authorization</h4><p>This section breaks down the access required on various API objects by the CCM to perform its operations.</p><h5><strong>Node Controller</strong></h5><p>The Node controller only works with Node objects. It requires full access to get, list, create, update, patch, watch, and delete Node objects.</p><p>v1/Node:</p><ul><li>Get</li><li>List</li><li>Create</li><li>Update</li><li>Patch</li><li>Watch</li><li>Delete</li></ul><h5><strong>Route controller</strong></h5><p>The route controller listens to Node object creation and configures routes appropriately. It requires get access to Node objects.</p><p>v1/Node:</p><ul><li>Get</li></ul><h5><strong>Service controller</strong></h5><p>The service controller listens to Service object create, update and delete events and then configures endpoints for those Services appropriately.</p><p>To access Services, it requires list, and watch access. To update Services, it requires patch and update access.</p><p>To set up endpoints for the Services, it requires access to create, list, get, watch, and update.</p><p>v1/Service:</p><ul><li>List</li><li>Get</li><li>Watch</li><li>Patch</li><li>Update</li></ul><h5><strong>PersistentVolumeLabels controller</strong></h5><p>The PersistentVolumeLabels controller listens on PersistentVolume (PV) create events and then updates them. This controller requires access to get and update PVs.</p><p>v1/PersistentVolume:</p><ul><li>Get</li><li>List</li><li>Watch</li><li>Update</li></ul><h5><strong>Others</strong></h5><p>The implementation of the core of CCM requires access to create events, and to ensure secure operation, it requires access to create ServiceAccounts.</p><p>v1/Event:</p><ul><li>Create</li><li>Patch</li><li>Update</li></ul><p>v1/ServiceAccount:</p><ul><li>Create</li></ul><p>The RBAC ClusterRole for the CCM looks like this:</p><p><strong>apiVersion: rbac.authorization.k8s.io/v1</strong></p><p><strong>kind: ClusterRole</strong></p><p><strong>metadata:</strong></p><p><strong>name: cloud-controller-manager</strong></p><p><strong>rules:</strong></p><p><strong>- apiGroups:</strong></p><p><strong>- &quot;&quot;</strong></p><p><strong>resources:</strong></p><p><strong>- events</strong></p><p><strong>verbs:</strong></p><p><strong>- create</strong></p><p><strong>- patch</strong></p><p><strong>- update</strong></p><p><strong>- apiGroups:</strong></p><p><strong>- &quot;&quot;</strong></p><p><strong>resources:</strong></p><p><strong>- nodes</strong></p><p><strong>verbs:</strong></p><p><strong>- \&#x27;<!-- -->*<!-- -->\&#x27;</strong></p><p><strong>- apiGroups:</strong></p><p><strong>- &quot;&quot;</strong></p><p><strong>resources:</strong></p><p><strong>- nodes/status</strong></p><p><strong>verbs:</strong></p><p><strong>- patch</strong></p><p><strong>- apiGroups:</strong></p><p><strong>- &quot;&quot;</strong></p><p><strong>resources:</strong></p><p><strong>- services</strong></p><p><strong>verbs:</strong></p><p><strong>- list</strong></p><p><strong>- patch</strong></p><p><strong>- update</strong></p><p><strong>- watch</strong></p><p><strong>- apiGroups:</strong></p><p><strong>- &quot;&quot;</strong></p><p><strong>resources:</strong></p><p><strong>- serviceaccounts</strong></p><p><strong>verbs:</strong></p><p><strong>- create</strong></p><p><strong>- apiGroups:</strong></p><p><strong>- &quot;&quot;</strong></p><p><strong>resources:</strong></p><p><strong>- persistentvolumes</strong></p><p><strong>verbs:</strong></p><p><strong>- get</strong></p><p><strong>- list</strong></p><p><strong>- update</strong></p><p><strong>- watch</strong></p><p><strong>- apiGroups:</strong></p><p><strong>- &quot;&quot;</strong></p><p><strong>resources:</strong></p><p><strong>- endpoints</strong></p><p><strong>verbs:</strong></p><p><strong>- create</strong></p><p><strong>- get</strong></p><p><strong>- list</strong></p><p><strong>- watch</strong></p><p><strong>- update</strong></p><h4>Vendor Implementations</h4><p>The following cloud providers have implemented CCMs:</p><ul><li>Digital Ocean</li><li>Oracle</li><li>Azure</li><li>GCE</li><li>AWS</li></ul><h4>Cluster Administration</h4><p>Complete instructions for configuring and running the CCM are provided <a href="https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/#cloud-controller-manager">here</a>.</p><h2>Extending Kubernetes</h2><h3>Extending your Kubernetes Cluster</h3><p>Kubernetes is highly configurable and extensible. As a result, there is rarely a need to fork or submit patches to the Kubernetes project code.</p><p>This guide describes the options for customizing a Kubernetes cluster. It is aimed at <a href="https://kubernetes.io/docs/reference/glossary/?all=true#term-cluster-operator">Cluster Operators</a> who want to understand how to adapt their Kubernetes cluster to the needs of their work environment. Developers who are prospective <a href="https://kubernetes.io/docs/reference/glossary/?all=true#term-platform-developer">Platform Developers</a> or Kubernetes Project <a href="https://kubernetes.io/docs/reference/glossary/?all=true#term-contributor">Contributors</a> will also find it useful as an introduction to what extension points and patterns exist, and their trade-offs and limitations.</p><h4>Overview</h4><p>Customization approaches can be broadly divided into configuration, which only involves changing flags, local configuration files, or API resources; and extensions, which involve running additional programs or services. This document is primarily about extensions.</p><h4>Configuration</h4><p>Configuration files and flags are documented in the Reference section of the online documentation, under each binary:</p><ul><li><a href="https://kubernetes.io/docs/admin/kubelet/">kubelet</a></li><li><a href="https://kubernetes.io/docs/admin/kube-apiserver/">kube-apiserver</a></li><li><a href="https://kubernetes.io/docs/admin/kube-controller-manager/">kube-controller-manager</a></li><li><a href="https://kubernetes.io/docs/admin/kube-scheduler/">kube-scheduler</a>.</li></ul><p>Flags and configuration files may not always be changeable in a hosted Kubernetes service or a distribution with managed installation. When they are changeable, they are usually only changeable by the cluster administrator. Also, they are subject to change in future Kubernetes versions, and setting them may require restarting processes. For those reasons, they should be used only when there are no other options.</p><p>Built-in Policy APIs, such as <a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/">ResourceQuota</a>, <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/">PodSecurityPolicies</a>, <a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/">NetworkPolicy</a> and Role-based Access Control (<a href="https://kubernetes.io/docs/admin/authorization/rbac/">RBAC</a>), are built-in Kubernetes APIs. APIs are typically used with hosted Kubernetes services and with managed Kubernetes installations. They are declarative and use the same conventions as other Kubernetes resources like pods, so new cluster configuration can be repeatable and be managed the same way as applications. And, where they are stable, they enjoy a <a href="https://kubernetes.io/docs/reference/deprecation-policy/">defined support policy</a> like other Kubernetes APIs. For these reasons, they are preferred over configuration files and flags where suitable.</p><h4>Extensions</h4><p>Extensions are software components that extend and deeply integrate with Kubernetes. They adapt it to support new types and new kinds of hardware.</p><p>Most cluster administrators will use a hosted or distribution instance of Kubernetes. As a result, most Kubernetes users will need to install extensions and fewer will need to author new ones.</p><h4>Extension Patterns</h4><p>Kubernetes is designed to be automated by writing client programs. Any program that reads and/or writes to the Kubernetes API can provide useful automation. Automation can run on the cluster or off it. By following the guidance in this doc you can write highly available and robust automation. Automation generally works with any Kubernetes cluster, including hosted clusters and managed installations.</p><p>There is a specific pattern for writing client programs that work well with Kubernetes called the Controller pattern. Controllers typically read an object&#x27;s <strong>.spec</strong>, possibly do things, and then update the object&#x27;s <strong>.status</strong>.</p><p>A controller is a client of Kubernetes. When Kubernetes is the client and calls out to a remote service, it is called a Webhook. The remote service is called a Webhook Backend. Like Controllers, Webhooks do add a point of failure.</p><p>In the webhook model, Kubernetes makes a network request to a remote service. In the Binary Pluginmodel, Kubernetes executes a binary (program). Binary plugins are used by the kubelet (e.g. <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/flexvolume.md">Flex Volume Plugins</a> and <a href="https://kubernetes.io/docs/concepts/cluster-administration/network-plugins/">Network Plugins</a>) and by kubectl.</p><p>Below is a diagram showing how the extensions points interact with the Kubernetes control plane.</p><h4>Extension Points</h4><p>This diagram shows the extension points in a Kubernetes system.</p><ol><li>Users often interact with the Kubernetes API using <strong>kubectl</strong>. <a href="https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/">Kubectl plugins</a> extend the kubectl binary. They only affect the individual user&#x27;s local environment, and so cannot enforce site-wide policies.</li><li>The apiserver handles all requests. Several types of extension points in the apiserver allow authenticating requests, or blocking them based on their content, editing content, and handling deletion. These are described in the <a href="https://kubernetes.io/docs/concepts/overview/extending#api-access-extensions">API Access Extensions</a> section.</li><li>The apiserver serves various kinds of resources. Built-in resource kinds, like <strong>pods</strong>, are defined by the Kubernetes project and can&#x27;t be changed. You can also add resources that you define, or that other projects have defined, called Custom Resources, as explained in the <a href="https://kubernetes.io/docs/concepts/overview/extending#user-defined-types">Custom Resources</a>section. Custom Resources are often used with API Access Extensions.</li><li>The Kubernetes scheduler decides which nodes to place pods on. There are several ways to extend scheduling. These are described in the <a href="https://kubernetes.io/docs/concepts/overview/extending#scheduler-extensions">Scheduler Extensions</a> section.</li><li>Much of the behavior of Kubernetes is implemented by programs called Controllers which are clients of the API-Server. Controllers are often used in conjunction with Custom Resources.</li><li>The kubelet runs on servers, and helps pods appear like virtual servers with their own IPs on the cluster network. <a href="https://kubernetes.io/docs/concepts/overview/extending#network-plugins">Network Plugins</a> allow for different implementations of pod networking.</li><li>The kubelet also mounts and unmounts volumes for containers. New types of storage can be supported via <a href="https://kubernetes.io/docs/concepts/overview/extending#storage-plugins">Storage Plugins</a>.</li></ol><p>If you are unsure where to start, this flowchart can help. Note that some solutions may involve several types of extensions.</p><h4>API Extensions</h4><h5><strong>User-Defined Types</strong></h5><p>Consider adding a Custom Resource to Kubernetes if you want to define new controllers, application configuration objects or other declarative APIs, and to manage them using Kubernetes tools, such as <strong>kubectl</strong>.</p><p>Do not use a Custom Resource as data storage for application, user, or monitoring data.</p><p>For more about Custom Resources, see the <a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/">Custom Resources concept guide</a>.</p><h5><strong>Combining New APIs with Automation</strong></h5><p>Often, when you add a new API, you also add a control loop that reads and/or writes the new APIs. When the combination of a Custom API and a control loop is used to manage a specific, usually stateful, application, this is called the Operator pattern. Custom APIs and control loops can also be used to control other resources, such as storage, policies, and so on.</p><h5><strong>Changing Built-in Resources</strong></h5><p>When you extend the Kubernetes API by adding custom resources, the added resources always fall into a new API Groups. You cannot replace or change existing API groups. Adding an API does not directly let you affect the behavior of existing APIs (e.g. Pods), but API Access Extensions do.</p><h5><strong>API Access Extensions</strong></h5><p>When a request reaches the Kubernetes API Server, it is first Authenticated, then Authorized, then subject to various types of Admission Control. See [<a href="https://kubernetes.io/docs/admin/accessing-the-api/">Accessing the API</a>] for more on this flow.</p><p>Each of these steps offers extension points.</p><p>Kubernetes has several built-in authentication methods that it supports. It can also sit behind an authenticating proxy, and it can send a token from an Authorization header to a remote service for verification (a webhook). All of these methods are covered in the <a href="https://kubernetes.io/docs/admin/authentication/">Authentication documentation</a>.</p><h5><strong>Authentication</strong></h5><p><a href="https://kubernetes.io/docs/admin/authentication">Authentication</a> maps headers or certificates in all requests to a username for the client making the request.</p><p>Kubernetes provides several built-in authentication methods, and an <a href="https://kubernetes.io/docs/admin/authentication/#webhook-token-authentication">Authentication webhook</a>method if those don&#x27;t meet your needs.</p><h5><strong>Authorization</strong></h5><p><a href="https://kubernetes.io/docs/admin/authorization/webhook/">Authorization</a> determines whether specific users can read, write, and do other operations on API resources. It just works at the level of whole resources -- it doesn&#x27;t discriminate based on arbitrary object fields. If the built-in authorization options don&#x27;t meet your needs, and <a href="https://kubernetes.io/docs/admin/authorization/webhook/">Authorization webhook</a>allows calling out to user-provided code to make an authorization decision.</p><h5><strong>Dynamic Admission Control</strong></h5><p>After a request is authorized, if it is a write operation, it also goes through <a href="https://kubernetes.io/docs/admin/admission-controllers/">Admission Control</a> steps. In addition to the built-in steps, there are several extensions:</p><ul><li>The <a href="https://kubernetes.io/docs/admin/admission-controllers/#imagepolicywebhook">Image Policy webhook</a> restricts what images can be run in containers.</li><li>To make arbitrary admission control decisions, a general <a href="https://kubernetes.io/docs/admin/extensible-admission-controllers/#external-admission-webhooks">Admission webhook</a> can be used. Admission Webhooks can reject creations or updates.</li><li><a href="https://kubernetes.io/docs/admin/extensible-admission-controllers/#initializers">Initializers</a> are controllers that can modify objects before they are created. Initializers can modify initial object creations but cannot affect updates to objects. Initializers can also reject objects.</li></ul><h4>Infrastructure Extensions</h4><h5><strong>Storage Plugins</strong></h5><p><a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/flexvolume-deployment.md">Flex Volumes</a> allow users to mount volume types without built-in support by having the Kubelet call a Binary Plugin to mount the volume.</p><h5><strong>Device Plugins</strong></h5><p>Device plugins allow a node to discover new Node resources (in addition to the builtin ones like cpu and memory) via a <a href="https://kubernetes.io/docs/concepts/cluster-administration/device-plugins/">Device Plugin</a>.</p><h5><strong>Network Plugins</strong></h5><p>Different networking fabrics can be supported via node-level <a href="https://kubernetes.io/docs/admin/network-plugins/">Network Plugins</a>.</p><h5><strong>Scheduler Extensions</strong></h5><p>The scheduler is a special type of controller that watches pods, and assigns pods to nodes. The default scheduler can be replaced entirely, while continuing to use other Kubernetes components, or <a href="https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/">multiple schedulers</a> can run at the same time.</p><p>This is a significant undertaking, and almost all Kubernetes users find they do not need to modify the scheduler.</p><p>The scheduler also supports a <a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/scheduler_extender.md">webhook</a> that permits a webhook backend (scheduler extension) to filter and prioritize the nodes chosen for a pod.</p><h3>Extending the Kubernetes API with the aggregation layer</h3><p>The aggregation layer allows Kubernetes to be extended with additional APIs, beyond what is offered by the core Kubernetes APIs.</p><ul><li><a href="https://kubernetes.io/docs/concepts/api-extension/apiserver-aggregation/#overview"><strong>Overview</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/api-extension/apiserver-aggregation/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Overview</h4><p>The aggregation layer enables installing additional Kubernetes-style APIs in your cluster. These can either be pre-built, existing 3rd party solutions, such as <a href="https://github.com/kubernetes-incubator/service-catalog/blob/master/README.md">service-catalog</a>, or user-created APIs like <a href="https://github.com/kubernetes-incubator/apiserver-builder/blob/master/README.md">apiserver-builder</a>, which can get you started.</p><p>In 1.7 the aggregation layer runs in-process with the kube-apiserver. Until an extension resource is registered, the aggregation layer will do nothing. To register an API, users must add an APIService object, which &quot;claims&quot; the URL path in the Kubernetes API. At that point, the aggregation layer will proxy anything sent to that API path (e.g. /apis/myextension.mycompany.io/v1/...) to the registered APIService.</p><p>Ordinarily, the APIService will be implemented by an extension-apiserver in a pod running in the cluster. This extension-apiserver will normally need to be paired with one or more controllers if active management of the added resources is needed. As a result, the apiserver-builder will actually provide a skeleton for both. As another example, when the service-catalog is installed, it provides both the extension-apiserver and controller for the services it provides.</p><h4>What&#x27;s next</h4><ul><li>To get the aggregator working in your environment, <a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/configure-aggregation-layer/">configure the aggregation layer</a>.</li><li>Then, <a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/setup-extension-api-server/">setup an extension api-server</a> to work with the aggregation layer.</li><li>Also, learn how to <a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/">extend the Kubernetes API using Custom Resource Definitions</a>.</li></ul><h3>Custom Resources</h3><p>This page explains <a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/">custom resources</a>, which are extensions of the Kubernetes API. This page explains when to add a custom resource to your Kubernetes cluster and when to use a standalone service. It describes the two methods for adding custom resources and how to choose between them.</p><ul><li><a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/#custom-resources"><strong>Custom resources</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/#custom-controllers"><strong>Custom controllers</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/#should-i-add-a-custom-resource-to-my-kubernetes-cluster"><strong>Should I add a custom resource to my Kubernetes Cluster?</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/#declarative-apis"><strong>Declarative APIs</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/#should-i-use-a-configmap-or-a-custom-resource"><strong>Should I use a configMap or a custom resource?</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/#adding-custom-resources"><strong>Adding custom resources</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/#customresourcedefinitions"><strong>CustomResourceDefinitions</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/#api-server-aggregation"><strong>API server aggregation</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/#choosing-a-method-for-adding-custom-resources"><strong>Choosing a method for adding custom resources</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/#comparing-ease-of-use"><strong>Comparing ease of use</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/#advanced-features-and-flexibility"><strong>Advanced features and flexibility</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/#common-features"><strong>Common Features</strong></a></li></ul></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/#preparing-to-install-a-custom-resource"><strong>Preparing to install a custom resource</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/#third-party-code-and-new-points-of-failure"><strong>Third party code and new points of failure</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/#storage"><strong>Storage</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/#authentication-authorization-and-auditing"><strong>Authentication, authorization, and auditing</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/#accessing-a-custom-resource"><strong>Accessing a custom resource</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Custom resources</h4><p>A resource is an endpoint in the <a href="https://kubernetes.io/docs/reference/api-overview/">Kubernetes API</a> that stores a collection of <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/">API objects</a> of a certain kind. For example, the built-in pods resource contains a collection of Pod objects.</p><p>A custom resource is an extension of the Kubernetes API that is not necessarily available on every Kubernetes cluster. In other words, it represents a customization of a particular Kubernetes installation.</p><p>Custom resources can appear and disappear in a running cluster through dynamic registration, and cluster admins can update custom resources independently of the cluster itself. Once a custom resource is installed, users can create and access its objects with <a href="https://kubernetes.io/docs/user-guide/kubectl-overview/">kubectl</a>, just as they do for built-in resources like pods.</p><h5><strong>Custom controllers</strong></h5><p>On their own, custom resources simply let you store and retrieve structured data. It is only when combined with a controller that they become a true <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/#understanding-kubernetes-objects">declarative API</a>. The controller interprets the structured data as a record of the user&#x27;s desired state, and continually takes action to achieve and maintain that state.</p><p>A custom controller is a controller that users can deploy and update on a running cluster, independently of the cluster&#x27;s own lifecycle. Custom controllers can work with any kind of resource, but they are especially effective when combined with custom resources. The <a href="https://coreos.com/blog/introducing-operators.html">Operator</a> pattern is one example of such a combination. It allows developers to encode domain knowledge for specific applications into an extension of the Kubernetes API.</p><h5><strong>Should I add a custom resource to my Kubernetes Cluster?</strong></h5><p>When creating a new API, consider whether to <a href="https://kubernetes.io/docs/concepts/api-extension/apiserver-aggregation/">aggregate your API with the Kubernetes cluster APIs</a>or let your API stand alone.</p><p>  Consider API aggregation if:                                                                                                                                                                                               Prefer a stand-alone API if:</p><hr/><p>  Your API is <a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/#declarative-apis">Declarative</a>.                                                                                                           Your API does not fit the <a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/#declarative-apis">Declarative</a> model.
You want your new types to be readable and writable using <strong>kubectl</strong>.                                                                                                                                                     <strong>kubectl</strong> support is not required
You want to view your new types in a Kubernetes UI, such as dashboard, alongside built-in types.                                                                                                                           Kubernetes UI support is not required.
You are developing a new API.                                                                                                                                                                                              You already have a program that serves your API and works well.
You are willing to accept the format restriction that Kubernetes puts on REST resource paths, such as API Groups and Namespaces. (See the <a href="https://kubernetes.io/docs/concepts/overview/kubernetes-api/">API Overview</a>.)   You need to have specific REST paths to be compatible with an already defined REST API.
Your resources are naturally scoped to a cluster or to namespaces of a cluster.                                                                                                                                            Cluster or namespace scoped resources are a poor fit; you need control over the specifics of resource paths.
You want to reuse <a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/#common-features">Kubernetes API support features</a>.                                                                                  You don&#x27;t need those features.</p><h6><strong>Declarative APIs</strong></h6><p>In a Declarative API, typically:</p><ul><li>Your API consists of a relatively small number of relatively small objects (resources).</li><li>The objects define configuration of applications or infrastructure.</li><li>The objects are updated relatively infrequently.</li><li>Humans often need to read and write the objects.</li><li>The main operations on the objects are CRUD-y (creating, reading, updating and deleting).</li><li>Transactions across objects are not required: the API represents a desired state, not an exact state.</li></ul><p>Imperative APIs are not declarative. Signs that your API might not be declarative include:</p><ul><li>The client says &quot;do this&quot;, and then gets a synchronous response back when it is done.</li><li>The client says &quot;do this&quot;, and then gets an operation ID back, and has to check a separate Operation objects to determine completion of the request.</li><li>You talk about Remote Procedure Calls (RPCs).</li><li>Directly storing large amounts of data (e.g. &gt; a few kB per object, or &gt;1000s of objects).</li><li>High bandwidth access (10s of requests per second sustained) needed.</li><li>Store end-user data (such as images, PII, etc) or other large-scale data processed by applications.</li><li>The natural operations on the objects are not CRUD-y.</li><li>The API is not easily modeled as objects.</li><li>You chose to represent pending operations with an operation ID or operation object.</li></ul><h5><strong>Should I use a configMap or a custom resource?</strong></h5><p>Use a ConfigMap if any of the following apply:</p><ul><li>There is an existing, well-documented config file format, such as a <strong>mysql.cnf</strong> or <strong>pom.xml</strong>.</li><li>You want to put the entire config file into one key of a configMap.</li><li>The main use of the config file is for a program running in a Pod on your cluster to consume the file to configure itself.</li><li>Consumers of the file prefer to consume via file in a Pod or environment variable in a pod, rather than the Kubernetes API.</li><li>You want to perform rolling updates via Deployment, etc, when the file is updated.</li></ul><p><strong>Note:</strong> Use a <a href="https://kubernetes.io/docs/concepts/configuration/secret/">secret</a> for sensitive data, which is similar to a configMap but more secure.</p><p>Use a custom resource (CRD or Aggregated API) if most of the following apply:</p><ul><li>You want to use Kubernetes client libraries and CLIs to create and update the new resource.</li><li>You want top-level support from kubectl (for example: <strong>kubectl get my-object object-name</strong>).</li><li>You want to build new automation that watches for updates on the new object, and then CRUD other objects, or vice versa.</li><li>You want to write automation that handles updates to the object.</li><li>You want to use Kubernetes API conventions like <strong>.spec</strong>, <strong>.status</strong>, and <strong>.metadata</strong>.</li><li>You want the object to be an abstraction over a collection of controlled resources, or a summarization of other resources.</li></ul><h4>Adding custom resources</h4><p>Kubernetes provides two ways to add custom resources to your cluster:</p><ul><li><a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/">Custom Resource Definitions</a> (CRDs) are easier to use: they do not require any programming in some cases.</li><li><a href="https://kubernetes.io/docs/concepts/api-extension/apiserver-aggregation/">API Aggregation</a> requires programming, but allows more control over API behaviors like how data is stored and conversion between API versions.</li></ul><p>Kubernetes provides these two options to meet the needs of different users, so that neither ease of use nor flexibility are compromised.</p><p>Aggregated APIs are subordinate APIServers that sit behind the primary API server, which acts as a proxy. This arrangement is called <a href="https://kubernetes.io/docs/concepts/api-extension/apiserver-aggregation/">API Aggregation</a> (AA). To users, it simply appears that the Kubernetes API is extended.</p><p>Custom Resource Definitions (CRDS) allow users to create new types of resources without adding another APIserver. You do not need to understand API Aggregation to use CRDs.</p><p>Regardless of whether they are installed via CRDs or AA, the new resources are called Custom Resources to distinguish them from built-in Kubernetes resources (like pods).</p><h4>CustomResourceDefinitions</h4><p>The <a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/">CustomResourceDefinition</a> (CRD) API resource allows you to define custom resources. Defining a CRD object creates a new custom resource with a name and schema that you specify. The Kubernetes API serves and handles the storage of your custom resource.</p><p>This frees you from writing your own API server to handle the custom resource, but the generic nature of the implementation means you have less flexibility than with <a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/#api-server-aggregation">API server aggregation</a>.</p><p>Refer to the <a href="https://github.com/kubernetes/sample-controller">Custom Controller example, which uses Custom Resources</a> for a demonstration of how to register a new custom resource, work with instances of your new resource type, and setup a controller to handle events.</p><p><strong>Note:</strong> CRD is the successor to the deprecated ThirdPartyResource (TPR) API, and is available as of Kubernetes 1.7.</p><h4>API server aggregation</h4><p>Usually, each resource in the Kubernetes API requires code that handles REST requests and manages persistent storage of objects. The main Kubernetes API server handles built-in resources like pods and services, and can also handle custom resources in a generic way through <a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/#customresourcedefinitions">CustomResourceDefinitions</a>.</p><p>The <a href="https://kubernetes.io/docs/concepts/api-extension/apiserver-aggregation/">aggregation layer</a> allows you to provide specialized implementations for your custom resources by writing and deploying your own standalone API server. The main API server delegates requests to you for the custom resources that you handle, making them available to all of its clients.</p><h5><strong>Choosing a method for adding custom resources</strong></h5><p>CRDs are easier to use. Aggregated APIs are more flexible. Choose the method that best meets your needs.</p><p>Typically, CRDs are a good fit if:</p><ul><li>You have a handful of fields</li><li>You are using the resource within your company, or as part of a small open-source project (as opposed to a commercial product)</li></ul><h6><strong>Comparing ease of use</strong></h6><p>CRDs are easier to create than Aggregated APIs.</p><p>  Custom Resource Definitions                                                                                                                             Aggregated API</p><hr/><p>  Do not require programming. Users can choose any language for a CRD controller.                                                                         Requires programming in Go and building binary and image. Users can choose any language for a CRD controller.
No additional service to run; CRs are handled by API Server.                                                                                            An additional service to create and that could fail.
No ongoing support once the CRD is created. Any bug fixes are picked up as part of normal Kubernetes Master upgrades.                                   May need to periodically pickup bug fixes from upstream and rebuild and update the Aggregated APIserver.
No need to handle multiple versions of your API. For example: when you control the client for this resource, you can upgrade it in sync with the API.   You need to handle multiple versions of your API, for example: when developing an extension to share with the world.</p><h5><strong>Advanced features and flexibility</strong></h5><p>Aggregated APIs offer more advanced API features and customization of other features, for example: the storage layer.</p><p>+-----------------+-----------------+-----------------+-----------------+
| Feature         | Description     | CRDs            | Aggregated API  |
+=================+=================+=================+=================+
| Validation      | Help users      | Beta feature of | Yes, arbitrary  |
|                 | prevent errors  | CRDs in v1.9.   | validation      |
|                 | and allow you   | Checks limited  | checks          |
|                 | to evolve your  | to what is      |                 |
|                 | API             | supported by    |                 |
|                 | independently   | OpenAPI v3.0.   |                 |
|                 | of your         |                 |                 |
|                 | clients. These  |                 |                 |
|                 | features are    |                 |                 |
|                 | most useful     |                 |                 |
|                 | when there are  |                 |                 |
|                 | many clients    |                 |                 |
|                 | who can&#x27;t all   |                 |                 |
|                 | update at the   |                 |                 |
|                 | same time.      |                 |                 |
+-----------------+-----------------+-----------------+-----------------+
| Defaulting      | See above       | No, but can     | Yes             |
|                 |                 | achieve the     |                 |
|                 |                 | same effect     |                 |
|                 |                 | with an         |                 |
|                 |                 | Initializer     |                 |
|                 |                 | (requires       |                 |
|                 |                 | programming)    |                 |
+-----------------+-----------------+-----------------+-----------------+
| M               | Allows serving  | No              | Yes             |
| ulti-versioning | the same object |                 |                 |
|                 | through two API |                 |                 |
|                 | versions. Can   |                 |                 |
|                 | help ease API   |                 |                 |
|                 | changes like    |                 |                 |
|                 | renaming        |                 |                 |
|                 | fields. Less    |                 |                 |
|                 | important if    |                 |                 |
|                 | you control     |                 |                 |
|                 | your client     |                 |                 |
|                 | versions.       |                 |                 |
+-----------------+-----------------+-----------------+-----------------+
| Custom Storage  | If you need     | No              | Yes             |
|                 | storage with a  |                 |                 |
|                 | different       |                 |                 |
|                 | performance     |                 |                 |
|                 | mode (for       |                 |                 |
|                 | example,        |                 |                 |
|                 | time-series     |                 |                 |
|                 | database        |                 |                 |
|                 | instead of      |                 |                 |
|                 | key-value       |                 |                 |
|                 | store) or       |                 |                 |
|                 | isolation for   |                 |                 |
|                 | security (for   |                 |                 |
|                 | example,        |                 |                 |
|                 | encryption      |                 |                 |
|                 | secrets or      |                 |                 |
|                 | different       |                 |                 |
+-----------------+-----------------+-----------------+-----------------+
| Custom Business | Perform         | No, but can get | Yes             |
| Logic           | arbitrary       | some of the     |                 |
|                 | checks or       | same effects    |                 |
|                 | actions when    | with            |                 |
|                 | creating,       | Initializers or |                 |
|                 | reading,        | Finalizers      |                 |
|                 | updating or     | (requires       |                 |
|                 | deleting an     | programming)    |                 |
|                 | object          |                 |                 |
+-----------------+-----------------+-----------------+-----------------+
| Subresources    | -   Add extra   | No but planned  | Yes, any        |
|                 |     operations  |                 | Subresource     |
|                 |     other than  |                 |                 |
|                 |     CRUD, such  |                 |                 |
|                 |     as          |                 |                 |
|                 |     &quot;scale&quot;   |                 |                 |
|                 |     or &quot;exec&quot; |                 |                 |
|                 | -   Allows      |                 |                 |
|                 |     systems     |                 |                 |
|                 |     like        |                 |                 |
|                 |     Horizont    |                 |                 |
|                 | alPodAutoscaler |                 |                 |
|                 |     and         |                 |                 |
|                 |     PodD        |                 |                 |
|                 | isruptionBudget |                 |                 |
|                 |     interact    |                 |                 |
|                 |     with your   |                 |                 |
|                 |     new         |                 |                 |
|                 |     resource    |                 |                 |
|                 | -               |                 |                 |
|                 |   Finer-grained |                 |                 |
|                 |     access      |                 |                 |
|                 |     control:    |                 |                 |
|                 |     user writes |                 |                 |
|                 |     spec        |                 |                 |
|                 |     section,    |                 |                 |
|                 |     controller  |                 |                 |
|                 |     writes      |                 |                 |
|                 |     status      |                 |                 |
|                 |     section.    |                 |                 |
|                 | -   Allows      |                 |                 |
|                 |                 |                 |                 |
|                 |    incrementing |                 |                 |
|                 |     object      |                 |                 |
|                 |     Generation  |                 |                 |
|                 |     on custom   |                 |                 |
|                 |     resource    |                 |                 |
|                 |     data        |                 |                 |
|                 |     mutation    |                 |                 |
|                 |     (requires   |                 |                 |
|                 |     separate    |                 |                 |
|                 |     spec and    |                 |                 |
|                 |     status      |                 |                 |
|                 |     sections in |                 |                 |
|                 |     the         |                 |                 |
|                 |     resource)   |                 |                 |
+-----------------+-----------------+-----------------+-----------------+
| strate          | The new         | No              | Yes             |
| gic-merge-patch | endpoints       |                 |                 |
|                 | support PATCH   |                 |                 |
|                 | with            |                 |                 |
|                 | <strong>Content-Type: |                 |                 |
|                 | application     |                 |                 |
|                 | /strategic-merg |                 |                 |
|                 | e-patch+json</strong>. |                 |                 |
|                 | Useful for      |                 |                 |
|                 | updating        |                 |                 |
|                 | objects that    |                 |                 |
|                 | may be modified |                 |                 |
|                 | both locally,   |                 |                 |
|                 | and by the      |                 |                 |
|                 | server. For     |                 |                 |
|                 | more            |                 |                 |
|                 | information,    |                 |                 |
|                 | see <!-- -->[&quot;Update    |                 |                 |
|                 | API Objects in  |                 |                 |
|                 | Place Using     |                 |                 |
|                 | kubectl         |                 |                 |
|                 | patch&quot;]<!-- -->(https:/ |                 |                 |
|                 | /kubernetes.io/ |                 |                 |
|                 | docs/tasks/run- |                 |                 |
|                 | application/upd |                 |                 |
|                 | ate-api-object- |                 |                 |
|                 | kubectl-patch/) |                 |                 |
+-----------------+-----------------+-----------------+-----------------+
| Protocol        | The new         | No              | Yes             |
| Buffers         | resource        |                 |                 |
|                 | supports        |                 |                 |
|                 | clients that    |                 |                 |
|                 | want to use     |                 |                 |
|                 | Protocol        |                 |                 |
|                 | Buffers         |                 |                 |
+-----------------+-----------------+-----------------+-----------------+
| OpenAPI Schema  | Is there an     | No but planned  | Yes             |
|                 | OpenAPI         |                 |                 |
|                 | (swagger)       |                 |                 |
|                 | schema for the  |                 |                 |
|                 | types that can  |                 |                 |
|                 | be dynamically  |                 |                 |
|                 | fetched from    |                 |                 |
|                 | the server? Is  |                 |                 |
|                 | the user        |                 |                 |
|                 | protected from  |                 |                 |
|                 | misspelling     |                 |                 |
|                 | field names by  |                 |                 |
|                 | ensuring only   |                 |                 |
|                 | allowed fields  |                 |                 |
|                 | are set? Are    |                 |                 |
|                 | types enforced  |                 |                 |
|                 | (in other       |                 |                 |
|                 | words, don&#x27;t    |                 |                 |
|                 | put             |                 |                 |
|                 | an <strong>int</strong> in   |                 |                 |
|                 | a <strong>            |                 |                 |
|                 | string</strong>field?) |                 |                 |
+-----------------+-----------------+-----------------+-----------------+</p><h6><strong>Common Features</strong></h6><p>When you create a custom resource, either via a CRDs or an AA, you get many features for your API, compared to implementing it outside the Kubernetes platform:</p><p>  Feature                       What it does</p><hr/><p>  CRUD                          The new endpoints support CRUD basic operations via HTTP and <strong>kubectl</strong>
Watch                         The new endpoints support Kubernetes Watch operations via HTTP
Discovery                     Clients like kubectl and dashboard automatically offer list, display, and field edit operations on your resources
json-patch                    The new endpoints support PATCH with <strong>Content-Type: application/json-patch+json</strong>
merge-patch                   The new endpoints support PATCH with <strong>Content-Type: application/merge-patch+json</strong>
HTTPS                         The new endpoints uses HTTPS
Built-in Authentication       Access to the extension uses the core apiserver (aggregation layer) for authentication
Built-in Authorization        Access to the extension can reuse the authorization used by the core apiserver (e.g. RBAC)
Finalizers                    Block deletion of extension resources until external cleanup happens.
Admission Webhooks            Set default values and validate extension resources during any create/update/delete operation.
UI/CLI Display                Kubectl, dashboard can display extension resources.
Unset vs Empty                Clients can distinguish unset fields from zero-valued fields.
Client Libraries Generation   Kubernetes provides generic client libraries, as well as tools to generate type-specific client libraries.
Labels and annotations        Common metadata across objects that tools know how to edit for core and custom resources.</p><h4>Preparing to install a custom resource</h4><p>There are several points to be aware of before adding a custom resource to your cluster.</p><h5><strong>Third party code and new points of failure</strong></h5><p>While creating a CRD does not automatically add any new points of failure (for example, by causing third party code to run on your API server), packages (for example, Charts) or other installation bundles often include CRDs as well as a Deployment of third-party code that implements the business logic for a new custom resource.</p><p>Installing an Aggregated APIserver always involves running a new Deployment.</p><h5><strong>Storage</strong></h5><p>Custom resources consume storage space in the same way that ConfigMaps do. Creating too many custom resources may overload your API server&#x27;s storage space.</p><p>Aggregated API servers may use the same storage as the main API server, in which case the same warning applies.</p><h5><strong>Authentication, authorization, and auditing</strong></h5><p>CRDs always use the same authentication, authorization, and audit logging as the built-in resources of your API Server.</p><p>If you use RBAC for authorization, most RBAC roles will not grant access to the new resources (except the cluster-admin role or any role created with wildcard rules). You&#x27;ll need to explicitly grant access to the new resources. CRDs and Aggregated APIs often come bundled with new role definitions for the types they add.</p><p>Aggregated API servers may or may not use the same authentication, authorization, and auditing as the primary API server.</p><h4>Accessing a custom resource</h4><p>Kubernetes <a href="https://kubernetes.io/docs/reference/client-libraries/">client libraries</a> can be used to access custom resources. Not all client libraries support custom resources. The go and python client libraries do.</p><p>When you add a custom resource, you can access it using:</p><ul><li>kubectl</li><li>The kubernetes dynamic client.</li><li>A REST client that you write.</li><li>A client generated using <a href="https://github.com/kubernetes/code-generator">Kubernetes client generation tools</a> (generating one is an advanced undertaking, but some projects may provide a client along with the CRD or AA).</li></ul><h4>What&#x27;s next</h4><ul><li>Learn how to <a href="https://kubernetes.io/docs/concepts/api-extension/apiserver-aggregation/">Extend the Kubernetes API with the aggregation layer</a>.</li><li>Learn how to <a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/">Extend the Kubernetes API with CustomResourceDefinition</a>.</li><li>Learn how to <a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/migrate-third-party-resource/">Migrate a ThirdPartyResource to CustomResourceDefinition</a>.</li></ul><h3>Network Plugins</h3><ul><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/network-plugins/#installation"><strong>Installation</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/network-plugins/#network-plugin-requirements"><strong>Network Plugin Requirements</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/network-plugins/#cni"><strong>CNI</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/network-plugins/#kubenet"><strong>kubenet</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/network-plugins/#customizing-the-mtu-with-kubenet"><strong>Customizing the MTU (with kubenet)</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/network-plugins/#usage-summary"><strong>Usage Summary</strong></a></li></ul><p><strong>Disclaimer</strong>: Network plugins are in alpha. Its contents will change rapidly.</p><p>Network plugins in Kubernetes come in a few flavors:</p><ul><li>CNI plugins: adhere to the appc/CNI specification, designed for interoperability.</li><li>Kubenet plugin: implements basic <strong>cbr0</strong> using the <strong>bridge</strong> and <strong>host-local</strong> CNI plugins</li></ul><h4>Installation</h4><p>The kubelet has a single default network plugin, and a default network common to the entire cluster. It probes for plugins when it starts up, remembers what it found, and executes the selected plugin at appropriate times in the pod lifecycle (this is only true for Docker, as rkt manages its own CNI plugins). There are two Kubelet command line parameters to keep in mind when using plugins:</p><ul><li><strong>cni-bin-dir</strong>: Kubelet probes this directory for plugins on startup</li><li><strong>network-plugin</strong>: The network plugin to use from <strong>cni-bin-dir</strong>. It must match the name reported by a plugin probed from the plugin directory. For CNI plugins, this is simply &quot;cni&quot;.</li></ul><h4>Network Plugin Requirements</h4><p>Besides providing the <a href="https://github.com/kubernetes/kubernetes/tree/v1.10.0/pkg/kubelet/network/plugins.go"><strong>NetworkPlugin</strong> interface</a> to configure and clean up pod networking, the plugin may also need specific support for kube-proxy. The iptables proxy obviously depends on iptables, and the plugin may need to ensure that container traffic is made available to iptables. For example, if the plugin connects containers to a Linux bridge, the plugin must set the <strong>net/bridge/bridge-nf-call-iptables</strong> sysctl to <strong>1</strong> to ensure that the iptables proxy functions correctly. If the plugin does not use a Linux bridge (but instead something like Open vSwitch or some other mechanism) it should ensure container traffic is appropriately routed for the proxy.</p><p>By default if no kubelet network plugin is specified, the <strong>noop</strong> plugin is used, which sets <strong>net/bridge/bridge-nf-call-iptables=1</strong> to ensure simple configurations (like Docker with a bridge) work correctly with the iptables proxy.</p><h5><strong>CNI</strong></h5><p>The CNI plugin is selected by passing Kubelet the <strong>--network-plugin=cni</strong> command-line option. Kubelet reads a file from <strong>--cni-conf-dir</strong> (default <strong>/etc/cni/net.d</strong>) and uses the CNI configuration from that file to set up each pod&#x27;s network. The CNI configuration file must match the <a href="https://github.com/containernetworking/cni/blob/master/SPEC.md#network-configuration">CNI specification</a>, and any required CNI plugins referenced by the configuration must be present in <strong>--cni-bin-dir</strong> (default <strong>/opt/cni/bin</strong>).</p><p>If there are multiple CNI configuration files in the directory, the first one in lexicographic order of file name is used.</p><p>In addition to the CNI plugin specified by the configuration file, Kubernetes requires the standard CNI <a href="https://github.com/containernetworking/plugins/blob/master/plugins/main/loopback/loopback.go"><strong>lo</strong></a> plugin, at minimum version 0.2.0</p><p>Limitation: Due to <a href="https://github.com/kubernetes/kubernetes/issues/31307">#31307</a>, <strong>HostPort</strong> won&#x27;t work with CNI networking plugin at the moment. That means all <strong>hostPort</strong> attribute in pod would be simply ignored.</p><h5><strong>kubenet</strong></h5><p>Kubenet is a very basic, simple network plugin, on Linux only. It does not, of itself, implement more advanced features like cross-node networking or network policy. It is typically used together with a cloud provider that sets up routing rules for communication between nodes, or in single-node environments.</p><p>Kubenet creates a Linux bridge named <strong>cbr0</strong> and creates a veth pair for each pod with the host end of each pair connected to <strong>cbr0</strong>. The pod end of the pair is assigned an IP address allocated from a range assigned to the node either through configuration or by the controller-manager. <strong>cbr0</strong> is assigned an MTU matching the smallest MTU of an enabled normal interface on the host.</p><p>The plugin requires a few things:</p><ul><li>The standard CNI <strong>bridge</strong>, <strong>lo</strong> and <strong>host-local</strong> plugins are required, at minimum version 0.2.0. Kubenet will first search for them in <strong>/opt/cni/bin</strong>. Specify <strong>cni-bin-dir</strong> to supply additional search path. The first found match will take effect.</li><li>Kubelet must be run with the <strong>--network-plugin=kubenet</strong> argument to enable the plugin</li><li>Kubelet should also be run with the <strong>--non-masquerade-cidr=<code>&lt;clusterCidr&gt;</code></strong> argument to ensure traffic to IPs outside this range will use IP masquerade.</li><li>The node must be assigned an IP subnet through either the <strong>--pod-cidr</strong> kubelet command-line option or the <strong>--allocate-node-cidrs=true --cluster-cidr=<code>&lt;cidr&gt;</code></strong> controller-manager command-line options.</li></ul><h5><strong>Customizing the MTU (with kubenet)</strong></h5><p>The MTU should always be configured correctly to get the best networking performance. Network plugins will usually try to infer a sensible MTU, but sometimes the logic will not result in an optimal MTU. For example, if the Docker bridge or another interface has a small MTU, kubenet will currently select that MTU. Or if you are using IPSEC encapsulation, the MTU must be reduced, and this calculation is out-of-scope for most network plugins.</p><p>Where needed, you can specify the MTU explicitly with the <strong>network-plugin-mtu</strong> kubelet option. For example, on AWS the <strong>eth0</strong> MTU is typically 9001, so you might specify <strong>--network-plugin-mtu=9001</strong>. If you&#x27;re using IPSEC you might reduce it to allow for encapsulation overhead e.g. <strong>--network-plugin-mtu=8873</strong>.</p><p>This option is provided to the network-plugin; currently <strong>only kubenet supports network-plugin-mtu</strong>.</p><h4>Usage Summary</h4><ul><li><strong>--network-plugin=cni</strong> specifies that we use the <strong>cni</strong> network plugin with actual CNI plugin binaries located in <strong>--cni-bin-dir</strong> (default <strong>/opt/cni/bin</strong>) and CNI plugin configuration located in <strong>--cni-conf-dir</strong> (default <strong>/etc/cni/net.d</strong>).</li><li><strong>--network-plugin=kubenet</strong> specifies that we use the <strong>kubenet</strong> network plugin with CNI <strong>bridge</strong> and <strong>host-local</strong> plugins placed in <strong>/opt/cni/bin</strong> or <strong>cni-bin-dir</strong>.</li><li><strong>--network-plugin-mtu=9001</strong> specifies the MTU to use, currently only used by the <strong>kubenet</strong>network plugin.</li></ul><h3>Device Plugins</h3><p><strong>FEATURE STATE:</strong> <strong>Kubernetes v1.10</strong> <a href="https://kubernetes.io/docs/concepts/cluster-administration/device-plugins/">beta</a></p><p>Starting in version 1.8, Kubernetes provides a <a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/resource-management/device-plugin.md">device plugin framework</a> for vendors to advertise their resources to the kubelet without changing Kubernetes core code. Instead of writing custom Kubernetes code, vendors can implement a device plugin that can be deployed manually or as a DaemonSet. The targeted devices include GPUs, High-performance NICs, FPGAs, InfiniBand, and other similar computing resources that may require vendor specific initialization and setup.</p><ul><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/device-plugins/#device-plugin-registration"><strong>Device plugin registration</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/device-plugins/#device-plugin-implementation"><strong>Device plugin implementation</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/device-plugins/#device-plugin-deployment"><strong>Device plugin deployment</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/device-plugins/#examples"><strong>Examples</strong></a></li></ul><h4>Device plugin registration</h4><p>The device plugins feature is gated by the <strong>DevicePlugins</strong> feature gate which is disabled by default before 1.10. When the device plugins feature is enabled, the kubelet exports a <strong>Registration</strong> gRPC service:</p><p><strong>service Registration {</strong></p><p><strong>rpc Register(RegisterRequest) returns (Empty) {}</strong></p><p><strong>}</strong></p><p>A device plugin can register itself with the kubelet through this gRPC service. During the registration, the device plugin needs to send:</p><ul><li>The name of its Unix socket.</li><li>The Device Plugin API version against which it was built.</li><li>The <strong>ResourceName</strong> it wants to advertise. Here <strong>ResourceName</strong> needs to follow the <a href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#extended-resources">extended resource naming scheme</a> as <strong>vendor-domain/resource</strong>. For example, an Nvidia GPU is advertised as <strong>nvidia.com/gpu</strong>.</li></ul><p>Following a successful registration, the device plugin sends the kubelet the list of devices it manages, and the kubelet is then in charge of advertising those resources to the API server as part of the kubelet node status update. For example, after a device plugin registers <strong>vendor-domain/foo</strong>with the kubelet and reports two healthy devices on a node, the node status is updated to advertise 2 <strong>vendor-domain/foo</strong>.</p><p>Then, users can request devices in a <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#container-v1-core">Container</a> specification as they request other types of resources, with the following limitations:</p><ul><li>Extended resources are only supported as integer resources and cannot be overcommitted.</li><li>Devices cannot be shared among Containers.</li></ul><p>Suppose a Kubernetes cluster is running a device plugin that advertises resource <strong>vendor-domain/resource</strong> on certain nodes, here is an example user pod requesting this resource:</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: demo-pod</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: demo-container-1</strong></p><p><strong>image: gcr.io/google_containers/pause:2.0</strong></p><p><strong>resources:</strong></p><p><strong>limits:</strong></p><p><strong>vendor-domain/resource: 2 <em># requesting 2 vendor-domain/resource</em></strong></p><h4>Device plugin implementation</h4><p>The general workflow of a device plugin includes the following steps:</p><ul><li>Initialization. During this phase, the device plugin performs vendor specific initialization and setup to make sure the devices are in a ready state.</li><li>The plugin starts a gRPC service, with a Unix socket under host path <strong>/var/lib/kubelet/device-plugins/</strong>, that implements the following interfaces:</li><li><strong>service DevicePlugin {</strong></li><li><strong>// ListAndWatch returns a stream of List of Devices</strong></li><li><strong>// Whenever a Device state change or a Device disappears, ListAndWatch</strong></li><li><strong>// returns the new list</strong></li><li><strong>rpc ListAndWatch(Empty) returns (stream ListAndWatchResponse) {}</strong></li><li><strong>// Allocate is called during container creation so that the Device</strong></li><li><strong>// Plugin can run device specific operations and instruct Kubelet</strong></li><li><strong>// of the steps to make the Device available in the container</strong></li><li><strong>rpc Allocate(AllocateRequest) returns (AllocateResponse) {}</strong></li><li><strong>}</strong></li><li>The plugin registers itself with the kubelet through the Unix socket at host path <strong>/var/lib/kubelet/device-plugins/kubelet.sock</strong>.</li><li>After successfully registering itself, the device plugin runs in serving mode, during which it keeps monitoring device health and reports back to the kubelet upon any device state changes. It is also responsible for serving <strong>Allocate</strong> gRPC requests. During <strong>Allocate</strong>, the device plugin may do device-specific preparation; for example, GPU cleanup or QRNG initialization. If the operations succeed, the device plugin returns an <strong>AllocateResponse</strong> that contains container runtime configurations for accessing the allocated devices. The kubelet passes this information to the container runtime.</li></ul><p>A device plugin is expected to detect kubelet restarts and re-register itself with the new kubelet instance. In the current implementation, a new kubelet instance deletes all the existing Unix sockets under <strong>/var/lib/kubelet/device-plugins</strong> when it starts. A device plugin can monitor the deletion of its Unix socket and re-register itself upon such an event.</p><h4>Device plugin deployment</h4><p>A device plugin can be deployed manually or as a DaemonSet. Being deployed as a DaemonSet has the benefit that Kubernetes can restart the device plugin if it fails. Otherwise, an extra mechanism is needed to recover from device plugin failures. The canonical directory <strong>/var/lib/kubelet/device-plugins</strong> requires privileged access, so a device plugin must run in a privileged security context. If a device plugin is running as a DaemonSet, <strong>/var/lib/kubelet/device-plugins</strong> must be mounted as a <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#volume-v1-core">Volume</a> in the plugin&#x27;s <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#podspec-v1-core">PodSpec</a>.</p><p>Kubernetes device plugin support is still in alpha. As development continues, its API version can change in incompatible ways. We recommend that device plugin developers do the following:</p><ul><li>Watch for changes in future releases.</li><li>Support multiple versions of the device plugin API for backward/forward compatibility.</li></ul><p>If you enable the DevicePlugins feature and run device plugins on nodes that need to be upgraded to a Kubernetes release with a newer device plugin API version, upgrade your device plugins to support both versions before upgrading these nodes to ensure the continuous functioning of the device allocations during the upgrade.</p><h4>Examples</h4><p>For examples of device plugin implementations, see:</p><ul><li>The official <a href="https://github.com/NVIDIA/k8s-device-plugin">NVIDIA GPU device plugin</a><ul><li>it requires using <a href="https://github.com/NVIDIA/nvidia-docker">nvidia-docker 2.0</a> which allows you to run GPU enabled docker containers</li></ul></li><li>The <a href="https://github.com/GoogleCloudPlatform/container-engine-accelerators/tree/master/cmd/nvidia_gpu">NVIDIA GPU device plugin for COS base OS</a>.</li><li>The <a href="https://github.com/hustcat/k8s-rdma-device-plugin">RDMA device plugin</a></li><li>The <a href="https://github.com/vikaschoudhary16/sfc-device-plugin">Solarflare device plugin</a></li></ul><h3>Service Catalog</h3><p>Service Catalog is an extension API that enables applications running in Kubernetes clusters to easily use external managed software offerings, such as a datastore service offered by a cloud provider.</p><p>It provides a way to list, provision, and bind with external <a href="https://kubernetes.io/docs/reference/glossary/?all=true#term-managed-service">Managed Services</a> from <a href="https://kubernetes.io/docs/reference/glossary/?all=true#term-service-broker">Service Brokers</a>without needing detailed knowledge about how those services are created or managed.</p><p>A service broker, as defined by the <a href="https://github.com/openservicebrokerapi/servicebroker/blob/v2.13/spec.md">Open service broker API spec</a>, is an endpoint for a set of managed services offered and maintained by a third-party, which could be a cloud provider such as AWS, GCP, or Azure. Some examples of managed services are Microsoft Azure Cloud Queue, Amazon Simple Queue Service, and Google Cloud Pub/Sub, but they can be any software offering that can be used by an application.</p><p>Using Service Catalog, a <a href="https://kubernetes.io/docs/reference/glossary/?all=true#term-cluster-operator">cluster operator</a> can browse the list of managed services offered by a service broker, provision an instance of a managed service, and bind with it to make it available to an application in the Kubernetes cluster.</p><ul><li><a href="https://kubernetes.io/docs/concepts/service-catalog/#example-use-case"><strong>Example use case</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/service-catalog/#architecture"><strong>Architecture</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/service-catalog/#api-resources"><strong>API Resources</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/service-catalog/#authentication"><strong>Authentication</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/service-catalog/#usage"><strong>Usage</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/service-catalog/#listing-managed-services-and-service-plans"><strong>Listing managed services and Service Plans</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/service-catalog/#provisioning-a-new-instance"><strong>Provisioning a new instance</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/service-catalog/#binding-to-a-managed-service"><strong>Binding to a managed service</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/service-catalog/#mapping-the-connection-credentials"><strong>Mapping the connection credentials</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/service-catalog/#pod-configuration-file"><strong>Pod configuration File</strong></a></li></ul></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/service-catalog/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Example use case</h4><p>An <a href="https://kubernetes.io/docs/reference/glossary/?all=true#term-application-developer">application developer</a> wants to use message queuing as part of their application running in a Kubernetes cluster. However, they do not want to deal with the overhead of setting such a service up and administering it themselves. Fortunately, there is a cloud provider that offers message queuing as a managed service through its service broker.</p><p>A cluster operator can setup Service Catalog and use it to communicate with the cloud provider&#x27;s service broker to provision an instance of the message queuing service and make it available to the application within the Kubernetes cluster. The application developer therefore does not need to be concerned with the implementation details or management of the message queue. The application can simply use it as a service.</p><h4>Architecture</h4><p>Service Catalog uses the <a href="https://github.com/openservicebrokerapi/servicebroker">Open service broker API</a> to communicate with service brokers, acting as an intermediary for the Kubernetes API Server to negotiate the initial provisioning and retrieve the credentials necessary for the application to use a managed service.</p><p>It is implemented as an extension API server and a controller, using etcd for storage. It also uses the <a href="https://kubernetes.io/docs/concepts/api-extension/apiserver-aggregation/">aggregation layer</a> available in Kubernetes 1.7+ to present its API.</p><h5><strong>API Resources</strong></h5><p>Service Catalog installs the <strong>servicecatalog.k8s.io</strong> API and provides the following Kubernetes resources:</p><ul><li><strong>ClusterServiceBroker</strong>: An in-cluster representation of a service broker, encapsulating its server connection details. These are created and managed by cluster operators who wish to use that broker server to make new types of managed services available within their cluster.</li><li><strong>ClusterServiceClass</strong>: A managed service offered by a particular service broker. When a new <strong>ClusterServiceBroker</strong> resource is added to the cluster, the Service Catalog controller connects to the service broker to obtain a list of available managed services. It then creates a new <strong>ClusterServiceClass</strong> resource corresponding to each managed service.</li><li><strong>ClusterServicePlan</strong>: A specific offering of a managed service. For example, a managed service may have different plans available, such as a free tier or paid tier, or it may have different configuration options, such as using SSD storage or having more resources. Similar to <strong>ClusterServiceClass</strong>, when a new <strong>ClusterServiceBroker</strong> is added to the cluster, Service Catalog creates a new <strong>ClusterServicePlan</strong> resource corresponding to each Service Plan available for each managed service.</li><li><strong>ServiceInstance</strong>: A provisioned instance of a <strong>ClusterServiceClass</strong>. These are created by cluster operators to make a specific instance of a managed service available for use by one or more in-cluster applications. When a new <strong>ServiceInstance</strong> resource is created, the Service Catalog controller connects to the appropriate service broker and instruct it to provision the service instance.</li><li><strong>ServiceBinding</strong>: Access credentials to a <strong>ServiceInstance</strong>. These are created by cluster operators who want their applications to make use of a <strong>ServiceInstance</strong>. Upon creation, the Service Catalog controller creates a Kubernetes <strong>Secret</strong> containing connection details and credentials for the Service Instance, which can be mounted into Pods.</li></ul><h5><strong>Authentication</strong></h5><p>Service Catalog supports these methods of authentication:</p><ul><li>Basic (username/password)</li><li><a href="https://tools.ietf.org/html/rfc6750">OAuth 2.0 Bearer Token</a></li></ul><h4>Usage</h4><p>A cluster operator can use Service Catalog API Resources to provision managed services and make them available within a Kubernetes cluster. The steps involved are:</p><ol><li>Listing the managed services and Service Plans available from a service broker.</li><li>Provisioning a new instance of the managed service.</li><li>Binding to the managed service, which returns the connection credentials.</li><li>Mapping the connection credentials into the application.</li></ol><h5><strong>Listing managed services and Service Plans</strong></h5><p>First, a cluster operator must create a <strong>ClusterServiceBroker</strong> resource within the <strong>servicecatalog.k8s.io</strong> group. This resource contains the URL and connection details necessary to access a service broker endpoint.</p><p>This is an example of a <strong>ClusterServiceBroker</strong> resource:</p><p><strong>apiVersion: servicecatalog.k8s.io/v1beta1</strong></p><p><strong>kind: ClusterServiceBroker</strong></p><p><strong>metadata:</strong></p><p><strong>name: cloud-broker</strong></p><p><strong>spec:</strong></p><p><strong><em># Points to the endpoint of a service broker. (This example is not a working URL.)</em></strong></p><p><strong>url: <a href="https://servicebroker.somecloudprovider.com/v1alpha1/projects/service-catalog/brokers/default">https://servicebroker.somecloudprovider.com/v1alpha1/projects/service-catalog/brokers/default</a></strong></p><p><strong><em>#####</em></strong></p><p><strong><em># Additional values can be added here, which may be used to communicate</em></strong></p><p><strong><em># with the service broker, such as bearer token info or a caBundle for TLS.</em></strong></p><p><strong><em>#####</em></strong></p><p>The following is a sequence diagram illustrating the steps involved in listing managed services and Plans available from a service broker:</p><ol><li>Once the <strong>ClusterServiceBroker</strong> resource is added to Service Catalog, it triggers a call to the external service broker for a list of available services.</li><li>The service broker returns a list of available managed services and a list of Service Plans, which are cached locally as <strong>ClusterServiceClass</strong> and <strong>ClusterServicePlan</strong> resources respectively.</li><li>A cluster operator can then get the list of available managed services using the following command:</li><li><strong>kubectl get clusterserviceclasses -o=custom-columns=SERVICE<!-- -->\<!-- --> NAME:.metadata.name,EXTERNAL<!-- -->\<!-- --> NAME:.spec.externalName</strong></li></ol><p>It should output a list of service names with a format similar to:</p><p><strong>SERVICE NAME EXTERNAL NAME</strong></p><p><strong>4f6e6cf6-ffdd-425f-a2c7-3c9258ad2468 cloud-provider-service</strong></p><p><strong>.<!-- -->.. <!-- -->.<!-- -->..</strong></p><p>They can also view the Service Plans available using the following command:</p><p><strong>kubectl get clusterserviceplans -o=custom-columns=PLAN<!-- -->\<!-- --> NAME:.metadata.name,EXTERNAL<!-- -->\<!-- --> NAME:.spec.externalName</strong></p><p>It should output a list of plan names with a format similar to:</p><p><strong>PLAN NAME EXTERNAL NAME</strong></p><p><strong>86064792-7ea2-467b-af93-ac9694d96d52 service-plan-name</strong></p><p><strong>.<!-- -->.. <!-- -->.<!-- -->..</strong></p><h5><strong>Provisioning a new instance</strong></h5><p>A cluster operator can initiate the provisioning of a new instance by creating a <strong>ServiceInstance</strong>resource.</p><p>This is an example of a <strong>ServiceInstance</strong> resource:</p><p><strong>apiVersion: servicecatalog.k8s.io/v1beta1</strong></p><p><strong>kind: ServiceInstance</strong></p><p><strong>metadata:</strong></p><p><strong>name: cloud-queue-instance</strong></p><p><strong>namespace: cloud-apps</strong></p><p><strong>spec:</strong></p><p><strong><em># References one of the previously returned services</em></strong></p><p><strong>clusterServiceClassExternalName: cloud-provider-service</strong></p><p><strong>clusterServicePlanExternalName: service-plan-name</strong></p><p><strong><em>#####</em></strong></p><p><strong><em># Additional parameters can be added here,</em></strong></p><p><strong><em># which may be used by the service broker.</em></strong></p><p><strong><em>#####</em></strong></p><p>The following sequence diagram illustrates the steps involved in provisioning a new instance of a managed service:</p><ol><li>When the <strong>ServiceInstance</strong> resource is created, Service Catalog initiates a call to the external service broker to provision an instance of the service.</li><li>The service broker creates a new instance of the managed service and returns an HTTP response.</li><li>A cluster operator can then check the status of the instance to see if it is ready.</li></ol><h5><strong>Binding to a managed service</strong></h5><p>After a new instance has been provisioned, a cluster operator must bind to the managed service to get the connection credentials and service account details necessary for the application to use the service. This is done by creating a <strong>ServiceBinding</strong> resource.</p><p>The following is an example of a <strong>ServiceBinding</strong> resource:</p><p><strong>apiVersion: servicecatalog.k8s.io/v1beta1</strong></p><p><strong>kind: ServiceBinding</strong></p><p><strong>metadata:</strong></p><p><strong>name: cloud-queue-binding</strong></p><p><strong>namespace: cloud-apps</strong></p><p><strong>spec:</strong></p><p><strong>instanceRef:</strong></p><p><strong>name: cloud-queue-instance</strong></p><p><strong><em>#####</em></strong></p><p><strong><em># Additional information can be added here, such as a secretName or</em></strong></p><p><strong><em># service account parameters, which may be used by the service broker.</em></strong></p><p><strong><em>#####</em></strong></p><p>The following sequence diagram illustrates the steps involved in binding to a managed service instance:</p><ol><li>After the <strong>ServiceBinding</strong> is created, Service Catalog makes a call to the external service broker requesting the information necessary to bind with the service instance.</li><li>The service broker enables the application permissions/roles for the appropriate service account.</li><li>The service broker returns the information necessary to connect and access the managed service instance. This is provider and service-specific so the information returned may differ between Service Providers and their managed services.</li></ol><h5><strong>Mapping the connection credentials</strong></h5><p>After binding, the final step involves mapping the connection credentials and service-specific information into the application. These pieces of information are stored in secrets that the application in the cluster can access and use to connect directly with the managed service.</p><h6><strong>Pod configuration File</strong></h6><p>One method to perform this mapping is to use a declarative Pod configuration.</p><p>The following example describes how to map service account credentials into the application. A key called <strong>sa-key</strong> is stored in a volume named <strong>provider-cloud-key</strong>, and the application mounts this volume at <strong>/var/secrets/provider/key.json</strong>. The environment variable <strong>PROVIDER_APPLICATION_CREDENTIALS</strong> is mapped from the value of the mounted file.</p><p><strong>.<!-- -->..</strong></p><p><strong>spec:</strong></p><p><strong>volumes:</strong></p><p><strong>- name: provider-cloud-key</strong></p><p><strong>secret:</strong></p><p><strong>secretName: sa-key</strong></p><p><strong>containers:</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>volumeMounts:</strong></p><p><strong>- name: provider-cloud-key</strong></p><p><strong>mountPath: /var/secrets/provider</strong></p><p><strong>env:</strong></p><p><strong>- name: PROVIDER_APPLICATION_CREDENTIALS</strong></p><p><strong>value: &quot;/var/secrets/provider/key.json&quot;</strong></p><p>The following example describes how to map secret values into application environment variables. In this example, the messaging queue topic name is mapped from a secret named <strong>provider-queue-credentials</strong> with a key named <strong>topic</strong> to the environment variable <strong>TOPIC</strong>.</p><p><strong>.<!-- -->..</strong></p><p><strong>env:</strong></p><p><strong>- name: &quot;TOPIC&quot;</strong></p><p><strong>valueFrom:</strong></p><p><strong>secretKeyRef:</strong></p><p><strong>name: provider-queue-credentials</strong></p><p><strong>key: topic</strong></p><h4>What&#x27;s next</h4><ul><li>If you are familiar with <a href="https://github.com/kubernetes/helm/blob/master/docs/charts.md">Helm Charts</a>, <a href="https://kubernetes.io/docs/tasks/service-catalog/install-service-catalog-using-helm/">install Service Catalog using Helm</a> into your Kubernetes cluster. Alternatively, you can <a href="https://kubernetes.io/docs/tasks/service-catalog/install-service-catalog-using-sc/">install Service Catalog using the SC tool</a>.</li><li>View <a href="https://github.com/openservicebrokerapi/servicebroker/blob/master/gettingStarted.md#sample-service-brokers">sample service brokers</a>.</li><li>Explore the <a href="https://github.com/kubernetes-incubator/service-catalog">kubernetes-incubator/service-catalog</a> project.</li></ul><h3>Containers</h3><h4>Images</h4><p>You create your Docker image and push it to a registry before referring to it in a Kubernetes pod.</p><p>The <strong>image</strong> property of a container supports the same syntax as the <strong>docker</strong> command does, including private registries and tags.</p><ul><li><a href="https://kubernetes.io/docs/concepts/containers/images/#updating-images"><strong>Updating Images</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/containers/images/#using-a-private-registry"><strong>Using a Private Registry</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/containers/images/#using-google-container-registry"><strong>Using Google Container Registry</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/containers/images/#using-aws-ec2-container-registry"><strong>Using AWS EC2 Container Registry</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/containers/images/#using-azure-container-registry-acr"><strong>Using Azure Container Registry (ACR)</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/containers/images/#configuring-nodes-to-authenticate-to-a-private-repository"><strong>Configuring Nodes to Authenticate to a Private Repository</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/containers/images/#pre-pulling-images"><strong>Pre-pulling Images</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod"><strong>Specifying ImagePullSecrets on a Pod</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/containers/images/#creating-a-secret-with-a-docker-config"><strong>Creating a Secret with a Docker Config</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/containers/images/#bypassing-kubectl-create-secrets"><strong>Bypassing kubectl create secrets</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/containers/images/#referring-to-an-imagepullsecrets-on-a-pod"><strong>Referring to an imagePullSecrets on a Pod</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/containers/images/#use-cases"><strong>Use Cases</strong></a></li></ul></li></ul><h5><strong>Updating Images</strong></h5><p>The default pull policy is <strong>IfNotPresent</strong> which causes the Kubelet to skip pulling an image if it already exists. If you would like to always force a pull, you can do one of the following:</p><ul><li>set the <strong>imagePullPolicy</strong> of the container to <strong>Always</strong>;</li><li>use <strong>:latest</strong> as the tag for the image to use;</li><li>enable the <a href="https://kubernetes.io/docs/admin/admission-controllers/#alwayspullimages">AlwaysPullImages</a> admission controller.</li></ul><p>If you did not specify tag of your image, it will be assumed as <strong>:latest</strong>, with pull image policy of <strong>Always</strong> correspondingly.</p><p>Note that you should avoid using <strong>:latest</strong> tag, see <a href="https://kubernetes.io/docs/concepts/configuration/overview/#container-images">Best Practices for Configuration</a> for more information.</p><h5><strong>Using a Private Registry</strong></h5><p>Private registries may require keys to read images from them. Credentials can be provided in several ways:</p><ul><li>Using Google Container Registry<ul><li>Per-cluster</li><li>automatically configured on Google Compute Engine or Google Kubernetes Engine</li><li>all pods can read the project&#x27;s private registry</li></ul></li><li>Using AWS EC2 Container Registry (ECR)<ul><li>use IAM roles and policies to control access to ECR repositories</li><li>automatically refreshes ECR login credentials</li></ul></li><li>Using Azure Container Registry (ACR)</li><li>Configuring Nodes to Authenticate to a Private Registry<ul><li>all pods can read any configured private registries</li><li>requires node configuration by cluster administrator</li></ul></li><li>Pre-pulling Images<ul><li>all pods can use any images cached on a node</li><li>requires root access to all nodes to setup</li></ul></li><li>Specifying ImagePullSecrets on a Pod<ul><li>only pods which provide own keys can access the private registry Each option is described in more detail below.</li></ul></li></ul><h6><strong>Using Google Container Registry</strong></h6><p>Kubernetes has native support for the <a href="https://cloud.google.com/tools/container-registry/">Google Container Registry (GCR)</a>, when running on Google Compute Engine (GCE). If you are running your cluster on GCE or Google Kubernetes Engine, simply use the full image name (e.g. gcr.io/my_project/image:tag).</p><p>All pods in a cluster will have read access to images in this registry.</p><p>The kubelet will authenticate to GCR using the instance&#x27;s Google service account. The service account on the instance will have a <strong><a href="https://www.googleapis.com/auth/devstorage.read_only">https://www.googleapis.com/auth/devstorage.read_only</a></strong>, so it can pull from the project&#x27;s GCR, but not push.</p><h6><strong>Using AWS EC2 Container Registry</strong></h6><p>Kubernetes has native support for the <a href="https://aws.amazon.com/ecr/">AWS EC2 Container Registry</a>, when nodes are AWS EC2 instances.</p><p>Simply use the full image name (e.g. <strong>ACCOUNT.dkr.ecr.REGION.amazonaws.com/imagename:tag</strong>) in the Pod definition.</p><p>All users of the cluster who can create pods will be able to run pods that use any of the images in the ECR registry.</p><p>The kubelet will fetch and periodically refresh ECR credentials. It needs the following permissions to do this:</p><ul><li><strong>ecr:GetAuthorizationToken</strong></li><li><strong>ecr:BatchCheckLayerAvailability</strong></li><li><strong>ecr:GetDownloadUrlForLayer</strong></li><li><strong>ecr:GetRepositoryPolicy</strong></li><li><strong>ecr:DescribeRepositories</strong></li><li><strong>ecr:ListImages</strong></li><li><strong>ecr:BatchGetImage</strong></li></ul><p>Requirements:</p><ul><li>You must be using kubelet version <strong>v1.2.0</strong> or newer. (e.g. run <strong>/usr/bin/kubelet --version=true</strong>).</li><li>If your nodes are in region A and your registry is in a different region B, you need version <strong>v1.3.0</strong>or newer.</li><li>ECR must be offered in your region</li></ul><p>Troubleshooting:</p><ul><li>Verify all requirements above.</li><li>Get $REGION (e.g. <strong>us-west-2</strong>) credentials on your workstation. SSH into the host and run Docker manually with those creds. Does it work?</li><li>Verify kubelet is running with <strong>--cloud-provider=aws</strong>.</li><li>Check kubelet logs (e.g. <strong>journalctl -u kubelet</strong>) for log lines like:<ul><li><strong>plugins.go:56] Registering credential provider: aws-ecr-key</strong></li><li><strong>provider.go:91] Refreshing cache for provider: <!-- -->*<!-- -->aws_credentials.ecrProvider</strong></li></ul></li></ul><h6><strong>Using Azure Container Registry (ACR)</strong></h6><p>When using <a href="https://azure.microsoft.com/en-us/services/container-registry/">Azure Container Registry</a> you can authenticate using either an admin user or a service principal. In either case, authentication is done via standard Docker authentication. These instructions assume the <a href="https://github.com/azure/azure-cli">azure-cli</a> command line tool.</p><p>You first need to create a registry and generate credentials, complete documentation for this can be found in the <a href="https://docs.microsoft.com/en-us/azure/container-registry/container-registry-get-started-azure-cli">Azure container registry documentation</a>.</p><p>Once you have created your container registry, you will use the following credentials to login:</p><ul><li><strong>DOCKER_USER</strong> : service principal, or admin username</li><li><strong>DOCKER_PASSWORD</strong>: service principal password, or admin user password</li><li><strong>DOCKER_REGISTRY_SERVER</strong>: <strong>${some-registry-name}.azurecr.io</strong></li><li><strong>DOCKER_EMAIL</strong>: <strong>${some-email-address}</strong></li></ul><p>Once you have those variables filled in you can <a href="https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod">configure a Kubernetes Secret and use it to deploy a Pod</a>.</p><h6><strong>Configuring Nodes to Authenticate to a Private Repository</strong></h6><p><strong>Note:</strong> if you are running on Google Kubernetes Engine, there will already be a <strong>.dockercfg</strong> on each node with credentials for Google Container Registry. You cannot use this approach.</p><p><strong>Note:</strong> if you are running on AWS EC2 and are using the EC2 Container Registry (ECR), the kubelet on each node will manage and update the ECR login credentials. You cannot use this approach.</p><p><strong>Note:</strong> this approach is suitable if you can control node configuration. It will not work reliably on GCE, and any other cloud provider that does automatic node replacement.</p><p>Docker stores keys for private registries in the <strong>$HOME/.dockercfg</strong> or <strong>$HOME/.docker/config.json</strong> file. If you put this in the <strong>$HOME</strong> of user <strong>root</strong> on a kubelet, then docker will use it.</p><p>Here are the recommended steps to configuring your nodes to use a private registry. In this example, run these on your desktop/laptop:</p><ol><li>Run <strong>docker login <!-- -->[server]</strong> for each set of credentials you want to use. This updates <strong>$HOME/.docker/config.json</strong>.</li><li>View <strong>$HOME/.docker/config.json</strong> in an editor to ensure it contains just the credentials you want to use.</li><li>Get a list of your nodes, for example:<ul><li>if you want the names: <strong>nodes=$(kubectl get nodes -o jsonpath=\&#x27;{range.items<!-- -->[*]<!-- -->.metadata}{.name} {end}\&#x27;)</strong></li><li>if you want to get the IPs: <strong>nodes=$(kubectl get nodes -o jsonpath=\&#x27;{range .items<!-- -->[*]<!-- -->.status.addresses<!-- -->[?(@.type==&quot;ExternalIP&quot;)]<!-- -->}{.address} {end}\&#x27;)</strong></li></ul></li><li>Copy your local <strong>.docker/config.json</strong> to the home directory of root on each node.<ul><li>for example: <strong>for n in $nodes; do scp <!-- -->~<!-- -->/.docker/config.json root@$n:/root/.docker/config.json; done</strong></li></ul></li></ol><p>Verify by creating a pod that uses a private image, e.g.:</p><p><strong>$ cat <code>&lt;&lt;EOF &gt;</code> /tmp/private-image-test-1.yaml</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: private-image-test-1</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: uses-private-image</strong></p><p><strong>image: $PRIVATE_IMAGE_NAME</strong></p><p><strong>imagePullPolicy: Always</strong></p><p><strong>command: <!-- -->[ &quot;echo&quot;, &quot;SUCCESS&quot; ]</strong></p><p><strong>EOF</strong></p><p><strong>$ kubectl create -f /tmp/private-image-test-1.yaml</strong></p><p><strong>pod &quot;private-image-test-1&quot; created</strong></p><p><strong>$</strong></p><p>If everything is working, then, after a few moments, you should see:</p><p><strong>$ kubectl logs private-image-test-1</strong></p><p><strong>SUCCESS</strong></p><p>If it failed, then you will see:</p><p><strong>$ kubectl describe pods/private-image-test-1 | grep &quot;Failed&quot;</strong></p><p><strong>Fri, 26 Jun 2015 15:36:13 -0700 Fri, 26 Jun 2015 15:39:13 -0700 19 {kubelet node-i2hq} spec.containers{uses-private-image} failed Failed to pull image &quot;user/privaterepo:v1&quot;: Error: image user/privaterepo:v1 not found</strong></p><p>You must ensure all nodes in the cluster have the same <strong>.docker/config.json</strong>. Otherwise, pods will run on some nodes and fail to run on others. For example, if you use node autoscaling, then each instance template needs to include the <strong>.docker/config.json</strong> or mount a drive that contains it.</p><p>All pods will have read access to images in any private registry once private registry keys are added to the <strong>.docker/config.json</strong>.</p><p><strong>This was tested with a private docker repository as of 26 June with Kubernetes version v0.19.3. It should also work for a private registry such as quay.io, but that has not been tested.</strong></p><h6><strong>Pre-pulling Images</strong></h6><p><strong>Note:</strong> if you are running on Google Kubernetes Engine, there will already be a <strong>.dockercfg</strong> on each node with credentials for Google Container Registry. You cannot use this approach.</p><p><strong>Note:</strong> this approach is suitable if you can control node configuration. It will not work reliably on GCE, and any other cloud provider that does automatic node replacement.</p><p>By default, the kubelet will try to pull each image from the specified registry. However, if the <strong>imagePullPolicy</strong> property of the container is set to <strong>IfNotPresent</strong> or <strong>Never</strong>, then a local image is used (preferentially or exclusively, respectively).</p><p>If you want to rely on pre-pulled images as a substitute for registry authentication, you must ensure all nodes in the cluster have the same pre-pulled images.</p><p>This can be used to preload certain images for speed or as an alternative to authenticating to a private registry.</p><p>All pods will have read access to any pre-pulled images.</p><h6><strong>Specifying ImagePullSecrets on a Pod</strong></h6><p><strong>Note:</strong> This approach is currently the recommended approach for Google Kubernetes Engine, GCE, and any cloud-providers where node creation is automated.</p><p>Kubernetes supports specifying registry keys on a pod.</p><p><strong>Creating a Secret with a Docker Config</strong></p><p>Run the following command, substituting the appropriate uppercase values:</p><p><strong>$ kubectl create secret docker-registry myregistrykey --docker-server=DOCKER_REGISTRY_SERVER --docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAIL</strong></p><p><strong>secret &quot;myregistrykey&quot; created.</strong></p><p>If you need access to multiple registries, you can create one secret for each registry. Kubelet will merge any <strong>imagePullSecrets</strong> into a single virtual <strong>.docker/config.json</strong> when pulling images for your Pods.</p><p>Pods can only reference image pull secrets in their own namespace, so this process needs to be done one time per namespace.</p><p><strong>Bypassing kubectl create secrets</strong></p><p>If for some reason you need multiple items in a single <strong>.docker/config.json</strong> or need control not given by the above command, then you can <a href="https://kubernetes.io/docs/user-guide/secrets/#creating-a-secret-manually">create a secret using json or yaml</a>.</p><p>Be sure to:</p><ul><li>set the name of the data item to <strong>.dockerconfigjson</strong></li><li>base64 encode the docker file and paste that string, unbroken as the value for field <strong>data<!-- -->[&quot;.dockerconfigjson&quot;]</strong></li><li>set <strong>type</strong> to <strong>kubernetes.io/dockerconfigjson</strong></li></ul><p>Example:</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Secret</strong></p><p><strong>metadata:</strong></p><p><strong>name: myregistrykey</strong></p><p><strong>namespace: awesomeapps</strong></p><p><strong>data:</strong></p><p><strong>.dockerconfigjson: UmVhbGx5IHJlYWxseSByZWVlZWVlZWVlZWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWxsbGxsbGxsbGxsbGxsbGxsbGxsbGxsbGxsbGxsbGx5eXl5eXl5eXl5eXl5eXl5eXl5eSBsbGxsbGxsbGxsbGxsbG9vb29vb29vb29vb29vb29vb29vb29vb29vb25ubm5ubm5ubm5ubm5ubm5ubm5ubm5ubmdnZ2dnZ2dnZ2dnZ2dnZ2dnZ2cgYXV0aCBrZXlzCg==</strong></p><p><strong>type: kubernetes.io/dockerconfigjson</strong></p><p>If you get the error message <strong>error: no objects passed to create</strong>, it may mean the base64 encoded string is invalid. If you get an error message like <strong>Secret &quot;myregistrykey&quot; is invalid: data<!-- -->[.dockerconfigjson]<!-- -->: invalid value <!-- -->.<!-- -->..</strong>, it means the data was successfully un-base64 encoded, but could not be parsed as a <strong>.docker/config.json</strong> file.</p><p><strong>Referring to an imagePullSecrets on a Pod</strong></p><p>Now, you can create pods which reference that secret by adding an <strong>imagePullSecrets</strong> section to a pod definition.</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: foo</strong></p><p><strong>namespace: awesomeapps</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: foo</strong></p><p><strong>image: janedoe/awesomeapp:v1</strong></p><p><strong>imagePullSecrets:</strong></p><p><strong>- name: myregistrykey</strong></p><p>This needs to be done for each pod that is using a private registry.</p><p>However, setting of this field can be automated by setting the imagePullSecrets in a <a href="https://kubernetes.io/docs/user-guide/service-accounts">serviceAccount</a>resource. Check <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account">Add ImagePullSecrets to a Service Account</a> for detailed instructions.</p><p>You can use this in conjunction with a per-node <strong>.docker/config.json</strong>. The credentials will be merged. This approach will work on Google Kubernetes Engine.</p><h6><strong>Use Cases</strong></h6><p>There are a number of solutions for configuring private registries. Here are some common use cases and suggested solutions.</p><ol><li>Cluster running only non-proprietary (e.g. open-source) images. No need to hide images.<ul><li>Use public images on the Docker hub.<ul><li>No configuration required.</li><li>On GCE/Google Kubernetes Engine, a local mirror is automatically used for improved speed and availability.</li></ul></li></ul></li><li>Cluster running some proprietary images which should be hidden to those outside the company, but visible to all cluster users.<ul><li>Use a hosted private <a href="https://docs.docker.com/registry/">Docker registry</a>.<ul><li>It may be hosted on the <a href="https://hub.docker.com/account/signup/">Docker Hub</a>, or elsewhere.</li><li>Manually configure .docker/config.json on each node as described above.</li></ul></li><li>Or, run an internal private registry behind your firewall with open read access.<ul><li>No Kubernetes configuration is required.</li></ul></li><li>Or, when on GCE/Google Kubernetes Engine, use the project&#x27;s Google Container Registry.<ul><li>It will work better with cluster autoscaling than manual node configuration.</li></ul></li><li>Or, on a cluster where changing the node configuration is inconvenient, use <strong>imagePullSecrets</strong>.</li></ul></li><li>Cluster with a proprietary images, a few of which require stricter access control.<ul><li>Ensure <a href="https://kubernetes.io/docs/admin/admission-controllers/#alwayspullimages">AlwaysPullImages admission controller</a> is active. Otherwise, all Pods potentially have access to all images.</li><li>Move sensitive data into a &quot;Secret&quot; resource, instead of packaging it in an image.</li></ul></li><li>A multi-tenant cluster where each tenant needs own private registry.<ul><li>Ensure <a href="https://kubernetes.io/docs/admin/admission-controllers/#alwayspullimages">AlwaysPullImages admission controller</a> is active. Otherwise, all Pods of all tenants potentially have access to all images.</li><li>Run a private registry with authorization required.</li><li>Generate registry credential for each tenant, put into secret, and populate secret to each tenant namespace.</li><li>The tenant adds that secret to imagePullSecrets of each namespace.</li></ul></li></ol><h4>Container Environment Variables</h4><p>This page describes the resources available to Containers in the Container environment.</p><ul><li><a href="https://kubernetes.io/docs/concepts/containers/container-environment-variables/#container-environment"><strong>Container environment</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/containers/container-environment-variables/#container-information"><strong>Container information</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/containers/container-environment-variables/#cluster-information"><strong>Cluster information</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/containers/container-environment-variables/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h5><strong>Container environment</strong></h5><p>The Kubernetes Container environment provides several important resources to Containers:</p><ul><li>A filesystem, which is a combination of an <a href="https://kubernetes.io/docs/concepts/containers/images/">image</a> and one or more <a href="https://kubernetes.io/docs/concepts/storage/volumes/">volumes</a>.</li><li>Information about the Container itself.</li><li>Information about other objects in the cluster.</li></ul><h6><strong>Container information</strong></h6><p>The hostname of a Container is the name of the Pod in which the Container is running. It is available through the <strong>hostname</strong> command or the <a href="http://man7.org/linux/man-pages/man2/gethostname.2.html"><strong>gethostname</strong></a> function call in libc.</p><p>The Pod name and namespace are available as environment variables through the <a href="https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/">downward API</a>.</p><p>User defined environment variables from the Pod definition are also available to the Container, as are any environment variables specified statically in the Docker image.</p><h6><strong>Cluster information</strong></h6><p>A list of all services that were running when a Container was created is available to that Container as environment variables. Those environment variables match the syntax of Docker links.</p><p>For a service named foo that maps to a Container named bar, the following variables are defined:</p><p><strong>FOO_SERVICE_HOST=<code>&lt;the host the service is running on&gt;</code></strong></p><p><strong>FOO_SERVICE_PORT=<code>&lt;the port the service is running on&gt;</code></strong></p><p>Services have dedicated IP addresses and are available to the Container via DNS, if <a href="http://releases.k8s.io/master/cluster/addons/dns/">DNS addon</a> is enabled. </p><h5><strong>What&#x27;s next</strong></h5><ul><li>Learn more about <a href="https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/">Container lifecycle hooks</a>.</li><li>Get hands-on experience <a href="https://kubernetes.io/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/">attaching handlers to Container lifecycle events</a>.</li></ul><h4>Container Lifecycle Hooks</h4><p>This page describes how kubelet managed Containers can use the Container lifecycle hook framework to run code triggered by events during their management lifecycle.</p><ul><li><a href="https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#overview"><strong>Overview</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks"><strong>Container hooks</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#hook-handler-implementations"><strong>Hook handler implementations</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#hook-handler-execution"><strong>Hook handler execution</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#hook-delivery-guarantees"><strong>Hook delivery guarantees</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#debugging-hook-handlers"><strong>Debugging Hook handlers</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h5><strong>Overview</strong></h5><p>Analogous to many programming language frameworks that have component lifecycle hooks, such as Angular, Kubernetes provides Containers with lifecycle hooks. The hooks enable Containers to be aware of events in their management lifecycle and run code implemented in a handler when the corresponding lifecycle hook is executed.</p><h5><strong>Container hooks</strong></h5><p>There are two hooks that are exposed to Containers:</p><p><strong>PostStart</strong></p><p>This hook executes immediately after a container is created. However, there is no guarantee that the hook will execute before the container ENTRYPOINT. No parameters are passed to the handler.</p><p><strong>PreStop</strong></p><p>This hook is called immediately before a container is terminated. It is blocking, meaning it is synchronous, so it must complete before the call to delete the container can be sent. No parameters are passed to the handler.</p><p>A more detailed description of the termination behavior can be found in <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods">Termination of Pods</a>.</p><h6><strong>Hook handler implementations</strong></h6><p>Containers can access a hook by implementing and registering a handler for that hook. There are two types of hook handlers that can be implemented for Containers:</p><ul><li>Exec - Executes a specific command, such as <strong>pre-stop.sh</strong>, inside the cgroups and namespaces of the Container. Resources consumed by the command are counted against the Container.</li><li>HTTP - Executes an HTTP request against a specific endpoint on the Container.</li></ul><h6><strong>Hook handler execution</strong></h6><p>When a Container lifecycle management hook is called, the Kubernetes management system executes the handler in the Container registered for that hook. </p><p>Hook handler calls are synchronous within the context of the Pod containing the Container. This means that for a <strong>PostStart</strong> hook, the Container ENTRYPOINT and hook fire asynchronously. However, if the hook takes too long to run or hangs, the Container cannot reach a <strong>running</strong> state.</p><p>The behavior is similar for a <strong>PreStop</strong> hook. If the hook hangs during execution, the Pod phase stays in a <strong>Terminating</strong> state and is killed after <strong>terminationGracePeriodSeconds</strong> of pod ends. If a <strong>PostStart</strong> or <strong>PreStop</strong> hook fails, it kills the Container.</p><p>Users should make their hook handlers as lightweight as possible. There are cases, however, when long running commands make sense, such as when saving state prior to stopping a Container.</p><h6><strong>Hook delivery guarantees</strong></h6><p>Hook delivery is intended to be at least once, which means that a hook may be called multiple times for any given event, such as for <strong>PostStart</strong> or <strong>PreStop</strong>. It is up to the hook implementation to handle this correctly.</p><p>Generally, only single deliveries are made. If, for example, an HTTP hook receiver is down and is unable to take traffic, there is no attempt to resend. In some rare cases, however, double delivery may occur. For instance, if a kubelet restarts in the middle of sending a hook, the hook might be resent after the kubelet comes back up.</p><h6><strong>Debugging Hook handlers</strong></h6><p>The logs for a Hook handler are not exposed in Pod events. If a handler fails for some reason, it broadcasts an event. For <strong>PostStart</strong>, this is the <strong>FailedPostStartHook</strong> event, and for <strong>PreStop</strong>, this is the <strong>FailedPreStopHook</strong> event. You can see these events by running <strong>kubectl describe pod <code>&lt;pod_name&gt;</code></strong>. Here is some example output of events from running this command:</p><p><strong>Events:</strong></p><p><strong>FirstSeen LastSeen Count From SubobjectPath Type Reason Message</strong></p><p><strong>--------- -------- ----- ---- ------------- -------- ------ -------</strong></p><p><strong>1m 1m 1 {default-scheduler } Normal Scheduled Successfully assigned test-1730497541-cq1d2 to gke-test-cluster-default-pool-a07e5d30-siqd</strong></p><p><strong>1m 1m 1 {kubelet gke-test-cluster-default-pool-a07e5d30-siqd} spec.containers{main} Normal Pulling pulling image &quot;test:1.0&quot;</strong></p><p><strong>1m 1m 1 {kubelet gke-test-cluster-default-pool-a07e5d30-siqd} spec.containers{main} Normal Created Created container with docker id 5c6a256a2567; Security:<!-- -->[seccomp=unconfined]</strong></p><p><strong>1m 1m 1 {kubelet gke-test-cluster-default-pool-a07e5d30-siqd} spec.containers{main} Normal Pulled Successfully pulled image &quot;test:1.0&quot;</strong></p><p><strong>1m 1m 1 {kubelet gke-test-cluster-default-pool-a07e5d30-siqd} spec.containers{main} Normal Started Started container with docker id 5c6a256a2567</strong></p><p><strong>38s 38s 1 {kubelet gke-test-cluster-default-pool-a07e5d30-siqd} spec.containers{main} Normal Killing Killing container with docker id 5c6a256a2567: PostStart handler: Error executing in Docker Container: 1</strong></p><p><strong>37s 37s 1 {kubelet gke-test-cluster-default-pool-a07e5d30-siqd} spec.containers{main} Normal Killing Killing container with docker id 8df9fdfd7054: PostStart handler: Error executing in Docker Container: 1</strong></p><p><strong>38s 37s 2 {kubelet gke-test-cluster-default-pool-a07e5d30-siqd} Warning FailedSync Error syncing pod, skipping: failed to &quot;StartContainer&quot; for &quot;main&quot; with RunContainerError: &quot;PostStart handler: Error executing in Docker Container: 1&quot;</strong></p><p><strong>1m 22s 2 {kubelet gke-test-cluster-default-pool-a07e5d30-siqd} spec.containers{main} Warning FailedPostStartHook</strong></p><h2>Workloads</h2><h3>Pods</h3><h4>Pod Overview</h4><p>This page provides an overview of <strong>Pod</strong>, the smallest deployable object in the Kubernetes object model.</p><h5>Understanding Pods</h5><p>A Pod is the basic building block of Kubernetes--the smallest and simplest unit in the Kubernetes object model that you create or deploy. A Pod represents a running process on your cluster.</p><p>A Pod encapsulates an application container (or, in some cases, multiple containers), storage resources, a unique network IP, and options that govern how the container(s) should run. A Pod represents a unit of deployment: a single instance of an application in Kubernetes, which might consist of either a single container or a small number of containers that are tightly coupled and that share resources.</p><p><a href="https://www.docker.com/">Docker</a> is the most common container runtime used in a Kubernetes Pod, but Pods support other container runtimes as well.</p><p>Pods in a Kubernetes cluster can be used in two main ways:</p><ul><li><strong>Pods that run a single container</strong>. The &quot;one-container-per-Pod&quot; model is the most common Kubernetes use case; in this case, you can think of a Pod as a wrapper around a single container, and Kubernetes manages the Pods rather than the containers directly.</li><li><strong>Pods that run multiple containers that need to work together</strong>. A Pod might encapsulate an application composed of multiple co-located containers that are tightly coupled and need to share resources. These co-located containers might form a single cohesive unit of service--one container serving files from a shared volume to the public, while a separate &quot;sidecar&quot; container refreshes or updates those files. The Pod wraps these containers and storage resources together as a single manageable entity.</li><li>The <a href="http://blog.kubernetes.io/">Kubernetes Blog</a> has some additional information on Pod use cases. For more information, see:</li><li><a href="http://blog.kubernetes.io/2015/06/the-distributed-system-toolkit-patterns.html">The Distributed System Toolkit: Patterns for Composite Containers</a></li><li><a href="http://blog.kubernetes.io/2016/06/container-design-patterns.html">Container Design Patterns</a></li></ul><p>Each Pod is meant to run a single instance of a given application. If you want to scale your application horizontally (e.g., run multiple instances), you should use multiple Pods, one for each instance. In Kubernetes, this is generally referred to as replication. Replicated Pods are usually created and managed as a group by an abstraction called a Controller. See <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/#pods-and-controllers">Pods and Controllers</a> for more information.</p><h6><strong>How Pods manage multiple Containers</strong></h6><p>Pods are designed to support multiple cooperating processes (as containers) that form a cohesive unit of service. The containers in a Pod are automatically co-located and co-scheduled on the same physical or virtual machine in the cluster. The containers can share resources and dependencies, communicate with one another, and coordinate when and how they are terminated.</p><p>Note that grouping multiple co-located and co-managed containers in a single Pod is a relatively advanced use case. You should use this pattern only in specific instances in which your containers are tightly coupled. For example, you might have a container that acts as a web server for files in a shared volume, and a separate &quot;sidecar&quot; container that updates those files from a remote source, as in the following diagram:</p><p>Pods provide two kinds of shared resources for their constituent containers: networking and storage.</p><p><strong>Networking</strong></p><p>Each Pod is assigned a unique IP address. Every container in a Pod shares the network namespace, including the IP address and network ports. Containers inside a Pod can communicate with one another using <strong>localhost</strong>. When containers in a Pod communicate with entities outside the Pod, they must coordinate how they use the shared network resources (such as ports).</p><p><strong>Storage</strong></p><p>A Pod can specify a set of shared storage volumes. All containers in the Pod can access the shared volumes, allowing those containers to share data. Volumes also allow persistent data in a Pod to survive in case one of the containers within needs to be restarted. See <a href="https://kubernetes.io/docs/concepts/storage/volumes/">Volumes</a> for more information on how Kubernetes implements shared storage in a Pod.</p><h5><strong>Working with Pods</strong></h5><p>You&#x27;ll rarely create individual Pods directly in Kubernetes--even singleton Pods. This is because Pods are designed as relatively ephemeral, disposable entities. When a Pod gets created (directly by you, or indirectly by a Controller), it is scheduled to run on a Node in your cluster. The Pod remains on that Node until the process is terminated, the pod object is deleted, the pod is evicted for lack of resources, or the Node fails.</p><p><strong>Note:</strong> Restarting a container in a Pod should not be confused with restarting the Pod. The Pod itself does not run, but is an environment the containers run in and persists until it is deleted.</p><p>Pods do not, by themselves, self-heal. If a Pod is scheduled to a Node that fails, or if the scheduling operation itself fails, the Pod is deleted; likewise, a Pod won&#x27;t survive an eviction due to a lack of resources or Node maintenance. Kubernetes uses a higher-level abstraction, called a Controller, that handles the work of managing the relatively disposable Pod instances. Thus, while it is possible to use Pod directly, it&#x27;s far more common in Kubernetes to manage your pods using a Controller. See <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/#pods-and-controllers">Pods and Controllers</a> for more information on how Kubernetes uses Controllers to implement Pod scaling and healing.</p><h6><strong>Pods and Controllers</strong></h6><p>A Controller can create and manage multiple Pods for you, handling replication and rollout and providing self-healing capabilities at cluster scope. For example, if a Node fails, the Controller might automatically replace the Pod by scheduling an identical replacement on a different Node.</p><p>Some examples of Controllers that contain one or more pods include:</p><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployment</a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">StatefulSet</a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/">DaemonSet</a></li></ul><p>In general, Controllers use a Pod Template that you provide to create the Pods for which it is responsible.</p><h5><strong>Pod Templates</strong></h5><p>Pod templates are pod specifications which are included in other objects, such as <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/">Replication Controllers</a>, <a href="https://kubernetes.io/docs/concepts/jobs/run-to-completion-finite-workloads/">Jobs</a>, and <a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/">DaemonSets</a>. Controllers use Pod Templates to make actual pods. The sample below is a simple manifest for a Pod which contains a container that prints a message.</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: myapp-pod</strong></p><p><strong>labels:</strong></p><p><strong>app: myapp</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: myapp-container</strong></p><p><strong>image: busybox</strong></p><p><strong>command: <!-- -->[\&#x27;sh\&#x27;, \&#x27;-c\&#x27;, \&#x27;echo Hello Kubernetes! &amp;&amp; sleep 3600\&#x27;]</strong></p><p>Rather than specifying the current desired state of all replicas, pod templates are like cookie cutters. Once a cookie has been cut, the cookie has no relationship to the cutter. There is no &quot;quantum entanglement&quot;. Subsequent changes to the template or even switching to a new template has no direct effect on the pods already created. Similarly, pods created by a replication controller may subsequently be updated directly. This is in deliberate contrast to pods, which do specify the current desired state of all containers belonging to the pod. This approach radically simplifies system semantics and increases the flexibility of the primitive.</p><h5><strong>What&#x27;s next</strong></h5><ul><li>Learn more about Pod behavior:<ul><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods">Pod Termination</a></li><li>Other Pod Topics</li></ul></li></ul><h4>Pods</h4><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/#what-is-a-pod"><strong>What is a Pod?</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/#motivation-for-pods"><strong>Motivation for pods</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/#management"><strong>Management</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/#resource-sharing-and-communication"><strong>Resource sharing and communication</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/#uses-of-pods"><strong>Uses of pods</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/#alternatives-considered"><strong>Alternatives considered</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/#durability-of-pods-or-lack-thereof"><strong>Durability of pods (or lack thereof)</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods"><strong>Termination of Pods</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/#force-deletion-of-pods"><strong>Force deletion of pods</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/#privileged-mode-for-pod-containers"><strong>Privileged mode for pod containers</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/#api-object"><strong>API Object</strong></a></li></ul><p>Pods are the smallest deployable units of computing that can be created and managed in Kubernetes.</p><h5><strong>What is a Pod?</strong></h5><p>A pod (as in a pod of whales or pea pod) is a group of one or more containers (such as Docker containers), with shared storage/network, and a specification for how to run the containers. A pod&#x27;s contents are always co-located and co-scheduled, and run in a shared context. A pod models an application-specific &quot;logical host&quot; - it contains one or more application containers which are relatively tightly coupled --- in a pre-container world, they would have executed on the same physical or virtual machine.</p><p>While Kubernetes supports more container runtimes than just Docker, Docker is the most commonly known runtime, and it helps to describe pods in Docker terms.</p><p>The shared context of a pod is a set of Linux namespaces, cgroups, and potentially other facets of isolation - the same things that isolate a Docker container. Within a pod&#x27;s context, the individual applications may have further sub-isolations applied.</p><p>Containers within a pod share an IP address and port space, and can find each other via <strong>localhost</strong>. They can also communicate with each other using standard inter-process communications like SystemV semaphores or POSIX shared memory. Containers in different pods have distinct IP addresses and can not communicate by IPC without <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/">special configuration</a>. These containers usually communicate with each other via Pod IP addresses.</p><p>Applications within a pod also have access to shared volumes, which are defined as part of a pod and are made available to be mounted into each application&#x27;s filesystem.</p><p>In terms of <a href="https://www.docker.com/">Docker</a> constructs, a pod is modelled as a group of Docker containers with shared namespaces and shared <a href="https://kubernetes.io/docs/concepts/storage/volumes/">volumes</a>.</p><p>Like individual application containers, pods are considered to be relatively ephemeral (rather than durable) entities. As discussed in <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/">life of a pod</a>, pods are created, assigned a unique ID (UID), and scheduled to nodes where they remain until termination (according to restart policy) or deletion. If a node dies, the pods scheduled to that node are scheduled for deletion, after a timeout period. A given pod (as defined by a UID) is not &quot;rescheduled&quot; to a new node; instead, it can be replaced by an identical pod, with even the same name if desired, but with a new UID (see <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/">replication controller</a> for more details).</p><p>When something is said to have the same lifetime as a pod, such as a volume, that means that it exists as long as that pod (with that UID) exists. If that pod is deleted for any reason, even if an identical replacement is created, the related thing (e.g. volume) is also destroyed and created anew.</p><p>A multi-container pod that contains a file puller and a web server that uses a persistent volume for shared storage between the containers.</p><h5><strong>Motivation for pods</strong></h5><h6><strong>Management</strong></h6><p>Pods are a model of the pattern of multiple cooperating processes which form a cohesive unit of service. They simplify application deployment and management by providing a higher-level abstraction than the set of their constituent applications. Pods serve as unit of deployment, horizontal scaling, and replication. Colocation (co-scheduling), shared fate (e.g. termination), coordinated replication, resource sharing, and dependency management are handled automatically for containers in a pod.</p><h6><strong>Resource sharing and communication</strong></h6><p>Pods enable data sharing and communication among their constituents.</p><p>The applications in a pod all use the same network namespace (same IP and port space), and can thus &quot;find&quot; each other and communicate using <strong>localhost</strong>. Because of this, applications in a pod must coordinate their usage of ports. Each pod has an IP address in a flat shared networking space that has full communication with other physical computers and pods across the network.</p><p>The hostname is set to the pod&#x27;s Name for the application containers within the pod. <a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/">More details on networking</a>.</p><p>In addition to defining the application containers that run in the pod, the pod specifies a set of shared storage volumes. Volumes enable data to survive container restarts and to be shared among the applications within the pod.</p><h5><strong>Uses of pods</strong></h5><p>Pods can be used to host vertically integrated application stacks (e.g. LAMP), but their primary motivation is to support co-located, co-managed helper programs, such as:</p><ul><li>content management systems, file and data loaders, local cache managers, etc.</li><li>log and checkpoint backup, compression, rotation, snapshotting, etc.</li><li>data change watchers, log tailers, logging and monitoring adapters, event publishers, etc.</li><li>proxies, bridges, and adapters</li><li>controllers, managers, configurators, and updaters</li></ul><p>Individual pods are not intended to run multiple instances of the same application, in general.</p><p>For a longer explanation, see <a href="http://blog.kubernetes.io/2015/06/the-distributed-system-toolkit-patterns.html">The Distributed System ToolKit: Patterns for Composite Containers</a>.</p><h5><strong>Alternatives considered</strong></h5><p>Why not just run multiple programs in a single (Docker) container?</p><ol><li>Transparency. Making the containers within the pod visible to the infrastructure enables the infrastructure to provide services to those containers, such as process management and resource monitoring. This facilitates a number of conveniences for users.</li><li>Decoupling software dependencies. The individual containers may be versioned, rebuilt and redeployed independently. Kubernetes may even support live updates of individual containers someday.</li><li>Ease of use. Users don&#x27;t need to run their own process managers, worry about signal and exit-code propagation, etc.</li><li>Efficiency. Because the infrastructure takes on more responsibility, containers can be lighter weight.</li></ol><p>Why not support affinity-based co-scheduling of containers?</p><p>That approach would provide co-location, but would not provide most of the benefits of pods, such as resource sharing, IPC, guaranteed fate sharing, and simplified management.</p><h5><strong>Durability of pods (or lack thereof)</strong></h5><p>Pods aren&#x27;t intended to be treated as durable entities. They won&#x27;t survive scheduling failures, node failures, or other evictions, such as due to lack of resources, or in the case of node maintenance.</p><p>In general, users shouldn&#x27;t need to create pods directly. They should almost always use controllers even for singletons, for example, <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployments</a>). Controllers provide self-healing with a cluster scope, as well as replication and rollout management. Controllers like <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset.md">StatefulSet</a> can also provide support to stateful pods.</p><p>The use of collective APIs as the primary user-facing primitive is relatively common among cluster scheduling systems, including <a href="https://research.google.com/pubs/pub43438.html">Borg</a>, <a href="https://mesosphere.github.io/marathon/docs/rest-api.html">Marathon</a>, <a href="http://aurora.apache.org/documentation/latest/reference/configuration/#job-schema">Aurora</a>, and <a href="http://www.slideshare.net/Docker/aravindnarayanan-facebook140613153626phpapp02-37588997">Tupperware</a>.</p><p>Pod is exposed as a primitive in order to facilitate:</p><ul><li>scheduler and controller pluggability</li><li>support for pod-level operations without the need to &quot;proxy&quot; them via controller APIs</li><li>decoupling of pod lifetime from controller lifetime, such as for bootstrapping</li><li>decoupling of controllers and services --- the endpoint controller just watches pods</li><li>clean composition of Kubelet-level functionality with cluster-level functionality --- Kubelet is effectively the &quot;pod controller&quot;</li><li>high-availability applications, which will expect pods to be replaced in advance of their termination and certainly in advance of deletion, such as in the case of planned evictions or image prefetching.</li></ul><h5><strong>Termination of Pods</strong></h5><p>Because pods represent running processes on nodes in the cluster, it is important to allow those processes to gracefully terminate when they are no longer needed (vs being violently killed with a KILL signal and having no chance to clean up). Users should be able to request deletion and know when processes terminate, but also be able to ensure that deletes eventually complete. When a user requests deletion of a pod the system records the intended grace period before the pod is allowed to be forcefully killed, and a TERM signal is sent to the main process in each container. Once the grace period has expired the KILL signal is sent to those processes and the pod is then deleted from the API server. If the Kubelet or the container manager is restarted while waiting for processes to terminate, the termination will be retried with the full grace period.</p><p>An example flow:</p><ol><li>User sends command to delete Pod, with default grace period (30s)</li><li>The Pod in the API server is updated with the time beyond which the Pod is considered &quot;dead&quot; along with the grace period.</li><li>Pod shows up as &quot;Terminating&quot; when listed in client commands</li><li>(simultaneous with 3) When the Kubelet sees that a Pod has been marked as terminating because the time in 2 has been set, it begins the pod shutdown process.<ol><li>If the pod has defined a <a href="https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#hook-details">preStop hook</a>, it is invoked inside of the pod. If the <strong>preStop</strong> hook is still running after the grace period expires, step 2 is then invoked with a small (2 second) extended grace period.</li><li>The processes in the Pod are sent the TERM signal.</li></ol></li><li>(simultaneous with 3) Pod is removed from endpoints list for service, and are no longer considered part of the set of running pods for replication controllers. Pods that shutdown slowly can continue to serve traffic as load balancers (like the service proxy) remove them from their rotations.</li><li>When the grace period expires, any processes still running in the Pod are killed with SIGKILL.</li><li>The Kubelet will finish deleting the Pod on the API server by setting grace period 0 (immediate deletion). The Pod disappears from the API and is no longer visible from the client.</li></ol><p>By default, all deletes are graceful within 30 seconds. The <strong>kubectl delete</strong> command supports the <strong>--grace-period=<code>&lt;seconds&gt; option which allows a user to override the default and specify their own value. The value 0 [force deletes](https://kubernetes.io/docs/concepts/workloads/pods/pod/#force-deletion-of-pods) the pod. In kubectl version &gt;</code>= 1.5, you must specify an additional flag </strong>--force<strong> along with </strong>--grace-period=0** in order to perform force deletions.</p><h6><strong>Force deletion of pods</strong></h6><p>Force deletion of a pod is defined as deletion of a pod from the cluster state and etcd immediately. When a force deletion is performed, the apiserver does not wait for confirmation from the kubelet that the pod has been terminated on the node it was running on. It removes the pod in the API immediately so a new pod can be created with the same name. On the node, pods that are set to terminate immediately will still be given a small grace period before being force killed.</p><p>Force deletions can be potentially dangerous for some pods and should be performed with caution. In case of StatefulSet pods, please refer to the task documentation for <a href="https://kubernetes.io/docs/tasks/run-application/force-delete-stateful-set-pod/">deleting Pods from a StatefulSet</a>.</p><h5><strong>Privileged mode for pod containers</strong></h5><p>From Kubernetes v1.1, any container in a pod can enable privileged mode, using the <strong>privileged</strong>flag on the <strong>SecurityContext</strong> of the container spec. This is useful for containers that want to use linux capabilities like manipulating the network stack and accessing devices. Processes within the container get almost the same privileges that are available to processes outside a container. With privileged mode, it should be easier to write network and volume plugins as separate pods that don&#x27;t need to be compiled into the kubelet.</p><p>If the master is running Kubernetes v1.1 or higher, and the nodes are running a version lower than v1.1, then new privileged pods will be accepted by api-server, but will not be launched. They will be pending state. If user calls <strong>kubectl describe pod FooPodName</strong>, user can see the reason why the pod is in pending state. The events table in the describe command output will say: <strong>Error validating pod &quot;FooPodName&quot;.&quot;FooPodNamespace&quot; from api, ignoring: spec.containers<!-- -->[0]<!-- -->.securityContext.privileged: forbidden \&#x27;<code>&lt;\*&gt;</code>(0xc2089d3248)true\&#x27;</strong></p><p>If the master is running a version lower than v1.1, then privileged pods cannot be created. If user attempts to create a pod, that has a privileged container, the user will get the following error: <strong>The Pod &quot;FooPodName&quot; is invalid. spec.containers<!-- -->[0]<!-- -->.securityContext.privileged: forbidden \&#x27;<code>&lt;\*&gt;</code>(0xc20b222db0)true\&#x27;</strong></p><h5><strong>API Object</strong></h5><p>Pod is a top-level resource in the Kubernetes REST API. More details about the API object can be found at: <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#pod-v1-core">Pod API object</a>.</p><h4>Pod Lifecycle</h4><p>This page describes the lifecycle of a Pod.</p><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase"><strong>Pod phase</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-conditions"><strong>Pod conditions</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes"><strong>Container probes</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#when-should-you-use-liveness-or-readiness-probes"><strong>When should you use liveness or readiness probes?</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-and-container-status"><strong>Pod and Container status</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy"><strong>Restart policy</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-lifetime"><strong>Pod lifetime</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#examples"><strong>Examples</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#advanced-liveness-probe-example"><strong>Advanced liveness probe example</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#example-states"><strong>Example states</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h5><strong>Pod phase</strong></h5><p>A Pod&#x27;s <strong>status</strong> field is a <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#podstatus-v1-core">PodStatus</a> object, which has a <strong>phase</strong> field.</p><p>The phase of a Pod is a simple, high-level summary of where the Pod is in its lifecycle. The phase is not intended to be a comprehensive rollup of observations of Container or Pod state, nor is it intended to be a comprehensive state machine.</p><p>The number and meanings of Pod phase values are tightly guarded. Other than what is documented here, nothing should be assumed about Pods that have a given <strong>phase</strong> value.</p><p>Here are the possible values for <strong>phase</strong>:</p><ul><li>Pending: The Pod has been accepted by the Kubernetes system, but one or more of the Container images has not been created. This includes time before being scheduled as well as time spent downloading images over the network, which could take a while.</li><li>Running: The Pod has been bound to a node, and all of the Containers have been created. At least one Container is still running, or is in the process of starting or restarting.</li><li>Succeeded: All Containers in the Pod have terminated in success, and will not be restarted.</li><li>Failed: All Containers in the Pod have terminated, and at least one Container has terminated in failure. That is, the Container either exited with non-zero status or was terminated by the system.</li><li>Unknown: For some reason the state of the Pod could not be obtained, typically due to an error in communicating with the host of the Pod.</li></ul><h5><strong>Pod conditions</strong></h5><p>A Pod has a PodStatus, which has an array of <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#podcondition-v1-core">PodConditions</a>. Each element of the PodCondition array has a <strong>type</strong> field and a <strong>status</strong> field. The <strong>type</strong> field is a string, with possible values PodScheduled, Ready, Initialized, and Unschedulable. The <strong>status</strong> field is a string, with possible values True, False, and Unknown.</p><h5><strong>Container probes</strong></h5><p>A <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#probe-v1-core">Probe</a> is a diagnostic performed periodically by the <a href="https://kubernetes.io/docs/admin/kubelet/">kubelet</a> on a Container. To perform a diagnostic, the kubelet calls a <a href="https://godoc.org/k8s.io/kubernetes/pkg/api/v1#Handler">Handler</a> implemented by the Container. There are three types of handlers:</p><ul><li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#execaction-v1-core">ExecAction</a>: Executes a specified command inside the Container. The diagnostic is considered successful if the command exits with a status code of 0.</li><li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#tcpsocketaction-v1-core">TCPSocketAction</a>: Performs a TCP check against the Container&#x27;s IP address on a specified port. The diagnostic is considered successful if the port is open.</li><li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#httpgetaction-v1-core">HTTPGetAction</a>: Performs an HTTP Get request against the Container&#x27;s IP address on a specified port and path. The diagnostic is considered successful if the response has a status code greater than or equal to 200 and less than 400.</li></ul><p>Each probe has one of three results:</p><ul><li>Success: The Container passed the diagnostic.</li><li>Failure: The Container failed the diagnostic.</li><li>Unknown: The diagnostic failed, so no action should be taken.</li></ul><p>The kubelet can optionally perform and react to two kinds of probes on running Containers:</p><ul><li><strong>livenessProbe</strong>: Indicates whether the Container is running. If the liveness probe fails, the kubelet kills the Container, and the Container is subjected to its <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy">restart policy</a>. If a Container does not provide a liveness probe, the default state is <strong>Success</strong>.</li><li><strong>readinessProbe</strong>: Indicates whether the Container is ready to service requests. If the readiness probe fails, the endpoints controller removes the Pod&#x27;s IP address from the endpoints of all Services that match the Pod. The default state of readiness before the initial delay is <strong>Failure</strong>. If a Container does not provide a readiness probe, the default state is <strong>Success</strong>.</li></ul><h6><strong>When should you use liveness or readiness probes?</strong></h6><p>If the process in your Container is able to crash on its own whenever it encounters an issue or becomes unhealthy, you do not necessarily need a liveness probe; the kubelet will automatically perform the correct action in accordance with the Pod&#x27;s <strong>restartPolicy</strong>.</p><p>If you&#x27;d like your Container to be killed and restarted if a probe fails, then specify a liveness probe, and specify a <strong>restartPolicy</strong> of Always or OnFailure.</p><p>If you&#x27;d like to start sending traffic to a Pod only when a probe succeeds, specify a readiness probe. In this case, the readiness probe might be the same as the liveness probe, but the existence of the readiness probe in the spec means that the Pod will start without receiving any traffic and only start receiving traffic after the probe starts succeeding.</p><p>If you want your Container to be able to take itself down for maintenance, you can specify a readiness probe that checks an endpoint specific to readiness that is different from the liveness probe.</p><p>Note that if you just want to be able to drain requests when the Pod is deleted, you do not necessarily need a readiness probe; on deletion, the Pod automatically puts itself into an unready state regardless of whether the readiness probe exists. The Pod remains in the unready state while it waits for the Containers in the Pod to stop.</p><h5><strong>Pod and Container status</strong></h5><p>For detailed information about Pod Container status, see <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#podstatus-v1-core">PodStatus</a> and <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#containerstatus-v1-core">ContainerStatus</a>. Note that the information reported as Pod status depends on the current <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#containerstatus-v1-core">ContainerState</a>.</p><h5><strong>Restart policy</strong></h5><p>A PodSpec has a <strong>restartPolicy</strong> field with possible values Always, OnFailure, and Never. The default value is Always. <strong>restartPolicy</strong> applies to all Containers in the Pod. <strong>restartPolicy</strong> only refers to restarts of the Containers by the kubelet on the same node. Failed Containers that are restarted by the kubelet are restarted with an exponential back-off delay (10s, 20s, 40s ...) capped at five minutes, and is reset after ten minutes of successful execution. As discussed in the <a href="https://kubernetes.io/docs/user-guide/pods/#durability-of-pods-or-lack-thereof">Pods document</a>, once bound to a node, a Pod will never be rebound to another node.</p><h5><strong>Pod lifetime</strong></h5><p>In general, Pods do not disappear until someone destroys them. This might be a human or a controller. The only exception to this rule is that Pods with a <strong>phase</strong> of Succeeded or Failed for more than some duration (determined by the master) will expire and be automatically destroyed.</p><p>Three types of controllers are available:</p><ul><li>Use a <a href="https://kubernetes.io/docs/concepts/jobs/run-to-completion-finite-workloads/">Job</a> for Pods that are expected to terminate, for example, batch computations. Jobs are appropriate only for Pods with <strong>restartPolicy</strong> equal to OnFailure or Never.</li><li>Use a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/">ReplicationController</a>, <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/">ReplicaSet</a>, or <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployment</a> for Pods that are not expected to terminate, for example, web servers. ReplicationControllers are appropriate only for Pods with a <strong>restartPolicy</strong> of Always.</li><li>Use a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/">DaemonSet</a> for Pods that need to run one per machine, because they provide a machine-specific system service.</li></ul><p>All three types of controllers contain a PodTemplate. It is recommended to create the appropriate controller and let it create Pods, rather than directly create Pods yourself. That is because Pods alone are not resilient to machine failures, but controllers are.</p><p>If a node dies or is disconnected from the rest of the cluster, Kubernetes applies a policy for setting the <strong>phase</strong> of all Pods on the lost node to Failed.</p><h5><strong>Examples</strong></h5><h6><strong>Advanced liveness probe example</strong></h6><p>Liveness probes are executed by the kubelet, so all requests are made in the kubelet network namespace.</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>labels:</strong></p><p><strong>test: liveness</strong></p><p><strong>name: liveness-http</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- args:</strong></p><p><strong>- /server</strong></p><p><strong>image: k8s.gcr.io/liveness</strong></p><p><strong>livenessProbe:</strong></p><p><strong>httpGet:</strong></p><p><strong><em># when &quot;host&quot; is not defined, &quot;PodIP&quot; will be used</em></strong></p><p><strong><em># host: my-host</em></strong></p><p><strong><em># when &quot;scheme&quot; is not defined, &quot;HTTP&quot; scheme will be used. Only &quot;HTTP&quot; and &quot;HTTPS&quot; are allowed</em></strong></p><p><strong><em># scheme: HTTPS</em></strong></p><p><strong>path: /healthz</strong></p><p><strong>port: 8080</strong></p><p><strong>httpHeaders:</strong></p><p><strong>- name: X-Custom-Header</strong></p><p><strong>value: Awesome</strong></p><p><strong>initialDelaySeconds: 15</strong></p><p><strong>timeoutSeconds: 1</strong></p><p><strong>name: liveness</strong></p><h6><strong>Example states</strong></h6><ul><li>Pod is running and has one Container. Container exits with success.<ul><li>Log completion event.</li><li>If <strong>restartPolicy</strong> is:<ul><li>Always: Restart Container; Pod <strong>phase</strong> stays Running.</li><li>OnFailure: Pod <strong>phase</strong> becomes Succeeded.</li><li>Never: Pod <strong>phase</strong> becomes Succeeded.</li></ul></li></ul></li><li>Pod is running and has one Container. Container exits with failure.<ul><li>Log failure event.</li><li>If <strong>restartPolicy</strong> is:<ul><li>Always: Restart Container; Pod <strong>phase</strong> stays Running.</li><li>OnFailure: Restart Container; Pod <strong>phase</strong> stays Running.</li><li>Never: Pod <strong>phase</strong> becomes Failed.</li></ul></li></ul></li><li>Pod is running and has two Containers. Container 1 exits with failure.<ul><li>Log failure event.</li><li>If <strong>restartPolicy</strong> is:<ul><li>Always: Restart Container; Pod <strong>phase</strong> stays Running.</li><li>OnFailure: Restart Container; Pod <strong>phase</strong> stays Running.</li><li>Never: Do not restart Container; Pod <strong>phase</strong> stays Running.</li></ul></li><li>If Container 1 is not running, and Container 2 exits:<ul><li>Log failure event.</li><li>If <strong>restartPolicy</strong> is:<ul><li>Always: Restart Container; Pod <strong>phase</strong> stays Running.</li><li>OnFailure: Restart Container; Pod <strong>phase</strong> stays Running.</li><li>Never: Pod <strong>phase</strong> becomes Failed.</li></ul></li></ul></li></ul></li><li>Pod is running and has one Container. Container runs out of memory.<ul><li>Container terminates in failure.</li><li>Log OOM event.</li><li>If <strong>restartPolicy</strong> is:<ul><li>Always: Restart Container; Pod <strong>phase</strong> stays Running.</li><li>OnFailure: Restart Container; Pod <strong>phase</strong> stays Running.</li><li>Never: Log failure event; Pod <strong>phase</strong> becomes Failed.</li></ul></li></ul></li><li>Pod is running, and a disk dies.<ul><li>Kill all Containers.</li><li>Log appropriate event.</li><li>Pod <strong>phase</strong> becomes Failed.</li><li>If running under a controller, Pod is recreated elsewhere.</li></ul></li><li>Pod is running, and its node is segmented out.<ul><li>Node controller waits for timeout.</li><li>Node controller sets Pod <strong>phase</strong> to Failed.</li><li>If running under a controller, Pod is recreated elsewhere.</li></ul></li></ul><h5><strong>What&#x27;s next</strong></h5><ul><li>Get hands-on experience <a href="https://kubernetes.io/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/">attaching handlers to Container lifecycle events</a>.</li><li>Get hands-on experience <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/">configuring liveness and readiness probes</a>.</li><li>Learn more about <a href="https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/">Container lifecycle hooks</a>.</li></ul><h4>Init Containers</h4><p>This feature has exited beta in 1.6. Init Containers can be specified in the PodSpec alongside the app <strong>containers</strong> array. The beta annotation value will still be respected and overrides the PodSpec field value, however, they are deprecated in 1.6 and 1.7. In 1.8, the annotations are no longer supported and must be converted to the PodSpec field.</p><p>This page provides an overview of Init Containers, which are specialized Containers that run before app Containers and can contain utilities or setup scripts not present in an app image.</p><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/#understanding-init-containers"><strong>Understanding Init Containers</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/#differences-from-regular-containers"><strong>Differences from regular Containers</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/#what-can-init-containers-be-used-for"><strong>What can Init Containers be used for?</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/#examples"><strong>Examples</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/#init-containers-in-use"><strong>Init Containers in use</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/#detailed-behavior"><strong>Detailed behavior</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/#resources"><strong>Resources</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/#pod-restart-reasons"><strong>Pod restart reasons</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/#support-and-compatibility"><strong>Support and compatibility</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h5><strong>Understanding Init Containers</strong></h5><p>A <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/">Pod</a> can have multiple Containers running apps within it, but it can also have one or more Init Containers, which are run before the app Containers are started.</p><p>Init Containers are exactly like regular Containers, except:</p><ul><li>They always run to completion.</li><li>Each one must complete successfully before the next one is started.</li></ul><p>If an Init Container fails for a Pod, Kubernetes restarts the Pod repeatedly until the Init Container succeeds. However, if the Pod has a <strong>restartPolicy</strong> of Never, it is not restarted.</p><p>To specify a Container as an Init Container, add the <strong>initContainers</strong> field on the PodSpec as a JSON array of objects of type <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#container-v1-core">Container</a> alongside the app <strong>containers</strong> array. The status of the init containers is returned in <strong>status.initContainerStatuses</strong> field as an array of the container statuses (similar to the <strong>status.containerStatuses</strong> field).</p><h6><strong>Differences from regular Containers</strong></h6><p>Init Containers support all the fields and features of app Containers, including resource limits, volumes, and security settings. However, the resource requests and limits for an Init Container are handled slightly differently, which are documented in <a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/#resources">Resources</a> below. Also, Init Containers do not support readiness probes because they must run to completion before the Pod can be ready.</p><p>If multiple Init Containers are specified for a Pod, those Containers are run one at a time in sequential order. Each must succeed before the next can run. When all of the Init Containers have run to completion, Kubernetes initializes the Pod and runs the application Containers as usual.</p><h5><strong>What can Init Containers be used for?</strong></h5><p>Because Init Containers have separate images from app Containers, they have some advantages for start-up related code:</p><ul><li>They can contain and run utilities that are not desirable to include in the app Container image for security reasons.</li><li>They can contain utilities or custom code for setup that is not present in an app image. For example, there is no need to make an image <strong>FROM</strong> another image just to use a tool like <strong>sed</strong>, <strong>awk</strong>, <strong>python</strong>, or <strong>dig</strong> during setup.</li><li>The application image builder and deployer roles can work independently without the need to jointly build a single app image.</li><li>They use Linux namespaces so that they have different filesystem views from app Containers. Consequently, they can be given access to Secrets that app Containers are not able to access.</li><li>They run to completion before any app Containers start, whereas app Containers run in parallel, so Init Containers provide an easy way to block or delay the startup of app Containers until some set of preconditions are met.</li></ul><h6><strong>Examples</strong></h6><p>Here are some ideas for how to use Init Containers:</p><ul><li>Wait for a service to be created with a shell command like:</li><li><strong>for i in {1..100}; do sleep 1; if dig myservice; then exit 0; fi; done; exit 1</strong></li><li>Register this Pod with a remote server from the downward API with a command like:</li><li><strong>curl -X POST http://$MANAGEMENT_SERVICE_HOST:$MANAGEMENT_SERVICE_PORT/register -d \&#x27;instance=$(<code>&lt;POD_NAME&gt;)&amp;ip=$(&lt;POD_IP&gt;</code>)\&#x27;</strong></li><li>Wait for some time before starting the app Container with a command like <strong>sleep 60</strong>.</li><li>Clone a git repository into a volume.</li><li>Place values into a configuration file and run a template tool to dynamically generate a configuration file for the main app Container. For example, place the POD_IP value in a configuration and generate the main app configuration file using Jinja.</li></ul><p>More detailed usage examples can be found in the <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">StatefulSets documentation</a> and the <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-initialization/">Production Pods guide</a>.</p><h6><strong>Init Containers in use</strong></h6><p>The following yaml file for Kubernetes 1.5 outlines a simple Pod which has two Init Containers. The first waits for <strong>myservice</strong> and the second waits for <strong>mydb</strong>. Once both containers complete, the Pod will begin.</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: myapp-pod</strong></p><p><strong>labels:</strong></p><p><strong>app: myapp</strong></p><p><strong>annotations:</strong></p><p><strong>pod.beta.kubernetes.io/init-containers: \&#x27;[</strong></p><p><strong>{</strong></p><p><strong>&quot;name&quot;: &quot;init-myservice&quot;,</strong></p><p><strong>&quot;image&quot;: &quot;busybox&quot;,</strong></p><p><strong>&quot;command&quot;: <!-- -->[&quot;sh&quot;, &quot;-c&quot;, &quot;until nslookup myservice; do echo waiting for myservice; sleep 2; done;&quot;]</strong></p><p><strong>},</strong></p><p><strong>{</strong></p><p><strong>&quot;name&quot;: &quot;init-mydb&quot;,</strong></p><p><strong>&quot;image&quot;: &quot;busybox&quot;,</strong></p><p><strong>&quot;command&quot;: <!-- -->[&quot;sh&quot;, &quot;-c&quot;, &quot;until nslookup mydb; do echo waiting for mydb; sleep 2; done;&quot;]</strong></p><p><strong>}</strong></p><p><strong>]\&#x27;</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: myapp-container</strong></p><p><strong>image: busybox</strong></p><p><strong>command: <!-- -->[\&#x27;sh\&#x27;, \&#x27;-c\&#x27;, \&#x27;echo The app is running! &amp;&amp; sleep 3600\&#x27;]</strong></p><p>There is a new syntax in Kubernetes 1.6, although the old annotation syntax still works for 1.6 and 1.7. The new syntax must be used for 1.8 or greater. We have moved the declaration of Init Containers to <strong>spec</strong>:</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: myapp-pod</strong></p><p><strong>labels:</strong></p><p><strong>app: myapp</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: myapp-container</strong></p><p><strong>image: busybox</strong></p><p><strong>command: <!-- -->[\&#x27;sh\&#x27;, \&#x27;-c\&#x27;, \&#x27;echo The app is running! &amp;&amp; sleep 3600\&#x27;]</strong></p><p><strong>initContainers:</strong></p><p><strong>- name: init-myservice</strong></p><p><strong>image: busybox</strong></p><p><strong>command: <!-- -->[\&#x27;sh\&#x27;, \&#x27;-c\&#x27;, \&#x27;until nslookup myservice; do echo waiting for myservice; sleep 2; done;\&#x27;]</strong></p><p><strong>- name: init-mydb</strong></p><p><strong>image: busybox</strong></p><p><strong>command: <!-- -->[\&#x27;sh\&#x27;, \&#x27;-c\&#x27;, \&#x27;until nslookup mydb; do echo waiting for mydb; sleep 2; done;\&#x27;]</strong></p><p>1.5 syntax still works on 1.6, but we recommend using 1.6 syntax. In Kubernetes 1.6, Init Containers were made a field in the API. The beta annotation is still respected in 1.6 and 1.7, but is not supported in 1.8 or greater.</p><p>Yaml file below outlines the <strong>mydb</strong> and <strong>myservice</strong> services:</p><p><strong>kind: Service</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>metadata:</strong></p><p><strong>name: myservice</strong></p><p><strong>spec:</strong></p><p><strong>ports:</strong></p><p><strong>- protocol: TCP</strong></p><p><strong>port: 80</strong></p><p><strong>targetPort: 9376</strong></p><p><strong>---</strong></p><p><strong>kind: Service</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>metadata:</strong></p><p><strong>name: mydb</strong></p><p><strong>spec:</strong></p><p><strong>ports:</strong></p><p><strong>- protocol: TCP</strong></p><p><strong>port: 80</strong></p><p><strong>targetPort: 9377</strong></p><p>This Pod can be started and debugged with the following commands:</p><p><strong>$ kubectl create -f myapp.yaml</strong></p><p><strong>pod &quot;myapp-pod&quot; created</strong></p><p><strong>$ kubectl get -f myapp.yaml</strong></p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>myapp-pod 0/1 Init:0/2 0 6m</strong></p><p><strong>$ kubectl describe -f myapp.yaml</strong></p><p><strong>Name: myapp-pod</strong></p><p><strong>Namespace: default</strong></p><p><strong>[.<!-- -->..]</strong></p><p><strong>Labels: app=myapp</strong></p><p><strong>Status: Pending</strong></p><p><strong>[.<!-- -->..]</strong></p><p><strong>Init Containers:</strong></p><p><strong>init-myservice:</strong></p><p><strong>[.<!-- -->..]</strong></p><p><strong>State: Running</strong></p><p><strong>[.<!-- -->..]</strong></p><p><strong>init-mydb:</strong></p><p><strong>[.<!-- -->..]</strong></p><p><strong>State: Waiting</strong></p><p><strong>Reason: PodInitializing</strong></p><p><strong>Ready: False</strong></p><p><strong>[.<!-- -->..]</strong></p><p><strong>Containers:</strong></p><p><strong>myapp-container:</strong></p><p><strong>[.<!-- -->..]</strong></p><p><strong>State: Waiting</strong></p><p><strong>Reason: PodInitializing</strong></p><p><strong>Ready: False</strong></p><p><strong>[.<!-- -->..]</strong></p><p><strong>Events:</strong></p><p><strong>FirstSeen LastSeen Count From SubObjectPath Type Reason Message</strong></p><p><strong>--------- -------- ----- ---- ------------- -------- ------ -------</strong></p><p><strong>16s 16s 1 {default-scheduler } Normal Scheduled Successfully assigned myapp-pod to 172.17.4.201</strong></p><p><strong>16s 16s 1 {kubelet 172.17.4.201} spec.initContainers{init-myservice} Normal Pulling pulling image &quot;busybox&quot;</strong></p><p><strong>13s 13s 1 {kubelet 172.17.4.201} spec.initContainers{init-myservice} Normal Pulled Successfully pulled image &quot;busybox&quot;</strong></p><p><strong>13s 13s 1 {kubelet 172.17.4.201} spec.initContainers{init-myservice} Normal Created Created container with docker id 5ced34a04634; Security:<!-- -->[seccomp=unconfined]</strong></p><p><strong>13s 13s 1 {kubelet 172.17.4.201} spec.initContainers{init-myservice} Normal Started Started container with docker id 5ced34a04634</strong></p><p><strong>$ kubectl logs myapp-pod -c init-myservice <em># Inspect the first init container</em></strong></p><p><strong>$ kubectl logs myapp-pod -c init-mydb <em># Inspect the second init container</em></strong></p><p>Once we start the <strong>mydb</strong> and <strong>myservice</strong> services, we can see the Init Containers complete and the <strong>myapp-pod</strong> is created:</p><p><strong>$ kubectl create -f services.yaml</strong></p><p><strong>service &quot;myservice&quot; created</strong></p><p><strong>service &quot;mydb&quot; created</strong></p><p><strong>$ kubectl get -f myapp.yaml</strong></p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>myapp-pod 1/1 Running 0 9m</strong></p><p>This example is very simple but should provide some inspiration for you to create your own Init Containers.</p><h5><strong>Detailed behavior</strong></h5><p>During the startup of a Pod, the Init Containers are started in order, after the network and volumes are initialized. Each Container must exit successfully before the next is started. If a Container fails to start due to the runtime or exits with failure, it is retried according to the Pod <strong>restartPolicy</strong>. However, if the Pod <strong>restartPolicy</strong> is set to Always, the Init Containers use <strong>RestartPolicy</strong>OnFailure.</p><p>A Pod cannot be <strong>Ready</strong> until all Init Containers have succeeded. The ports on an Init Container are not aggregated under a service. A Pod that is initializing is in the <strong>Pending</strong> state but should have a condition <strong>Initializing</strong> set to true.</p><p>If the Pod is <a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/#pod-restart-reasons">restarted</a>, all Init Containers must execute again.</p><p>Changes to the Init Container spec are limited to the container image field. Altering an Init Container image field is equivalent to restarting the Pod.</p><p>Because Init Containers can be restarted, retried, or re-executed, Init Container code should be idempotent. In particular, code that writes to files on <strong>EmptyDirs</strong> should be prepared for the possibility that an output file already exists.</p><p>Init Containers have all of the fields of an app Container. However, Kubernetes prohibits <strong>readinessProbe</strong> from being used because Init Containers cannot define readiness distinct from completion. This is enforced during validation.</p><p>Use <strong>activeDeadlineSeconds</strong> on the Pod and <strong>livenessProbe</strong> on the Container to prevent Init Containers from failing forever. The active deadline includes Init Containers.</p><p>The name of each app and Init Container in a Pod must be unique; a validation error is thrown for any Container sharing a name with another.</p><h6><strong>Resources</strong></h6><p>Given the ordering and execution for Init Containers, the following rules for resource usage apply:</p><ul><li>The highest of any particular resource request or limit defined on all Init Containers is the effective init request/limit</li><li>The Pod&#x27;s effective request/limit for a resource is the higher of:<ul><li>the sum of all app Containers request/limit for a resource</li><li>the effective init request/limit for a resource</li></ul></li><li>Scheduling is done based on effective requests/limits, which means Init Containers can reserve resources for initialization that are not used during the life of the Pod.</li><li>QoS tier of the Pod&#x27;s effective QoS tier is the QoS tier for Init Containers and app containers alike.</li></ul><p>Quota and limits are applied based on the effective Pod request and limit.</p><p>Pod level cgroups are based on the effective Pod request and limit, the same as the scheduler.</p><h6><strong>Pod restart reasons</strong></h6><p>A Pod can restart, causing re-execution of Init Containers, for the following reasons:</p><ul><li>A user updates the PodSpec causing the Init Container image to change. App Container image changes only restart the app Container.</li><li>The Pod infrastructure container is restarted. This is uncommon and would have to be done by someone with root access to nodes.</li><li>All containers in a Pod are terminated while <strong>restartPolicy</strong> is set to Always, forcing a restart, and the Init Container completion record has been lost due to garbage collection.</li></ul><h5><strong>Support and compatibility</strong></h5><p>A cluster with Apiserver version 1.6.0 or greater supports Init Containers using the <strong>spec.initContainers</strong> field. Previous versions support Init Containers using the alpha or beta annotations. The <strong>spec.initContainers</strong> field is also mirrored into alpha and beta annotations so that Kubelets version 1.3.0 or greater can execute Init Containers, and so that a version 1.6 apiserver can safely be rolled back to version 1.5.x without losing Init Container functionality for existing created pods.</p><p>In Apiserver and Kubelet versions 1.8.0 or greater, support for the alpha and beta annotations is removed, requiring a conversion from the deprecated annotations to the <strong>spec.initContainers</strong>field.</p><h5><strong>What&#x27;s next</strong></h5><ul><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-initialization/#creating-a-pod-that-has-an-init-container">Creating a Pod that has an Init Container</a></li></ul><h4>Pod Preset</h4><p>This page provides an overview of PodPresets, which are objects for injecting certain information into pods at creation time. The information can include secrets, volumes, volume mounts, and environment variables.</p><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/podpreset/#understanding-pod-presets"><strong>Understanding Pod Presets</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/podpreset/#how-it-works"><strong>How It Works</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/podpreset/#disable-pod-preset-for-a-specific-pod"><strong>Disable Pod Preset for a Specific Pod</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/podpreset/#enable-pod-preset"><strong>Enable Pod Preset</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/podpreset/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h5><strong>Understanding Pod Presets</strong></h5><p>A <strong>Pod Preset</strong> is an API resource for injecting additional runtime requirements into a Pod at creation time. You use <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors">label selectors</a> to specify the Pods to which a given Pod Preset applies.</p><p>Using a Pod Preset allows pod template authors to not have to explicitly provide all information for every pod. This way, authors of pod templates consuming a specific service do not need to know all the details about that service.</p><p>For more information about the background, see the <a href="https://git.k8s.io/community/contributors/design-proposals/service-catalog/pod-preset.md">design proposal for PodPreset</a>.</p><h5><strong>How It Works</strong></h5><p>Kubernetes provides an admission controller (<strong>PodPreset</strong>) which, when enabled, applies Pod Presets to incoming pod creation requests. When a pod creation request occurs, the system does the following:</p><ol><li>Retrieve all <strong>PodPresets</strong> available for use.</li><li>Check if the label selectors of any <strong>PodPreset</strong> matches the labels on the pod being created.</li><li>Attempt to merge the various resources defined by the <strong>PodPreset</strong> into the Pod being created.</li><li>On error, throw an event documenting the merge error on the pod, and create the pod without any injected resources from the <strong>PodPreset</strong>.</li><li>Annotate the resulting modified Pod spec to indicate that it has been modified by a <strong>PodPreset</strong>. The annotation is of the form <strong>podpreset.admission.kubernetes.io/podpreset-<code>&lt;pod-preset name&gt;: &quot;&lt;resource version&gt;</code>&quot;</strong>.</li></ol><p>Each Pod can be matched zero or more Pod Presets; and each <strong>PodPreset</strong> can be applied to zero or more pods. When a <strong>PodPreset</strong> is applied to one or more Pods, Kubernetes modifies the Pod Spec. For changes to <strong>Env</strong>, <strong>EnvFrom</strong>, and <strong>VolumeMounts</strong>, Kubernetes modifies the container spec for all containers in the Pod; for changes to <strong>Volume</strong>, Kubernetes modifies the Pod Spec.</p><p><strong>Note:</strong> A Pod Preset is capable of modifying the <strong>spec.containers</strong> field in a Pod spec when appropriate. No resource definition from the Pod Preset will be applied to the <strong>initContainers</strong> field.</p><h6><strong>Disable Pod Preset for a Specific Pod</strong></h6><p>There may be instances where you wish for a Pod to not be altered by any Pod Preset mutations. In these cases, you can add an annotation in the Pod Spec of the form: <strong>podpreset.admission.kubernetes.io/exclude: &quot;true&quot;</strong>.</p><h5><strong>Enable Pod Preset</strong></h5><p>In order to use Pod Presets in your cluster you must ensure the following:</p><ol><li>You have enabled the API type <strong>settings.k8s.io/v1alpha1/podpreset</strong>. For example, this can be done by including <strong>settings.k8s.io/v1alpha1=true</strong> in the <strong>--runtime-config</strong> option for the API server.</li><li>You have enabled the admission controller <strong>PodPreset</strong>. One way to doing this is to include <strong>PodPreset</strong> in the <strong>--enable-admission-plugins</strong> option value specified for the API server.</li><li>You have defined your Pod Presets by creating <strong>PodPreset</strong> objects in the namespace you will use.</li></ol><h5><strong>What&#x27;s next</strong></h5><ul><li><a href="https://kubernetes.io/docs/tasks/inject-data-application/podpreset/">Injecting data into a Pod using PodPreset</a></li></ul><h4>Disruptions</h4><p>This guide is for application owners who want to build highly available applications, and thus need to understand what types of Disruptions can happen to Pods.</p><p>It is also for Cluster Administrators who want to perform automated cluster actions, like upgrading and autoscaling clusters.</p><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#voluntary-and-involuntary-disruptions"><strong>Voluntary and Involuntary Disruptions</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#dealing-with-disruptions"><strong>Dealing with Disruptions</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#how-disruption-budgets-work"><strong>How Disruption Budgets Work</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#pdb-example"><strong>PDB Example</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#separating-cluster-owner-and-application-owner-roles"><strong>Separating Cluster Owner and Application Owner Roles</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#how-to-perform-disruptive-actions-on-your-cluster"><strong>How to perform Disruptive Actions on your Cluster</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h5>Voluntary and Involuntary Disruptions</h5><p>Pods do not disappear until someone (a person or a controller) destroys them, or there is an unavoidable hardware or system software error.</p><p>We call these unavoidable cases involuntary disruptions to an application. Examples are:</p><ul><li>a hardware failure of the physical machine backing the node</li><li>cluster administrator deletes VM (instance) by mistake</li><li>cloud provider or hypervisor failure makes VM disappear</li><li>a kernel panic</li><li>the node disappears from the cluster due to cluster network partition</li><li>eviction of a pod due to the node being <a href="https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/">out-of-resources</a>.</li></ul><p>Except for the out-of-resources condition, all these conditions should be familiar to most users; they are not specific to Kubernetes.</p><p>We call other cases voluntary disruptions. These include both actions initiated by the application owner and those initiated by a Cluster Administrator. Typical application owner actions include:</p><ul><li>deleting the deployment or other controller that manages the pod</li><li>updating a deployment&#x27;s pod template causing a restart</li><li>directly deleting a pod (e.g. by accident)</li></ul><p>Cluster Administrator actions include:</p><ul><li><a href="https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/">Draining a node</a> for repair or upgrade.</li><li>Draining a node from a cluster to scale the cluster down (learn about <a href="https://kubernetes.io/docs/tasks/administer-cluster/cluster-management/#cluster-autoscaler">Cluster Autoscaling</a> ).</li><li>Removing a pod from a node to permit something else to fit on that node.</li></ul><p>These actions might be taken directly by the cluster administrator, or by automation run by the cluster administrator, or by your cluster hosting provider.</p><p>Ask your cluster administrator or consult your cloud provider or distribution documentation to determine if any sources of voluntary disruptions are enabled for your cluster. If none are enabled, you can skip creating Pod Disruption Budgets.</p><h5>Dealing with Disruptions</h5><p>Here are some ways to mitigate involuntary disruptions:</p><ul><li>Ensure your pod <a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-ram-container">requests the resources</a> it needs.</li><li>Replicate your application if you need higher availability. (Learn about running replicated <a href="https://kubernetes.io/docs/tasks/run-application/run-stateless-application-deployment/">stateless</a>and <a href="https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/">stateful</a> applications.)</li><li>For even higher availability when running replicated applications, spread applications across racks (using <a href="https://kubernetes.io/docs/user-guide/node-selection/#inter-pod-affinity-and-anti-affinity-beta-feature">anti-affinity</a>) or across zones (if using a <a href="https://kubernetes.io/docs/admin/multiple-zones">multi-zone cluster</a>.)</li></ul><p>The frequency of voluntary disruptions varies. On a basic Kubernetes cluster, there are no voluntary disruptions at all. However, your cluster administrator or hosting provider may run some additional services which cause voluntary disruptions. For example, rolling out node software updates can cause voluntary disruptions. Also, some implementations of cluster (node) autoscaling may cause voluntary disruptions to defragment and compact nodes. Your cluster administrator or hosting provider should have documented what level of voluntary disruptions, if any, to expect.</p><p>Kubernetes offers features to help run highly available applications at the same time as frequent voluntary disruptions. We call this set of features Disruption Budgets.</p><h5>How Disruption Budgets Work</h5><p>An Application Owner can create a <strong>PodDisruptionBudget</strong> object (PDB) for each application. A PDB limits the number pods of a replicated application that are down simultaneously from voluntary disruptions. For example, a quorum-based application would like to ensure that the number of replicas running is never brought below the number needed for a quorum. A web front end might want to ensure that the number of replicas serving load never falls below a certain percentage of the total.</p><p>Cluster managers and hosting providers should use tools which respect Pod Disruption Budgets by calling the <a href="https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/#the-eviction-api">Eviction API</a> instead of directly deleting pods. Examples are the <strong>kubectl drain</strong>command and the Kubernetes-on-GCE cluster upgrade script (<strong>cluster/gce/upgrade.sh</strong>).</p><p>When a cluster administrator wants to drain a node they use the <strong>kubectl drain</strong> command. That tool tries to evict all the pods on the machine. The eviction request may be temporarily rejected, and the tool periodically retries all failed requests until all pods are terminated, or until a configurable timeout is reached.</p><p>A PDB specifies the number of replicas that an application can tolerate having, relative to how many it is intended to have. For example, a Deployment which has a <strong>spec.replicas: 5</strong> is supposed to have 5 pods at any given time. If its PDB allows for there to be 4 at a time, then the Eviction API will allow voluntary disruption of one, but not two pods, at a time.</p><p>The group of pods that comprise the application is specified using a label selector, the same as the one used by the application&#x27;s controller (deployment, stateful-set, etc).</p><p>The &quot;intended&quot; number of pods is computed from the <strong>.spec.replicas</strong> of the pods controller. The controller is discovered from the pods using the <strong>.metadata.ownerReferences</strong> of the object.</p><p>PDBs cannot prevent <a href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#voluntary-and-involuntary-disruptions">involuntary disruptions</a> from occurring, but they do count against the budget.</p><p>Pods which are deleted or unavailable due to a rolling upgrade to an application do count against the disruption budget, but controllers (like deployment and stateful-set) are not limited by PDBs when doing rolling upgrades -- the handling of failures during application updates is configured in the controller spec. (Learn about <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#updating-a-deployment">updating a deployment</a>.)</p><p>When a pod is evicted using the eviction API, it is gracefully terminated (see <strong>terminationGracePeriodSeconds</strong> in <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#podspec-v1-core">PodSpec</a>.)</p><h5><strong>PDB Example</strong></h5><p>Consider a cluster with 3 nodes, <strong>node-1</strong> through <strong>node-3</strong>. The cluster is running several applications. One of them has 3 replicas initially called <strong>pod-a</strong>, <strong>pod-b</strong>, and <strong>pod-c</strong>. Another, unrelated pod without a PDB, called <strong>pod-x</strong>, is also shown. Initially, the pods are laid out as follows:</p><p>  node-1            node-2            node-3</p><hr/><p>  pod-a available   pod-b available   pod-c available
pod-x available                      </p><p>All 3 pods are part of a deployment, and they collectively have a PDB which requires there be at least 2 of the 3 pods to be available at all times.</p><p>For example, assume the cluster administrator wants to reboot into a new kernel version to fix a bug in the kernel. The cluster administrator first tries to drain <strong>node-1</strong> using the <strong>kubectl drain</strong>command. That tool tries to evict <strong>pod-a</strong> and <strong>pod-x</strong>. This succeeds immediately. Both pods go into the <strong>terminating</strong> state at the same time. This puts the cluster in this state:</p><p>  node-1 draining     node-2            node-3</p><hr/><p>  pod-a terminating   pod-b available   pod-c available
pod-x terminating                      </p><p>The deployment notices that one of the pods is terminating, so it creates a replacement called <strong>pod-d</strong>. Since <strong>node-1</strong> is cordoned, it lands on another node. Something has also created <strong>pod-y</strong> as a replacement for <strong>pod-x</strong>.</p><p>(Note: for a StatefulSet, <strong>pod-a</strong>, which would be called something like <strong>pod-1</strong>, would need to terminate completely before its replacement, which is also called <strong>pod-1</strong> but has a different UID, could be created. Otherwise, the example applies to a StatefulSet as well.)</p><p>Now the cluster is in this state:</p><p>  node-1 draining     node-2            node-3</p><hr/><p>  pod-a terminating   pod-b available   pod-c available
pod-x terminating   pod-d starting    pod-y</p><p>At some point, the pods terminate, and the cluster looks like this:</p><p>  node-1 drained   node-2            node-3</p><hr/><p>                   pod-b available   pod-c available
                 pod-d starting    pod-y</p><p>At this point, if an impatient cluster administrator tries to drain <strong>node-2</strong> or <strong>node-3</strong>, the drain command will block, because there are only 2 available pods for the deployment, and its PDB requires at least 2. After some time passes, <strong>pod-d</strong> becomes available.</p><p>The cluster state now looks like this:</p><p>  node-1 drained   node-2            node-3</p><hr/><p>                   pod-b available   pod-c available
                 pod-d available   pod-y</p><p>Now, the cluster administrator tries to drain <strong>node-2</strong>. The drain command will try to evict the two pods in some order, say <strong>pod-b</strong> first and then <strong>pod-d</strong>. It will succeed at evicting <strong>pod-b</strong>. But, when it tries to evict <strong>pod-d</strong>, it will be refused because that would leave only one pod available for the deployment.</p><p>The deployment creates a replacement for <strong>pod-b</strong> called <strong>pod-e</strong>. Because there are not enough resources in the cluster to schedule <strong>pod-e</strong> the drain will again block. The cluster may end up in this state:</p><p>  node-1 drained   node-2            node-3            no node</p><hr/><p>                   pod-b available   pod-c available   pod-e pending
                 pod-d available   pod-y              </p><p>At this point, the cluster administrator needs to add a node back to the cluster to proceed with the upgrade.</p><p>You can see how Kubernetes varies the rate at which disruptions can happen, according to:</p><ul><li>how many replicas an application needs</li><li>how long it takes to gracefully shutdown an instance</li><li>how long it takes a new instance to start up</li><li>the type of controller</li><li>the cluster&#x27;s resource capacity</li></ul><h5><strong>Separating Cluster Owner and Application Owner Roles</strong></h5><p>Often, it is useful to think of the Cluster Manager and Application Owner as separate roles with limited knowledge of each other. This separation of responsibilities may make sense in these scenarios:</p><ul><li>when there are many application teams sharing a Kubernetes cluster, and there is natural specialization of roles</li><li>when third-party tools or services are used to automate cluster management</li></ul><p>Pod Disruption Budgets support this separation of roles by providing an interface between the roles.</p><p>If you do not have such a separation of responsibilities in your organization, you may not need to use Pod Disruption Budgets.</p><h5><strong>How to perform Disruptive Actions on your Cluster</strong></h5><p>If you are a Cluster Administrator, and you need to perform a disruptive action on all the nodes in your cluster, such as a node or system software upgrade, here are some options:</p><ul><li>Accept downtime during the upgrade.</li><li>Fail over to another complete replica cluster.<ul><li>No downtime, but may be costly both for the duplicated nodes, and for human effort to orchestrate the switchover.</li></ul></li><li>Write disruption tolerant applications and use PDBs.<ul><li>No downtime.</li><li>Minimal resource duplication.</li><li>Allows more automation of cluster administration.</li><li>Writing disruption-tolerant applications is tricky, but the work to tolerate voluntary disruptions largely overlaps with work to support autoscaling and tolerating involuntary disruptions.</li></ul></li></ul><h3>Controllers</h3><h4>ReplicaSet</h4><p>ReplicaSet is the next-generation Replication Controller. The only difference between a ReplicaSetand a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/">Replication Controller</a> right now is the selector support. ReplicaSet supports the new set-based selector requirements as described in the <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors">labels user guide</a> whereas a Replication Controller only supports equality-based selector requirements.</p><h5><strong>How to use a ReplicaSet</strong></h5><p>Most <a href="https://kubernetes.io/docs/user-guide/kubectl/"><strong>kubectl</strong></a> commands that support Replication Controllers also support ReplicaSets. One exception is the <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#rolling-update"><strong>rolling-update</strong></a> command. If you want the rolling update functionality please consider using Deployments instead. Also, the <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#rolling-update"><strong>rolling-update</strong></a> command is imperative whereas Deployments are declarative, so we recommend using Deployments through the <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#rollout"><strong>rollout</strong></a>command.</p><p>While ReplicaSets can be used independently, today it&#x27;s mainly used by <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployments</a> as a mechanism to orchestrate pod creation, deletion and updates. When you use Deployments you don&#x27;t have to worry about managing the ReplicaSets that they create. Deployments own and manage their ReplicaSets.</p><h5><strong>When to use a ReplicaSet</strong></h5><p>A ReplicaSet ensures that a specified number of pod replicas are running at any given time. However, a Deployment is a higher-level concept that manages ReplicaSets and provides declarative updates to pods along with a lot of other useful features. Therefore, we recommend using Deployments instead of directly using ReplicaSets, unless you require custom update orchestration or don&#x27;t require updates at all.</p><p>This actually means that you may never need to manipulate ReplicaSet objects: use a Deployment instead, and define your application in the spec section.</p><h5><strong>Example</strong></h5><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>frontend.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kuberne">https://raw.githubusercontent.com/kuberne</a>        |
| tes/website/master/docs/concepts/workloads/controllers/frontend.yaml) |
+=======================================================================+
| <strong>apiVersion: apps/v1</strong>                                               |
|                                                                       |
| <strong>kind: ReplicaSet</strong>                                                  |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: frontend</strong>                                                    |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: guestbook</strong>                                                    |
|                                                                       |
| <strong>tier: frontend</strong>                                                    |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong><em># this replicas value is default</em></strong>                               |
|                                                                       |
| <strong><em># modify it according to your case</em></strong>                             |
|                                                                       |
| <strong>replicas: 3</strong>                                                       |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>matchLabels:</strong>                                                      |
|                                                                       |
| <strong>tier: frontend</strong>                                                    |
|                                                                       |
| <strong>matchExpressions:</strong>                                                 |
|                                                                       |
| <strong>- {key: tier, operator: In, values: <!-- -->[frontend]<!-- -->}</strong>                 |
|                                                                       |
| <strong>template:</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: guestbook</strong>                                                    |
|                                                                       |
| <strong>tier: frontend</strong>                                                    |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: php-redis</strong>                                                 |
|                                                                       |
| <strong>image: gcr.io/google_samples/gb-frontend:v3</strong>                       |
|                                                                       |
| <strong>resources:</strong>                                                        |
|                                                                       |
| <strong>requests:</strong>                                                         |
|                                                                       |
| <strong>cpu: 100m</strong>                                                         |
|                                                                       |
| <strong>memory: 100Mi</strong>                                                     |
|                                                                       |
| <strong>env:</strong>                                                              |
|                                                                       |
| <strong>- name: GET_HOSTS_FROM</strong>                                            |
|                                                                       |
| <strong>value: dns</strong>                                                        |
|                                                                       |
| <strong><em># If your cluster config does not include a dns service, then     |
| to</em></strong>                                                                 |
|                                                                       |
| <strong><em># instead access environment variables to find service host</em></strong>    |
|                                                                       |
| <strong><em># info, comment out the \&#x27;value: dns\&#x27; line above, and uncomment  |
| the</em></strong>                                                                |
|                                                                       |
| <strong><em># line below.</em></strong>                                                  |
|                                                                       |
| <strong><em># value: env</em></strong>                                                   |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- containerPort: 80</strong>                                               |
+-----------------------------------------------------------------------+</p><p>Saving this manifest into <strong>frontend.yaml</strong> and submitting it to a Kubernetes cluster should create the defined ReplicaSet and the pods that it manages.</p><p><strong>$ kubectl create -f frontend.yaml</strong></p><p><strong>replicaset &quot;frontend&quot; created</strong></p><p><strong>$ kubectl describe rs/frontend</strong></p><p><strong>Name: frontend</strong></p><p><strong>Namespace: default</strong></p><p><strong>Selector: tier=frontend,tier in (frontend)</strong></p><p><strong>Labels: app=guestbook</strong></p><p><strong>tier=frontend</strong></p><p><strong>Annotations: <code>&lt;none&gt;</code></strong></p><p><strong>Replicas: 3 current / 3 desired</strong></p><p><strong>Pods Status: 3 Running / 0 Waiting / 0 Succeeded / 0 Failed</strong></p><p><strong>Pod Template:</strong></p><p><strong>Labels: app=guestbook</strong></p><p><strong>tier=frontend</strong></p><p><strong>Containers:</strong></p><p><strong>php-redis:</strong></p><p><strong>Image: gcr.io/google_samples/gb-frontend:v3</strong></p><p><strong>Port: 80/TCP</strong></p><p><strong>Requests:</strong></p><p><strong>cpu: 100m</strong></p><p><strong>memory: 100Mi</strong></p><p><strong>Environment:</strong></p><p><strong>GET_HOSTS_FROM: dns</strong></p><p><strong>Mounts: <code>&lt;none&gt;</code></strong></p><p><strong>Volumes: <code>&lt;none&gt;</code></strong></p><p><strong>Events:</strong></p><p><strong>FirstSeen LastSeen Count From SubobjectPath Type Reason Message</strong></p><p><strong>--------- -------- ----- ---- ------------- -------- ------ -------</strong></p><p><strong>1m 1m 1 {replicaset-controller } Normal SuccessfulCreate Created pod: frontend-qhloh</strong></p><p><strong>1m 1m 1 {replicaset-controller } Normal SuccessfulCreate Created pod: frontend-dnjpy</strong></p><p><strong>1m 1m 1 {replicaset-controller } Normal SuccessfulCreate Created pod: frontend-9si5l</strong></p><p><strong>$ kubectl get pods</strong></p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>frontend-9si5l 1/1 Running 0 1m</strong></p><p><strong>frontend-dnjpy 1/1 Running 0 1m</strong></p><p><strong>frontend-qhloh 1/1 Running 0 1m</strong></p><h5><strong>Writing a ReplicaSet Spec</strong></h5><p>As with all other Kubernetes API objects, a ReplicaSet needs the <strong>apiVersion</strong>, <strong>kind</strong>, and <strong>metadata</strong>fields. For general information about working with manifests, see <a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/overview/">object management using kubectl</a>.</p><p>A ReplicaSet also needs a <a href="https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status"><strong>.spec</strong> section</a>.</p><h6><strong>Pod Template</strong></h6><p>The <strong>.spec.template</strong> is the only required field of the <strong>.spec</strong>. The <strong>.spec.template</strong> is a <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/#pod-templates">pod template</a>. It has exactly the same schema as a <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/">pod</a>, except that it is nested and does not have an <strong>apiVersion</strong> or <strong>kind</strong>.</p><p>In addition to required fields of a pod, a pod template in a ReplicaSet must specify appropriate labels and an appropriate restart policy.</p><p>For labels, make sure to not overlap with other controllers. For more information, see <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/#pod-selector">pod selector</a>.</p><p>For <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/">restart policy</a>, the only allowed value for <strong>.spec.template.spec.restartPolicy</strong> is <strong>Always</strong>, which is the default.</p><p>For local container restarts, ReplicaSet delegates to an agent on the node, for example the <a href="https://kubernetes.io/docs/admin/kubelet/">Kubelet</a>or Docker.</p><h6><strong>Pod Selector</strong></h6><p>The <strong>.spec.selector</strong> field is a <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/">label selector</a>. A ReplicaSet manages all the pods with labels that match the selector. It does not distinguish between pods that it created or deleted and pods that another person or process created or deleted. This allows the ReplicaSet to be replaced without affecting the running pods.</p><p>The <strong>.spec.template.metadata.labels</strong> must match the <strong>.spec.selector</strong>, or it will be rejected by the API.</p><p>In Kubernetes 1.9 the API version <strong>apps/v1</strong> on the ReplicaSet kind is the current version and is enabled by default. The API version <strong>apps/v1beta2</strong> is deprecated.</p><p>Also you should not normally create any pods whose labels match this selector, either directly, with another ReplicaSet, or with another controller such as a Deployment. If you do so, the ReplicaSet thinks that it created the other pods. Kubernetes does not stop you from doing this.</p><p>If you do end up with multiple controllers that have overlapping selectors, you will have to manage the deletion yourself.</p><h6><strong>Labels on a ReplicaSet</strong></h6><p>The ReplicaSet can itself have labels (<strong>.metadata.labels</strong>). Typically, you would set these the same as the <strong>.spec.template.metadata.labels</strong>. However, they are allowed to be different, and the <strong>.metadata.labels</strong> do not affect the behavior of the ReplicaSet.</p><h6><strong>Replicas</strong></h6><p>You can specify how many pods should run concurrently by setting <strong>.spec.replicas</strong>. The number running at any time may be higher or lower, such as if the replicas were just increased or decreased, or if a pod is gracefully shut down, and a replacement starts early.</p><p>If you do not specify <strong>.spec.replicas</strong>, then it defaults to 1.</p><h5><strong>Working with ReplicaSets</strong></h5><h6><strong>Deleting a ReplicaSet and its Pods</strong></h6><p>To delete a ReplicaSet and all its pods, use <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#delete"><strong>kubectl delete</strong></a>. Kubectl will scale the ReplicaSet to zero and wait for it to delete each pod before deleting the ReplicaSet itself. If this kubectl command is interrupted, it can be restarted.</p><p>When using the REST API or go client library, you need to do the steps explicitly (scale replicas to 0, wait for pod deletions, then delete the ReplicaSet).</p><h6><strong>Deleting just a ReplicaSet</strong></h6><p>You can delete a ReplicaSet without affecting any of its pods, using <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#delete"><strong>kubectl delete</strong></a> with the <strong>--cascade=false</strong> option.</p><p>When using the REST API or go client library, simply delete the ReplicaSet object.</p><p>Once the original is deleted, you can create a new ReplicaSet to replace it. As long as the old and new <strong>.spec.selector</strong> are the same, then the new one will adopt the old pods. However, it will not make any effort to make existing pods match a new, different pod template. To update pods to a new spec in a controlled way, use a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/#rolling-updates">rolling update</a>.</p><h6><strong>Isolating pods from a ReplicaSet</strong></h6><p>Pods may be removed from a ReplicaSet&#x27;s target set by changing their labels. This technique may be used to remove pods from service for debugging, data recovery, etc. Pods that are removed in this way will be replaced automatically ( assuming that the number of replicas is not also changed).</p><h6><strong>Scaling a ReplicaSet</strong></h6><p>A ReplicaSet can be easily scaled up or down by simply updating the <strong>.spec.replicas</strong> field. The ReplicaSet controller ensures that a desired number of pods with a matching label selector are available and operational.</p><h6><strong>ReplicaSet as an Horizontal Pod Autoscaler Target</strong></h6><p>A ReplicaSet can also be a target for <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">Horizontal Pod Autoscalers (HPA)</a>. That is, a ReplicaSet can be auto-scaled by an HPA. Here is an example HPA targeting the ReplicaSet we created in the previous example.</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>hpa-rs.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kuber">https://raw.githubusercontent.com/kuber</a>            |
| netes/website/master/docs/concepts/workloads/controllers/hpa-rs.yaml) |
+=======================================================================+
| <strong>apiVersion: autoscaling/v1</strong>                                        |
|                                                                       |
| <strong>kind: HorizontalPodAutoscaler</strong>                                     |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: frontend-scaler</strong>                                             |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>scaleTargetRef:</strong>                                                   |
|                                                                       |
| <strong>kind: ReplicaSet</strong>                                                  |
|                                                                       |
| <strong>name: frontend</strong>                                                    |
|                                                                       |
| <strong>minReplicas: 3</strong>                                                    |
|                                                                       |
| <strong>maxReplicas: 10</strong>                                                   |
|                                                                       |
| <strong>targetCPUUtilizationPercentage: 50</strong>                                |
+-----------------------------------------------------------------------+</p><p>Saving this manifest into <strong>hpa-rs.yaml</strong> and submitting it to a Kubernetes cluster should create the defined HPA that autoscales the target ReplicaSet depending on the CPU usage of the replicated pods.</p><p><strong>kubectl create -f hpa-rs.yaml</strong></p><p>Alternatively, you can use the <strong>kubectl autoscale</strong> command to accomplish the same (and it&#x27;s easier!)</p><p><strong>kubectl autoscale rs frontend</strong></p><h5><strong>Alternatives to ReplicaSet</strong></h5><h6><strong>Deployment (Recommended)</strong></h6><p><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/"><strong>Deployment</strong></a> is a higher-level API object that updates its underlying ReplicaSets and their Pods in a similar fashion as <strong>kubectl rolling-update</strong>. Deployments are recommended if you want this rolling update functionality, because unlike <strong>kubectl rolling-update</strong>, they are declarative, server-side, and have additional features. For more information on running a stateless application using a Deployment, please read <a href="https://kubernetes.io/docs/tasks/run-application/run-stateless-application-deployment/">Run a Stateless Application Using a Deployment</a>.</p><h6><strong>Bare Pods</strong></h6><p>Unlike the case where a user directly created pods, a ReplicaSet replaces pods that are deleted or terminated for any reason, such as in the case of node failure or disruptive node maintenance, such as a kernel upgrade. For this reason, we recommend that you use a ReplicaSet even if your application requires only a single pod. Think of it similarly to a process supervisor, only it supervises multiple pods across multiple nodes instead of individual processes on a single node. A ReplicaSet delegates local container restarts to some agent on the node (for example, Kubelet or Docker).</p><h6><strong>Job</strong></h6><p>Use a <a href="https://kubernetes.io/docs/concepts/jobs/run-to-completion-finite-workloads/"><strong>Job</strong></a> instead of a ReplicaSet for pods that are expected to terminate on their own (that is, batch jobs).</p><h6><strong>DaemonSet</strong></h6><p>Use a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/"><strong>DaemonSet</strong></a> instead of a ReplicaSet for pods that provide a machine-level function, such as machine monitoring or machine logging. These pods have a lifetime that is tied to a machine lifetime: the pod needs to be running on the machine before other pods start, and are safe to terminate when the machine is otherwise ready to be rebooted/shutdown.</p><h4>ReplicationController</h4><p><strong>NOTE:</strong> A <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/"><strong>Deployment</strong></a> that configures a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/"><strong>ReplicaSet</strong></a> is now the recommended way to set up replication.</p><p>A ReplicationController ensures that a specified number of pod replicas are running at any one time. In other words, a ReplicationController makes sure that a pod or a homogeneous set of pods is always up and available.</p><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/#how-a-replicationcontroller-works"><strong>How a ReplicationController Works</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/#running-an-example-replicationcontroller"><strong>Running an example ReplicationController</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/#writing-a-replicationcontroller-spec"><strong>Writing a ReplicationController Spec</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/#pod-template"><strong>Pod Template</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/#labels-on-the-replicationcontroller"><strong>Labels on the ReplicationController</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/#pod-selector"><strong>Pod Selector</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/#multiple-replicas"><strong>Multiple Replicas</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/#working-with-replicationcontrollers"><strong>Working with ReplicationControllers</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/#deleting-a-replicationcontroller-and-its-pods"><strong>Deleting a ReplicationController and its Pods</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/#deleting-just-a-replicationcontroller"><strong>Deleting just a ReplicationController</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/#isolating-pods-from-a-replicationcontroller"><strong>Isolating pods from a ReplicationController</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/#common-usage-patterns"><strong>Common usage patterns</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/#rescheduling"><strong>Rescheduling</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/#scaling"><strong>Scaling</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/#rolling-updates"><strong>Rolling updates</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/#multiple-release-tracks"><strong>Multiple release tracks</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/#using-replicationcontrollers-with-services"><strong>Using ReplicationControllers with Services</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/#writing-programs-for-replication"><strong>Writing programs for Replication</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/#responsibilities-of-the-replicationcontroller"><strong>Responsibilities of the ReplicationController</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/#api-object"><strong>API Object</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/#alternatives-to-replicationcontroller"><strong>Alternatives to ReplicationController</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/#replicaset"><strong>ReplicaSet</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/#deployment-recommended"><strong>Deployment (Recommended)</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/#bare-pods"><strong>Bare Pods</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/#job"><strong>Job</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/#daemonset"><strong>DaemonSet</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/#for-more-information"><strong>For more information</strong></a></li></ul><h5><strong>How a ReplicationController Works</strong></h5><p>If there are too many pods, the ReplicationController terminates the extra pods. If there are too few, the ReplicationController starts more pods. Unlike manually created pods, the pods maintained by a ReplicationController are automatically replaced if they fail, are deleted, or are terminated. For example, your pods are re-created on a node after disruptive maintenance such as a kernel upgrade. For this reason, you should use a ReplicationController even if your application requires only a single pod. A ReplicationController is similar to a process supervisor, but instead of supervising individual processes on a single node, the ReplicationController supervises multiple pods across multiple nodes.</p><p>ReplicationController is often abbreviated to &quot;rc&quot; or &quot;rcs&quot; in discussion, and as a shortcut in kubectl commands.</p><p>A simple case is to create one ReplicationController object to reliably run one instance of a Pod indefinitely. A more complex use case is to run several identical replicas of a replicated service, such as web servers.</p><h5><strong>Running an example ReplicationController</strong></h5><p>This example ReplicationController config runs three copies of the nginx web server.</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>replication.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes">https://raw.githubusercontent.com/kubernetes</a>  |
| /website/master/docs/concepts/workloads/controllers/replication.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: ReplicationController</strong>                                       |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: nginx</strong>                                                       |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>replicas: 3</strong>                                                       |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>app: nginx</strong>                                                        |
|                                                                       |
| <strong>template:</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: nginx</strong>                                                       |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: nginx</strong>                                                        |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: nginx</strong>                                                     |
|                                                                       |
| <strong>image: nginx</strong>                                                      |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- containerPort: 80</strong>                                               |
+-----------------------------------------------------------------------+</p><p>Run the example job by downloading the example file and then running this command:</p><p><strong>$ kubectl create -f ./replication.yaml</strong></p><p><strong>replicationcontroller &quot;nginx&quot; created</strong></p><p>Check on the status of the ReplicationController using this command:</p><p><strong>$ kubectl describe replicationcontrollers/nginx</strong></p><p><strong>Name: nginx</strong></p><p><strong>Namespace: default</strong></p><p><strong>Selector: app=nginx</strong></p><p><strong>Labels: app=nginx</strong></p><p><strong>Annotations: <code>&lt;none&gt;</code></strong></p><p><strong>Replicas: 3 current / 3 desired</strong></p><p><strong>Pods Status: 0 Running / 3 Waiting / 0 Succeeded / 0 Failed</strong></p><p><strong>Pod Template:</strong></p><p><strong>Labels: app=nginx</strong></p><p><strong>Containers:</strong></p><p><strong>nginx:</strong></p><p><strong>Image: nginx</strong></p><p><strong>Port: 80/TCP</strong></p><p><strong>Environment: <code>&lt;none&gt;</code></strong></p><p><strong>Mounts: <code>&lt;none&gt;</code></strong></p><p><strong>Volumes: <code>&lt;none&gt;</code></strong></p><p><strong>Events:</strong></p><p><strong>FirstSeen LastSeen Count From SubobjectPath Type Reason Message</strong></p><p><strong>--------- -------- ----- ---- ------------- ---- ------ -------</strong></p><p><strong>20s 20s 1 {replication-controller } Normal SuccessfulCreate Created pod: nginx-qrm3m</strong></p><p><strong>20s 20s 1 {replication-controller } Normal SuccessfulCreate Created pod: nginx-3ntk0</strong></p><p><strong>20s 20s 1 {replication-controller } Normal SuccessfulCreate Created pod: nginx-4ok8v</strong></p><p>Here, three pods are created, but none is running yet, perhaps because the image is being pulled. A little later, the same command may show:</p><p><strong>Pods Status: 3 Running / 0 Waiting / 0 Succeeded / 0 Failed</strong></p><p>To list all the pods that belong to the ReplicationController in a machine readable form, you can use a command like this:</p><p><strong>$ pods=$(kubectl get pods --selector=app=nginx --output=jsonpath={.items..metadata.name})</strong></p><p><strong>echo $pods</strong></p><p><strong>nginx-3ntk0 nginx-4ok8v nginx-qrm3m</strong></p><p>Here, the selector is the same as the selector for the ReplicationController (seen in the <strong>kubectl describe</strong> output, and in a different form in <strong>replication.yaml</strong>. The <strong>--output=jsonpath</strong> option specifies an expression that just gets the name from each pod in the returned list.</p><h5><strong>Writing a ReplicationController Spec</strong></h5><p>As with all other Kubernetes config, a ReplicationController needs <strong>apiVersion</strong>, <strong>kind</strong>, and <strong>metadata</strong> fields. For general information about working with config files, see <a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/overview/">object management </a>.</p><p>A ReplicationController also needs a <a href="https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status"><strong>.spec</strong> section</a>.</p><h6><strong>Pod Template</strong></h6><p>The <strong>.spec.template</strong> is the only required field of the <strong>.spec</strong>.</p><p>The <strong>.spec.template</strong> is a <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/#pod-templates">pod template</a>. It has exactly the same schema as a <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/">pod</a>, except it is nested and does not have an <strong>apiVersion</strong> or <strong>kind</strong>.</p><p>In addition to required fields for a Pod, a pod template in a ReplicationController must specify appropriate labels and an appropriate restart policy. For labels, make sure not to overlap with other controllers. See <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/#pod-selector">pod selector</a>.</p><p>Only a <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/"><strong>.spec.template.spec.restartPolicy</strong></a> equal to <strong>Always</strong> is allowed, which is the default if not specified.</p><p>For local container restarts, ReplicationControllers delegate to an agent on the node, for example the <a href="https://kubernetes.io/docs/admin/kubelet/">Kubelet</a> or Docker.</p><h6><strong>Labels on the ReplicationController</strong></h6><p>The ReplicationController can itself have labels (<strong>.metadata.labels</strong>). Typically, you would set these the same as the <strong>.spec.template.metadata.labels</strong>; if <strong>.metadata.labels</strong> is not specified then it defaults to <strong>.spec.template.metadata.labels</strong>. However, they are allowed to be different, and the <strong>.metadata.labels</strong> do not affect the behavior of the ReplicationController.</p><h6><strong>Pod Selector</strong></h6><p>The <strong>.spec.selector</strong> field is a <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors">label selector</a>. A ReplicationController manages all the pods with labels that match the selector. It does not distinguish between pods that it created or deleted and pods that another person or process created or deleted. This allows the ReplicationController to be replaced without affecting the running pods.</p><p>If specified, the <strong>.spec.template.metadata.labels</strong> must be equal to the <strong>.spec.selector</strong>, or it will be rejected by the API. If <strong>.spec.selector</strong> is unspecified, it will be defaulted to <strong>.spec.template.metadata.labels</strong>.</p><p>Also you should not normally create any pods whose labels match this selector, either directly, with another ReplicationController, or with another controller such as Job. If you do so, the ReplicationController thinks that it created the other pods. Kubernetes does not stop you from doing this.</p><p>If you do end up with multiple controllers that have overlapping selectors, you will have to manage the deletion yourself (see <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/#working-with-replicationcontrollers">below</a>).</p><h6><strong>Multiple Replicas</strong></h6><p>You can specify how many pods should run concurrently by setting <strong>.spec.replicas</strong> to the number of pods you would like to have running concurrently. The number running at any time may be higher or lower, such as if the replicas were just increased or decreased, or if a pod is gracefully shutdown, and a replacement starts early.</p><p>If you do not specify <strong>.spec.replicas</strong>, then it defaults to 1.</p><h5><strong>Working with ReplicationControllers</strong></h5><h6><strong>Deleting a ReplicationController and its Pods</strong></h6><p>To delete a ReplicationController and all its pods, use <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#delete"><strong>kubectl delete</strong></a>. Kubectl will scale the ReplicationController to zero and wait for it to delete each pod before deleting the ReplicationController itself. If this kubectl command is interrupted, it can be restarted.</p><p>When using the REST API or go client library, you need to do the steps explicitly (scale replicas to 0, wait for pod deletions, then delete the ReplicationController).</p><h6><strong>Deleting just a ReplicationController</strong></h6><p>You can delete a ReplicationController without affecting any of its pods.</p><p>Using kubectl, specify the <strong>--cascade=false</strong> option to <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#delete"><strong>kubectl delete</strong></a>.</p><p>When using the REST API or go client library, simply delete the ReplicationController object.</p><p>Once the original is deleted, you can create a new ReplicationController to replace it. As long as the old and new <strong>.spec.selector</strong> are the same, then the new one will adopt the old pods. However, it will not make any effort to make existing pods match a new, different pod template. To update pods to a new spec in a controlled way, use a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/#rolling-updates">rolling update</a>.</p><h6><strong>Isolating pods from a ReplicationController</strong></h6><p>Pods may be removed from a ReplicationController&#x27;s target set by changing their labels. This technique may be used to remove pods from service for debugging, data recovery, etc. Pods that are removed in this way will be replaced automatically (assuming that the number of replicas is not also changed).</p><h5><strong>Common usage patterns</strong></h5><h6><strong>Rescheduling</strong></h6><p>As mentioned above, whether you have 1 pod you want to keep running, or 1000, a ReplicationController will ensure that the specified number of pods exists, even in the event of node failure or pod termination (for example, due to an action by another control agent).</p><h6><strong>Scaling</strong></h6><p>The ReplicationController makes it easy to scale the number of replicas up or down, either manually or by an auto-scaling control agent, by simply updating the <strong>replicas</strong> field.</p><h6><strong>Rolling updates</strong></h6><p>The ReplicationController is designed to facilitate rolling updates to a service by replacing pods one-by-one.</p><p>As explained in <a href="http://issue.k8s.io/1353">#1353</a>, the recommended approach is to create a new ReplicationController with 1 replica, scale the new (+1) and old (-1) controllers one by one, and then delete the old controller after it reaches 0 replicas. This predictably updates the set of pods regardless of unexpected failures.</p><p>Ideally, the rolling update controller would take application readiness into account, and would ensure that a sufficient number of pods were productively serving at any given time.</p><p>The two ReplicationControllers would need to create pods with at least one differentiating label, such as the image tag of the primary container of the pod, since it is typically image updates that motivate rolling updates.</p><p>Rolling update is implemented in the client tool <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#rolling-update"><strong>kubectl rolling-update</strong></a>. Visit <a href="https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/"><strong>kubectl rolling-update</strong> task</a> for more concrete examples.</p><h6><strong>Multiple release tracks</strong></h6><p>In addition to running multiple releases of an application while a rolling update is in progress, it&#x27;s common to run multiple releases for an extended period of time, or even continuously, using multiple release tracks. The tracks would be differentiated by labels.</p><p>For instance, a service might target all pods with <strong>tier in (frontend), environment in (prod)</strong>. Now say you have 10 replicated pods that make up this tier. But you want to be able to &#x27;canary&#x27; a new version of this component. You could set up a ReplicationController with <strong>replicas</strong> set to 9 for the bulk of the replicas, with labels <strong>tier=frontend, environment=prod, track=stable</strong>, and another ReplicationController with <strong>replicas</strong> set to 1 for the canary, with labels <strong>tier=frontend, environment=prod, track=canary</strong>. Now the service is covering both the canary and non-canary pods. But you can mess with the ReplicationControllers separately to test things out, monitor the results, etc.</p><h6><strong>Using ReplicationControllers with Services</strong></h6><p>Multiple ReplicationControllers can sit behind a single service, so that, for example, some traffic goes to the old version, and some goes to the new version.</p><p>A ReplicationController will never terminate on its own, but it isn&#x27;t expected to be as long-lived as services. Services may be composed of pods controlled by multiple ReplicationControllers, and it is expected that many ReplicationControllers may be created and destroyed over the lifetime of a service (for instance, to perform an update of pods that run the service). Both services themselves and their clients should remain oblivious to the ReplicationControllers that maintain the pods of the services.</p><h5><strong>Writing programs for Replication</strong></h5><p>Pods created by a ReplicationController are intended to be fungible and semantically identical, though their configurations may become heterogeneous over time. This is an obvious fit for replicated stateless servers, but ReplicationControllers can also be used to maintain availability of master-elected, sharded, and worker-pool applications. Such applications should use dynamic work assignment mechanisms, such as the <a href="https://www.rabbitmq.com/tutorials/tutorial-two-python.html">RabbitMQ work queues</a>, as opposed to static/one-time customization of the configuration of each pod, which is considered an anti-pattern. Any pod customization performed, such as vertical auto-sizing of resources (for example, cpu or memory), should be performed by another online controller process, not unlike the ReplicationController itself.</p><h5><strong>Responsibilities of the ReplicationController</strong></h5><p>The ReplicationController simply ensures that the desired number of pods matches its label selector and are operational. Currently, only terminated pods are excluded from its count. In the future, <a href="http://issue.k8s.io/620">readiness</a> and other information available from the system may be taken into account, we may add more controls over the replacement policy, and we plan to emit events that could be used by external clients to implement arbitrarily sophisticated replacement and/or scale-down policies.</p><p>The ReplicationController is forever constrained to this narrow responsibility. It itself will not perform readiness nor liveness probes. Rather than performing auto-scaling, it is intended to be controlled by an external auto-scaler (as discussed in <a href="http://issue.k8s.io/492">#492</a>), which would change its <strong>replicas</strong> field. We will not add scheduling policies (for example, <a href="http://issue.k8s.io/367#issuecomment-48428019">spreading</a>) to the ReplicationController. Nor should it verify that the pods controlled match the currently specified template, as that would obstruct auto-sizing and other automated processes. Similarly, completion deadlines, ordering dependencies, configuration expansion, and other features belong elsewhere. We even plan to factor out the mechanism for bulk pod creation (<a href="http://issue.k8s.io/170">#170</a>).</p><p>The ReplicationController is intended to be a composable building-block primitive. We expect higher-level APIs and/or tools to be built on top of it and other complementary primitives for user convenience in the future. The &quot;macro&quot; operations currently supported by kubectl (run, scale, rolling-update) are proof-of-concept examples of this. For instance, we could imagine something like <a href="http://techblog.netflix.com/2012/06/asgard-web-based-cloud-management-and.html">Asgard</a> managing ReplicationControllers, auto-scalers, services, scheduling policies, canaries, etc.</p><h5><strong>API Object</strong></h5><p>Replication controller is a top-level resource in the Kubernetes REST API. More details about the API object can be found at: <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#replicationcontroller-v1-core">ReplicationController API object</a>.</p><h5><strong>Alternatives to ReplicationController</strong></h5><h6><strong>ReplicaSet</strong></h6><p><a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/"><strong>ReplicaSet</strong></a> is the next-generation ReplicationController that supports the new <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#set-based-requirement">set-based label selector</a>. It&#x27;s mainly used by <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/"><strong>Deployment</strong></a> as a mechanism to orchestrate pod creation, deletion and updates. Note that we recommend using Deployments instead of directly using Replica Sets, unless you require custom update orchestration or don&#x27;t require updates at all.</p><h6><strong>Deployment (Recommended)</strong></h6><p><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/"><strong>Deployment</strong></a> is a higher-level API object that updates its underlying Replica Sets and their Pods in a similar fashion as <strong>kubectl rolling-update</strong>. Deployments are recommended if you want this rolling update functionality, because unlike <strong>kubectl rolling-update</strong>, they are declarative, server-side, and have additional features.</p><h6><strong>Bare Pods</strong></h6><p>Unlike in the case where a user directly created pods, a ReplicationController replaces pods that are deleted or terminated for any reason, such as in the case of node failure or disruptive node maintenance, such as a kernel upgrade. For this reason, we recommend that you use a ReplicationController even if your application requires only a single pod. Think of it similarly to a process supervisor, only it supervises multiple pods across multiple nodes instead of individual processes on a single node. A ReplicationController delegates local container restarts to some agent on the node (for example, Kubelet or Docker).</p><h6><strong>Job</strong></h6><p>Use a <a href="https://kubernetes.io/docs/concepts/jobs/run-to-completion-finite-workloads/"><strong>Job</strong></a> instead of a ReplicationController for pods that are expected to terminate on their own (that is, batch jobs).</p><h6><strong>DaemonSet</strong></h6><p>Use a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/"><strong>DaemonSet</strong></a> instead of a ReplicationController for pods that provide a machine-level function, such as machine monitoring or machine logging. These pods have a lifetime that is tied to a machine lifetime: the pod needs to be running on the machine before other pods start, and are safe to terminate when the machine is otherwise ready to be rebooted/shutdown.</p><h5><strong>For more information</strong></h5><p>Read <a href="https://kubernetes.io/docs/tutorials/stateless-application/run-stateless-ap-replication-controller/">Run Stateless AP Replication Controller</a>.</p><h4>Deployments</h4><p>A Deployment controller provides declarative updates for <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/">Pods</a> and <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/">ReplicaSets</a>.</p><p>You describe a desired state in a Deployment object, and the Deployment controller changes the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments.</p><p><strong>Note:</strong> You should not manage ReplicaSets owned by a Deployment. All the use cases should be covered by manipulating the Deployment object. Consider opening an issue in the main Kubernetes repository if your use case is not covered below.</p><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#use-case"><strong>Use Case</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#creating-a-deployment"><strong>Creating a Deployment</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#pod-template-hash-label"><strong>Pod-template-hash label</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#updating-a-deployment"><strong>Updating a Deployment</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rollover-aka-multiple-updates-in-flight"><strong>Rollover (aka multiple updates in-flight)</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#label-selector-updates"><strong>Label selector updates</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-back-a-deployment"><strong>Rolling Back a Deployment</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#checking-rollout-history-of-a-deployment"><strong>Checking Rollout History of a Deployment</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-back-to-a-previous-revision"><strong>Rolling Back to a Previous Revision</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#scaling-a-deployment"><strong>Scaling a Deployment</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#proportional-scaling"><strong>Proportional scaling</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#pausing-and-resuming-a-deployment"><strong>Pausing and Resuming a Deployment</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#deployment-status"><strong>Deployment status</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#progressing-deployment"><strong>Progressing Deployment</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#complete-deployment"><strong>Complete Deployment</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#failed-deployment"><strong>Failed Deployment</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#operating-on-a-failed-deployment"><strong>Operating on a failed deployment</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#clean-up-policy"><strong>Clean up Policy</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#use-cases"><strong>Use Cases</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#canary-deployment"><strong>Canary Deployment</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#writing-a-deployment-spec"><strong>Writing a Deployment Spec</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#pod-template"><strong>Pod Template</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#replicas"><strong>Replicas</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#selector"><strong>Selector</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy"><strong>Strategy</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#recreate-deployment"><strong>Recreate Deployment</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-update-deployment"><strong>Rolling Update Deployment</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#max-unavailable"><strong>Max Unavailable</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#max-surge"><strong>Max Surge</strong></a></li></ul></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#progress-deadline-seconds"><strong>Progress Deadline Seconds</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#min-ready-seconds"><strong>Min Ready Seconds</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rollback-to"><strong>Rollback To</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#revision-history-limit"><strong>Revision History Limit</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#paused"><strong>Paused</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#alternative-to-deployments"><strong>Alternative to Deployments</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#kubectl-rolling-update"><strong>kubectl rolling update</strong></a></li></ul></li></ul><h5><strong>Use Case</strong></h5><p>The following are typical use cases for Deployments:</p><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#creating-a-deployment">Create a Deployment to rollout a ReplicaSet</a>. The ReplicaSet creates Pods in the background. Check the status of the rollout to see if it succeeds or not.</li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#updating-a-deployment">Declare the new state of the Pods</a> by updating the PodTemplateSpec of the Deployment. A new ReplicaSet is created and the Deployment manages moving the Pods from the old ReplicaSet to the new one at a controlled rate. Each new ReplicaSet updates the revision of the Deployment.</li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-back-a-deployment">Rollback to an earlier Deployment revision</a> if the current state of the Deployment is not stable. Each rollback updates the revision of the Deployment.</li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#scaling-a-deployment">Scale up the Deployment to facilitate more load</a>.</li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#pausing-and-resuming-a-deployment">Pause the Deployment</a> to apply multiple fixes to its PodTemplateSpec and then resume it to start a new rollout.</li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#deployment-status">Use the status of the Deployment</a> as an indicator that a rollout has stuck.</li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#clean-up-policy">Clean up older ReplicaSets</a> that you don&#x27;t need anymore.</li></ul><h5><strong>Creating a Deployment</strong></h5><p>The following is an example of a Deployment. It creates a ReplicaSet to bring up three <strong>nginx</strong> Pods:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>nginx-                                                             |
| deployment.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/webs">https://raw.githubusercontent.com/kubernetes/webs</a> |
| ite/master/docs/concepts/workloads/controllers/nginx-deployment.yaml) |
+=======================================================================+
| <strong>apiVersion: apps/v1</strong>                                               |
|                                                                       |
| <strong>kind: Deployment</strong>                                                  |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: nginx-deployment</strong>                                            |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: nginx</strong>                                                        |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>replicas: 3</strong>                                                       |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>matchLabels:</strong>                                                      |
|                                                                       |
| <strong>app: nginx</strong>                                                        |
|                                                                       |
| <strong>template:</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: nginx</strong>                                                        |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: nginx</strong>                                                     |
|                                                                       |
| <strong>image: nginx:1.7.9</strong>                                                |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- containerPort: 80</strong>                                               |
+-----------------------------------------------------------------------+</p><p>In this example:</p><ul><li>A Deployment named <strong>nginx-deployment</strong> is created, indicated by the <strong>metadata: name</strong> field.</li><li>The Deployment creates three replicated Pods, indicated by the <strong>replicas</strong> field.</li><li>The <strong>selector</strong> field defines how the Deployment finds which Pods to manage. In this case, we simply select on one label defined in the Pod template (<strong>app: nginx</strong>). However, more sophisticated selection rules are possible, as long as the Pod template itself satisfies the rule.</li><li>The Pod template&#x27;s specification, or <strong>template: spec</strong> field, indicates that the Pods run one container, <strong>nginx</strong>, which runs the <strong>nginx</strong> <a href="https://hub.docker.com/">Docker Hub</a> image at version 1.7.9.</li><li>The Deployment opens port 80 for use by the Pods.</li></ul><p><strong>Note:</strong> <strong>matchLabels</strong> is a map of {key,value} pairs. A single {key,value} in the <strong>matchLabels</strong>map is equivalent to an element of <strong>matchExpressions</strong>, whose key field is &quot;key&quot;, the operator is &quot;In&quot;, and the values array contains only &quot;value&quot;. The requirements are ANDed.</p><p>The <strong>template</strong> field contains the following instructions:</p><ul><li>The Pods are labeled <strong>app: nginx</strong></li><li>Create one container and name it <strong>nginx</strong>.</li><li>Run the <strong>nginx</strong> image at version <strong>1.7.9</strong>.</li><li>Open port <strong>80</strong> so that the container can send and accept traffic.</li></ul><p>To create this Deployment, run the following command:</p><p><strong>kubectl create -f <a href="https://raw.githubusercontent.com/kubernetes/website/master/docs/concepts/workloads/controllers/nginx-deployment.yaml">https://raw.githubusercontent.com/kubernetes/website/master/docs/concepts/workloads/controllers/nginx-deployment.yaml</a></strong></p><p><strong>Note:</strong> You can append <strong>--record</strong> to this command to record the current command in the annotations of the created or updated resource. This is useful for future review, such as investigating which commands were executed in each Deployment revision.</p><p>Next, run <strong>kubectl get deployments</strong>. The output is similar to the following:</p><p><strong>NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE</strong></p><p><strong>nginx-deployment 3 0 0 0 1s</strong></p><p>When you inspect the Deployments in your cluster, the following fields are displayed:</p><ul><li><strong>NAME</strong> lists the names of the Deployments in the cluster.</li><li><strong>DESIRED</strong> displays the desired number of replicas of the application, which you define when you create the Deployment. This is the desired state.</li><li><strong>CURRENT</strong> displays how many replicas are currently running.</li><li><strong>UP-TO-DATE</strong> displays the number of replicas that have been updated to achieve the desired state.</li><li><strong>AVAILABLE</strong> displays how many replicas of the application are available to your users.</li><li><strong>AGE</strong> displays the amount of time that the application has been running.</li></ul><p>Notice how the values in each field correspond to the values in the Deployment specification:</p><ul><li>The number of desired replicas is 3 according to <strong>spec: replicas</strong> field.</li><li>The number of current replicas is 0 according to the <strong>.status.replicas</strong> field.</li><li>The number of up-to-date replicas is 0 according to the <strong>.status.updatedReplicas</strong> field.</li><li>The number of available replicas is 0 according to the <strong>.status.availableReplicas</strong> field.</li></ul><p>To see the Deployment rollout status, run <strong>kubectl rollout status deployment/nginx-deployment</strong>. This command returns the following output:</p><p><strong>Waiting for rollout to finish: 2 out of 3 new replicas have been updated<!-- -->.<!-- -->..</strong></p><p><strong>deployment &quot;nginx-deployment&quot; successfully rolled out</strong></p><p>Run the <strong>kubectl get deployments</strong> again a few seconds later:</p><p><strong>NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE</strong></p><p><strong>nginx-deployment 3 3 3 3 18s</strong></p><p>Notice that the Deployment has created all three replicas, and all replicas are up-to-date (they contain the latest Pod template) and available (the Pod status is Ready for at least the value of the Deployment&#x27;s <strong>.spec.minReadySeconds</strong> field).</p><p>To see the ReplicaSet (<strong>rs</strong>) created by the deployment, run <strong>kubectl get rs</strong>:</p><p><strong>NAME DESIRED CURRENT READY AGE</strong></p><p><strong>nginx-deployment-2035384211 3 3 3 18s</strong></p><p>Notice that the name of the ReplicaSet is always formatted as <strong>[DEPLOYMENT-NAME]<!-- -->-<!-- -->[POD-TEMPLATE-HASH-VALUE]</strong>. The hash value is automatically generated when the Deployment is created.</p><p>To see the labels automatically generated for each pod, run <strong>kubectl get pods --show-labels</strong>. The following output is returned:</p><p><strong>NAME READY STATUS RESTARTS AGE LABELS</strong></p><p><strong>nginx-deployment-2035384211-7ci7o 1/1 Running 0 18s app=nginx,pod-template-hash=2035384211</strong></p><p><strong>nginx-deployment-2035384211-kzszj 1/1 Running 0 18s app=nginx,pod-template-hash=2035384211</strong></p><p><strong>nginx-deployment-2035384211-qqcnn 1/1 Running 0 18s app=nginx,pod-template-hash=2035384211</strong></p><p>The created ReplicaSet ensures that there are three <strong>nginx</strong> Pods running at all times.</p><p><strong>Note:</strong> You must specify an appropriate selector and Pod template labels in a Deployment (in this case, <strong>app: nginx</strong>). Do not overlap labels or selectors with other controllers (including other Deployments and StatefulSets). Kubernetes doesn&#x27;t stop you from overlapping, and if multiple controllers have overlapping selectors those controllers might conflict and behave unexpectedly.</p><h6><strong>Pod-template-hash label</strong></h6><p><strong>Note:</strong> Do not change this label.</p><p>The <strong>pod-template-hash</strong> label is added by the Deployment controller to every ReplicaSet that a Deployment creates or adopts.</p><p>This label ensures that child ReplicaSets of a Deployment do not overlap. It is generated by hashing the <strong>PodTemplate</strong> of the ReplicaSet and using the resulting hash as the label value that is added to the ReplicaSet selector, Pod template labels, and in any existing Pods that the ReplicaSet might have.</p><h5><strong>Updating a Deployment</strong></h5><p><strong>Note:</strong> A Deployment&#x27;s rollout is triggered if and only if the Deployment&#x27;s pod template (that is, <strong>.spec.template</strong>) is changed, for example if the labels or container images of the template are updated. Other updates, such as scaling the Deployment, do not trigger a rollout.</p><p>Suppose that we now want to update the nginx Pods to use the <strong>nginx:1.9.1</strong> image instead of the <strong>nginx:1.7.9</strong> image.</p><p><strong>$ kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1</strong></p><p><strong>deployment &quot;nginx-deployment&quot; image updated</strong></p><p>Alternatively, we can <strong>edit</strong> the Deployment and change <strong>.spec.template.spec.containers<!-- -->[0]<!-- -->.image</strong> from <strong>nginx:1.7.9</strong> to <strong>nginx:1.9.1</strong>:</p><p><strong>$ kubectl edit deployment/nginx-deployment</strong></p><p><strong>deployment &quot;nginx-deployment&quot; edited</strong></p><p>To see the rollout status, run:</p><p><strong>$ kubectl rollout status deployment/nginx-deployment</strong></p><p><strong>Waiting for rollout to finish: 2 out of 3 new replicas have been updated<!-- -->.<!-- -->..</strong></p><p><strong>deployment &quot;nginx-deployment&quot; successfully rolled out</strong></p><p>After the rollout succeeds, you may want to <strong>get</strong> the Deployment:</p><p><strong>$ kubectl get deployments</strong></p><p><strong>NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE</strong></p><p><strong>nginx-deployment 3 3 3 3 36s</strong></p><p>The number of up-to-date replicas indicates that the Deployment has updated the replicas to the latest configuration. The current replicas indicates the total replicas this Deployment manages, and the available replicas indicates the number of current replicas that are available.</p><p>We can run <strong>kubectl get rs</strong> to see that the Deployment updated the Pods by creating a new ReplicaSet and scaling it up to 3 replicas, as well as scaling down the old ReplicaSet to 0 replicas.</p><p><strong>$ kubectl get rs</strong></p><p><strong>NAME DESIRED CURRENT READY AGE</strong></p><p><strong>nginx-deployment-1564180365 3 3 3 6s</strong></p><p><strong>nginx-deployment-2035384211 0 0 0 36s</strong></p><p>Running <strong>get pods</strong> should now show only the new Pods:</p><p><strong>$ kubectl get pods</strong></p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>nginx-deployment-1564180365-khku8 1/1 Running 0 14s</strong></p><p><strong>nginx-deployment-1564180365-nacti 1/1 Running 0 14s</strong></p><p><strong>nginx-deployment-1564180365-z9gth 1/1 Running 0 14s</strong></p><p>Next time we want to update these Pods, we only need to update the Deployment&#x27;s pod template again.</p><p>Deployment can ensure that only a certain number of Pods may be down while they are being updated. By default, it ensures that at least 25% less than the desired number of Pods are up (25% max unavailable).</p><p>Deployment can also ensure that only a certain number of Pods may be created above the desired number of Pods. By default, it ensures that at most 25% more than the desired number of Pods are up (25% max surge).</p><p>For example, if you look at the above Deployment closely, you will see that it first created a new Pod, then deleted some old Pods and created new ones. It does not kill old Pods until a sufficient number of new Pods have come up, and does not create new Pods until a sufficient number of old Pods have been killed. It makes sure that number of available Pods is at least 2 and the number of total Pods is at most 4.</p><p><strong>$ kubectl describe deployments</strong></p><p><strong>Name: nginx-deployment</strong></p><p><strong>Namespace: default</strong></p><p><strong>CreationTimestamp: Thu, 30 Nov 2017 10:56:25 +0000</strong></p><p><strong>Labels: app=nginx</strong></p><p><strong>Annotations: deployment.kubernetes.io/revision=2</strong></p><p><strong>Selector: app=nginx</strong></p><p><strong>Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable</strong></p><p><strong>StrategyType: RollingUpdate</strong></p><p><strong>MinReadySeconds: 0</strong></p><p><strong>RollingUpdateStrategy: 25% max unavailable, 25% max surge</strong></p><p><strong>Pod Template:</strong></p><p><strong>Labels: app=nginx</strong></p><p><strong>Containers:</strong></p><p><strong>nginx:</strong></p><p><strong>Image: nginx:1.9.1</strong></p><p><strong>Port: 80/TCP</strong></p><p><strong>Environment: <code>&lt;none&gt;</code></strong></p><p><strong>Mounts: <code>&lt;none&gt;</code></strong></p><p><strong>Volumes: <code>&lt;none&gt;</code></strong></p><p><strong>Conditions:</strong></p><p><strong>Type Status Reason</strong></p><p><strong>---- ------ ------</strong></p><p><strong>Available True MinimumReplicasAvailable</strong></p><p><strong>Progressing True NewReplicaSetAvailable</strong></p><p><strong>OldReplicaSets: <code>&lt;none&gt;</code></strong></p><p><strong>NewReplicaSet: nginx-deployment-1564180365 (3/3 replicas created)</strong></p><p><strong>Events:</strong></p><p><strong>Type Reason Age From Message</strong></p><p><strong>---- ------ ---- ---- -------</strong></p><p><strong>Normal ScalingReplicaSet 2m deployment-controller Scaled up replica set nginx-deployment-2035384211 to 3</strong></p><p><strong>Normal ScalingReplicaSet 24s deployment-controller Scaled up replica set nginx-deployment-1564180365 to 1</strong></p><p><strong>Normal ScalingReplicaSet 22s deployment-controller Scaled down replica set nginx-deployment-2035384211 to 2</strong></p><p><strong>Normal ScalingReplicaSet 22s deployment-controller Scaled up replica set nginx-deployment-1564180365 to 2</strong></p><p><strong>Normal ScalingReplicaSet 19s deployment-controller Scaled down replica set nginx-deployment-2035384211 to 1</strong></p><p><strong>Normal ScalingReplicaSet 19s deployment-controller Scaled up replica set nginx-deployment-1564180365 to 3</strong></p><p><strong>Normal ScalingReplicaSet 14s deployment-controller Scaled down replica set nginx-deployment-2035384211 to 0</strong></p><p>Here we see that when we first created the Deployment, it created a ReplicaSet (nginx-deployment-2035384211) and scaled it up to 3 replicas directly. When we updated the Deployment, it created a new ReplicaSet (nginx-deployment-1564180365) and scaled it up to 1 and then scaled down the old ReplicaSet to 2, so that at least 2 Pods were available and at most 4 Pods were created at all times. It then continued scaling up and down the new and the old ReplicaSet, with the same rolling update strategy. Finally, we&#x27;ll have 3 available replicas in the new ReplicaSet, and the old ReplicaSet is scaled down to 0.</p><h6><strong>Rollover (aka multiple updates in-flight)</strong></h6><p>Each time a new deployment object is observed by the Deployment controller, a ReplicaSet is created to bring up the desired Pods if there is no existing ReplicaSet doing so. Existing ReplicaSet controlling Pods whose labels match <strong>.spec.selector</strong> but whose template does not match <strong>.spec.template</strong> are scaled down. Eventually, the new ReplicaSet will be scaled to <strong>.spec.replicas</strong> and all old ReplicaSets will be scaled to 0.</p><p>If you update a Deployment while an existing rollout is in progress, the Deployment will create a new ReplicaSet as per the update and start scaling that up, and will roll over the ReplicaSet that it was scaling up previously -- it will add it to its list of old ReplicaSets and will start scaling it down.</p><p>For example, suppose you create a Deployment to create 5 replicas of <strong>nginx:1.7.9</strong>, but then updates the Deployment to create 5 replicas of <strong>nginx:1.9.1</strong>, when only 3 replicas of <strong>nginx:1.7.9</strong>had been created. In that case, Deployment will immediately start killing the 3 <strong>nginx:1.7.9</strong> Pods that it had created, and will start creating <strong>nginx:1.9.1</strong> Pods. It will not wait for 5 replicas of <strong>nginx:1.7.9</strong> to be created before changing course.</p><h6><strong>Label selector updates</strong></h6><p>It is generally discouraged to make label selector updates and it is suggested to plan your selectors up front. In any case, if you need to perform a label selector update, exercise great caution and make sure you have grasped all of the implications.</p><p><strong>Note:</strong> In API version <strong>apps/v1</strong>, a Deployment&#x27;s label selector is immutable after it gets created.</p><ul><li>Selector additions require the pod template labels in the Deployment spec to be updated with the new label too, otherwise a validation error is returned. This change is a non-overlapping one, meaning that the new selector does not select ReplicaSets and Pods created with the old selector, resulting in orphaning all old ReplicaSets and creating a new ReplicaSet.</li><li>Selector updates -- that is, changing the existing value in a selector key -- result in the same behavior as additions.</li><li>Selector removals -- that is, removing an existing key from the Deployment selector -- do not require any changes in the pod template labels. No existing ReplicaSet is orphaned, and a new ReplicaSet is not created, but note that the removed label still exists in any existing Pods and ReplicaSets.</li></ul><h5><strong>Rolling Back a Deployment</strong></h5><p>Sometimes you may want to rollback a Deployment; for example, when the Deployment is not stable, such as crash looping. By default, all of the Deployment&#x27;s rollout history is kept in the system so that you can rollback anytime you want (you can change that by modifying revision history limit).</p><p><strong>Note:</strong> A Deployment&#x27;s revision is created when a Deployment&#x27;s rollout is triggered. This means that the new revision is created if and only if the Deployment&#x27;s pod template (<strong>.spec.template</strong>) is changed, for example if you update the labels or container images of the template. Other updates, such as scaling the Deployment, do not create a Deployment revision, so that we can facilitate simultaneous manual- or auto-scaling. This means that when you roll back to an earlier revision, only the Deployment&#x27;s pod template part is rolled back.</p><p>Suppose that we made a typo while updating the Deployment, by putting the image name as <strong>nginx:1.91</strong> instead of <strong>nginx:1.9.1</strong>:</p><p><strong>$ kubectl set image deployment/nginx-deployment nginx=nginx:1.91</strong></p><p><strong>deployment &quot;nginx-deployment&quot; image updated</strong></p><p>The rollout will be stuck.</p><p><strong>$ kubectl rollout status deployments nginx-deployment</strong></p><p><strong>Waiting for rollout to finish: 2 out of 3 new replicas have been updated<!-- -->.<!-- -->..</strong></p><p>Press Ctrl-C to stop the above rollout status watch. For more information on stuck rollouts, <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#deployment-status">read more here</a>.</p><p>You will also see that both the number of old replicas (nginx-deployment-1564180365 and nginx-deployment-2035384211) and new replicas (nginx-deployment-3066724191) are 2.</p><p><strong>$ kubectl get rs</strong></p><p><strong>NAME DESIRED CURRENT READY AGE</strong></p><p><strong>nginx-deployment-1564180365 2 2 0 25s</strong></p><p><strong>nginx-deployment-2035384211 0 0 0 36s</strong></p><p><strong>nginx-deployment-3066724191 2 2 2 6s</strong></p><p>Looking at the Pods created, you will see that the 2 Pods created by new ReplicaSet are stuck in an image pull loop.</p><p><strong>$ kubectl get pods</strong></p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>nginx-deployment-1564180365-70iae 1/1 Running 0 25s</strong></p><p><strong>nginx-deployment-1564180365-jbqqo 1/1 Running 0 25s</strong></p><p><strong>nginx-deployment-3066724191-08mng 0/1 ImagePullBackOff 0 6s</strong></p><p><strong>nginx-deployment-3066724191-eocby 0/1 ImagePullBackOff 0 6s</strong></p><p><strong>Note:</strong> The Deployment controller will stop the bad rollout automatically, and will stop scaling up the new ReplicaSet. This depends on the rollingUpdate parameters (<strong>maxUnavailable</strong>specifically) that you have specified. Kubernetes by default sets the value to 1 and <strong>spec.replicas</strong> to 1 so if you haven&#x27;t cared about setting those parameters, your Deployment can have 100% unavailability by default! This will be fixed in Kubernetes in a future version.</p><p><strong>$ kubectl describe deployment</strong></p><p><strong>Name: nginx-deployment</strong></p><p><strong>Namespace: default</strong></p><p><strong>CreationTimestamp: Tue, 15 Mar 2016 14:48:04 -0700</strong></p><p><strong>Labels: app=nginx</strong></p><p><strong>Selector: app=nginx</strong></p><p><strong>Replicas: 2 updated | 3 total | 2 available | 2 unavailable</strong></p><p><strong>StrategyType: RollingUpdate</strong></p><p><strong>MinReadySeconds: 0</strong></p><p><strong>RollingUpdateStrategy: 1 max unavailable, 1 max surge</strong></p><p><strong>OldReplicaSets: nginx-deployment-1564180365 (2/2 replicas created)</strong></p><p><strong>NewReplicaSet: nginx-deployment-3066724191 (2/2 replicas created)</strong></p><p><strong>Events:</strong></p><p><strong>FirstSeen LastSeen Count From SubobjectPath Type Reason Message</strong></p><p><strong>--------- -------- ----- ---- ------------- -------- ------ -------</strong></p><p><strong>1m 1m 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-2035384211 to 3</strong></p><p><strong>22s 22s 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-1564180365 to 1</strong></p><p><strong>22s 22s 1 {deployment-controller } Normal ScalingReplicaSet Scaled down replica set nginx-deployment-2035384211 to 2</strong></p><p><strong>22s 22s 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-1564180365 to 2</strong></p><p><strong>21s 21s 1 {deployment-controller } Normal ScalingReplicaSet Scaled down replica set nginx-deployment-2035384211 to 0</strong></p><p><strong>21s 21s 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-1564180365 to 3</strong></p><p><strong>13s 13s 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-3066724191 to 1</strong></p><p><strong>13s 13s 1 {deployment-controller } Normal ScalingReplicaSet Scaled down replica set nginx-deployment-1564180365 to 2</strong></p><p><strong>13s 13s 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-3066724191 to 2</strong></p><p>To fix this, we need to rollback to a previous revision of Deployment that is stable.</p><h6><strong>Checking Rollout History of a Deployment</strong></h6><p>First, check the revisions of this deployment:</p><p><strong>$ kubectl rollout history deployment/nginx-deployment</strong></p><p><strong>deployments &quot;nginx-deployment&quot;</strong></p><p><strong>REVISION CHANGE-CAUSE</strong></p><p><strong>1 kubectl create -f docs/user-guide/nginx-deployment.yaml --record</strong></p><p><strong>2 kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1</strong></p><p><strong>3 kubectl set image deployment/nginx-deployment nginx=nginx:1.91</strong></p><p>Because we recorded the command while creating this Deployment using <strong>--record</strong>, we can easily see the changes we made in each revision.</p><p>To further see the details of each revision, run:</p><p><strong>$ kubectl rollout history deployment/nginx-deployment --revision=2</strong></p><p><strong>deployments &quot;nginx-deployment&quot; revision 2</strong></p><p><strong>Labels: app=nginx</strong></p><p><strong>pod-template-hash=1159050644</strong></p><p><strong>Annotations: kubernetes.io/change-cause=kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1</strong></p><p><strong>Containers:</strong></p><p><strong>nginx:</strong></p><p><strong>Image: nginx:1.9.1</strong></p><p><strong>Port: 80/TCP</strong></p><p><strong>QoS Tier:</strong></p><p><strong>cpu: BestEffort</strong></p><p><strong>memory: BestEffort</strong></p><p><strong>Environment Variables: <code>&lt;none&gt;</code></strong></p><p><strong>No volumes.</strong></p><h6><strong>Rolling Back to a Previous Revision</strong></h6><p>Now we&#x27;ve decided to undo the current rollout and rollback to the previous revision:</p><p><strong>$ kubectl rollout undo deployment/nginx-deployment</strong></p><p><strong>deployment &quot;nginx-deployment&quot; rolled back</strong></p><p>Alternatively, you can rollback to a specific revision by specify that in <strong>--to-revision</strong>:</p><p><strong>$ kubectl rollout undo deployment/nginx-deployment --to-revision=2</strong></p><p><strong>deployment &quot;nginx-deployment&quot; rolled back</strong></p><p>For more details about rollout related commands, read <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#rollout"><strong>kubectl rollout</strong></a>.</p><p>The Deployment is now rolled back to a previous stable revision. As you can see, a <strong>DeploymentRollback</strong> event for rolling back to revision 2 is generated from Deployment controller.</p><p><strong>$ kubectl get deployment</strong></p><p><strong>NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE</strong></p><p><strong>nginx-deployment 3 3 3 3 30m</strong></p><p><strong>$ kubectl describe deployment</strong></p><p><strong>Name: nginx-deployment</strong></p><p><strong>Namespace: default</strong></p><p><strong>CreationTimestamp: Tue, 15 Mar 2016 14:48:04 -0700</strong></p><p><strong>Labels: app=nginx</strong></p><p><strong>Selector: app=nginx</strong></p><p><strong>Replicas: 3 updated | 3 total | 3 available | 0 unavailable</strong></p><p><strong>StrategyType: RollingUpdate</strong></p><p><strong>MinReadySeconds: 0</strong></p><p><strong>RollingUpdateStrategy: 1 max unavailable, 1 max surge</strong></p><p><strong>OldReplicaSets: <code>&lt;none&gt;</code></strong></p><p><strong>NewReplicaSet: nginx-deployment-1564180365 (3/3 replicas created)</strong></p><p><strong>Events:</strong></p><p><strong>FirstSeen LastSeen Count From SubobjectPath Type Reason Message</strong></p><p><strong>--------- -------- ----- ---- ------------- -------- ------ -------</strong></p><p><strong>30m 30m 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-2035384211 to 3</strong></p><p><strong>29m 29m 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-1564180365 to 1</strong></p><p><strong>29m 29m 1 {deployment-controller } Normal ScalingReplicaSet Scaled down replica set nginx-deployment-2035384211 to 2</strong></p><p><strong>29m 29m 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-1564180365 to 2</strong></p><p><strong>29m 29m 1 {deployment-controller } Normal ScalingReplicaSet Scaled down replica set nginx-deployment-2035384211 to 0</strong></p><p><strong>29m 29m 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-3066724191 to 2</strong></p><p><strong>29m 29m 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-3066724191 to 1</strong></p><p><strong>29m 29m 1 {deployment-controller } Normal ScalingReplicaSet Scaled down replica set nginx-deployment-1564180365 to 2</strong></p><p><strong>2m 2m 1 {deployment-controller } Normal ScalingReplicaSet Scaled down replica set nginx-deployment-3066724191 to 0</strong></p><p><strong>2m 2m 1 {deployment-controller } Normal DeploymentRollback Rolled back deployment &quot;nginx-deployment&quot; to revision 2</strong></p><p><strong>29m 2m 2 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-1564180365 to 3</strong></p><h5><strong>Scaling a Deployment</strong></h5><p>You can scale a Deployment by using the following command:</p><p><strong>$ kubectl scale deployment nginx-deployment --replicas=10</strong></p><p><strong>deployment &quot;nginx-deployment&quot; scaled</strong></p><p>Assuming <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/">horizontal pod autoscaling</a> is enabled in your cluster, you can setup an autoscaler for your Deployment and choose the minimum and maximum number of Pods you want to run based on the CPU utilization of your existing Pods.</p><p><strong>$ kubectl autoscale deployment nginx-deployment --min=10 --max=15 --cpu-percent=80</strong></p><p><strong>deployment &quot;nginx-deployment&quot; autoscaled</strong></p><h6><strong>Proportional scaling</strong></h6><p>RollingUpdate Deployments support running multiple versions of an application at the same time. When you or an autoscaler scales a RollingUpdate Deployment that is in the middle of a rollout (either in progress or paused), then the Deployment controller will balance the additional replicas in the existing active ReplicaSets (ReplicaSets with Pods) in order to mitigate risk. This is called proportional scaling.</p><p>For example, you are running a Deployment with 10 replicas, <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#max-surge">maxSurge</a>=3, and <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#max-unavailable">maxUnavailable</a>=2.</p><p><strong>$ kubectl get deploy</strong></p><p><strong>NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE</strong></p><p><strong>nginx-deployment 10 10 10 10 50s</strong></p><p>You update to a new image which happens to be unresolvable from inside the cluster.</p><p><strong>$ kubectl set image deploy/nginx-deployment nginx=nginx:sometag</strong></p><p><strong>deployment &quot;nginx-deployment&quot; image updated</strong></p><p>The image update starts a new rollout with ReplicaSet nginx-deployment-1989198191, but it&#x27;s blocked due to the <strong>maxUnavailable</strong> requirement that we mentioned above.</p><p><strong>$ kubectl get rs</strong></p><p><strong>NAME DESIRED CURRENT READY AGE</strong></p><p><strong>nginx-deployment-1989198191 5 5 0 9s</strong></p><p><strong>nginx-deployment-618515232 8 8 8 1m</strong></p><p>Then a new scaling request for the Deployment comes along. The autoscaler increments the Deployment replicas to 15. The Deployment controller needs to decide where to add these new 5 replicas. If we weren&#x27;t using proportional scaling, all 5 of them would be added in the new ReplicaSet. With proportional scaling, we spread the additional replicas across all ReplicaSets. Bigger proportions go to the ReplicaSets with the most replicas and lower proportions go to ReplicaSets with less replicas. Any leftovers are added to the ReplicaSet with the most replicas. ReplicaSets with zero replicas are not scaled up.</p><p>In our example above, 3 replicas will be added to the old ReplicaSet and 2 replicas will be added to the new ReplicaSet. The rollout process should eventually move all replicas to the new ReplicaSet, assuming the new replicas become healthy.</p><p><strong>$ kubectl get deploy</strong></p><p><strong>NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE</strong></p><p><strong>nginx-deployment 15 18 7 8 7m</strong></p><p><strong>$ kubectl get rs</strong></p><p><strong>NAME DESIRED CURRENT READY AGE</strong></p><p><strong>nginx-deployment-1989198191 7 7 0 7m</strong></p><p><strong>nginx-deployment-618515232 11 11 11 7m</strong></p><h5><strong>Pausing and Resuming a Deployment</strong></h5><p>You can pause a Deployment before triggering one or more updates and then resume it. This will allow you to apply multiple fixes in between pausing and resuming without triggering unnecessary rollouts.</p><p>For example, with a Deployment that was just created:</p><p><strong>$ kubectl get deploy</strong></p><p><strong>NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE</strong></p><p><strong>nginx 3 3 3 3 1m</strong></p><p><strong>$ kubectl get rs</strong></p><p><strong>NAME DESIRED CURRENT READY AGE</strong></p><p><strong>nginx-2142116321 3 3 3 1m</strong></p><p>Pause by running the following command:</p><p><strong>$ kubectl rollout pause deployment/nginx-deployment</strong></p><p><strong>deployment &quot;nginx-deployment&quot; paused</strong></p><p>Then update the image of the Deployment:</p><p><strong>$ kubectl set image deploy/nginx-deployment nginx=nginx:1.9.1</strong></p><p><strong>deployment &quot;nginx-deployment&quot; image updated</strong></p><p>Notice that no new rollout started:</p><p><strong>$ kubectl rollout history deploy/nginx-deployment</strong></p><p><strong>deployments &quot;nginx&quot;</strong></p><p><strong>REVISION CHANGE-CAUSE</strong></p><p><strong>1 <code>&lt;none&gt;</code></strong></p><p><strong>$ kubectl get rs</strong></p><p><strong>NAME DESIRED CURRENT READY AGE</strong></p><p><strong>nginx-2142116321 3 3 3 2m</strong></p><p>You can make as many updates as you wish, for example, update the resources that will be used:</p><p><strong>$ kubectl set resources deployment nginx-deployment -c=nginx --limits=cpu=200m,memory=512Mi</strong></p><p><strong>deployment &quot;nginx-deployment&quot; resource requirements updated</strong></p><p>The initial state of the Deployment prior to pausing it will continue its function, but new updates to the Deployment will not have any effect as long as the Deployment is paused.</p><p>Eventually, resume the Deployment and observe a new ReplicaSet coming up with all the new updates:</p><p><strong>$ kubectl rollout resume deploy/nginx-deployment</strong></p><p><strong>deployment &quot;nginx&quot; resumed</strong></p><p><strong>$ kubectl get rs -w</strong></p><p><strong>NAME DESIRED CURRENT READY AGE</strong></p><p><strong>nginx-2142116321 2 2 2 2m</strong></p><p><strong>nginx-3926361531 2 2 0 6s</strong></p><p><strong>nginx-3926361531 2 2 1 18s</strong></p><p><strong>nginx-2142116321 1 2 2 2m</strong></p><p><strong>nginx-2142116321 1 2 2 2m</strong></p><p><strong>nginx-3926361531 3 2 1 18s</strong></p><p><strong>nginx-3926361531 3 2 1 18s</strong></p><p><strong>nginx-2142116321 1 1 1 2m</strong></p><p><strong>nginx-3926361531 3 3 1 18s</strong></p><p><strong>nginx-3926361531 3 3 2 19s</strong></p><p><strong>nginx-2142116321 0 1 1 2m</strong></p><p><strong>nginx-2142116321 0 1 1 2m</strong></p><p><strong>nginx-2142116321 0 0 0 2m</strong></p><p><strong>nginx-3926361531 3 3 3 20s</strong></p><p><strong>\^C</strong></p><p><strong>$ kubectl get rs</strong></p><p><strong>NAME DESIRED CURRENT READY AGE</strong></p><p><strong>nginx-2142116321 0 0 0 2m</strong></p><p><strong>nginx-3926361531 3 3 3 28s</strong></p><p><strong>Note:</strong> You cannot rollback a paused Deployment until you resume it.</p><h5><strong>Deployment status</strong></h5><p>A Deployment enters various states during its lifecycle. It can be <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#progressing-deployment">progressing</a> while rolling out a new ReplicaSet, it can be <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#complete-deployment">complete</a>, or it can <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#failed-deployment">fail to progress</a>.</p><h6><strong>Progressing Deployment</strong></h6><p>Kubernetes marks a Deployment as progressing when one of the following tasks is performed:</p><ul><li>The Deployment creates a new ReplicaSet.</li><li>The Deployment is scaling up its newest ReplicaSet.</li><li>The Deployment is scaling down its older ReplicaSet(s).</li><li>New Pods become ready or available (ready for at least <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#min-ready-seconds">MinReadySeconds</a>).</li></ul><p>You can monitor the progress for a Deployment by using <strong>kubectl rollout status</strong>.</p><h6><strong>Complete Deployment</strong></h6><p>Kubernetes marks a Deployment as complete when it has the following characteristics:</p><ul><li>All of the replicas associated with the Deployment have been updated to the latest version you&#x27;ve specified, meaning any updates you&#x27;ve requested have been completed.</li><li>All of the replicas associated with the Deployment are available.</li><li>No old replicas for the Deployment are running.</li></ul><p>You can check if a Deployment has completed by using <strong>kubectl rollout status</strong>. If the rollout completed successfully, <strong>kubectl rollout status</strong> returns a zero exit code.</p><p><strong>$ kubectl rollout status deploy/nginx-deployment</strong></p><p><strong>Waiting for rollout to finish: 2 of 3 updated replicas are available<!-- -->.<!-- -->..</strong></p><p><strong>deployment &quot;nginx&quot; successfully rolled out</strong></p><p><strong>$ echo $?</strong></p><p><strong>0</strong></p><h6><strong>Failed Deployment</strong></h6><p>Your Deployment may get stuck trying to deploy its newest ReplicaSet without ever completing. This can occur due to some of the following factors:</p><ul><li>Insufficient quota</li><li>Readiness probe failures</li><li>Image pull errors</li><li>Insufficient permissions</li><li>Limit ranges</li><li>Application runtime misconfiguration</li></ul><p>One way you can detect this condition is to specify a deadline parameter in your Deployment spec: (<a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#progress-deadline-seconds"><strong>spec.progressDeadlineSeconds</strong></a>). <strong>spec.progressDeadlineSeconds</strong> denotes the number of seconds the Deployment controller waits before indicating (in the Deployment status) that the Deployment progress has stalled.</p><p>The following <strong>kubectl</strong> command sets the spec with <strong>progressDeadlineSeconds</strong> to make the controller report lack of progress for a Deployment after 10 minutes:</p><p><strong>$ kubectl patch deployment/nginx-deployment -p \&#x27;{&quot;spec&quot;:{&quot;progressDeadlineSeconds&quot;:600}}\&#x27;</strong></p><p><strong>deployment &quot;nginx-deployment&quot; patched</strong></p><p>Once the deadline has been exceeded, the Deployment controller adds a DeploymentCondition with the following attributes to the Deployment&#x27;s <strong>status.conditions</strong>:</p><ul><li>Type=Progressing</li><li>Status=False</li><li>Reason=ProgressDeadlineExceeded</li></ul><p>See the <a href="https://git.k8s.io/community/contributors/devel/api-conventions.md#typical-status-properties">Kubernetes API conventions</a> for more information on status conditions.</p><p><strong>Note:</strong> Kubernetes will take no action on a stalled Deployment other than to report a status condition with <strong>Reason=ProgressDeadlineExceeded</strong>. Higher level orchestrators can take advantage of it and act accordingly, for example, rollback the Deployment to its previous version.</p><p><strong>Note:</strong> If you pause a Deployment, Kubernetes does not check progress against your specified deadline. You can safely pause a Deployment in the middle of a rollout and resume without triggering the condition for exceeding the deadline.</p><p>You may experience transient errors with your Deployments, either due to a low timeout that you have set or due to any other kind of error that can be treated as transient. For example, let&#x27;s suppose you have insufficient quota. If you describe the Deployment you will notice the following section:</p><p><strong>$ kubectl describe deployment nginx-deployment</strong></p><p><strong><code>&lt;\...&gt;</code></strong></p><p><strong>Conditions:</strong></p><p><strong>Type Status Reason</strong></p><p><strong>---- ------ ------</strong></p><p><strong>Available True MinimumReplicasAvailable</strong></p><p><strong>Progressing True ReplicaSetUpdated</strong></p><p><strong>ReplicaFailure True FailedCreate</strong></p><p><strong><code>&lt;\...&gt;</code></strong></p><p>If you run <strong>kubectl get deployment nginx-deployment -o yaml</strong>, the Deployment status might look like this:</p><p><strong>status:</strong></p><p><strong>availableReplicas: 2</strong></p><p><strong>conditions:</strong></p><p><strong>- lastTransitionTime: 2016-10-04T12:25:39Z</strong></p><p><strong>lastUpdateTime: 2016-10-04T12:25:39Z</strong></p><p><strong>message: Replica set &quot;nginx-deployment-4262182780&quot; is progressing.</strong></p><p><strong>reason: ReplicaSetUpdated</strong></p><p><strong>status: &quot;True&quot;</strong></p><p><strong>type: Progressing</strong></p><p><strong>- lastTransitionTime: 2016-10-04T12:25:42Z</strong></p><p><strong>lastUpdateTime: 2016-10-04T12:25:42Z</strong></p><p><strong>message: Deployment has minimum availability.</strong></p><p><strong>reason: MinimumReplicasAvailable</strong></p><p><strong>status: &quot;True&quot;</strong></p><p><strong>type: Available</strong></p><p><strong>- lastTransitionTime: 2016-10-04T12:25:39Z</strong></p><p><strong>lastUpdateTime: 2016-10-04T12:25:39Z</strong></p><p><strong>message: \&#x27;Error creating: pods &quot;nginx-deployment-4262182780-&quot; is forbidden: exceeded quota:</strong></p><p><strong>object-counts, requested: pods=1, used: pods=3, limited: pods=2\&#x27;</strong></p><p><strong>reason: FailedCreate</strong></p><p><strong>status: &quot;True&quot;</strong></p><p><strong>type: ReplicaFailure</strong></p><p><strong>observedGeneration: 3</strong></p><p><strong>replicas: 2</strong></p><p><strong>unavailableReplicas: 2</strong></p><p>Eventually, once the Deployment progress deadline is exceeded, Kubernetes updates the status and the reason for the Progressing condition:</p><p><strong>Conditions:</strong></p><p><strong>Type Status Reason</strong></p><p><strong>---- ------ ------</strong></p><p><strong>Available True MinimumReplicasAvailable</strong></p><p><strong>Progressing False ProgressDeadlineExceeded</strong></p><p><strong>ReplicaFailure True FailedCreate</strong></p><p>You can address an issue of insufficient quota by scaling down your Deployment, by scaling down other controllers you may be running, or by increasing quota in your namespace. If you satisfy the quota conditions and the Deployment controller then completes the Deployment rollout, you&#x27;ll see the Deployment&#x27;s status update with a successful condition (<strong>Status=True</strong> and <strong>Reason=NewReplicaSetAvailable</strong>).</p><p><strong>Conditions:</strong></p><p><strong>Type Status Reason</strong></p><p><strong>---- ------ ------</strong></p><p><strong>Available True MinimumReplicasAvailable</strong></p><p><strong>Progressing True NewReplicaSetAvailable</strong></p><p><strong>Type=Available</strong> with <strong>Status=True</strong> means that your Deployment has minimum availability. Minimum availability is dictated by the parameters specified in the deployment strategy. <strong>Type=Progressing</strong> with <strong>Status=True</strong> means that your Deployment is either in the middle of a rollout and it is progressing or that it has successfully completed its progress and the minimum required new replicas are available (see the Reason of the condition for the particulars - in our case <strong>Reason=NewReplicaSetAvailable</strong> means that the Deployment is complete).</p><p>You can check if a Deployment has failed to progress by using <strong>kubectl rollout status</strong>. <strong>kubectl rollout status</strong> returns a non-zero exit code if the Deployment has exceeded the progression deadline.</p><p><strong>$ kubectl rollout status deploy/nginx-deployment</strong></p><p><strong>Waiting for rollout to finish: 2 out of 3 new replicas have been updated<!-- -->.<!-- -->..</strong></p><p><strong>error: deployment &quot;nginx&quot; exceeded its progress deadline</strong></p><p><strong>$ echo $?</strong></p><p><strong>1</strong></p><h6><strong>Operating on a failed deployment</strong></h6><p>All actions that apply to a complete Deployment also apply to a failed Deployment. You can scale it up/down, roll back to a previous revision, or even pause it if you need to apply multiple tweaks in the Deployment pod template.</p><h5><strong>Clean up Policy</strong></h5><p>You can set <strong>.spec.revisionHistoryLimit</strong> field in a Deployment to specify how many old ReplicaSets for this Deployment you want to retain. The rest will be garbage-collected in the background. By default, all revision history will be kept. In a future version, it will default to switch to 2.</p><p><strong>Note:</strong> Explicitly setting this field to 0, will result in cleaning up all the history of your Deployment thus that Deployment will not be able to roll back.</p><h5><strong>Use Cases</strong></h5><h6><strong>Canary Deployment</strong></h6><p>If you want to roll out releases to a subset of users or servers using the Deployment, you can create multiple Deployments, one for each release, following the canary pattern described in <a href="https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#canary-deployments">managing resources</a>.</p><h5><strong>Writing a Deployment Spec</strong></h5><p>As with all other Kubernetes configs, a Deployment needs <strong>apiVersion</strong>, <strong>kind</strong>, and <strong>metadata</strong>fields. For general information about working with config files, see <a href="https://kubernetes.io/docs/tutorials/stateless-application/run-stateless-application-deployment/">deploying applications</a>, configuring containers, and <a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/overview/">using kubectl to manage resources</a> documents.</p><p>A Deployment also needs a <a href="https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status"><strong>.spec</strong> section</a>.</p><h6><strong>Pod Template</strong></h6><p>The <strong>.spec.template</strong> is the only required field of the <strong>.spec</strong>.</p><p>The <strong>.spec.template</strong> is a <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/#pod-templates">pod template</a>. It has exactly the same schema as a <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/">Pod</a>, except it is nested and does not have an <strong>apiVersion</strong> or <strong>kind</strong>.</p><p>In addition to required fields for a Pod, a pod template in a Deployment must specify appropriate labels and an appropriate restart policy. For labels, make sure not to overlap with other controllers. See <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#selector">selector</a>).</p><p>Only a <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/"><strong>.spec.template.spec.restartPolicy</strong></a> equal to <strong>Always</strong> is allowed, which is the default if not specified.</p><h6><strong>Replicas</strong></h6><p><strong>.spec.replicas</strong> is an optional field that specifies the number of desired Pods. It defaults to 1.</p><h6><strong>Selector</strong></h6><p><strong>.spec.selector</strong> is an optional field that specifies a <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/">label selector</a> for the Pods targeted by this deployment.</p><p><strong>.spec.selector</strong> must match <strong>.spec.template.metadata.labels</strong>, or it will be rejected by the API.</p><p>In API version <strong>apps/v1</strong>, <strong>.spec.selector</strong> and <strong>.metadata.labels</strong> do not default to <strong>.spec.template.metadata.labels</strong> if not set. So they must be set explicitly. Also note that <strong>.spec.selector</strong> is immutable after creation of the Deployment in <strong>apps/v1</strong>.</p><p>A Deployment may terminate Pods whose labels match the selector if their template is different from <strong>.spec.template</strong> or if the total number of such Pods exceeds <strong>.spec.replicas</strong>. It brings up new Pods with <strong>.spec.template</strong> if the number of Pods is less than the desired number.</p><p><strong>Note:</strong> You should not create other pods whose labels match this selector, either directly, by creating another Deployment, or by creating another controller such as a ReplicaSet or a ReplicationController. If you do so, the first Deployment thinks that it created these other pods. Kubernetes does not stop you from doing this.</p><p>If you have multiple controllers that have overlapping selectors, the controllers will fight with each other and won&#x27;t behave correctly.</p><h6><strong>Strategy</strong></h6><p><strong>.spec.strategy</strong> specifies the strategy used to replace old Pods by new ones. <strong>.spec.strategy.type</strong> can be &quot;Recreate&quot; or &quot;RollingUpdate&quot;. &quot;RollingUpdate&quot; is the default value.</p><p><strong>Recreate Deployment</strong></p><p>All existing Pods are killed before new ones are created when <strong>.spec.strategy.type==Recreate</strong>.</p><p><strong>Rolling Update Deployment</strong></p><p>The Deployment updates Pods in a <a href="https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/">rolling update</a> fashion when <strong>.spec.strategy.type==RollingUpdate</strong>. You can specify <strong>maxUnavailable</strong> and <strong>maxSurge</strong> to control the rolling update process.</p><p><strong>Max Unavailable</strong></p><p><strong>.spec.strategy.rollingUpdate.maxUnavailable</strong> is an optional field that specifies the maximum number of Pods that can be unavailable during the update process. The value can be an absolute number (for example, 5) or a percentage of desired Pods (for example, 10%). The absolute number is calculated from percentage by rounding down. The value cannot be 0 if <strong>.spec.strategy.rollingUpdate.maxSurge</strong> is 0. The default value is 25%.</p><p>For example, when this value is set to 30%, the old ReplicaSet can be scaled down to 70% of desired Pods immediately when the rolling update starts. Once new Pods are ready, old ReplicaSet can be scaled down further, followed by scaling up the new ReplicaSet, ensuring that the total number of Pods available at all times during the update is at least 70% of the desired Pods.</p><p><strong>Max Surge</strong></p><p><strong>.spec.strategy.rollingUpdate.maxSurge</strong> is an optional field that specifies the maximum number of Pods that can be created over the desired number of Pods. The value can be an absolute number (for example, 5) or a percentage of desired Pods (for example, 10%). The value cannot be 0 if <strong>MaxUnavailable</strong> is 0. The absolute number is calculated from the percentage by rounding up. The default value is 25%.</p><p>For example, when this value is set to 30%, the new ReplicaSet can be scaled up immediately when the rolling update starts, such that the total number of old and new Pods does not exceed 130% of desired Pods. Once old Pods have been killed, the new ReplicaSet can be scaled up further, ensuring that the total number of Pods running at any time during the update is at most 130% of desired Pods.</p><h6><strong>Progress Deadline Seconds</strong></h6><p><strong>.spec.progressDeadlineSeconds</strong> is an optional field that specifies the number of seconds you want to wait for your Deployment to progress before the system reports back that the Deployment has <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#failed-deployment">failed progressing</a> - surfaced as a condition with <strong>Type=Progressing</strong>, <strong>Status=False</strong>. and <strong>Reason=ProgressDeadlineExceeded</strong> in the status of the resource. The deployment controller will keep retrying the Deployment. In the future, once automatic rollback will be implemented, the deployment controller will roll back a Deployment as soon as it observes such a condition.</p><p>If specified, this field needs to be greater than <strong>.spec.minReadySeconds</strong>.</p><h6><strong>Min Ready Seconds</strong></h6><p><strong>.spec.minReadySeconds</strong> is an optional field that specifies the minimum number of seconds for which a newly created Pod should be ready without any of its containers crashing, for it to be considered available. This defaults to 0 (the Pod will be considered available as soon as it is ready). To learn more about when a Pod is considered ready, see <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes">Container Probes</a>.</p><h6><strong>Rollback To</strong></h6><p>Field <strong>.spec.rollbackTo</strong> has been deprecated in API versions <strong>extensions/v1beta1</strong> and <strong>apps/v1beta1</strong>, and is no longer supported in API versions starting <strong>apps/v1beta2</strong>. Instead, <strong>kubectl rollout undo</strong> as introduced in <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-back-to-a-previous-revision">Rolling Back to a Previous Revision</a> should be used.</p><h6><strong>Revision History Limit</strong></h6><p>A Deployment&#x27;s revision history is stored in the replica sets it controls.</p><p><strong>.spec.revisionHistoryLimit</strong> is an optional field that specifies the number of old ReplicaSets to retain to allow rollback. Its ideal value depends on the frequency and stability of new Deployments. All old ReplicaSets will be kept by default, consuming resources in <strong>etcd</strong> and crowding the output of <strong>kubectl get rs</strong>, if this field is not set. The configuration of each Deployment revision is stored in its ReplicaSets; therefore, once an old ReplicaSet is deleted, you lose the ability to rollback to that revision of Deployment.</p><p>More specifically, setting this field to zero means that all old ReplicaSets with 0 replica will be cleaned up. In this case, a new Deployment rollout cannot be undone, since its revision history is cleaned up.</p><h6><strong>Paused</strong></h6><p><strong>.spec.paused</strong> is an optional boolean field for pausing and resuming a Deployment. The only difference between a paused Deployment and one that is not paused, is that any changes into the PodTemplateSpec of the paused Deployment will not trigger new rollouts as long as it is paused. A Deployment is not paused by default when it is created.</p><h5><strong>Alternative to Deployments</strong></h5><h6><strong>kubectl rolling update</strong></h6><p><a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#rolling-update">Kubectl rolling update</a> updates Pods and ReplicationControllers in a similar fashion. But Deployments are recommended, since they are declarative, server side, and have additional features, such as rolling back to any previous revision even after the rolling update is done.</p><h4>StatefulSets</h4><p>StatefulSet is the workload API object used to manage stateful applications.</p><p><strong>Note:</strong> StatefulSets are stable (GA) in 1.9.</p><p>Manages the deployment and scaling of a set of <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/">Pods</a>, and provides guarantees about the ordering and uniqueness of these Pods.</p><p>Like a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployment</a>, a StatefulSet manages Pods that are based on an identical container spec. Unlike a Deployment, a StatefulSet maintains a sticky identity for each of their Pods. These pods are created from the same spec, but are not interchangeable: each has a persistent identifier that it maintains across any rescheduling.</p><p>A StatefulSet operates under the same pattern as any other Controller. You define your desired state in a StatefulSet object, and the StatefulSet controller makes any necessary updates to get there from the current state.</p><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#using-statefulsets"><strong>Using StatefulSets</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#limitations"><strong>Limitations</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#components"><strong>Components</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-selector"><strong>Pod Selector</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-identity"><strong>Pod Identity</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#ordinal-index"><strong>Ordinal Index</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#stable-network-id"><strong>Stable Network ID</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#stable-storage"><strong>Stable Storage</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-name-label"><strong>Pod Name Label</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#deployment-and-scaling-guarantees"><strong>Deployment and Scaling Guarantees</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies"><strong>Pod Management Policies</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#orderedready-pod-management"><strong>OrderedReady Pod Management</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#parallel-pod-management"><strong>Parallel Pod Management</strong></a></li></ul></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies"><strong>Update Strategies</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#on-delete"><strong>On Delete</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#rolling-updates"><strong>Rolling Updates</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#partitions"><strong>Partitions</strong></a></li></ul></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h5><strong>Using StatefulSets</strong></h5><p>StatefulSets are valuable for applications that require one or more of the following.</p><ul><li>Stable, unique network identifiers.</li><li>Stable, persistent storage.</li><li>Ordered, graceful deployment and scaling.</li><li>Ordered, graceful deletion and termination.</li><li>Ordered, automated rolling updates.</li></ul><p>In the above, stable is synonymous with persistence across Pod (re)scheduling. If an application doesn&#x27;t require any stable identifiers or ordered deployment, deletion, or scaling, you should deploy your application with a controller that provides a set of stateless replicas. Controllers such as<a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployment</a> or <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/">ReplicaSet</a> may be better suited to your stateless needs.</p><h5><strong>Limitations</strong></h5><ul><li>StatefulSet was a beta resource prior to 1.9 and not available in any Kubernetes release prior to 1.5.</li><li>As with all alpha/beta resources, you can disable StatefulSet through the <strong>--runtime-config</strong>option passed to the apiserver.</li><li>The storage for a given Pod must either be provisioned by a <a href="https://github.com/kubernetes/examples/tree/master/staging/persistent-volume-provisioning/README.md">PersistentVolume Provisioner</a> based on the requested <strong>storage class</strong>, or pre-provisioned by an admin.</li><li>Deleting and/or scaling a StatefulSet down will not delete the volumes associated with the StatefulSet. This is done to ensure data safety, which is generally more valuable than an automatic purge of all related StatefulSet resources.</li><li>StatefulSets currently require a <a href="https://kubernetes.io/docs/concepts/services-networking/service/#headless-services">Headless Service</a> to be responsible for the network identity of the Pods. You are responsible for creating this Service.</li></ul><h5><strong>Components</strong></h5><p>The example below demonstrates the components of a StatefulSet.</p><ul><li>A Headless Service, named nginx, is used to control the network domain.</li><li>The StatefulSet, named web, has a Spec that indicates that 3 replicas of the nginx container will be launched in unique Pods.</li><li>The volumeClaimTemplates will provide stable storage using <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">PersistentVolumes</a> provisioned by a PersistentVolume Provisioner.</li></ul><p><strong>apiVersion: v1</strong></p><p><strong>kind: Service</strong></p><p><strong>metadata:</strong></p><p><strong>name: nginx</strong></p><p><strong>labels:</strong></p><p><strong>app: nginx</strong></p><p><strong>spec:</strong></p><p><strong>ports:</strong></p><p><strong>- port: 80</strong></p><p><strong>name: web</strong></p><p><strong>clusterIP: None</strong></p><p><strong>selector:</strong></p><p><strong>app: nginx</strong></p><p><strong>---</strong></p><p><strong>apiVersion: apps/v1</strong></p><p><strong>kind: StatefulSet</strong></p><p><strong>metadata:</strong></p><p><strong>name: web</strong></p><p><strong>spec:</strong></p><p><strong>selector:</strong></p><p><strong>matchLabels:</strong></p><p><strong>app: nginx <em># has to match .spec.template.metadata.labels</em></strong></p><p><strong>serviceName: &quot;nginx&quot;</strong></p><p><strong>replicas: 3 <em># by default is 1</em></strong></p><p><strong>template:</strong></p><p><strong>metadata:</strong></p><p><strong>labels:</strong></p><p><strong>app: nginx <em># has to match .spec.selector.matchLabels</em></strong></p><p><strong>spec:</strong></p><p><strong>terminationGracePeriodSeconds: 10</strong></p><p><strong>containers:</strong></p><p><strong>- name: nginx</strong></p><p><strong>image: k8s.gcr.io/nginx-slim:0.8</strong></p><p><strong>ports:</strong></p><p><strong>- containerPort: 80</strong></p><p><strong>name: web</strong></p><p><strong>volumeMounts:</strong></p><p><strong>- name: www</strong></p><p><strong>mountPath: /usr/share/nginx/html</strong></p><p><strong>volumeClaimTemplates:</strong></p><p><strong>- metadata:</strong></p><p><strong>name: www</strong></p><p><strong>spec:</strong></p><p><strong>accessModes: <!-- -->[ &quot;ReadWriteOnce&quot; ]</strong></p><p><strong>storageClassName: &quot;my-storage-class&quot;</strong></p><p><strong>resources:</strong></p><p><strong>requests:</strong></p><p><strong>storage: 1Gi</strong></p><h5><strong>Pod Selector</strong></h5><p>You must set the <strong>spec.selector</strong> field of a StatefulSet to match the labels of its <strong>.spec.template.metadata.labels</strong>. Prior to Kubernetes 1.8, the <strong>spec.selector</strong> field was defaulted when omitted. In 1.8 and later versions, failing to specify a matching Pod Selector will result in a validation error during StatefulSet creation.</p><h5><strong>Pod Identity</strong></h5><p>StatefulSet Pods have a unique identity that is comprised of an ordinal, a stable network identity, and stable storage. The identity sticks to the Pod, regardless of which node it&#x27;s (re)scheduled on.</p><h6><strong>Ordinal Index</strong></h6><p>For a StatefulSet with N replicas, each Pod in the StatefulSet will be assigned an integer ordinal, from 0 up through N-1, that is unique over the Set.</p><h6><strong>Stable Network ID</strong></h6><p>Each Pod in a StatefulSet derives its hostname from the name of the StatefulSet and the ordinal of the Pod. The pattern for the constructed hostname is <strong>$(statefulset name)-$(ordinal)</strong>. The example above will create three Pods named <strong>web-0,web-1,web-2</strong>. A StatefulSet can use a <a href="https://kubernetes.io/docs/concepts/services-networking/service/#headless-services">Headless Service</a> to control the domain of its Pods. The domain managed by this Service takes the form: <strong>$(service name).$(namespace).svc.cluster.local</strong>, where &quot;cluster.local&quot; is the cluster domain. As each Pod is created, it gets a matching DNS subdomain, taking the form: <strong>$(podname).$(governing service domain)</strong>, where the governing service is defined by the <strong>serviceName</strong> field on the StatefulSet.</p><p>Here are some examples of choices for Cluster Domain, Service name, StatefulSet name, and how that affects the DNS names for the StatefulSet&#x27;s Pods.</p><p>  Cluster Domain   Service (ns/name)   StatefulSet (ns/name)   StatefulSet Domain                Pod DNS                                        Pod Hostname</p><hr/><p>  cluster.local    default/nginx       default/web             nginx.default.svc.cluster.local   web-{0..N-1}.nginx.default.svc.cluster.local   web-{0..N-1}
cluster.local    foo/nginx           foo/web                 nginx.foo.svc.cluster.local       web-{0..N-1}.nginx.foo.svc.cluster.local       web-{0..N-1}
kube.local       foo/nginx           foo/web                 nginx.foo.svc.kube.local          web-{0..N-1}.nginx.foo.svc.kube.local          web-{0..N-1}</p><p>Note that Cluster Domain will be set to <strong>cluster.local</strong> unless <a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#how-it-works">otherwise configured</a>.</p><h6><strong>Stable Storage</strong></h6><p>Kubernetes creates one <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">PersistentVolume</a> for each VolumeClaimTemplate. In the nginx example above, each Pod will receive a single PersistentVolume with a StorageClass of <strong>my-storage-class</strong>and 1 Gib of provisioned storage. If no StorageClass is specified, then the default StorageClass will be used. When a Pod is (re)scheduled onto a node, its <strong>volumeMounts</strong> mount the PersistentVolumes associated with its PersistentVolume Claims. Note that, the PersistentVolumes associated with the Pods&#x27; PersistentVolume Claims are not deleted when the Pods, or StatefulSet are deleted. This must be done manually.</p><h6><strong>Pod Name Label</strong></h6><p>When the StatefulSet controller creates a Pod, it adds a label, <strong>statefulset.kubernetes.io/pod-name</strong>, that is set to the name of the Pod. This label allows you to attach a Service to a specific Pod in the StatefulSet.</p><h5><strong>Deployment and Scaling Guarantees</strong></h5><ul><li>For a StatefulSet with N replicas, when Pods are being deployed, they are created sequentially, in order from {0..N-1}.</li><li>When Pods are being deleted, they are terminated in reverse order, from {N-1..0}.</li><li>Before a scaling operation is applied to a Pod, all of its predecessors must be Running and Ready.</li><li>Before a Pod is terminated, all of its successors must be completely shutdown.</li></ul><p>The StatefulSet should not specify a <strong>pod.Spec.TerminationGracePeriodSeconds</strong> of 0. This practice is unsafe and strongly discouraged. For further explanation, please refer to <a href="https://kubernetes.io/docs/tasks/run-application/force-delete-stateful-set-pod/">force deleting StatefulSet Pods</a>.</p><p>When the nginx example above is created, three Pods will be deployed in the order web-0, web-1, web-2. web-1 will not be deployed before web-0 is <a href="https://kubernetes.io/docs/user-guide/pod-states/">Running and Ready</a>, and web-2 will not be deployed until web-1 is Running and Ready. If web-0 should fail, after web-1 is Running and Ready, but before web-2 is launched, web-2 will not be launched until web-0 is successfully relaunched and becomes Running and Ready.</p><p>If a user were to scale the deployed example by patching the StatefulSet such that <strong>replicas=1</strong>, web-2 would be terminated first. web-1 would not be terminated until web-2 is fully shutdown and deleted. If web-0 were to fail after web-2 has been terminated and is completely shutdown, but prior to web-1&#x27;s termination, web-1 would not be terminated until web-0 is Running and Ready.</p><h6><strong>Pod Management Policies</strong></h6><p>In Kubernetes 1.7 and later, StatefulSet allows you to relax its ordering guarantees while preserving its uniqueness and identity guarantees via its <strong>.spec.podManagementPolicy</strong> field.</p><p><strong>OrderedReady Pod Management</strong></p><p><strong>OrderedReady</strong> pod management is the default for StatefulSets. It implements the behavior described <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#deployment-and-scaling-guarantees">above</a>.</p><p><strong>Parallel Pod Management</strong></p><p><strong>Parallel</strong> pod management tells the StatefulSet controller to launch or terminate all Pods in parallel, and to not wait for Pods to become Running and Ready or completely terminated prior to launching or terminating another Pod.</p><h5><strong>Update Strategies</strong></h5><p>In Kubernetes 1.7 and later, StatefulSet&#x27;s <strong>.spec.updateStrategy</strong> field allows you to configure and disable automated rolling updates for containers, labels, resource request/limits, and annotations for the Pods in a StatefulSet.</p><h6><strong>On Delete</strong></h6><p>The <strong>OnDelete</strong> update strategy implements the legacy (1.6 and prior) behavior. When a StatefulSet&#x27;s <strong>.spec.updateStrategy.type</strong> is set to <strong>OnDelete</strong>, the StatefulSet controller will not automatically update the Pods in a StatefulSet. Users must manually delete Pods to cause the controller to create new Pods that reflect modifications made to a StatefulSet&#x27;s <strong>.spec.template</strong>.</p><h6><strong>Rolling Updates</strong></h6><p>The <strong>RollingUpdate</strong> update strategy implements automated, rolling update for the Pods in a StatefulSet. It is the default strategy when <strong>spec.updateStrategy</strong> is left unspecified. When a StatefulSet&#x27;s <strong>.spec.updateStrategy.type</strong> is set to <strong>RollingUpdate</strong>, the StatefulSet controller will delete and recreate each Pod in the StatefulSet. It will proceed in the same order as Pod termination (from the largest ordinal to the smallest), updating each Pod one at a time. It will wait until an updated Pod is Running and Ready prior to updating its predecessor.</p><p><strong>Partitions</strong></p><p>The <strong>RollingUpdate</strong> update strategy can be partitioned, by specifying a <strong>.spec.updateStrategy.rollingUpdate.partition</strong>. If a partition is specified, all Pods with an ordinal that is greater than or equal to the partition will be updated when the StatefulSet&#x27;s <strong>.spec.template</strong> is updated. All Pods with an ordinal that is less than the partition will not be updated, and, even if they are deleted, they will be recreated at the previous version. If a StatefulSet&#x27;s <strong>.spec.updateStrategy.rollingUpdate.partition</strong> is greater than its <strong>.spec.replicas</strong>, updates to its <strong>.spec.template</strong> will not be propagated to its Pods. In most cases you will not need to use a partition, but they are useful if you want to stage an update, roll out a canary, or perform a phased roll out.</p><h5><strong>What&#x27;s next</strong></h5><ul><li>Follow an example of <a href="https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/">deploying a stateful application</a>.</li><li>Follow an example of <a href="https://kubernetes.io/docs/tutorials/stateful-application/cassandra/">deploying Cassandra with Stateful Sets</a>.</li></ul><h4>DaemonSet</h4><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#what-is-a-daemonset"><strong>What is a DaemonSet?</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#writing-a-daemonset-spec"><strong>Writing a DaemonSet Spec</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#create-a-daemonset"><strong>Create a DaemonSet</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#required-fields"><strong>Required Fields</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#pod-template"><strong>Pod Template</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#pod-selector"><strong>Pod Selector</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#running-pods-on-only-some-nodes"><strong>Running Pods on Only Some Nodes</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#how-daemon-pods-are-scheduled"><strong>How Daemon Pods are Scheduled</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#communicating-with-daemon-pods"><strong>Communicating with Daemon Pods</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#updating-a-daemonset"><strong>Updating a DaemonSet</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#alternatives-to-daemonset"><strong>Alternatives to DaemonSet</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#init-scripts"><strong>Init Scripts</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#bare-pods"><strong>Bare Pods</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#static-pods"><strong>Static Pods</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#deployments"><strong>Deployments</strong></a></li></ul></li></ul><h5><strong>What is a DaemonSet?</strong></h5><p>A DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them. As nodes are removed from the cluster, those Pods are garbage collected. Deleting a DaemonSet will clean up the Pods it created.</p><p>Some typical uses of a DaemonSet are:</p><ul><li>running a cluster storage daemon, such as <strong>glusterd</strong>, <strong>ceph</strong>, on each node.</li><li>running a logs collection daemon on every node, such as <strong>fluentd</strong> or <strong>logstash</strong>.</li><li>running a node monitoring daemon on every node, such as <a href="https://github.com/prometheus/node_exporter">Prometheus Node Exporter</a>, <strong>collectd</strong>, Datadog agent, New Relic agent, or Ganglia <strong>gmond</strong>.</li></ul><p>In a simple case, one DaemonSet, covering all nodes, would be used for each type of daemon. A more complex setup might use multiple DaemonSets for a single type of daemon, but with different flags and/or different memory and cpu requests for different hardware types.</p><h5><strong>Writing a DaemonSet Spec</strong></h5><h6><strong>Create a DaemonSet</strong></h6><p>You can describe a DaemonSet in a YAML file. For example, the <strong>daemonset.yaml</strong> file below describes a DaemonSet that runs the fluentd-elasticsearch Docker image:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>daemonset.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernet">https://raw.githubusercontent.com/kubernet</a>      |
| es/website/master/docs/concepts/workloads/controllers/daemonset.yaml) |
+=======================================================================+
| <strong>apiVersion: apps/v1</strong>                                               |
|                                                                       |
| <strong>kind: DaemonSet</strong>                                                   |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: fluentd-elasticsearch</strong>                                       |
|                                                                       |
| <strong>namespace: kube-system</strong>                                            |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>k8s-app: fluentd-logging</strong>                                          |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>matchLabels:</strong>                                                      |
|                                                                       |
| <strong>name: fluentd-elasticsearch</strong>                                       |
|                                                                       |
| <strong>template:</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>name: fluentd-elasticsearch</strong>                                       |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>tolerations:</strong>                                                      |
|                                                                       |
| <strong>- key: node-role.kubernetes.io/master</strong>                             |
|                                                                       |
| <strong>effect: NoSchedule</strong>                                                |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: fluentd-elasticsearch</strong>                                     |
|                                                                       |
| <strong>image: gcr.io/google-containers/fluentd-elasticsearch:1.20</strong>        |
|                                                                       |
| <strong>resources:</strong>                                                        |
|                                                                       |
| <strong>limits:</strong>                                                           |
|                                                                       |
| <strong>memory: 200Mi</strong>                                                     |
|                                                                       |
| <strong>requests:</strong>                                                         |
|                                                                       |
| <strong>cpu: 100m</strong>                                                         |
|                                                                       |
| <strong>memory: 200Mi</strong>                                                     |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- name: varlog</strong>                                                    |
|                                                                       |
| <strong>mountPath: /var/log</strong>                                               |
|                                                                       |
| <strong>- name: varlibdockercontainers</strong>                                    |
|                                                                       |
| <strong>mountPath: /var/lib/docker/containers</strong>                             |
|                                                                       |
| <strong>readOnly: true</strong>                                                    |
|                                                                       |
| <strong>terminationGracePeriodSeconds: 30</strong>                                 |
|                                                                       |
| <strong>volumes:</strong>                                                          |
|                                                                       |
| <strong>- name: varlog</strong>                                                    |
|                                                                       |
| <strong>hostPath:</strong>                                                         |
|                                                                       |
| <strong>path: /var/log</strong>                                                    |
|                                                                       |
| <strong>- name: varlibdockercontainers</strong>                                    |
|                                                                       |
| <strong>hostPath:</strong>                                                         |
|                                                                       |
| <strong>path: /var/lib/docker/containers</strong>                                  |
+-----------------------------------------------------------------------+</p><ul><li>Create a DaemonSet based on the YAML file:</li><li><strong>kubectl create -f daemonset.yaml</strong></li></ul><h6><strong>Required Fields</strong></h6><p>As with all other Kubernetes config, a DaemonSet needs <strong>apiVersion</strong>, <strong>kind</strong>, and <strong>metadata</strong> fields. For general information about working with config files, see <a href="https://kubernetes.io/docs/user-guide/deploying-applications/">deploying applications</a>, <a href="https://kubernetes.io/docs/tasks/">configuring containers</a>, and <a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/overview/">object management using kubectl</a> documents.</p><p>A DaemonSet also needs a <a href="https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status"><strong>.spec</strong></a> section.</p><h6><strong>Pod Template</strong></h6><p>The <strong>.spec.template</strong> is one of the required fields in <strong>.spec</strong>.</p><p>The <strong>.spec.template</strong> is a <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/#pod-templates">pod template</a>. It has exactly the same schema as a <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/">Pod</a>, except it is nested and does not have an <strong>apiVersion</strong> or <strong>kind</strong>.</p><p>In addition to required fields for a Pod, a Pod template in a DaemonSet has to specify appropriate labels (see <a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#pod-selector">pod selector</a>).</p><p>A Pod Template in a DaemonSet must have a <a href="https://kubernetes.io/docs/user-guide/pod-states"><strong>RestartPolicy</strong></a> equal to <strong>Always</strong>, or be unspecified, which defaults to <strong>Always</strong>.</p><h6><strong>Pod Selector</strong></h6><p>The <strong>.spec.selector</strong> field is a pod selector. It works the same as the <strong>.spec.selector</strong> of a <a href="https://kubernetes.io/docs/concepts/jobs/run-to-completion-finite-workloads/">Job</a>.</p><p>As of Kubernetes 1.8, you must specify a pod selector that matches the labels of the <strong>.spec.template</strong>. The pod selector will no longer be defaulted when left empty. Selector defaulting was not compatible with <strong>kubectl apply</strong>. Also, once a DaemonSet is created, its <strong>spec.selector</strong>can not be mutated. Mutating the pod selector can lead to the unintentional orphaning of Pods, and it was found to be confusing to users.</p><p>The <strong>spec.selector</strong> is an object consisting of two fields:</p><ul><li><strong>matchLabels</strong> - works the same as the <strong>.spec.selector</strong> of a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/">ReplicationController</a>.</li><li><strong>matchExpressions</strong> - allows to build more sophisticated selectors by specifying key, list of values and an operator that relates the key and values.</li></ul><p>When the two are specified the result is ANDed.</p><p>If the <strong>.spec.selector</strong> is specified, it must match the <strong>.spec.template.metadata.labels</strong>. Config with these not matching will be rejected by the API.</p><p>Also you should not normally create any Pods whose labels match this selector, either directly, via another DaemonSet, or via other controller such as ReplicaSet. Otherwise, the DaemonSet controller will think that those Pods were created by it. Kubernetes will not stop you from doing this. One case where you might want to do this is manually create a Pod with a different value on a node for testing.</p><h6><strong>Running Pods on Only Some Nodes</strong></h6><p>If you specify a <strong>.spec.template.spec.nodeSelector</strong>, then the DaemonSet controller will create Pods on nodes which match that <a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/">node selector</a>. Likewise if you specify a <strong>.spec.template.spec.affinity</strong>, then DaemonSet controller will create Pods on nodes which match that <a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/">node affinity</a>. If you do not specify either, then the DaemonSet controller will create Pods on all nodes.</p><h5><strong>How Daemon Pods are Scheduled</strong></h5><p>Normally, the machine that a Pod runs on is selected by the Kubernetes scheduler. However, Pods created by the DaemonSet controller have the machine already selected (<strong>.spec.nodeName</strong> is specified when the Pod is created, so it is ignored by the scheduler). Therefore:</p><ul><li>The <a href="https://kubernetes.io/docs/admin/node/#manual-node-administration"><strong>unschedulable</strong></a> field of a node is not respected by the DaemonSet controller.</li><li>The DaemonSet controller can make Pods even when the scheduler has not been started, which can help cluster bootstrap.</li></ul><p>Daemon Pods do respect <a href="https://kubernetes.io/docs/concepts/configuration/taint-and-toleration">taints and tolerations</a>, but they are created with <strong>NoExecute</strong> tolerations for the following taints with no <strong>tolerationSeconds</strong>:</p><ul><li><strong>node.kubernetes.io/not-ready</strong></li><li><strong>node.alpha.kubernetes.io/unreachable</strong></li></ul><p>This ensures that when the <strong>TaintBasedEvictions</strong> alpha feature is enabled, they will not be evicted when there are node problems such as a network partition. (When the <strong>TaintBasedEvictions</strong>feature is not enabled, they are also not evicted in these scenarios, but due to hard-coded behavior of the NodeController rather than due to tolerations).</p><p>They also tolerate following <strong>NoSchedule</strong> taints:</p><ul><li><strong>node.kubernetes.io/memory-pressure</strong></li><li><strong>node.kubernetes.io/disk-pressure</strong></li></ul><p>When the support to critical pods is enabled and the pods in a DaemonSet are labeled as critical, the Daemon pods are created with an additional <strong>NoSchedule</strong> toleration for the <strong>node.kubernetes.io/out-of-disk</strong> taint.</p><p>Note that all above <strong>NoSchedule</strong> taints above are created only in version 1.8 or later if the alpha feature <strong>TaintNodesByCondition</strong> is enabled.</p><p>Also note that the <strong>node-role.kubernetes.io/master</strong> <strong>NoSchedule</strong> toleration specified in the above example is needed on 1.6 or later to schedule on master nodes as this is not a default toleration.</p><h5><strong>Communicating with Daemon Pods</strong></h5><p>Some possible patterns for communicating with Pods in a DaemonSet are:</p><ul><li><strong>Push</strong>: Pods in the DaemonSet are configured to send updates to another service, such as a stats database. They do not have clients.</li><li><strong>NodeIP and Known Port</strong>: Pods in the DaemonSet can use a <strong>hostPort</strong>, so that the pods are reachable via the node IPs. Clients know the list of node IPs somehow, and know the port by convention.</li><li><strong>DNS</strong>: Create a <a href="https://kubernetes.io/docs/concepts/services-networking/service/#headless-services">headless service</a> with the same pod selector, and then discover DaemonSets using the <strong>endpoints</strong> resource or retrieve multiple A records from DNS.</li><li><strong>Service</strong>: Create a service with the same Pod selector, and use the service to reach a daemon on a random node. (No way to reach specific node.)</li></ul><h5><strong>Updating a DaemonSet</strong></h5><p>If node labels are changed, the DaemonSet will promptly add Pods to newly matching nodes and delete Pods from newly not-matching nodes.</p><p>You can modify the Pods that a DaemonSet creates. However, Pods do not allow all fields to be updated. Also, the DaemonSet controller will use the original template the next time a node (even with the same name) is created.</p><p>You can delete a DaemonSet. If you specify <strong>--cascade=false</strong> with <strong>kubectl</strong>, then the Pods will be left on the nodes. You can then create a new DaemonSet with a different template. The new DaemonSet with the different template will recognize all the existing Pods as having matching labels. It will not modify or delete them despite a mismatch in the Pod template. You will need to force new Pod creation by deleting the Pod or deleting the node.</p><p>In Kubernetes version 1.6 and later, you can <a href="https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/">perform a rolling update</a> on a DaemonSet.</p><h5><strong>Alternatives to DaemonSet</strong></h5><h6><strong>Init Scripts</strong></h6><p>It is certainly possible to run daemon processes by directly starting them on a node (e.g. using <strong>init</strong>, <strong>upstartd</strong>, or <strong>systemd</strong>). This is perfectly fine. However, there are several advantages to running such processes via a DaemonSet:</p><ul><li>Ability to monitor and manage logs for daemons in the same way as applications.</li><li>Same config language and tools (e.g. Pod templates, <strong>kubectl</strong>) for daemons and applications.</li><li>Running daemons in containers with resource limits increases isolation between daemons from app containers. However, this can also be accomplished by running the daemons in a container but not in a Pod (e.g. start directly via Docker).</li></ul><h6><strong>Bare Pods</strong></h6><p>It is possible to create Pods directly which specify a particular node to run on. However, a DaemonSet replaces Pods that are deleted or terminated for any reason, such as in the case of node failure or disruptive node maintenance, such as a kernel upgrade. For this reason, you should use a DaemonSet rather than creating individual Pods.</p><h6><strong>Static Pods</strong></h6><p>It is possible to create Pods by writing a file to a certain directory watched by Kubelet. These are called <a href="https://kubernetes.io/docs/concepts/cluster-administration/static-pod/">static pods</a>. Unlike DaemonSet, static Pods cannot be managed with kubectl or other Kubernetes API clients. Static Pods do not depend on the apiserver, making them useful in cluster bootstrapping cases. Also, static Pods may be deprecated in the future.</p><h6><strong>Deployments</strong></h6><p>DaemonSets are similar to <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployments</a> in that they both create Pods, and those Pods have processes which are not expected to terminate (e.g. web servers, storage servers).</p><p>Use a Deployment for stateless services, like frontends, where scaling up and down the number of replicas and rolling out updates are more important than controlling exactly which host the Pod runs on. Use a DaemonSet when it is important that a copy of a Pod always run on all or certain hosts, and when it needs to start before other Pods.</p><h4>Garbage Collection</h4><p>The role of the Kubernetes garbage collector is to delete certain objects that once had an owner, but no longer have an owner.</p><p><strong>Note</strong>: Garbage collection is a beta feature and is enabled by default in Kubernetes version 1.4 and later.</p><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/garbage-collection/#owners-and-dependents"><strong>Owners and dependents</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/garbage-collection/#controlling-how-the-garbage-collector-deletes-dependents"><strong>Controlling how the garbage collector deletes dependents</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/garbage-collection/#foreground-cascading-deletion"><strong>Foreground cascading deletion</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/garbage-collection/#background-cascading-deletion"><strong>Background cascading deletion</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/garbage-collection/#setting-the-cascading-deletion-policy"><strong>Setting the cascading deletion policy</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/garbage-collection/#additional-note-on-deployments"><strong>Additional note on Deployments</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/garbage-collection/#known-issues"><strong>Known issues</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/garbage-collection/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h5><strong>Owners and dependents</strong></h5><p>Some Kubernetes objects are owners of other objects. For example, a ReplicaSet is the owner of a set of Pods. The owned objects are called dependents of the owner object. Every dependent object has a <strong>metadata.ownerReferences</strong> field that points to the owning object.</p><p>Sometimes, Kubernetes sets the value of <strong>ownerReference</strong> automatically. For example, when you create a ReplicaSet, Kubernetes automatically sets the <strong>ownerReference</strong> field of each Pod in the ReplicaSet. In 1.8, Kubernetes automatically sets the value of <strong>ownerReference</strong> for objects created or adopted by ReplicationController, ReplicaSet, StatefulSet, DaemonSet, Deployment, Job and CronJob.</p><p>You can also specify relationships between owners and dependents by manually setting the <strong>ownerReference</strong> field.</p><p>Here&#x27;s a configuration file for a ReplicaSet that has three Pods:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>my-repset.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernet">https://raw.githubusercontent.com/kubernet</a>      |
| es/website/master/docs/concepts/workloads/controllers/my-repset.yaml) |
+=======================================================================+
| <strong>apiVersion: extensions/v1beta1</strong>                                    |
|                                                                       |
| <strong>kind: ReplicaSet</strong>                                                  |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: my-repset</strong>                                                   |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>replicas: 3</strong>                                                       |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>matchLabels:</strong>                                                      |
|                                                                       |
| <strong>pod-is-for: garbage-collection-example</strong>                            |
|                                                                       |
| <strong>template:</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>pod-is-for: garbage-collection-example</strong>                            |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: nginx</strong>                                                     |
|                                                                       |
| <strong>image: nginx</strong>                                                      |
+-----------------------------------------------------------------------+</p><p>If you create the ReplicaSet and then view the Pod metadata, you can see OwnerReferences field:</p><p><strong>kubectl create -f <a href="https://k8s.io/docs/concepts/controllers/my-repset.yaml">https://k8s.io/docs/concepts/controllers/my-repset.yaml</a></strong></p><p><strong>kubectl get pods --output=yaml</strong></p><p>The output shows that the Pod owner is a ReplicaSet named my-repset:</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>ownerReferences:</strong></p><p><strong>- apiVersion: extensions/v1beta1</strong></p><p><strong>controller: true</strong></p><p><strong>blockOwnerDeletion: true</strong></p><p><strong>kind: ReplicaSet</strong></p><p><strong>name: my-repset</strong></p><p><strong>uid: d9607e19-f88f-11e6-a518-42010a800195</strong></p><p><strong>.<!-- -->..</strong></p><h5><strong>Controlling how the garbage collector deletes dependents</strong></h5><p>When you delete an object, you can specify whether the object&#x27;s dependents are also deleted automatically. Deleting dependents automatically is called cascading deletion. There are two modes of cascading deletion: background and foreground.</p><p>If you delete an object without deleting its dependents automatically, the dependents are said to be orphaned.</p><h6><strong>Foreground cascading deletion</strong></h6><p>In foreground cascading deletion, the root object first enters a &quot;deletion in progress&quot; state. In the &quot;deletion in progress&quot; state, the following things are true:</p><ul><li>The object is still visible via the REST API</li><li>The object&#x27;s <strong>deletionTimestamp</strong> is set</li><li>The object&#x27;s <strong>metadata.finalizers</strong> contains the value &quot;foregroundDeletion&quot;.</li></ul><p>Once the &quot;deletion in progress&quot; state is set, the garbage collector deletes the object&#x27;s dependents. Once the garbage collector has deleted all &quot;blocking&quot; dependents (objects with <strong>ownerReference.blockOwnerDeletion=true</strong>), it delete the owner object.</p><p>Note that in the &quot;foregroundDeletion&quot;, only dependents with <strong>ownerReference.blockOwnerDeletion</strong>block the deletion of the owner object. Kubernetes version 1.7 added an <a href="https://kubernetes.io/docs/admin/admission-controllers/#ownerreferencespermissionenforcement">admission controller</a> that controls user access to set <strong>blockOwnerDeletion</strong> to true based on delete permissions on the owner object, so that unauthorized dependents cannot delay deletion of an owner object.</p><p>If an object&#x27;s <strong>ownerReferences</strong> field is set by a controller (such as Deployment or ReplicaSet), blockOwnerDeletion is set automatically and you do not need to manually modify this field.</p><h6><strong>Background cascading deletion</strong></h6><p>In background cascading deletion, Kubernetes deletes the owner object immediately and the garbage collector then deletes the dependents in the background.</p><h6><strong>Setting the cascading deletion policy</strong></h6><p>To control the cascading deletion policy, set the <strong>propagationPolicy</strong> field on the <strong>deleteOptions</strong>argument when deleting an Object. Possible values include &quot;Orphan&quot;, &quot;Foreground&quot;, or &quot;Background&quot;.</p><p>Prior to Kubernetes 1.9, the default garbage collection policy for many controller resources was <strong>orphan</strong>. This included ReplicationController, ReplicaSet, StatefulSet, DaemonSet, and Deployment. For kinds in the extensions/v1beta1, apps/v1beta1, and apps/v1beta2 group versions, unless you specify otherwise, dependent objects are orphaned by default. In Kubernetes 1.9, for all kinds in the apps/v1 group version, dependent objects are deleted by default.</p><p>Here&#x27;s an example that deletes dependents in background:</p><p><strong>kubectl proxy --port=8080</strong></p><p><strong>curl -X DELETE localhost:8080/apis/extensions/v1beta1/namespaces/default/replicasets/my-repset <!-- -->\</strong></p><p><strong>-d \&#x27;{&quot;kind&quot;:&quot;DeleteOptions&quot;,&quot;apiVersion&quot;:&quot;v1&quot;,&quot;propagationPolicy&quot;:&quot;Background&quot;}\&#x27; <!-- -->\</strong></p><p><strong>-H &quot;Content-Type: application/json&quot;</strong></p><p>Here&#x27;s an example that deletes dependents in foreground:</p><p><strong>kubectl proxy --port=8080</strong></p><p><strong>curl -X DELETE localhost:8080/apis/extensions/v1beta1/namespaces/default/replicasets/my-repset <!-- -->\</strong></p><p><strong>-d \&#x27;{&quot;kind&quot;:&quot;DeleteOptions&quot;,&quot;apiVersion&quot;:&quot;v1&quot;,&quot;propagationPolicy&quot;:&quot;Foreground&quot;}\&#x27; <!-- -->\</strong></p><p><strong>-H &quot;Content-Type: application/json&quot;</strong></p><p>Here&#x27;s an example that orphans dependents:</p><p><strong>kubectl proxy --port=8080</strong></p><p><strong>curl -X DELETE localhost:8080/apis/extensions/v1beta1/namespaces/default/replicasets/my-repset <!-- -->\</strong></p><p><strong>-d \&#x27;{&quot;kind&quot;:&quot;DeleteOptions&quot;,&quot;apiVersion&quot;:&quot;v1&quot;,&quot;propagationPolicy&quot;:&quot;Orphan&quot;}\&#x27; <!-- -->\</strong></p><p><strong>-H &quot;Content-Type: application/json&quot;</strong></p><p>kubectl also supports cascading deletion. To delete dependents automatically using kubectl, set <strong>--cascade</strong> to true. To orphan dependents, set <strong>--cascade</strong> to false. The default value for <strong>--cascade</strong> is true.</p><p>Here&#x27;s an example that orphans the dependents of a ReplicaSet:</p><p><strong>kubectl delete replicaset my-repset --cascade=false</strong></p><h6><strong>Additional note on Deployments</strong></h6><p>When using cascading deletes with Deployments you must use <strong>propagationPolicy: Foreground</strong>to delete not only the ReplicaSets created, but also their Pods. If this type of propagationPolicy is not used, only the ReplicaSets will be deleted, and the Pods will be orphaned. See <a href="https://github.com/kubernetes/kubeadm/issues/149#issuecomment-284766613">kubeadm/#149</a> for more information.</p><h5><strong>Known issues</strong></h5><p>Tracked at <a href="https://github.com/kubernetes/kubernetes/issues/26120">#26120</a></p><h5><strong>What&#x27;s next</strong></h5><p><a href="https://git.k8s.io/community/contributors/design-proposals/api-machinery/garbage-collection.md">Design Doc 1</a></p><p><a href="https://git.k8s.io/community/contributors/design-proposals/api-machinery/synchronous-garbage-collection.md">Design Doc 2</a></p><h4>Jobs - Run to Completion</h4><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#what-is-a-job"><strong>What is a Job?</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#running-an-example-job"><strong>Running an example Job</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#writing-a-job-spec"><strong>Writing a Job Spec</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#pod-template"><strong>Pod Template</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#pod-selector"><strong>Pod Selector</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#parallel-jobs"><strong>Parallel Jobs</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#controlling-parallelism"><strong>Controlling Parallelism</strong></a></li></ul></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#handling-pod-and-container-failures"><strong>Handling Pod and Container Failures</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#pod-backoff-failure-policy"><strong>Pod Backoff failure policy</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#job-termination-and-cleanup"><strong>Job Termination and Cleanup</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#job-patterns"><strong>Job Patterns</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#advanced-usage"><strong>Advanced Usage</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#specifying-your-own-pod-selector"><strong>Specifying your own pod selector</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#alternatives"><strong>Alternatives</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#bare-pods"><strong>Bare Pods</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#replication-controller"><strong>Replication Controller</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#single-job-starts-controller-pod"><strong>Single Job starts Controller Pod</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#cron-jobs"><strong>Cron Jobs</strong></a></li></ul><h5><strong>What is a Job?</strong></h5><p>A job creates one or more pods and ensures that a specified number of them successfully terminate. As pods successfully complete, the job tracks the successful completions. When a specified number of successful completions is reached, the job itself is complete. Deleting a Job will cleanup the pods it created.</p><p>A simple case is to create one Job object in order to reliably run one Pod to completion. The Job object will start a new Pod if the first pod fails or is deleted (for example due to a node hardware failure or a node reboot).</p><p>A Job can also be used to run multiple pods in parallel.</p><h5><strong>Running an example Job</strong></h5><p>Here is an example Job config. It computes π to 2000 places and prints it out. It takes around 10s to complete.</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>job.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/ku">https://raw.githubusercontent.com/ku</a>                  |
| bernetes/website/master/docs/concepts/workloads/controllers/job.yaml) |
+=======================================================================+
| <strong>apiVersion: batch/v1</strong>                                              |
|                                                                       |
| <strong>kind: Job</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: pi</strong>                                                          |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>template:</strong>                                                         |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: pi</strong>                                                        |
|                                                                       |
| <strong>image: perl</strong>                                                       |
|                                                                       |
| <strong>command: <!-- -->[&quot;perl&quot;, &quot;-Mbignum=bpi&quot;, &quot;-wle&quot;, &quot;print            |
| bpi(2000)&quot;]</strong>                                                       |
|                                                                       |
| <strong>restartPolicy: Never</strong>                                              |
|                                                                       |
| <strong>backoffLimit: 4</strong>                                                   |
+-----------------------------------------------------------------------+</p><p>Run the example job by downloading the example file and then running this command:</p><p><strong>$ kubectl create -f ./job.yaml</strong></p><p><strong>job &quot;pi&quot; created</strong></p><p>Check on the status of the job using this command:</p><p><strong>$ kubectl describe jobs/pi</strong></p><p><strong>Name: pi</strong></p><p><strong>Namespace: default</strong></p><p><strong>Selector: controller-uid=b1db589a-2c8d-11e6-b324-0209dc45a495</strong></p><p><strong>Labels: controller-uid=b1db589a-2c8d-11e6-b324-0209dc45a495</strong></p><p><strong>job-name=pi</strong></p><p><strong>Annotations: <code>&lt;none&gt;</code></strong></p><p><strong>Parallelism: 1</strong></p><p><strong>Completions: 1</strong></p><p><strong>Start Time: Tue, 07 Jun 2016 10:56:16 +0200</strong></p><p><strong>Pods Statuses: 0 Running / 1 Succeeded / 0 Failed</strong></p><p><strong>Pod Template:</strong></p><p><strong>Labels: controller-uid=b1db589a-2c8d-11e6-b324-0209dc45a495</strong></p><p><strong>job-name=pi</strong></p><p><strong>Containers:</strong></p><p><strong>pi:</strong></p><p><strong>Image: perl</strong></p><p><strong>Port:</strong></p><p><strong>Command:</strong></p><p><strong>perl</strong></p><p><strong>-Mbignum=bpi</strong></p><p><strong>-wle</strong></p><p><strong>print bpi(2000)</strong></p><p><strong>Environment: <code>&lt;none&gt;</code></strong></p><p><strong>Mounts: <code>&lt;none&gt;</code></strong></p><p><strong>Volumes: <code>&lt;none&gt;</code></strong></p><p><strong>Events:</strong></p><p><strong>FirstSeen LastSeen Count From SubobjectPath Type Reason Message</strong></p><p><strong>--------- -------- ----- ---- ------------- -------- ------ -------</strong></p><p><strong>1m 1m 1 {job-controller } Normal SuccessfulCreate Created pod: pi-dtn4q</strong></p><p>To view completed pods of a job, use <strong>kubectl get pods</strong>.</p><p>To list all the pods that belong to a job in a machine readable form, you can use a command like this:</p><p><strong>$ pods=$(kubectl get pods --selector=job-name=pi --output=jsonpath={.items..metadata.name})</strong></p><p><strong>$ echo $pods</strong></p><p><strong>pi-aiw0a</strong></p><p>Here, the selector is the same as the selector for the job. The <strong>--output=jsonpath</strong> option specifies an expression that just gets the name from each pod in the returned list.</p><p>View the standard output of one of the pods:</p><p><strong>$ kubectl logs $pods</strong></p><p><strong>3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679821480865132823066470938446095505822317253594081284811174502841027019385211055596446229489549303819644288109756659334461284756482337867831652712019091456485669234603486104543266482133936072602491412737245870066063155881748815209209628292540917153643678925903600113305305488204665213841469519415116094330572703657595919530921861173819326117931051185480744623799627495673518857527248912279381830119491298336733624406566430860213949463952247371907021798609437027705392171762931767523846748184676694051320005681271452635608277857713427577896091736371787214684409012249534301465495853710507922796892589235420199561121290219608640344181598136297747713099605187072113499999983729780499510597317328160963185950244594553469083026425223082533446850352619311881710100031378387528865875332083814206171776691473035982534904287554687311595628638823537875937519577818577805321712268066130019278766111959092164201989380952572010654858632788659361533818279682303019520353018529689957736225994138912497217752834791315155748572424541506959508295331168617278558890750983817546374649393192550604009277016711390098488240128583616035637076601047101819429555961989467678374494482553797747268471040475346462080466842590694912933136770289891521047521620569660240580381501935112533824300355876402474964732639141992726042699227967823547816360093417216412199245863150302861829745557067498385054945885869269956909272107975093029553211653449872027559602364806654991198818347977535663698074265425278625518184175746728909777727938000816470600161452491921732172147723501414419735685481613611573525521334757418494684385233239073941433345477624168625189835694855620992192221842725502542568876717904946016534668049886272327917860857843838279679766814541009538837863609506800642251252051173929848960841284886269456042419652850222106611863067442786220391949450471237137869609563643719172874677646575739624138908658326459958133904780275901</strong></p><h5><strong>Writing a Job Spec</strong></h5><p>As with all other Kubernetes config, a Job needs <strong>apiVersion</strong>, <strong>kind</strong>, and <strong>metadata</strong> fields.</p><p>A Job also needs a <a href="https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status"><strong>.spec</strong> section</a>.</p><h6><strong>Pod Template</strong></h6><p>The <strong>.spec.template</strong> is the only required field of the <strong>.spec</strong>.</p><p>The <strong>.spec.template</strong> is a <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/#pod-templates">pod template</a>. It has exactly the same schema as a <a href="https://kubernetes.io/docs/user-guide/pods">pod</a>, except it is nested and does not have an <strong>apiVersion</strong> or <strong>kind</strong>.</p><p>In addition to required fields for a Pod, a pod template in a job must specify appropriate labels (see <a href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#pod-selector">pod selector</a>) and an appropriate restart policy.</p><p>Only a <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy"><strong>RestartPolicy</strong></a> equal to <strong>Never</strong> or <strong>OnFailure</strong> is allowed.</p><h6><strong>Pod Selector</strong></h6><p>The <strong>.spec.selector</strong> field is optional. In almost all cases you should not specify it. See section <a href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#specifying-your-own-pod-selector">specifying your own pod selector</a>.</p><h6><strong>Parallel Jobs</strong></h6><p>There are three main types of jobs:</p><ol><li>Non-parallel Jobs<ul><li>normally only one pod is started, unless the pod fails.</li><li>job is complete as soon as Pod terminates successfully.</li></ul></li><li>Parallel Jobs with a fixed completion count:<ul><li>specify a non-zero positive value for <strong>.spec.completions</strong>.</li><li>the job is complete when there is one successful pod for each value in the range 1 to <strong>.spec.completions</strong>.</li><li><strong>not implemented yet:</strong> each pod passed a different index in the range 1 to <strong>.spec.completions</strong>.</li></ul></li><li>Parallel Jobs with a work queue:  - do not specify <strong>.spec.completions</strong>, default to <strong>.spec.parallelism</strong>.  - the pods must coordinate with themselves or an external service to determine what each should work on.<ul><li>each pod is independently capable of determining whether or not all its peers are done, thus the entire Job is done.</li><li>when any pod terminates with success, no new pods are created.</li><li>once at least one pod has terminated with success and all pods are terminated, then the job is completed with success.</li><li>once any pod has exited with success, no other pod should still be doing any work or writing any output. They should all be in the process of exiting.</li></ul></li></ol><p>For a Non-parallel job, you can leave both <strong>.spec.completions</strong> and <strong>.spec.parallelism</strong> unset. When both are unset, both are defaulted to 1.</p><p>For a Fixed Completion Count job, you should set <strong>.spec.completions</strong> to the number of completions needed. You can set <strong>.spec.parallelism</strong>, or leave it unset and it will default to 1.</p><p>For a Work Queue Job, you must leave <strong>.spec.completions</strong> unset, and set <strong>.spec.parallelism</strong> to a non-negative integer.</p><p>For more information about how to make use of the different types of job, see the <a href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#job-patterns">job patterns</a>section.</p><p><strong>Controlling Parallelism</strong></p><p>The requested parallelism (<strong>.spec.parallelism</strong>) can be set to any non-negative value. If it is unspecified, it defaults to 1. If it is specified as 0, then the Job is effectively paused until it is increased.</p><p>Actual parallelism (number of pods running at any instant) may be more or less than requested parallelism, for a variety of reasons:</p><ul><li>For Fixed Completion Count jobs, the actual number of pods running in parallel will not exceed the number of remaining completions. Higher values of <strong>.spec.parallelism</strong> are effectively ignored.</li><li>For work queue jobs, no new pods are started after any pod has succeeded -- remaining pods are allowed to complete, however.</li><li>If the controller has not had time to react.</li><li>If the controller failed to create pods for any reason (lack of ResourceQuota, lack of permission, etc.), then there may be fewer pods than requested.</li><li>The controller may throttle new pod creation due to excessive previous pod failures in the same Job.</li><li>When a pod is gracefully shutdown, it takes time to stop.</li></ul><h5><strong>Handling Pod and Container Failures</strong></h5><p>A Container in a Pod may fail for a number of reasons, such as because the process in it exited with a non-zero exit code, or the Container was killed for exceeding a memory limit, etc. If this happens, and the <strong>.spec.template.spec.restartPolicy = &quot;OnFailure&quot;</strong>, then the Pod stays on the node, but the Container is re-run. Therefore, your program needs to handle the case when it is restarted locally, or else specify <strong>.spec.template.spec.restartPolicy = &quot;Never&quot;</strong>. See <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#example-states">pods-states</a> for more information on <strong>restartPolicy</strong>.</p><p>An entire Pod can also fail, for a number of reasons, such as when the pod is kicked off the node (node is upgraded, rebooted, deleted, etc.), or if a container of the Pod fails and the <strong>.spec.template.spec.restartPolicy = &quot;Never&quot;</strong>. When a Pod fails, then the Job controller starts a new Pod. Therefore, your program needs to handle the case when it is restarted in a new pod. In particular, it needs to handle temporary files, locks, incomplete output and the like caused by previous runs.</p><p>Note that even if you specify <strong>.spec.parallelism = 1</strong> and <strong>.spec.completions = 1</strong> and <strong>.spec.template.spec.restartPolicy = &quot;Never&quot;</strong>, the same program may sometimes be started twice.</p><p>If you do specify <strong>.spec.parallelism</strong> and <strong>.spec.completions</strong> both greater than 1, then there may be multiple pods running at once. Therefore, your pods must also be tolerant of concurrency.</p><h6><strong>Pod Backoff failure policy</strong></h6><p>There are situations where you want to fail a Job after some amount of retries due to a logical error in configuration etc. To do so, set <strong>.spec.backoffLimit</strong> to specify the number of retries before considering a Job as failed. The back-off limit is set by default to 6. Failed Pods associated with the Job are recreated by the Job controller with an exponential back-off delay (10s, 20s, 40s ...) capped at six minutes, The back-off limit is reset if no new failed Pods appear before the Job&#x27;s next status check.</p><p><strong>Note:</strong> Due to a known issue <a href="https://github.com/kubernetes/kubernetes/issues/54870">#54870</a>, when the <strong>spec.template.spec.restartPolicy</strong> field is set to &quot;<strong>OnFailure</strong>&quot;, the back-off limit may be ineffective. As a short-term workaround, set the restart policy for the embedded template to &quot;<strong>Never</strong>&quot;.</p><h5><strong>Job Termination and Cleanup</strong></h5><p>When a Job completes, no more Pods are created, but the Pods are not deleted either. Since they are terminated, they don&#x27;t show up with <strong>kubectl get pods</strong>, but they will show up with <strong>kubectl get pods -a</strong>. Keeping them around allows you to still view the logs of completed pods to check for errors, warnings, or other diagnostic output. The job object also remains after it is completed so that you can view its status. It is up to the user to delete old jobs after noting their status. Delete the job with <strong>kubectl</strong> (e.g. <strong>kubectl delete jobs/pi</strong> or <strong>kubectl delete -f ./job.yaml</strong>). When you delete the job using <strong>kubectl</strong>, all the pods it created are deleted too.</p><p>By default, a Job will run uninterrupted unless a Pod fails, at which point the Job defers to the <strong>.spec.backoffLimit</strong> described above. Another way to terminate a Job is by setting an active deadline. Do this by setting the <strong>.spec.activeDeadlineSeconds</strong> field of the Job to a number of seconds.</p><p>The <strong>activeDeadlineSeconds</strong> applies to the duration of the job, no matter how many Pods are created. Once a Job reaches <strong>activeDeadlineSeconds</strong>, the Job and all of its Pods are terminated. The result is that the job has a status with <strong>reason: DeadlineExceeded</strong>.</p><p>Note that a Job&#x27;s <strong>.spec.activeDeadlineSeconds</strong> takes precedence over its <strong>.spec.backoffLimit</strong>. Therefore, a Job that is retrying one or more failed Pods will not deploy additional Pods once it reaches the time limit specified by <strong>activeDeadlineSeconds</strong>, even if the <strong>backoffLimit</strong> is not yet reached.</p><p>Example:</p><p><strong>apiVersion: batch/v1</strong></p><p><strong>kind: Job</strong></p><p><strong>metadata:</strong></p><p><strong>name: pi-with-timeout</strong></p><p><strong>spec:</strong></p><p><strong>backoffLimit: 5</strong></p><p><strong>activeDeadlineSeconds: 100</strong></p><p><strong>template:</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: pi</strong></p><p><strong>image: perl</strong></p><p><strong>command: <!-- -->[&quot;perl&quot;, &quot;-Mbignum=bpi&quot;, &quot;-wle&quot;, &quot;print bpi(2000)&quot;]</strong></p><p><strong>restartPolicy: Never</strong></p><p>Note that both the Job Spec and the <a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/#detailed-behavior">Pod Template Spec</a> within the Job have an <strong>activeDeadlineSeconds</strong> field. Ensure that you set this field at the proper level.</p><h5><strong>Job Patterns</strong></h5><p>The Job object can be used to support reliable parallel execution of Pods. The Job object is not designed to support closely-communicating parallel processes, as commonly found in scientific computing. It does support parallel processing of a set of independent but related work items. These might be emails to be sent, frames to be rendered, files to be transcoded, ranges of keys in a NoSQL database to scan, and so on.</p><p>In a complex system, there may be multiple different sets of work items. Here we are just considering one set of work items that the user wants to manage together --- a batch job.</p><p>There are several different patterns for parallel computation, each with strengths and weaknesses. The tradeoffs are:</p><ul><li>One Job object for each work item, vs. a single Job object for all work items. The latter is better for large numbers of work items. The former creates some overhead for the user and for the system to manage large numbers of Job objects.</li><li>Number of pods created equals number of work items, vs. each pod can process multiple work items. The former typically requires less modification to existing code and containers. The latter is better for large numbers of work items, for similar reasons to the previous bullet.</li><li>Several approaches use a work queue. This requires running a queue service, and modifications to the existing program or container to make it use the work queue. Other approaches are easier to adapt to an existing containerised application.</li></ul><p>The tradeoffs are summarized here, with columns 2 to 4 corresponding to the above tradeoffs. The pattern names are also links to examples and more detailed description.</p><p>  Pattern                                                                                                       Single Job object   Fewer pods than work items?   Use app unmodified?   Works in Kube 1.1?</p><hr/><p>  <a href="https://kubernetes.io/docs/tasks/job/parallel-processing-expansion/">Job Template Expansion</a>                                                                   ✓                     ✓
<a href="https://kubernetes.io/docs/tasks/job/coarse-parallel-processing-work-queue/">Queue with Pod Per Work Item</a>   ✓                                                 sometimes             ✓
<a href="https://kubernetes.io/docs/tasks/job/fine-parallel-processing-work-queue/">Queue with Variable Pod Count</a>    ✓                   ✓                                                   ✓
Single Job with Static Work Assignment                                                                        ✓                                                 ✓                      </p><p>When you specify completions with <strong>.spec.completions</strong>, each Pod created by the Job controller has an identical <a href="https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status"><strong>spec</strong></a>. This means that all pods will have the same command line and the same image, the same volumes, and (almost) the same environment variables. These patterns are different ways to arrange for pods to work on different things.</p><p>This table shows the required settings for <strong>.spec.parallelism</strong> and <strong>.spec.completions</strong> for each of the patterns. Here, <strong>W</strong> is the number of work items.</p><p>  Pattern                                                                                                       <strong>.spec.completions</strong>   <strong>.spec.parallelism</strong></p><hr/><p>  <a href="https://kubernetes.io/docs/tasks/job/parallel-processing-expansion/">Job Template Expansion</a>                 1                       should be 1
<a href="https://kubernetes.io/docs/tasks/job/coarse-parallel-processing-work-queue/">Queue with Pod Per Work Item</a>   W                       any
<a href="https://kubernetes.io/docs/tasks/job/fine-parallel-processing-work-queue/">Queue with Variable Pod Count</a>    1                       any
Single Job with Static Work Assignment                                                                        W                       any</p><h5><strong>Advanced Usage</strong></h5><h6><strong>Specifying your own pod selector</strong></h6><p>Normally, when you create a job object, you do not specify <strong>spec.selector</strong>. The system defaulting logic adds this field when the job is created. It picks a selector value that will not overlap with any other jobs.</p><p>However, in some cases, you might need to override this automatically set selector. To do this, you can specify the <strong>spec.selector</strong> of the job.</p><p>Be very careful when doing this. If you specify a label selector which is not unique to the pods of that job, and which matches unrelated pods, then pods of the unrelated job may be deleted, or this job may count other pods as completing it, or one or both of the jobs may refuse to create pods or run to completion. If a non-unique selector is chosen, then other controllers (e.g. ReplicationController) and their pods may behave in unpredictable ways too. Kubernetes will not stop you from making a mistake when specifying <strong>spec.selector</strong>.</p><p>Here is an example of a case when you might want to use this feature.</p><p>Say job <strong>old</strong> is already running. You want existing pods to keep running, but you want the rest of the pods it creates to use a different pod template and for the job to have a new name. You cannot update the job because these fields are not updatable. Therefore, you delete job <strong>old</strong> but leave its pods running, using <strong>kubectl delete jobs/old --cascade=false</strong>. Before deleting it, you make a note of what selector it uses:</p><p><strong>kind: Job</strong></p><p><strong>metadata:</strong></p><p><strong>name: old</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>spec:</strong></p><p><strong>selector:</strong></p><p><strong>matchLabels:</strong></p><p><strong>job-uid: a8f3d00d-c6d2-11e5-9f87-42010af00002</strong></p><p><strong>.<!-- -->..</strong></p><p>Then you create a new job with name <strong>new</strong> and you explicitly specify the same selector. Since the existing pods have label <strong>job-uid=a8f3d00d-c6d2-11e5-9f87-42010af00002</strong>, they are controlled by job <strong>new</strong> as well.</p><p>You need to specify <strong>manualSelector: true</strong> in the new job since you are not using the selector that the system normally generates for you automatically.</p><p><strong>kind: Job</strong></p><p><strong>metadata:</strong></p><p><strong>name: new</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>spec:</strong></p><p><strong>manualSelector: true</strong></p><p><strong>selector:</strong></p><p><strong>matchLabels:</strong></p><p><strong>job-uid: a8f3d00d-c6d2-11e5-9f87-42010af00002</strong></p><p><strong>.<!-- -->..</strong></p><p>The new Job itself will have a different uid from <strong>a8f3d00d-c6d2-11e5-9f87-42010af00002</strong>. Setting<strong>manualSelector: true</strong> tells the system to that you know what you are doing and to allow this mismatch.</p><h5><strong>Alternatives</strong></h5><h6><strong>Bare Pods</strong></h6><p>When the node that a pod is running on reboots or fails, the pod is terminated and will not be restarted. However, a Job will create new pods to replace terminated ones. For this reason, we recommend that you use a job rather than a bare pod, even if your application requires only a single pod.</p><h6><strong>Replication Controller</strong></h6><p>Jobs are complementary to <a href="https://kubernetes.io/docs/user-guide/replication-controller">Replication Controllers</a>. A Replication Controller manages pods which are not expected to terminate (e.g. web servers), and a Job manages pods that are expected to terminate (e.g. batch jobs).</p><p>As discussed in <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/">Pod Lifecycle</a>, <strong>Job</strong> is only appropriate for pods with <strong>RestartPolicy</strong> equal to <strong>OnFailure</strong> or <strong>Never</strong>. (Note: If <strong>RestartPolicy</strong> is not set, the default value is <strong>Always</strong>.)</p><h6><strong>Single Job starts Controller Pod</strong></h6><p>Another pattern is for a single Job to create a pod which then creates other pods, acting as a sort of custom controller for those pods. This allows the most flexibility, but may be somewhat complicated to get started with and offers less integration with Kubernetes.</p><p>One example of this pattern would be a Job which starts a Pod which runs a script that in turn starts a Spark master controller (see <a href="https://github.com/kubernetes/examples/tree/master/staging/spark/README.md">spark example</a>), runs a spark driver, and then cleans up.</p><p>An advantage of this approach is that the overall process gets the completion guarantee of a Job object, but complete control over what pods are created and how work is assigned to them.</p><h5><strong>Cron Jobs</strong></h5><p>Support for creating Jobs at specified times/dates (i.e. cron) is available in Kubernetes <a href="https://github.com/kubernetes/kubernetes/pull/11980">1.4</a>. More information is available in the <a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/">cron job documents</a></p><h4>CronJob</h4><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#what-is-a-cron-job"><strong>What is a cron job?</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#prerequisites"><strong>Prerequisites</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#creating-a-cron-job"><strong>Creating a Cron Job</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#deleting-a-cron-job"><strong>Deleting a Cron Job</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#cron-job-limitations"><strong>Cron Job Limitations</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#writing-a-cron-job-spec"><strong>Writing a Cron Job Spec</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#schedule"><strong>Schedule</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#job-template"><strong>Job Template</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#starting-deadline-seconds"><strong>Starting Deadline Seconds</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#concurrency-policy"><strong>Concurrency Policy</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#suspend"><strong>Suspend</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#jobs-history-limits"><strong>Jobs History Limits</strong></a></li></ul></li></ul><h5><strong>What is a cron job?</strong></h5><p>A Cron Job manages time based <a href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/">Jobs</a>, namely:</p><ul><li>Once at a specified point in time</li><li>Repeatedly at a specified point in time</li></ul><p>One CronJob object is like one line of a crontab (cron table) file. It runs a job periodically on a given schedule, written in <a href="https://en.wikipedia.org/wiki/Cron">Cron</a> format.</p><p><strong>Note:</strong> The question mark (<strong>?</strong>) in the schedule has the same meaning as an asterisk <strong>*</strong>, that is, it stands for any of available value for a given field.</p><p><strong>Note:</strong> CronJob resource in <strong>batch/v2alpha1</strong> API group has been deprecated starting from cluster version 1.8. You should switch to using <strong>batch/v1beta1</strong>, instead, which is enabled by default in the API server. Further in this document, we will be using <strong>batch/v1beta1</strong> in all the examples.</p><p>A typical use case is:</p><ul><li>Schedule a job execution at a given point in time.</li><li>Create a periodic job, e.g. database backup, sending emails.</li></ul><h6><strong>Prerequisites</strong></h6><p>You need a working Kubernetes cluster at version &gt;= 1.8 (for CronJob). For previous versions of cluster (&lt; 1.8) you need to explicitly enable <strong>batch/v2alpha1</strong> API by passing <strong>--runtime-config=batch/v2alpha1=true</strong> to the API server (see <a href="https://kubernetes.io/docs/admin/cluster-management/#turn-on-or-off-an-api-version-for-your-cluster">Turn on or off an API version for your cluster</a> for more), and then restart both the API server and the controller manager component.</p><h5><strong>Creating a Cron Job</strong></h5><p>Here is an example Cron Job. Every minute, it runs a simple job to print current time and then say hello.</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>cronjob.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubern">https://raw.githubusercontent.com/kubern</a>          |
| etes/website/master/docs/concepts/workloads/controllers/cronjob.yaml) |
+=======================================================================+
| <strong>apiVersion: batch/v1beta1</strong>                                         |
|                                                                       |
| <strong>kind: CronJob</strong>                                                     |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: hello</strong>                                                       |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>schedule: &quot;<!-- -->*<!-- -->/1 <!-- -->*<!-- --> <!-- -->*<!-- --> <!-- -->*<!-- --> <!-- -->*<!-- -->&quot;</strong>                                    |
|                                                                       |
| <strong>jobTemplate:</strong>                                                      |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>template:</strong>                                                         |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: hello</strong>                                                     |
|                                                                       |
| <strong>image: busybox</strong>                                                    |
|                                                                       |
| <strong>args:</strong>                                                             |
|                                                                       |
| <strong>- /bin/sh</strong>                                                         |
|                                                                       |
| <strong>- -c</strong>                                                              |
|                                                                       |
| <strong>- date; echo Hello from the Kubernetes cluster</strong>                    |
|                                                                       |
| <strong>restartPolicy: OnFailure</strong>                                          |
+-----------------------------------------------------------------------+</p><p>Run the example cron job by downloading the example file and then running this command:</p><p><strong>$ kubectl create -f ./cronjob.yaml</strong></p><p><strong>cronjob &quot;hello&quot; created</strong></p><p>Alternatively, use <strong>kubectl run</strong> to create a cron job without writing full config:</p><p><strong>$ kubectl run hello --schedule=&quot;<!-- -->*<!-- -->/1 <!-- -->*<!-- --> <!-- -->*<!-- --> <!-- -->*<!-- --> <!-- -->*<!-- -->&quot; --restart=OnFailure --image=busybox -- /bin/sh -c &quot;date; echo Hello from the Kubernetes cluster&quot;</strong></p><p><strong>cronjob &quot;hello&quot; created</strong></p><p>After creating the cron job, get its status using this command:</p><p><strong>$ kubectl get cronjob hello</strong></p><p><strong>NAME SCHEDULE SUSPEND ACTIVE LAST-SCHEDULE</strong></p><p><strong>hello <!-- -->*<!-- -->/1 <!-- -->*<!-- --> <!-- -->*<!-- --> <!-- -->*<!-- --> <!-- -->*<!-- --> False 0 <code>&lt;none&gt;</code></strong></p><p>As you can see above, there&#x27;s no active job yet, and no job has been scheduled, either.</p><p>Watch for the job to be created in around one minute:</p><p><strong>$ kubectl get jobs --watch</strong></p><p><strong>NAME DESIRED SUCCESSFUL AGE</strong></p><p><strong>hello-4111706356 1 1 2s</strong></p><p>Now you&#x27;ve seen one running job scheduled by &quot;hello&quot;. We can stop watching it and get the cron job again:</p><p><strong>$ kubectl get cronjob hello</strong></p><p><strong>NAME SCHEDULE SUSPEND ACTIVE LAST-SCHEDULE</strong></p><p><strong>hello <!-- -->*<!-- -->/1 <!-- -->*<!-- --> <!-- -->*<!-- --> <!-- -->*<!-- --> <!-- -->*<!-- --> False 0 Mon, 29 Aug 2016 14:34:00 -0700</strong></p><p>You should see that &quot;hello&quot; successfully scheduled a job at the time specified in <strong>LAST-SCHEDULE</strong>. There are currently 0 active jobs, meaning that the job that&#x27;s scheduled is completed or failed.</p><p>Now, find the pods created by the job last scheduled and view the standard output of one of the pods. Note that your job name and pod name would be different.</p><p><strong><em># Replace &quot;hello-4111706356&quot; with the job name in your system</em></strong></p><p><strong>$ pods=$(kubectl get pods --selector=job-name=hello-4111706356 --output=jsonpath={.items..metadata.name})</strong></p><p><strong>$ echo $pods</strong></p><p><strong>hello-4111706356-o9qcm</strong></p><p><strong>$ kubectl logs $pods</strong></p><p><strong>Mon Aug 29 21:34:09 UTC 2016</strong></p><p><strong>Hello from the Kubernetes cluster</strong></p><h5><strong>Deleting a Cron Job</strong></h5><p>Once you don&#x27;t need a cron job anymore, simply delete it with <strong>kubectl</strong>:</p><p><strong>$ kubectl delete cronjob hello</strong></p><p><strong>cronjob &quot;hello&quot; deleted</strong></p><p>This stops new jobs from being created and removes all the jobs and pods created by this cronjob. You can read more about it in <a href="https://kubernetes.io/docs/concepts/workloads/controllers/garbage-collection/">garbage collection section</a>.</p><h5><strong>Cron Job Limitations</strong></h5><p>A cron job creates a job object about once per execution time of its schedule. We say &quot;about&quot; because there are certain circumstances where two jobs might be created, or no job might be created. We attempt to make these rare, but do not completely prevent them. Therefore, jobs should be idempotent.</p><p>If <strong>startingDeadlineSeconds</strong> is set to a large value or left unset (the default) and if <strong>concurrencyPolicy</strong> is set to <strong>Allow</strong>, the jobs will always run at least once.</p><p>Jobs may fail to run if the CronJob controller is not running or broken for a span of time from before the start time of the CronJob to start time plus <strong>startingDeadlineSeconds</strong>, or if the span covers multiple start times and <strong>concurrencyPolicy</strong> does not allow concurrency. For example, suppose a cron job is set to start at exactly <strong>08:30:00</strong> and its <strong>startingDeadlineSeconds</strong> is set to 10, if the CronJob controller happens to be down from <strong>08:29:00</strong> to <strong>08:42:00</strong>, the job will not start. Set a longer <strong>startingDeadlineSeconds</strong> if starting later is better than not starting at all.</p><p>The Cronjob is only responsible for creating Jobs that match its schedule, and the Job in turn is responsible for the management of the Pods it represents.</p><h5><strong>Writing a Cron Job Spec</strong></h5><p>As with all other Kubernetes configs, a cron job needs <strong>apiVersion</strong>, <strong>kind</strong>, and <strong>metadata</strong> fields. For general information about working with config files, see <a href="https://kubernetes.io/docs/user-guide/deploying-applications">deploying applications</a>, and <a href="https://kubernetes.io/docs/user-guide/working-with-resources">using kubectl to manage resources</a> documents.</p><p>A cron job also needs a <a href="https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status"><strong>.spec</strong> section</a>.</p><p><strong>Note:</strong> All modifications to a cron job, especially its <strong>.spec</strong>, will be applied only to the next run.</p><h6><strong>Schedule</strong></h6><p>The <strong>.spec.schedule</strong> is a required field of the <strong>.spec</strong>. It takes a <a href="https://en.wikipedia.org/wiki/Cron">Cron</a> format string, e.g. <strong>0 <!-- -->*<!-- --> <!-- -->*<!-- --> <!-- -->*<!-- --> <!-- -->*</strong>or <strong>@hourly</strong>, as schedule time of its jobs to be created and executed.</p><h6><strong>Job Template</strong></h6><p>The <strong>.spec.jobTemplate</strong> is another required field of the <strong>.spec</strong>. It is a job template. It has exactly the same schema as a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/">Job</a>, except it is nested and does not have an <strong>apiVersion</strong> or <strong>kind</strong>, see<a href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#writing-a-job-spec">Writing a Job Spec</a>.</p><h6><strong>Starting Deadline Seconds</strong></h6><p>The <strong>.spec.startingDeadlineSeconds</strong> field is optional. It stands for the deadline (in seconds) for starting the job if it misses its scheduled time for any reason. Missed jobs executions will be counted as failed ones. If not specified, there&#x27;s no deadline.</p><h6><strong>Concurrency Policy</strong></h6><p>The <strong>.spec.concurrencyPolicy</strong> field is also optional. It specifies how to treat concurrent executions of a job created by this cron job. Only one of the following concurrent policies may be specified:</p><ul><li><strong>Allow</strong> (default): allows concurrently running jobs</li><li><strong>Forbid</strong>: forbids concurrent runs, skipping next run if previous hasn&#x27;t finished yet</li><li><strong>Replace</strong>: cancels currently running job and replaces it with a new one</li></ul><p>Note that concurrency policy only applies to the jobs created by the same cron job. If there are multiple cron jobs, their respective jobs are always allowed to run concurrently.</p><h6><strong>Suspend</strong></h6><p>The <strong>.spec.suspend</strong> field is also optional. If set to <strong>true</strong>, all subsequent executions will be suspended. It does not apply to already started executions. Defaults to false.</p><h6><strong>Jobs History Limits</strong></h6><p>The <strong>.spec.successfulJobsHistoryLimit</strong> and <strong>.spec.failedJobsHistoryLimit</strong> fields are optional. These fields specify how many completed and failed jobs should be kept. By default, they are set to 3 and 1 respectively. Setting a limit to <strong>0</strong> corresponds to keeping none of the corresponding kind of jobs after they finish.</p><h2>Configuration</h2><h3>Configuration Best Practices</h3><p>This document highlights and consolidates configuration best practices that are introduced throughout the user guide, Getting Started documentation, and examples.</p><p>This is a living document. If you think of something that is not on this list but might be useful to others, please don&#x27;t hesitate to file an issue or submit a PR.</p><ul><li><a href="https://kubernetes.io/docs/concepts/configuration/overview/#general-configuration-tips"><strong>General Configuration Tips</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/overview/#naked-pods-vs-replicasets-deployments-and-jobs"><strong>&quot;Naked&quot; Pods vs ReplicaSets, Deployments, and Jobs</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/overview/#services"><strong>Services</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/overview/#using-labels"><strong>Using Labels</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/overview/#container-images"><strong>Container Images</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/overview/#using-kubectl"><strong>Using kubectl</strong></a></li></ul><h4>General Configuration Tips</h4><ul><li>When defining configurations, specify the latest stable API version.</li><li>Configuration files should be stored in version control before being pushed to the cluster. This allows you to quickly roll back a configuration change if necessary. It also aids cluster re-creation and restoration.</li><li>Write your configuration files using YAML rather than JSON. Though these formats can be used interchangeably in almost all scenarios, YAML tends to be more user-friendly.</li><li>Group related objects into a single file whenever it makes sense. One file is often easier to manage than several. See the <a href="https://github.com/kubernetes/examples/tree/master/guestbook/all-in-one/guestbook-all-in-one.yaml">guestbook-all-in-one.yaml</a> file as an example of this syntax.</li><li>Note also that many <strong>kubectl</strong> commands can be called on a directory. For example, you can call <strong>kubectl create</strong> on a directory of config files.</li><li>Don&#x27;t specify default values unnecessarily: simple, minimal configuration will make errors less likely.</li><li>Put object descriptions in annotations, to allow better introspection.</li></ul><h4>&quot;Naked&quot; Pods vs ReplicaSets, Deployments, and Jobs</h4><ul><li>Don&#x27;t use naked Pods (that is, Pods not bound to a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/">ReplicaSet</a> or <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployment</a>) if you can avoid it. Naked Pods will not be rescheduled in the event of a node failure.</li></ul><p>A Deployment, which both creates a ReplicaSet to ensure that the desired number of Pods is always available, and specifies a strategy to replace Pods (such as <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-update-deployment">RollingUpdate</a>), is almost always preferable to creating Pods directly, except for some explicit <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy"><strong>restartPolicy: Never</strong></a>scenarios. A <a href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/">Job</a> may also be appropriate.</p><h4>Services</h4><ul><li>Create a <a href="https://kubernetes.io/docs/concepts/services-networking/service/">Service</a> before its corresponding backend workloads (Deployments or ReplicaSets), and before any workloads that need to access it. When Kubernetes starts a container, it provides environment variables pointing to all the Services which were running when the container was started. For example, if a Service named <strong>foo</strong> exists, all containers will get the following variables in their initial environment:</li><li><strong>FOO_SERVICE_HOST=<code>&lt;the host the Service is running on&gt;</code></strong></li><li><strong>FOO_SERVICE_PORT=<code>&lt;the port the Service is running on&gt;</code></strong></li></ul><p>If you are writing code that talks to a Service, don&#x27;t use these environment variables; use the <a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/">DNS name of the Service</a> instead. Service environment variables are provided only for older software which can&#x27;t be modified to use DNS lookups, and are a much less flexible way of accessing Services.</p><ul><li>Don&#x27;t specify a <strong>hostPort</strong> for a Pod unless it is absolutely necessary. When you bind a Pod to a <strong>hostPort</strong>, it limits the number of places the Pod can be scheduled, because each <code>&lt;**hostIP**, **hostPort**, **protocol**&gt;</code> combination must be unique. If you don&#x27;t specify the <strong>hostIP</strong> and <strong>protocol</strong> explicitly, Kubernetes will use <strong>0.0.0.0</strong> as the default <strong>hostIP</strong> and <strong>TCP</strong> as the default <strong>protocol</strong>.</li></ul><p>If you only need access to the port for debugging purposes, you can use the <a href="https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#manually-constructing-apiserver-proxy-urls">apiserver proxy</a> or <a href="https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/"><strong>kubectl port-forward</strong></a>.</p><p>If you explicitly need to expose a Pod&#x27;s port on the node, consider using a <a href="https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport">NodePort</a> Service before resorting to <strong>hostPort</strong>.</p><ul><li>Avoid using <strong>hostNetwork</strong>, for the same reasons as <strong>hostPort</strong>.</li><li>Use <a href="https://kubernetes.io/docs/concepts/services-networking/service/#headless-services">headless Services</a> (which have a <strong>ClusterIP</strong> of <strong>None</strong>) for easy service discovery when you don&#x27;t need <strong>kube-proxy</strong> load balancing.</li></ul><h4>Using Labels</h4><ul><li>Define and use <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/">labels</a> that identify <strong>semantic attributes</strong> of your application or Deployment, such as <strong>{ app: myapp, tier: frontend, phase: test, deployment: v3 }</strong>. You can use these labels to select the appropriate Pods for other resources; for example, a Service that selects all <strong>tier: frontend</strong> Pods, or all <strong>phase: test</strong> components of <strong>app: myapp</strong>. See the <a href="https://github.com/kubernetes/examples/tree/master/guestbook/">guestbook</a>app for examples of this approach.</li></ul><p>A Service can be made to span multiple Deployments by omitting release-specific labels from its selector. <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployments</a> make it easy to update a running service without downtime.</p><p>A desired state of an object is described by a Deployment, and if changes to that spec are applied, the deployment controller changes the actual state to the desired state at a controlled rate.</p><ul><li>You can manipulate labels for debugging. Because Kubernetes controllers (such as ReplicaSet) and Services match to Pods using selector labels, removing the relevant labels from a Pod will stop it from being considered by a controller or from being served traffic by a Service. If you remove the labels of an existing Pod, its controller will create a new Pod to take its place. This is a useful way to debug a previously &quot;live&quot; Pod in a &quot;quarantine&quot; environment. To interactively remove or add labels, use <a href="https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#label"><strong>kubectl label</strong></a>.</li></ul><h4>Container Images</h4><ul><li>The default <a href="https://kubernetes.io/docs/concepts/containers/images/#updating-images">imagePullPolicy</a> for a container is <strong>IfNotPresent</strong>, which causes the <a href="https://kubernetes.io/docs/admin/kubelet/">kubelet</a> to pull an image only if it does not already exist locally. If you want the image to be pulled every time Kubernetes starts the container, specify <strong>imagePullPolicy: Always</strong>.</li></ul><p>An alternative, but deprecated way to have Kubernetes always pull the image is to use the <strong>:latest</strong> tag, which will implicitly set the <strong>imagePullPolicy</strong> to <strong>Always</strong>.</p><p><strong>Note:</strong> You should avoid using the <strong>:latest</strong> tag when deploying containers in production, because this makes it hard to track which version of the image is running and hard to roll back.</p><ul><li>To make sure the container always uses the same version of the image, you can specify its <a href="https://docs.docker.com/engine/reference/commandline/pull/#pull-an-image-by-digest-immutable-identifier">digest</a>(for example <strong>sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2</strong>). This uniquely identifies a specific version of the image, so it will never be updated by Kubernetes unless you change the digest value.</li></ul><h4>Using kubectl</h4><ul><li>Use <code>kubectl apply -f &lt;directory&gt;</code> or <code>kubectl create -f &lt;directory&gt;</code>. This looks for Kubernetes configuration in all <code>.yaml</code>, <code>.yml</code>, and <code>.json</code> files in <code>&lt;directory&gt;</code> and passes it to <code>apply</code> or <code>create</code>.</li><li>Use label selectors for <strong>get</strong> and <strong>delete</strong> operations instead of specific object names. See the sections on <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors">label selectors</a> and <a href="https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#using-labels-effectively">using labels effectively</a>.</li><li>Use <strong>kubectl run</strong> and <strong>kubectl expose</strong> to quickly create single-container Deployments and Services. See <a href="https://kubernetes.io/docs/tasks/access-application-cluster/service-access-application-cluster/">Use a Service to Access an Application in a Cluster</a> for an example.</li></ul><h3>Managing Compute Resources for Containers</h3><p>When you specify a <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/">Pod</a>, you can optionally specify how much CPU and memory (RAM) each Container needs. When Containers have resource requests specified, the scheduler can make better decisions about which nodes to place Pods on. And when Containers have their limits specified, contention for resources on a node can be handled in a specified manner. For more details about the difference between requests and limits, see <a href="https://git.k8s.io/community/contributors/design-proposals/node/resource-qos.md">Resource QoS</a>.</p><ul><li><a href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#resource-types"><strong>Resource types</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#resource-requests-and-limits-of-pod-and-container"><strong>Resource requests and limits of Pod and Container</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#meaning-of-cpu"><strong>Meaning of CPU</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#meaning-of-memory"><strong>Meaning of memory</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#how-pods-with-resource-requests-are-scheduled"><strong>How Pods with resource requests are scheduled</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#how-pods-with-resource-limits-are-run"><strong>How Pods with resource limits are run</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#monitoring-compute-resource-usage"><strong>Monitoring compute resource usage</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#troubleshooting"><strong>Troubleshooting</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#my-pods-are-pending-with-event-message-failedscheduling"><strong>My Pods are pending with event message failedScheduling</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#my-container-is-terminated"><strong>My Container is terminated</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#local-ephemeral-storage"><strong>Local ephemeral storage</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#requests-and-limits-setting-for-local-ephemeral-storage"><strong>Requests and limits setting for local ephemeral storage</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#how-pods-with-ephemeral-storage-requests-are-scheduled"><strong>How Pods with ephemeral-storage requests are scheduled</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#how-pods-with-ephemeral-storage-limits-run"><strong>How Pods with ephemeral-storage limits run</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#extended-resources"><strong>Extended Resources</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#managing-extended-resources"><strong>Managing extended resources</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#node-level-extended-resources"><strong>Node-level extended resources</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#device-plugin-managed-resources"><strong>Device plugin managed resources</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#other-resources"><strong>Other resources</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#cluster-level-extended-resources"><strong>Cluster-level extended resources</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#consuming-extended-resources"><strong>Consuming extended resources</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#planned-improvements"><strong>Planned Improvements</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Resource types</h4><p>CPU and memory are each a resource type. A resource type has a base unit. CPU is specified in units of cores, and memory is specified in units of bytes.</p><p>CPU and memory are collectively referred to as compute resources, or just resources. Compute resources are measurable quantities that can be requested, allocated, and consumed. They are distinct from <a href="https://kubernetes.io/docs/concepts/overview/kubernetes-api/">API resources</a>. API resources, such as Pods and <a href="https://kubernetes.io/docs/concepts/services-networking/service/">Services</a> are objects that can be read and modified through the Kubernetes API server.</p><h4>Resource requests and limits of Pod and Container</h4><p>Each Container of a Pod can specify one or more of the following:</p><ul><li><strong>spec.containers[].resources.limits.cpu</strong></li><li><strong>spec.containers[].resources.limits.memory</strong></li><li><strong>spec.containers[].resources.requests.cpu</strong></li><li><strong>spec.containers[].resources.requests.memory</strong></li></ul><p>Although requests and limits can only be specified on individual Containers, it is convenient to talk about Pod resource requests and limits. A Pod resource request/limit for a particular resource type is the sum of the resource requests/limits of that type for each Container in the Pod.</p><h4>Meaning of CPU</h4><p>Limits and requests for CPU resources are measured in cpu units. One cpu, in Kubernetes, is equivalent to:</p><ul><li>1 AWS vCPU</li><li>1 GCP Core</li><li>1 Azure vCore</li><li>1 Hyperthread on a bare-metal Intel processor with Hyperthreading</li></ul><p>Fractional requests are allowed. A Container with <strong>spec.containers[].resources.requests.cpu</strong>of <strong>0.5</strong> is guaranteed half as much CPU as one that asks for 1 CPU. The expression <strong>0.1</strong> is equivalent to the expression <strong>100m</strong>, which can be read as &quot;one hundred millicpu&quot;. Some people say &quot;one hundred millicores&quot;, and this is understood to mean the same thing. A request with a decimal point, like <strong>0.1</strong>, is converted to <strong>100m</strong> by the API, and precision finer than <strong>1m</strong> is not allowed. For this reason, the form <strong>100m</strong> might be preferred.</p><p>CPU is always requested as an absolute quantity, never as a relative quantity; 0.1 is the same amount of CPU on a single-core, dual-core, or 48-core machine.</p><h4>Meaning of memory</h4><p>Limits and requests for <strong>memory</strong> are measured in bytes. You can express memory as a plain integer or as a fixed-point integer using one of these suffixes: E, P, T, G, M, K. You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi, Mi, Ki. For example, the following represent roughly the same value:</p><p><strong>128974848, 129e6, 129M, 123Mi</strong></p><p>Here&#x27;s an example. The following Pod has two Containers. Each Container has a request of 0.25 cpu and 64MiB (2^26^ bytes) of memory. Each Container has a limit of 0.5 cpu and 128MiB of memory. You can say the Pod has a request of 0.5 cpu and 128 MiB of memory, and a limit of 1 cpu and 256MiB of memory.</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: frontend</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: db</strong></p><p><strong>image: mysql</strong></p><p><strong>env:</strong></p><p><strong>- name: MYSQL_ROOT_PASSWORD</strong></p><p><strong>value: &quot;password&quot;</strong></p><p><strong>resources:</strong></p><p><strong>requests:</strong></p><p><strong>memory: &quot;64Mi&quot;</strong></p><p><strong>cpu: &quot;250m&quot;</strong></p><p><strong>limits:</strong></p><p><strong>memory: &quot;128Mi&quot;</strong></p><p><strong>cpu: &quot;500m&quot;</strong></p><p><strong>- name: wp</strong></p><p><strong>image: wordpress</strong></p><p><strong>resources:</strong></p><p><strong>requests:</strong></p><p><strong>memory: &quot;64Mi&quot;</strong></p><p><strong>cpu: &quot;250m&quot;</strong></p><p><strong>limits:</strong></p><p><strong>memory: &quot;128Mi&quot;</strong></p><p><strong>cpu: &quot;500m&quot;</strong></p><h4>How Pods with resource requests are scheduled</h4><p>When you create a Pod, the Kubernetes scheduler selects a node for the Pod to run on. Each node has a maximum capacity for each of the resource types: the amount of CPU and memory it can provide for Pods. The scheduler ensures that, for each resource type, the sum of the resource requests of the scheduled Containers is less than the capacity of the node. Note that although actual memory or CPU resource usage on nodes is very low, the scheduler still refuses to place a Pod on a node if the capacity check fails. This protects against a resource shortage on a node when resource usage later increases, for example, during a daily peak in request rate.</p><h4>How Pods with resource limits are run</h4><p>When the kubelet starts a Container of a Pod, it passes the CPU and memory limits to the container runtime.</p><p>When using Docker:</p><ul><li>The <strong>spec.containers[].resources.requests.cpu</strong> is converted to its core value, which is potentially fractional, and multiplied by 1024. The greater of this number or 2 is used as the value of the <a href="https://docs.docker.com/engine/reference/run/#/cpu-share-constraint"><strong>--cpu-shares</strong></a> flag in the <strong>docker run</strong> command.</li><li>The <strong>spec.containers[].resources.limits.cpu</strong> is converted to its millicore value and multiplied by 100. The resulting value is the total amount of CPU time that a container can use every 100ms. A container cannot use more than its share of CPU time during this interval.</li></ul><p><strong>Note</strong>: The default quota period is 100ms. The minimum resolution of CPU quota is 1ms.</p><ul><li>The <strong>spec.containers[].resources.limits.memory</strong> is converted to an integer, and used as the value of the <a href="https://docs.docker.com/engine/reference/run/#/user-memory-constraints"><strong>--memory</strong></a> flag in the <strong>docker run</strong> command.</li></ul><p>If a Container exceeds its memory limit, it might be terminated. If it is restartable, the kubelet will restart it, as with any other type of runtime failure.</p><p>If a Container exceeds its memory request, it is likely that its Pod will be evicted whenever the node runs out of memory.</p><p>A Container might or might not be allowed to exceed its CPU limit for extended periods of time. However, it will not be killed for excessive CPU usage.</p><p>To determine whether a Container cannot be scheduled or is being killed due to resource limits, see the <a href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#troubleshooting">Troubleshooting</a> section.</p><h4>Monitoring compute resource usage</h4><p>The resource usage of a Pod is reported as part of the Pod status.</p><p>If <a href="http://releases.k8s.io/master/cluster/addons/cluster-monitoring/README.md">optional monitoring</a> is configured for your cluster, then Pod resource usage can be retrieved from the monitoring system.</p><h4>Troubleshooting</h4><h5><strong>My Pods are pending with event message failedScheduling</strong></h5><p>If the scheduler cannot find any node where a Pod can fit, the Pod remains unscheduled until a place can be found. An event is produced each time the scheduler fails to find a place for the Pod, like this:</p><p><strong>$ kubectl describe pod frontend | grep -A 3 Events</strong></p><p><strong>Events:</strong></p><p><strong>FirstSeen LastSeen Count From Subobject PathReason Message</strong></p><p><strong>36s 5s 6 {scheduler } FailedScheduling Failed for reason PodExceedsFreeCPU and possibly others</strong></p><p>In the preceding example, the Pod named &quot;frontend&quot; fails to be scheduled due to insufficient CPU resource on the node. Similar error messages can also suggest failure due to insufficient memory (PodExceedsFreeMemory). In general, if a Pod is pending with a message of this type, there are several things to try:</p><ul><li>Add more nodes to the cluster.</li><li>Terminate unneeded Pods to make room for pending Pods.</li><li>Check that the Pod is not larger than all the nodes. For example, if all the nodes have a capacity of <strong>cpu: 1</strong>, then a Pod with a request of <strong>cpu: 1.1</strong> will never be scheduled.</li></ul><p>You can check node capacities and amounts allocated with the <strong>kubectl describe nodes</strong>command. For example:</p><p><strong>$ kubectl describe nodes e2e-test-minion-group-4lw4</strong></p><p><strong>Name: e2e-test-minion-group-4lw4</strong></p><p><strong>[ <!-- -->.<!-- -->.. lines removed for clarity <!-- -->.<!-- -->..]</strong></p><p><strong>Capacity:</strong></p><p><strong>alpha.kubernetes.io/nvidia-gpu: 0</strong></p><p><strong>cpu: 2</strong></p><p><strong>memory: 7679792Ki</strong></p><p><strong>pods: 110</strong></p><p><strong>Allocatable:</strong></p><p><strong>alpha.kubernetes.io/nvidia-gpu: 0</strong></p><p><strong>cpu: 1800m</strong></p><p><strong>memory: 7474992Ki</strong></p><p><strong>pods: 110</strong></p><p><strong>[ <!-- -->.<!-- -->.. lines removed for clarity <!-- -->.<!-- -->..]</strong></p><p><strong>Non-terminated Pods: (5 in total)</strong></p><p><strong>Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits</strong></p><p><strong>--------- ---- ------------ ---------- --------------- -------------</strong></p><p><strong>kube-system fluentd-gcp-v1.38-28bv1 100m (5%) 0 (0%) 200Mi (2%) 200Mi (2%)</strong></p><p><strong>kube-system kube-dns-3297075139-61lj3 260m (13%) 0 (0%) 100Mi (1%) 170Mi (2%)</strong></p><p><strong>kube-system kube-proxy-e2e-test-<!-- -->.<!-- -->.. 100m (5%) 0 (0%) 0 (0%) 0 (0%)</strong></p><p><strong>kube-system monitoring-influxdb-grafana-v4-z1m12 200m (10%) 200m (10%) 600Mi (8%) 600Mi (8%)</strong></p><p><strong>kube-system node-problem-detector-v0.1-fj7m3 20m (1%) 200m (10%) 20Mi (0%) 100Mi (1%)</strong></p><p><strong>Allocated resources:</strong></p><p><strong>(Total limits may be over 100 percent, i.e., overcommitted.)</strong></p><p><strong>CPU Requests CPU Limits Memory Requests Memory Limits</strong></p><p><strong>------------ ---------- --------------- -------------</strong></p><p><strong>680m (34%) 400m (20%) 920Mi (12%) 1070Mi (14%)</strong></p><p>In the preceding output, you can see that if a Pod requests more than 1120m CPUs or 6.23Gi of memory, it will not fit on the node.</p><p>By looking at the <strong>Pods</strong> section, you can see which Pods are taking up space on the node.</p><p>The amount of resources available to Pods is less than the node capacity, because system daemons use a portion of the available resources. The <strong>allocatable</strong> field <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#nodestatus-v1-core">NodeStatus</a> gives the amount of resources that are available to Pods. For more information, see <a href="https://git.k8s.io/community/contributors/design-proposals/node/node-allocatable.md">Node Allocatable Resources</a>.</p><p>The <a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/">resource quota</a> feature can be configured to limit the total amount of resources that can be consumed. If used in conjunction with namespaces, it can prevent one team from hogging all the resources.</p><h5><strong>My Container is terminated</strong></h5><p>Your Container might get terminated because it is resource-starved. To check whether a Container is being killed because it is hitting a resource limit, call <strong>kubectl describe pod</strong> on the Pod of interest:</p><p><strong>[12:54:41]<!-- --> $ kubectl describe pod simmemleak-hra99</strong></p><p><strong>Name: simmemleak-hra99</strong></p><p><strong>Namespace: default</strong></p><p><strong>Image(s): saadali/simmemleak</strong></p><p><strong>Node: kubernetes-node-tf0f/10.240.216.66</strong></p><p><strong>Labels: name=simmemleak</strong></p><p><strong>Status: Running</strong></p><p><strong>Reason:</strong></p><p><strong>Message:</strong></p><p><strong>IP: 10.244.2.75</strong></p><p><strong>Replication Controllers: simmemleak (1/1 replicas created)</strong></p><p><strong>Containers:</strong></p><p><strong>simmemleak:</strong></p><p><strong>Image: saadali/simmemleak</strong></p><p><strong>Limits:</strong></p><p><strong>cpu: 100m</strong></p><p><strong>memory: 50Mi</strong></p><p><strong>State: Running</strong></p><p><strong>Started: Tue, 07 Jul 2015 12:54:41 -0700</strong></p><p><strong>Last Termination State: Terminated</strong></p><p><strong>Exit Code: 1</strong></p><p><strong>Started: Fri, 07 Jul 2015 12:54:30 -0700</strong></p><p><strong>Finished: Fri, 07 Jul 2015 12:54:33 -0700</strong></p><p><strong>Ready: False</strong></p><p><strong>Restart Count: 5</strong></p><p><strong>Conditions:</strong></p><p><strong>Type Status</strong></p><p><strong>Ready False</strong></p><p><strong>Events:</strong></p><p><strong>FirstSeen LastSeen Count From SubobjectPath Reason Message</strong></p><p><strong>Tue, 07 Jul 2015 12:53:51 -0700 Tue, 07 Jul 2015 12:53:51 -0700 1 {scheduler } scheduled Successfully assigned simmemleak-hra99 to kubernetes-node-tf0f</strong></p><p><strong>Tue, 07 Jul 2015 12:53:51 -0700 Tue, 07 Jul 2015 12:53:51 -0700 1 {kubelet kubernetes-node-tf0f} implicitly required container POD pulled Pod container image &quot;k8s.gcr.io/pause:0.8.0&quot; already present on machine</strong></p><p><strong>Tue, 07 Jul 2015 12:53:51 -0700 Tue, 07 Jul 2015 12:53:51 -0700 1 {kubelet kubernetes-node-tf0f} implicitly required container POD created Created with docker id 6a41280f516d</strong></p><p><strong>Tue, 07 Jul 2015 12:53:51 -0700 Tue, 07 Jul 2015 12:53:51 -0700 1 {kubelet kubernetes-node-tf0f} implicitly required container POD started Started with docker id 6a41280f516d</strong></p><p><strong>Tue, 07 Jul 2015 12:53:51 -0700 Tue, 07 Jul 2015 12:53:51 -0700 1 {kubelet kubernetes-node-tf0f} spec.containers{simmemleak} created Created with docker id 87348f12526a</strong></p><p>In the preceding example, the <strong>Restart Count: 5</strong> indicates that the <strong>simmemleak</strong> Container in the Pod was terminated and restarted five times.</p><p>You can call <strong>kubectl get pod</strong> with the <strong>-o go-template=<!-- -->.<!-- -->..</strong> option to fetch the status of previously terminated Containers:</p><p><strong>[13:59:01]<!-- --> $ kubectl get pod -o go-template=\&#x27;{{range.status.containerStatuses}}{{&quot;Container Name: &quot;}}{{.name}}{{&quot;<!-- -->\<!-- -->r<!-- -->\<!-- -->nLastState: &quot;}}{{.lastState}}{{end}}\&#x27; simmemleak-hra99</strong></p><p><strong>Container Name: simmemleak</strong></p><p><strong>LastState: map[terminated:map<!-- -->[exitCode:137 reason:OOM Killed startedAt:2015-07-07T20:58:43Z finishedAt:2015-07-07T20:58:43Z containerID:docker://0e4095bba1feccdfe7ef9fb6ebffe972b4b14285d5acdec6f0d3ae8a22fad8b2]<!-- -->]</strong></p><p>You can see that the Container was terminated because of <strong>reason:OOM Killed</strong>, where <strong>OOM</strong> stands for Out Of Memory.</p><h4>Local ephemeral storage</h4><p><strong>FEATURE STATE:</strong> <strong>Kubernetes v1.10</strong> <a href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/">beta</a></p><p>Kubernetes version 1.8 introduces a new resource, ephemeral-storage for managing local ephemeral storage. In each Kubernetes node, kubelet&#x27;s root directory (/var/lib/kubelet by default) and log directory (/var/log) are stored on the root partition of the node. This partition is also shared and consumed by pods via EmptyDir volumes, container logs, image layers and container writable layers.</p><p>This partition is &quot;ephemeral&quot; and applications cannot expect any performance SLAs (Disk IOPS for example) from this partition. Local ephemeral storage management only applies for the root partition; the optional partition for image layer and writable layer is out of scope.</p><p><strong>Note:</strong> If an optional runtime partition is used, root partition will not hold any image layer or writable layers.</p><h5><strong>Requests and limits setting for local ephemeral storage</strong></h5><p>Each Container of a Pod can specify one or more of the following:</p><ul><li><strong>spec.containers[].resources.limits.ephemeral-storage</strong></li><li><strong>spec.containers[].resources.requests.ephemeral-storage</strong></li></ul><p>Limits and requests for <strong>ephemeral-storage</strong> are measured in bytes. You can express storage as a plain integer or as a fixed-point integer using one of these suffixes: E, P, T, G, M, K. You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi, Mi, Ki. For example, the following represent roughly the same value:</p><p><strong>128974848, 129e6, 129M, 123Mi</strong></p><p>For example, the following Pod has two Containers. Each Container has a request of 2GiB of local ephemeral storage. Each Container has a limit of 4GiB of local ephemeral storage. Therefore, the Pod has a request of 4GiB of local ephemeral storage, and a limit of 8GiB of storage.</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: frontend</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: db</strong></p><p><strong>image: mysql</strong></p><p><strong>env:</strong></p><p><strong>- name: MYSQL_ROOT_PASSWORD</strong></p><p><strong>value: &quot;password&quot;</strong></p><p><strong>resources:</strong></p><p><strong>requests:</strong></p><p><strong>ephemeral-storage: &quot;2Gi&quot;</strong></p><p><strong>limits:</strong></p><p><strong>ephemeral-storage: &quot;4Gi&quot;</strong></p><p><strong>- name: wp</strong></p><p><strong>image: wordpress</strong></p><p><strong>resources:</strong></p><p><strong>requests:</strong></p><p><strong>ephemeral-storage: &quot;2Gi&quot;</strong></p><p><strong>limits:</strong></p><p><strong>ephemeral-storage: &quot;4Gi&quot;</strong></p><h5><strong>How Pods with ephemeral-storage requests are scheduled</strong></h5><p>When you create a Pod, the Kubernetes scheduler selects a node for the Pod to run on. Each node has a maximum amount of local ephemeral storage it can provide for Pods. (For more information, see <a href="https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable">&quot;Node Allocatable&quot;</a> The scheduler ensures that the sum of the resource requests of the scheduled Containers is less than the capacity of the node.</p><h5><strong>How Pods with ephemeral-storage limits run</strong></h5><p>For container-level isolation, if a Container&#x27;s writable layer and logs usage exceeds its storage limit, the pod will be evicted. For pod-level isolation, if the sum of the local ephemeral storage usage from all containers and also the pod&#x27;s EmptyDir volumes exceeds the limit, the pod will be evicted.</p><h4>Extended Resources</h4><p>Extended Resources are fully-qualified resource names outside the <strong>kubernetes.io</strong> domain. They allow cluster operators to advertise and users to consume the non-Kubernetes-built-in resources.</p><p>There are two steps required to use Extended Resources. First, the cluster operator must advertise an Extended Resource. Second, users must request the Extended Resource in Pods.</p><h5><strong>Managing extended resources</strong></h5><h6><strong>Node-level extended resources</strong></h6><p>Node-level extended resources are tied to nodes.</p><p><strong>Device plugin managed resources</strong></p><p>See <a href="https://kubernetes.io/docs/concepts/cluster-administration/device-plugins/">Device Plugin</a> for how to advertise device plugin managed resources on each node.</p><p><strong>Other resources</strong></p><p>To advertise a new node-level extended resource, the cluster operator can submit a <strong>PATCH</strong> HTTP request to the API server to specify the available quantity in the <strong>status.capacity</strong> for a node in the cluster. After this operation, the node&#x27;s <strong>status.capacity</strong> will include a new resource. The <strong>status.allocatable</strong> field is updated automatically with the new resource asynchronously by the kubelet. Note that because the scheduler uses the node <strong>status.allocatable</strong> value when evaluating Pod fitness, there may be a short delay between patching the node capacity with a new resource and the first pod that requests the resource to be scheduled on that node.</p><p><strong>Example:</strong></p><p>Here is an example showing how to use <strong>curl</strong> to form an HTTP request that advertises five &quot;example.com/foo&quot; resources on node <strong>k8s-node-1</strong> whose master is <strong>k8s-master</strong>.</p><p><strong>curl --header &quot;Content-Type: application/json-patch+json&quot; <!-- -->\</strong></p><p><strong>--request PATCH <!-- -->\</strong></p><p><strong>--data \&#x27;<!-- -->[{&quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/status/capacity/example.com<!-- -->~<!-- -->1foo&quot;, &quot;value&quot;: &quot;5&quot;}]<!-- -->\&#x27; <!-- -->\</strong></p><p><strong>http://k8s-master:8080/api/v1/nodes/k8s-node-1/status</strong></p><p><strong>Note</strong>: In the preceding request, <strong>~<!-- -->1</strong> is the encoding for the character <strong>/</strong> in the patch path. The operation path value in JSON-Patch is interpreted as a JSON-Pointer. For more details, see<a href="https://tools.ietf.org/html/rfc6901#section-3">IETF RFC 6901, section 3</a>.</p><h6><strong>Cluster-level extended resources</strong></h6><p>Cluster-level extended resources are not tied to nodes. They are usually managed by scheduler extenders, which handle the resource comsumption, quota and so on.</p><p>You can specify the extended resources that are handled by scheduler extenders in <a href="https://github.com/kubernetes/kubernetes/blob/release-1.10/pkg/scheduler/api/v1/types.go#L31">scheduler policy configuration</a>.</p><p><strong>Example:</strong></p><p>The following configuration for a scheduler policy indicates that the cluster-level extended resource &quot;example.com/foo&quot; is handled by scheduler extender.</p><ul><li>The scheduler sends a pod to the scheduler extender only if the pod requests &quot;example.com/foo&quot;.</li><li>The <strong>ignoredByScheduler</strong> field specifies that the scheduler does not check the &quot;example.com/foo&quot; resource in its <strong>PodFitsResources</strong> predicate.</li></ul><p><strong>{</strong></p><p><strong>&quot;kind&quot;: &quot;Policy&quot;,</strong></p><p><strong>&quot;apiVersion&quot;: &quot;v1&quot;,</strong></p><p><strong>&quot;extenders&quot;: [</strong></p><p><strong>{</strong></p><p><strong>&quot;urlPrefix&quot;:&quot;<code>&lt;extender-endpoint&gt;</code>&quot;,</strong></p><p><strong>&quot;bindVerb&quot;: &quot;bind&quot;,</strong></p><p><strong>&quot;ManagedResources&quot;: [</strong></p><p><strong>{</strong></p><p><strong>&quot;name&quot;: &quot;example.com/foo&quot;,</strong></p><p><strong>&quot;ignoredByScheduler&quot;: true</strong></p><p><strong>}</strong></p><p><strong>]</strong></p><p><strong>}</strong></p><p><strong>]</strong></p><p><strong>}</strong></p><h5><strong>Consuming extended resources</strong></h5><p>Users can consume Extended Resources in Pod specs just like CPU and memory. The scheduler takes care of the resource accounting so that no more than the available amount is simultaneously allocated to Pods.</p><p>The API server restricts quantities of Extended Resources to whole numbers. Examples of validquantities are <strong>3</strong>, <strong>3000m</strong> and <strong>3Ki</strong>. Examples of invalid quantities are <strong>0.5</strong> and <strong>1500m</strong>.</p><p><strong>Note:</strong> Extended Resources replace Opaque Integer Resources. Users can use any domain name prefix other than &quot;<strong>kubernetes.io</strong>&quot; which is reserved.</p><p>To consume an Extended Resource in a Pod, include the resource name as a key in the <strong>spec.containers[].resources.limits</strong> map in the container spec.</p><p><strong>Note:</strong> Extended resources cannot be overcommitted, so request and limit must be equal if both are present in a container spec.</p><p>A Pod is scheduled only if all of the resource requests are satisfied, including CPU, memory and any Extended Resources. The Pod remains in the <strong>PENDING</strong> state as long as the resource request cannot be satisfied.</p><p><strong>Example:</strong></p><p>The Pod below requests 2 CPUs and 1 &quot;example.com/foo&quot; (an extended resource).</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: my-pod</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: my-container</strong></p><p><strong>image: myimage</strong></p><p><strong>resources:</strong></p><p><strong>requests:</strong></p><p><strong>cpu: 2</strong></p><p><strong>example.com/foo: 1</strong></p><p><strong>limits:</strong></p><p><strong>example.com/foo: 1</strong></p><h4>Planned Improvements</h4><p>Kubernetes version 1.5 only allows resource quantities to be specified on a Container. It is planned to improve accounting for resources that are shared by all Containers in a Pod, such as <a href="https://kubernetes.io/docs/concepts/storage/volumes/#emptydir">emptyDir volumes</a>.</p><p>Kubernetes version 1.5 only supports Container requests and limits for CPU and memory. It is planned to add new resource types, including a node disk space resource, and a framework for adding custom <a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/resources.md">resource types</a>.</p><p>Kubernetes supports overcommitment of resources by supporting multiple levels of <a href="http://issue.k8s.io/168">Quality of Service</a>.</p><p>In Kubernetes version 1.5, one unit of CPU means different things on different cloud providers, and on different machine types within the same cloud providers. For example, on AWS, the capacity of a node is reported in <a href="http://aws.amazon.com/ec2/faqs/">ECUs</a>, while in GCE it is reported in logical cores. We plan to revise the definition of the cpu resource to allow for more consistency across providers and platforms.</p><h4>What&#x27;s next</h4><ul><li>Get hands-on experience <a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/">assigning Memory resources to containers and pods</a>.</li><li>Get hands-on experience <a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/">assigning CPU resources to containers and pods</a>.</li><li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#container-v1-core">Container</a></li><li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#resourcerequirements-v1-core">ResourceRequirements</a></li></ul><h3>Assigning Pods to Nodes</h3><p>You can constrain a <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/">pod</a> to only be able to run on particular <a href="https://kubernetes.io/docs/concepts/architecture/nodes/">nodes</a> or to prefer to run on particular nodes. There are several ways to do this, and they all use <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/">label selectors</a> to make the selection. Generally such constraints are unnecessary, as the scheduler will automatically do a reasonable placement (e.g. spread your pods across nodes, not place the pod on a node with insufficient free resources, etc.) but there are some circumstances where you may want more control on a node where a pod lands, e.g. to ensure that a pod ends up on a machine with an SSD attached to it, or to co-locate pods from two different services that communicate a lot into the same availability zone.</p><p>You can find all the files for these examples <a href="https://github.com/kubernetes/website/tree/master/docs/user-guide/node-selection">in our docs repo here</a>.</p><ul><li><a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector"><strong>nodeSelector</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#step-zero-prerequisites"><strong>Step Zero: Prerequisites</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#step-one-attach-label-to-the-node"><strong>Step One: Attach label to the node</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#step-two-add-a-nodeselector-field-to-your-pod-configuration"><strong>Step Two: Add a nodeSelector field to your pod configuration</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#interlude-built-in-node-labels"><strong>Interlude: built-in node labels</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity"><strong>Affinity and anti-affinity</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity-beta-feature"><strong>Node affinity (beta feature)</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity-beta-feature"><strong>Inter-pod affinity and anti-affinity (beta feature)</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#an-example-of-a-pod-that-uses-pod-affinity"><strong>An example of a pod that uses pod affinity:</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#more-practical-use-cases"><strong>More Practical Use-cases</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#always-co-located-in-the-same-node"><strong>Always co-located in the same node</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#never-co-located-in-the-same-node"><strong>Never co-located in the same node</strong></a></li></ul></li></ul></li></ul></li></ul><h4>nodeSelector</h4><p><strong>nodeSelector</strong> is the simplest form of constraint. <strong>nodeSelector</strong> is a field of PodSpec. It specifies a map of key-value pairs. For the pod to be eligible to run on a node, the node must have each of the indicated key-value pairs as labels (it can have additional labels as well). The most common usage is one key-value pair.</p><p>Let&#x27;s walk through an example of how to use <strong>nodeSelector</strong>.</p><h5><strong>Step Zero: Prerequisites</strong></h5><p>This example assumes that you have a basic understanding of Kubernetes pods and that you have <a href="https://github.com/kubernetes/kubernetes#documentation">turned up a Kubernetes cluster</a>.</p><h5><strong>Step One: Attach label to the node</strong></h5><p>Run <strong>kubectl get nodes</strong> to get the names of your cluster&#x27;s nodes. Pick out the one that you want to add a label to, and then run <strong>kubectl label nodes <code>&lt;node-name&gt; &lt;label-key&gt;=&lt;label-value&gt;</code></strong>to add a label to the node you&#x27;ve chosen. For example, if my node name is &#x27;kubernetes-foo-node-1.c.a-robinson.internal&#x27; and my desired label is &#x27;disktype=ssd&#x27;, then I can run <strong>kubectl label nodes kubernetes-foo-node-1.c.a-robinson.internal disktype=ssd</strong>.</p><p>If this fails with an &quot;invalid command&quot; error, you&#x27;re likely using an older version of kubectl that doesn&#x27;t have the <strong>label</strong> command. In that case, see the <a href="https://github.com/kubernetes/kubernetes/blob/a053dbc313572ed60d89dae9821ecab8bfd676dc/examples/node-selection/README.md">previous version</a> of this guide for instructions on how to manually set labels on a node.</p><p>You can verify that it worked by re-running <strong>kubectl get nodes --show-labels</strong> and checking that the node now has a label.</p><h5><strong>Step Two: Add a nodeSelector field to your pod configuration</strong></h5><p>Take whatever pod config file you want to run, and add a nodeSelector section to it, like this. For example, if this is my pod config:</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: nginx</strong></p><p><strong>labels:</strong></p><p><strong>env: test</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: nginx</strong></p><p><strong>image: nginx</strong></p><p>Then add a nodeSelector like so:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>pod.yaml</strong> ]<!-- -->(<a href="https://raw.githubuserconten">https://raw.githubuserconten</a>                          |
| t.com/kubernetes/website/master/docs/concepts/configuration/pod.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: nginx</strong>                                                       |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>env: test</strong>                                                         |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: nginx</strong>                                                     |
|                                                                       |
| <strong>image: nginx</strong>                                                      |
|                                                                       |
| <strong>imagePullPolicy: IfNotPresent</strong>                                     |
|                                                                       |
| <strong>nodeSelector:</strong>                                                     |
|                                                                       |
| <strong>disktype: ssd</strong>                                                     |
+-----------------------------------------------------------------------+</p><p>When you then run <strong>kubectl create -f pod.yaml</strong>, the pod will get scheduled on the node that you attached the label to! You can verify that it worked by running <strong>kubectl get pods -o wide</strong> and looking at the &quot;NODE&quot; that the pod was assigned to.</p><h4>Interlude: built-in node labels</h4><p>In addition to labels you <a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#step-one-attach-label-to-the-node">attach</a>, nodes come pre-populated with a standard set of labels. As of Kubernetes v1.4 these labels are</p><ul><li><strong>kubernetes.io/hostname</strong></li><li><strong>failure-domain.beta.kubernetes.io/zone</strong></li><li><strong>failure-domain.beta.kubernetes.io/region</strong></li><li><strong>beta.kubernetes.io/instance-type</strong></li><li><strong>beta.kubernetes.io/os</strong></li><li><strong>beta.kubernetes.io/arch</strong></li></ul><p><strong>Note:</strong> The value of these labels is cloud provider specific and is not guaranteed to be reliable. For example, the value of <strong>kubernetes.io/hostname</strong> may be the same as the Node name in some environments and a different value in other environments.</p><h4>Affinity and anti-affinity</h4><p><strong>nodeSelector</strong> provides a very simple way to constrain pods to nodes with particular labels. The affinity/anti-affinity feature, currently in beta, greatly expands the types of constraints you can express. The key enhancements are</p><ol><li>the language is more expressive (not just &quot;AND of exact match&quot;)</li><li>you can indicate that the rule is &quot;soft&quot;/&quot;preference&quot; rather than a hard requirement, so if the scheduler can&#x27;t satisfy it, the pod will still be scheduled</li><li>you can constrain against labels on other pods running on the node (or other topological domain), rather than against labels on the node itself, which allows rules about which pods can and cannot be co-located</li></ol><p>The affinity feature consists of two types of affinity, &quot;node affinity&quot; and &quot;inter-pod affinity/anti-affinity&quot;. Node affinity is like the existing <strong>nodeSelector</strong> (but with the first two benefits listed above), while inter-pod affinity/anti-affinity constrains against pod labels rather than node labels, as described in the third item listed above, in addition to having the first and second properties listed above.</p><p><strong>nodeSelector</strong> continues to work as usual, but will eventually be deprecated, as node affinity can express everything that <strong>nodeSelector</strong> can express.</p><h5><strong>Node affinity (beta feature)</strong></h5><p>Node affinity was introduced as alpha in Kubernetes 1.2. Node affinity is conceptually similar to <strong>nodeSelector</strong> -- it allows you to constrain which nodes your pod is eligible to be scheduled on, based on labels on the node.</p><p>There are currently two types of node affinity, called <strong>requiredDuringSchedulingIgnoredDuringExecution</strong> and <strong>preferredDuringSchedulingIgnoredDuringExecution</strong>. You can think of them as &quot;hard&quot; and &quot;soft&quot; respectively, in the sense that the former specifies rules that must be met for a pod to be scheduled onto a node (just like <strong>nodeSelector</strong> but using a more expressive syntax), while the latter specifies preferences that the scheduler will try to enforce but will not guarantee. The &quot;IgnoredDuringExecution&quot; part of the names means that, similar to how <strong>nodeSelector</strong> works, if labels on a node change at runtime such that the affinity rules on a pod are no longer met, the pod will still continue to run on the node. In the future we plan to offer <strong>requiredDuringSchedulingRequiredDuringExecution</strong> which will be just like <strong>requiredDuringSchedulingIgnoredDuringExecution</strong> except that it will evict pods from nodes that cease to satisfy the pods&#x27; node affinity requirements.</p><p>Thus an example of <strong>requiredDuringSchedulingIgnoredDuringExecution</strong> would be &quot;only run the pod on nodes with Intel CPUs&quot; and an example <strong>preferredDuringSchedulingIgnoredDuringExecution</strong> would be &quot;try to run this set of pods in availability zone XYZ, but if it&#x27;s not possible, then allow some to run elsewhere&quot;.</p><p>Node affinity is specified as field <strong>nodeAffinity</strong> of field <strong>affinity</strong> in the PodSpec.</p><p>Here&#x27;s an example of a pod that uses node affinity:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>pod-with-n                                                         |
| ode-affinity.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/we">https://raw.githubusercontent.com/kubernetes/we</a> |
| bsite/master/docs/concepts/configuration/pod-with-node-affinity.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: with-node-affinity</strong>                                          |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>affinity:</strong>                                                         |
|                                                                       |
| <strong>nodeAffinity:</strong>                                                     |
|                                                                       |
| <strong>requiredDuringSchedulingIgnoredDuringExecution:</strong>                   |
|                                                                       |
| <strong>nodeSelectorTerms:</strong>                                                |
|                                                                       |
| <strong>- matchExpressions:</strong>                                               |
|                                                                       |
| <strong>- key: kubernetes.io/e2e-az-name</strong>                                  |
|                                                                       |
| <strong>operator: In</strong>                                                      |
|                                                                       |
| <strong>values:</strong>                                                           |
|                                                                       |
| <strong>- e2e-az1</strong>                                                         |
|                                                                       |
| <strong>- e2e-az2</strong>                                                         |
|                                                                       |
| <strong>preferredDuringSchedulingIgnoredDuringExecution:</strong>                  |
|                                                                       |
| <strong>- weight: 1</strong>                                                       |
|                                                                       |
| <strong>preference:</strong>                                                       |
|                                                                       |
| <strong>matchExpressions:</strong>                                                 |
|                                                                       |
| <strong>- key: another-node-label-key</strong>                                     |
|                                                                       |
| <strong>operator: In</strong>                                                      |
|                                                                       |
| <strong>values:</strong>                                                           |
|                                                                       |
| <strong>- another-node-label-value</strong>                                        |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: with-node-affinity</strong>                                        |
|                                                                       |
| <strong>image: k8s.gcr.io/pause:2.0</strong>                                       |
+-----------------------------------------------------------------------+</p><p>This node affinity rule says the pod can only be placed on a node with a label whose key is <strong>kubernetes.io/e2e-az-name</strong> and whose value is either <strong>e2e-az1</strong> or <strong>e2e-az2</strong>. In addition, among nodes that meet that criteria, nodes with a label whose key is <strong>another-node-label-key</strong> and whose value is <strong>another-node-label-value</strong> should be preferred.</p><p>You can see the operator <strong>In</strong> being used in the example. The new node affinity syntax supports the following operators: <strong>In</strong>, <strong>NotIn</strong>, <strong>Exists</strong>, <strong>DoesNotExist</strong>, <strong>Gt</strong>, <strong>Lt</strong>. There is no explicit &quot;node anti-affinity&quot; concept, but <strong>NotIn</strong> and <strong>DoesNotExist</strong> give that behavior.</p><p>If you specify both <strong>nodeSelector</strong> and <strong>nodeAffinity</strong>, both must be satisfied for the pod to be scheduled onto a candidate node.</p><p>If you specify multiple <strong>nodeSelectorTerms</strong> associated with <strong>nodeAffinity</strong> types, then the pod can be scheduled onto a node <strong>if one of</strong> the <strong>nodeSelectorTerms</strong> is satisfied.</p><p>If you specify multiple <strong>matchExpressions</strong> associated with <strong>nodeSelectorTerms</strong>, then the pod can be scheduled onto a node <strong>only if all</strong> <strong>matchExpressions</strong> can be satisfied.</p><p>If you remove or change the label of the node where the pod is scheduled, the pod won&#x27;t be removed. In other words, the affinity selection works only at the time of scheduling the pod.</p><p>For more information on node affinity, see the design doc <a href="https://git.k8s.io/community/contributors/design-proposals/scheduling/nodeaffinity.md">here</a>.</p><h5><strong>Inter-pod affinity and anti-affinity (beta feature)</strong></h5><p>Inter-pod affinity and anti-affinity were introduced in Kubernetes 1.4. Inter-pod affinity and anti-affinity allow you to constrain which nodes your pod is eligible to be scheduled based on labels on pods that are already running on the node rather than based on labels on nodes. The rules are of the form &quot;this pod should (or, in the case of anti-affinity, should not) run in an X if that X is already running one or more pods that meet rule Y&quot;. Y is expressed as a LabelSelector with an associated list of namespaces (or &quot;all&quot; namespaces); unlike nodes, because pods are namespaced (and therefore the labels on pods are implicitly namespaced), a label selector over pod labels must specify which namespaces the selector should apply to. Conceptually X is a topology domain like node, rack, cloud provider zone, cloud provider region, etc. You express it using a <strong>topologyKey</strong> which is the key for the node label that the system uses to denote such a topology domain, e.g. see the label keys listed above in the section <a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#interlude-built-in-node-labels">Interlude: built-in node labels</a>.</p><p><strong>Note:</strong> Inter-pod affinity and anti-affinity require substantial amount of processing which can slow down scheduling in large clusters significantly. We do not recommend using them in clusters larger than several hundred nodes.</p><p>As with node affinity, there are currently two types of pod affinity and anti-affinity, called <strong>requiredDuringSchedulingIgnoredDuringExecution</strong> and <strong>preferredDuringSchedulingIgnoredDuringExecution</strong> which denote &quot;hard&quot; vs. &quot;soft&quot; requirements. See the description in the node affinity section earlier. An example of <strong>requiredDuringSchedulingIgnoredDuringExecution</strong> affinity would be &quot;co-locate the pods of service A and service B in the same zone, since they communicate a lot with each other&quot; and an example <strong>preferredDuringSchedulingIgnoredDuringExecution</strong> anti-affinity would be &quot;spread the pods from this service across zones&quot; (a hard requirement wouldn&#x27;t make sense, since you probably have more pods than zones).</p><p>Inter-pod affinity is specified as field <strong>podAffinity</strong> of field <strong>affinity</strong> in the PodSpec. And inter-pod anti-affinity is specified as field <strong>podAntiAffinity</strong> of field <strong>affinity</strong> in the PodSpec.</p><h6><strong>An example of a pod that uses pod affinity:</strong></h6><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>pod-with                                                           |
| -pod-affinity.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/w">https://raw.githubusercontent.com/kubernetes/w</a> |
| ebsite/master/docs/concepts/configuration/pod-with-pod-affinity.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: with-pod-affinity</strong>                                           |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>affinity:</strong>                                                         |
|                                                                       |
| <strong>podAffinity:</strong>                                                      |
|                                                                       |
| <strong>requiredDuringSchedulingIgnoredDuringExecution:</strong>                   |
|                                                                       |
| <strong>- labelSelector:</strong>                                                  |
|                                                                       |
| <strong>matchExpressions:</strong>                                                 |
|                                                                       |
| <strong>- key: security</strong>                                                   |
|                                                                       |
| <strong>operator: In</strong>                                                      |
|                                                                       |
| <strong>values:</strong>                                                           |
|                                                                       |
| <strong>- S1</strong>                                                              |
|                                                                       |
| <strong>topologyKey: failure-domain.beta.kubernetes.io/zone</strong>               |
|                                                                       |
| <strong>podAntiAffinity:</strong>                                                  |
|                                                                       |
| <strong>preferredDuringSchedulingIgnoredDuringExecution:</strong>                  |
|                                                                       |
| <strong>- weight: 100</strong>                                                     |
|                                                                       |
| <strong>podAffinityTerm:</strong>                                                  |
|                                                                       |
| <strong>labelSelector:</strong>                                                    |
|                                                                       |
| <strong>matchExpressions:</strong>                                                 |
|                                                                       |
| <strong>- key: security</strong>                                                   |
|                                                                       |
| <strong>operator: In</strong>                                                      |
|                                                                       |
| <strong>values:</strong>                                                           |
|                                                                       |
| <strong>- S2</strong>                                                              |
|                                                                       |
| <strong>topologyKey: kubernetes.io/hostname</strong>                               |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: with-pod-affinity</strong>                                         |
|                                                                       |
| <strong>image: k8s.gcr.io/pause:2.0</strong>                                       |
+-----------------------------------------------------------------------+</p><p>The affinity on this pod defines one pod affinity rule and one pod anti-affinity rule. In this example, the<strong>podAffinity</strong> is <strong>requiredDuringSchedulingIgnoredDuringExecution</strong> while the <strong>podAntiAffinity</strong> is <strong>preferredDuringSchedulingIgnoredDuringExecution</strong>. The pod affinity rule says that the pod can be scheduled onto a node only if that node is in the same zone as at least one already-running pod that has a label with key &quot;security&quot; and value &quot;S1&quot;. (More precisely, the pod is eligible to run on node N if node N has a label with key <strong>failure-domain.beta.kubernetes.io/zone</strong> and some value V such that there is at least one node in the cluster with key <strong>failure-domain.beta.kubernetes.io/zone</strong> and value V that is running a pod that has a label with key &quot;security&quot; and value &quot;S1&quot;.) The pod anti-affinity rule says that the pod prefers not to be scheduled onto a node if that node is already running a pod with label having key &quot;security&quot; and value &quot;S2&quot;. (If the <strong>topologyKey</strong> were <strong>failure-domain.beta.kubernetes.io/zone</strong> then it would mean that the pod cannot be scheduled onto a node if that node is in the same zone as a pod with label having key &quot;security&quot; and value &quot;S2&quot;.) See the <a href="https://git.k8s.io/community/contributors/design-proposals/scheduling/podaffinity.md">design doc</a>. For many more examples of pod affinity and anti-affinity, both the <strong>requiredDuringSchedulingIgnoredDuringExecution</strong> flavor and the <strong>preferredDuringSchedulingIgnoredDuringExecution</strong> flavor.</p><p>The legal operators for pod affinity and anti-affinity are <strong>In</strong>, <strong>NotIn</strong>, <strong>Exists</strong>, <strong>DoesNotExist</strong>.</p><p>In principle, the <strong>topologyKey</strong> can be any legal label-key. However, for performance and security reasons, there are some constraints on topologyKey:</p><ol><li>For affinity and for <strong>requiredDuringSchedulingIgnoredDuringExecution</strong> pod anti-affinity, empty <strong>topologyKey</strong> is not allowed.</li><li>For <strong>requiredDuringSchedulingIgnoredDuringExecution</strong> pod anti-affinity, the admission controller <strong>LimitPodHardAntiAffinityTopology</strong> was introduced to limit <strong>topologyKey</strong> to <strong>kubernetes.io/hostname</strong>. If you want to make it available for custom topologies, you may modify the admission controller, or simply disable it.</li><li>For <strong>preferredDuringSchedulingIgnoredDuringExecution</strong> pod anti-affinity, empty <strong>topologyKey</strong> is interpreted as &quot;all topologies&quot; (&quot;all topologies&quot; here is now limited to the combination of <strong>kubernetes.io/hostname</strong>, <strong>failure-domain.beta.kubernetes.io/zone</strong> and <strong>failure-domain.beta.kubernetes.io/region</strong>).</li><li>Except for the above cases, the <strong>topologyKey</strong> can be any legal label-key.</li></ol><p>In addition to <strong>labelSelector</strong> and <strong>topologyKey</strong>, you can optionally specify a list <strong>namespaces</strong> of namespaces which the <strong>labelSelector</strong> should match against (this goes at the same level of the definition as <strong>labelSelector</strong> and <strong>topologyKey</strong>). If omitted, it defaults to the namespace of the pod where the affinity/anti-affinity definition appears. If defined but empty, it means &quot;all namespaces&quot;.</p><p>All <strong>matchExpressions</strong> associated with <strong>requiredDuringSchedulingIgnoredDuringExecution</strong>affinity and anti-affinity must be satisfied for the pod to be scheduled onto a node.</p><h6><strong>More Practical Use-cases</strong></h6><p>Interpod Affinity and AntiAffinity can be even more useful when they are used with higher level collections such as ReplicaSets, Statefulsets, Deployments, etc. One can easily configure that a set of workloads should be co-located in the same defined topology, eg., the same node.</p><p><strong>Always co-located in the same node</strong></p><p>In a three node cluster, a web application has in-memory cache such as redis. We want the web-servers to be co-located with the cache as much as possible. Here is the yaml snippet of a simple redis deployment with three replicas and selector label <strong>app=store</strong>. The deployment has <strong>PodAntiAffinity</strong> configured to ensure the scheduler does not co-locate replicas on a single node.</p><p><strong>apiVersion: apps/v1</strong></p><p><strong>kind: Deployment</strong></p><p><strong>metadata:</strong></p><p><strong>name: redis-cache</strong></p><p><strong>spec:</strong></p><p><strong>selector:</strong></p><p><strong>matchLabels:</strong></p><p><strong>app: store</strong></p><p><strong>replicas: 3</strong></p><p><strong>template:</strong></p><p><strong>metadata:</strong></p><p><strong>labels:</strong></p><p><strong>app: store</strong></p><p><strong>spec:</strong></p><p><strong>affinity:</strong></p><p><strong>podAntiAffinity:</strong></p><p><strong>requiredDuringSchedulingIgnoredDuringExecution:</strong></p><p><strong>- labelSelector:</strong></p><p><strong>matchExpressions:</strong></p><p><strong>- key: app</strong></p><p><strong>operator: In</strong></p><p><strong>values:</strong></p><p><strong>- store</strong></p><p><strong>topologyKey: &quot;kubernetes.io/hostname&quot;</strong></p><p><strong>containers:</strong></p><p><strong>- name: redis-server</strong></p><p><strong>image: redis:3.2-alpine</strong></p><p>The below yaml snippet of the webserver deployment has <strong>podAntiAffinity</strong> and <strong>podAffinity</strong>configured. This informs the scheduler that all its replicas are to be co-located with pods that have selector label <strong>app=store</strong>. This will also ensure that each web-server replica does not co-locate on a single node.</p><p><strong>apiVersion: apps/v1</strong></p><p><strong>kind: Deployment</strong></p><p><strong>metadata:</strong></p><p><strong>name: web-server</strong></p><p><strong>spec:</strong></p><p><strong>selector:</strong></p><p><strong>matchLabels:</strong></p><p><strong>app: web-store</strong></p><p><strong>replicas: 3</strong></p><p><strong>template:</strong></p><p><strong>metadata:</strong></p><p><strong>labels:</strong></p><p><strong>app: web-store</strong></p><p><strong>spec:</strong></p><p><strong>affinity:</strong></p><p><strong>podAntiAffinity:</strong></p><p><strong>requiredDuringSchedulingIgnoredDuringExecution:</strong></p><p><strong>- labelSelector:</strong></p><p><strong>matchExpressions:</strong></p><p><strong>- key: app</strong></p><p><strong>operator: In</strong></p><p><strong>values:</strong></p><p><strong>- web-store</strong></p><p><strong>topologyKey: &quot;kubernetes.io/hostname&quot;</strong></p><p><strong>podAffinity:</strong></p><p><strong>requiredDuringSchedulingIgnoredDuringExecution:</strong></p><p><strong>- labelSelector:</strong></p><p><strong>matchExpressions:</strong></p><p><strong>- key: app</strong></p><p><strong>operator: In</strong></p><p><strong>values:</strong></p><p><strong>- store</strong></p><p><strong>topologyKey: &quot;kubernetes.io/hostname&quot;</strong></p><p><strong>containers:</strong></p><p><strong>- name: web-app</strong></p><p><strong>image: nginx:1.12-alpine</strong></p><p>If we create the above two deployments, our three node cluster should look like below.</p><p>  node-1        node-2        node-3</p><hr/><p>  webserver-1   webserver-2   webserver-3
cache-1       cache-2       cache-3</p><p>As you can see, all the 3 replicas of the <strong>web-server</strong> are automatically co-located with the cache as expected.</p><p><strong>$ kubectl get pods -o wide</strong></p><p><strong>NAME READY STATUS RESTARTS AGE IP NODE</strong></p><p><strong>redis-cache-1450370735-6dzlj 1/1 Running 0 8m 10.192.4.2 kube-node-3</strong></p><p><strong>redis-cache-1450370735-j2j96 1/1 Running 0 8m 10.192.2.2 kube-node-1</strong></p><p><strong>redis-cache-1450370735-z73mh 1/1 Running 0 8m 10.192.3.1 kube-node-2</strong></p><p><strong>web-server-1287567482-5d4dz 1/1 Running 0 7m 10.192.2.3 kube-node-1</strong></p><p><strong>web-server-1287567482-6f7v5 1/1 Running 0 7m 10.192.4.3 kube-node-3</strong></p><p><strong>web-server-1287567482-s330j 1/1 Running 0 7m 10.192.3.2 kube-node-2</strong></p><p>Best practice is to configure these highly available stateful workloads such as redis with AntiAffinity rules for more guaranteed spreading.</p><p><strong>Never co-located in the same node</strong></p><p>Highly Available database statefulset has one master and three replicas, one may prefer none of the database instances to be co-located in the same node.</p><p>  node-1      node-2         node-3         node-4</p><hr/><p>  DB-MASTER   DB-REPLICA-1   DB-REPLICA-2   DB-REPLICA-3</p><p><a href="https://kubernetes.io/docs/tutorials/stateful-application/zookeeper/#tolerating-node-failure">Here</a> is an example of Zookeeper statefulset configured with anti-affinity for high availability.</p><p>For more information on inter-pod affinity/anti-affinity, see the design doc <a href="https://git.k8s.io/community/contributors/design-proposals/scheduling/podaffinity.md">here</a>.</p><p>You may want to check <a href="https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/">Taints</a> as well, which allow a node to repel a set of pods.</p><h3>Taints and Tolerations</h3><p>Node affinity, described <a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity-beta-feature">here</a>, is a property of pods that attracts them to a set of nodes (either as a preference or a hard requirement). Taints are the opposite -- they allow a node to repel a set of pods.</p><p>Taints and tolerations work together to ensure that pods are not scheduled onto inappropriate nodes. One or more taints are applied to a node; this marks that the node should not accept any pods that do not tolerate the taints. Tolerations are applied to pods, and allow (but do not require) the pods to schedule onto nodes with matching taints.</p><ul><li><a href="https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/#concepts"><strong>Concepts</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/#example-use-cases"><strong>Example Use Cases</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/#taint-based-evictions"><strong>Taint based Evictions</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/#taint-nodes-by-condition"><strong>Taint Nodes by Condition</strong></a></li></ul><h4>Concepts</h4><p>You add a taint to a node using <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#taint">kubectl taint</a>. For example,</p><p><strong>kubectl taint nodes node1 key=value:NoSchedule</strong></p><p>places a taint on node <strong>node1</strong>. The taint has key <strong>key</strong>, value <strong>value</strong>, and taint effect <strong>NoSchedule</strong>. This means that no pod will be able to schedule onto <strong>node1</strong> unless it has a matching toleration.</p><p>To remove the taint added by the command above, you can run:</p><p><strong>kubectl taint nodes node1 key:NoSchedule-</strong></p><p>You specify a toleration for a pod in the PodSpec. Both of the following tolerations &quot;match&quot; the taint created by the <strong>kubectl taint</strong> line above, and thus a pod with either toleration would be able to schedule onto <strong>node1</strong>:</p><p><strong>tolerations:</strong></p><p><strong>- key: &quot;key&quot;</strong></p><p><strong>operator: &quot;Equal&quot;</strong></p><p><strong>value: &quot;value&quot;</strong></p><p><strong>effect: &quot;NoSchedule&quot;</strong></p><p><strong>tolerations:</strong></p><p><strong>- key: &quot;key&quot;</strong></p><p><strong>operator: &quot;Exists&quot;</strong></p><p><strong>effect: &quot;NoSchedule&quot;</strong></p><p>A toleration &quot;matches&quot; a taint if the keys are the same and the effects are the same, and:</p><ul><li>the <strong>operator</strong> is <strong>Exists</strong> (in which case no <strong>value</strong> should be specified), or</li><li>the <strong>operator</strong> is <strong>Equal</strong> and the <strong>value</strong>s are equal</li></ul><p><strong>Operator</strong> defaults to <strong>Equal</strong> if not specified.</p><p><strong>NOTE:</strong> There are two special cases:</p><ul><li>An empty <strong>key</strong> with operator <strong>Exists</strong> matches all keys, values and effects which means this will tolerate everything.</li></ul><p><strong>tolerations:</strong></p><p><strong>- operator: &quot;Exists&quot;</strong></p><ul><li>An empty <strong>effect</strong> matches all effects with key <strong>key</strong>.</li></ul><p><strong>tolerations:</strong></p><p><strong>- key: &quot;key&quot;</strong></p><p><strong>operator: &quot;Exists&quot;</strong></p><p>The above example used <strong>effect</strong> of <strong>NoSchedule</strong>. Alternatively, you can use <strong>effect</strong> of <strong>PreferNoSchedule</strong>. This is a &quot;preference&quot; or &quot;soft&quot; version of <strong>NoSchedule</strong> -- the system will try to avoid placing a pod that does not tolerate the taint on the node, but it is not required. The third kind of <strong>effect</strong> is <strong>NoExecute</strong>, described later.</p><p>You can put multiple taints on the same node and multiple tolerations on the same pod. The way Kubernetes processes multiple taints and tolerations is like a filter: start with all of a node&#x27;s taints, then ignore the ones for which the pod has a matching toleration; the remaining un-ignored taints have the indicated effects on the pod. In particular,</p><ul><li>if there is at least one un-ignored taint with effect <strong>NoSchedule</strong> then Kubernetes will not schedule the pod onto that node</li><li>if there is no un-ignored taint with effect <strong>NoSchedule</strong> but there is at least one un-ignored taint with effect <strong>PreferNoSchedule</strong> then Kubernetes will try to not schedule the pod onto the node</li><li>if there is at least one un-ignored taint with effect <strong>NoExecute</strong> then the pod will be evicted from the node (if it is already running on the node), and will not be scheduled onto the node (if it is not yet running on the node).</li></ul><p>For example, imagine you taint a node like this</p><p><strong>kubectl taint nodes node1 key1=value1:NoSchedule</strong></p><p><strong>kubectl taint nodes node1 key1=value1:NoExecute</strong></p><p><strong>kubectl taint nodes node1 key2=value2:NoSchedule</strong></p><p>And a pod has two tolerations:</p><p><strong>tolerations:</strong></p><p><strong>- key: &quot;key1&quot;</strong></p><p><strong>operator: &quot;Equal&quot;</strong></p><p><strong>value: &quot;value1&quot;</strong></p><p><strong>effect: &quot;NoSchedule&quot;</strong></p><p><strong>- key: &quot;key1&quot;</strong></p><p><strong>operator: &quot;Equal&quot;</strong></p><p><strong>value: &quot;value1&quot;</strong></p><p><strong>effect: &quot;NoExecute&quot;</strong></p><p>In this case, the pod will not be able to schedule onto the node, because there is no toleration matching the third taint. But it will be able to continue running if it is already running on the node when the taint is added, because the third taint is the only one of the three that is not tolerated by the pod.</p><p>Normally, if a taint with effect <strong>NoExecute</strong> is added to a node, then any pods that do not tolerate the taint will be evicted immediately, and any pods that do tolerate the taint will never be evicted. However, a toleration with <strong>NoExecute</strong> effect can specify an optional <strong>tolerationSeconds</strong> field that dictates how long the pod will stay bound to the node after the taint is added. For example,</p><p><strong>tolerations:</strong></p><p><strong>- key: &quot;key1&quot;</strong></p><p><strong>operator: &quot;Equal&quot;</strong></p><p><strong>value: &quot;value1&quot;</strong></p><p><strong>effect: &quot;NoExecute&quot;</strong></p><p><strong>tolerationSeconds: 3600</strong></p><p>means that if this pod is running and a matching taint is added to the node, then the pod will stay bound to the node for 3600 seconds, and then be evicted. If the taint is removed before that time, the pod will not be evicted.</p><h4>Example Use Cases</h4><p>Taints and tolerations are a flexible way to steer pods away from nodes or evict pods that shouldn&#x27;t be running. A few of the use cases are</p><ul><li><strong>Dedicated Nodes</strong>: If you want to dedicate a set of nodes for exclusive use by a particular set of users, you can add a taint to those nodes (say, <strong>kubectl taint nodes nodename dedicated=groupName:NoSchedule</strong>) and then add a corresponding toleration to their pods (this would be done most easily by writing a custom<a href="https://kubernetes.io/docs/admin/admission-controllers/">admission controller</a>). The pods with the tolerations will then be allowed to use the tainted (dedicated) nodes as well as any other nodes in the cluster. If you want to dedicate the nodes to them and ensure they only use the dedicated nodes, then you should additionally add a label similar to the taint to the same set of nodes (e.g. <strong>dedicated=groupName</strong>), and the admission controller should additionally add a node affinity to require that the pods can only schedule onto nodes labeled with <strong>dedicated=groupName</strong>.</li><li><strong>Nodes with Special Hardware</strong>: In a cluster where a small subset of nodes have specialized hardware (for example GPUs), it is desirable to keep pods that don&#x27;t need the specialized hardware off of those nodes, thus leaving room for later-arriving pods that do need the specialized hardware. This can be done by tainting the nodes that have the specialized hardware (e.g. <strong>kubectl taint nodes nodename special=true:NoSchedule</strong> or <strong>kubectl taint nodes nodename special=true:PreferNoSchedule</strong>) and adding a corresponding toleration to pods that use the special hardware. As in the dedicated nodes use case, it is probably easiest to apply the tolerations using a custom <a href="https://kubernetes.io/docs/admin/admission-controllers/">admission controller</a>). For example, the admission controller could use some characteristic(s) of the pod to determine that the pod should be allowed to use the special nodes and hence the admission controller should add the toleration. To ensure that the pods that need the special hardware only schedule onto the nodes that have the special hardware, you will need some additional mechanism, e.g. you could represent the special resource using <a href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#extended-resources">extended resources</a> and request it as a resource in the PodSpec, or you could label the nodes that have the special hardware and use node affinity on the pods that need the hardware.</li><li><strong>Taint based Evictions (alpha feature)</strong>: A per-pod-configurable eviction behavior when there are node problems, which is described in the next section.</li></ul><h4>Taint based Evictions</h4><p>Earlier we mentioned the <strong>NoExecute</strong> taint effect, which affects pods that are already running on the node as follows</p><ul><li>pods that do not tolerate the taint are evicted immediately</li><li>pods that tolerate the taint without specifying <strong>tolerationSeconds</strong> in their toleration specification remain bound forever</li><li>pods that tolerate the taint with a specified <strong>tolerationSeconds</strong> remain bound for the specified amount of time</li></ul><p>In addition, Kubernetes 1.6 has alpha support for representing node problems. In other words, the node controller automatically taints a node when certain condition is true. The built-in taints currently include:</p><ul><li><strong>node.kubernetes.io/not-ready</strong>: Node is not ready. This corresponds to the NodeCondition <strong>Ready</strong> being &quot;<strong>False</strong>&quot;.</li><li><strong>node.alpha.kubernetes.io/unreachable</strong>: Node is unreachable from the node controller. This corresponds to the NodeCondition <strong>Ready</strong> being &quot;<strong>Unknown</strong>&quot;.</li><li><strong>node.kubernetes.io/out-of-disk</strong>: Node becomes out of disk.</li><li><strong>node.kubernetes.io/memory-pressure</strong>: Node has memory pressure.</li><li><strong>node.kubernetes.io/disk-pressure</strong>: Node has disk pressure.</li><li><strong>node.kubernetes.io/network-unavailable</strong>: Node&#x27;s network is unavailable.</li><li><strong>node.cloudprovider.kubernetes.io/uninitialized</strong>: When kubelet is started with &quot;external&quot; cloud provider, it sets this taint on a node to mark it as unusable. When a controller from the cloud-controller-manager initializes this node, kubelet removes this taint.</li></ul><p>When the <strong>TaintBasedEvictions</strong> alpha feature is enabled (you can do this by including <strong>TaintBasedEvictions=true</strong> in <strong>--feature-gates</strong> for Kubernetes controller manager, such as <strong>--feature-gates=FooBar=true,TaintBasedEvictions=true</strong>), the taints are automatically added by the NodeController (or kubelet) and the normal logic for evicting pods from nodes based on the Ready NodeCondition is disabled. (Note: To maintain the existing <a href="https://kubernetes.io/docs/concepts/architecture/nodes/">rate limiting</a> behavior of pod evictions due to node problems, the system actually adds the taints in a rate-limited way. This prevents massive pod evictions in scenarios such as the master becoming partitioned from the nodes.) This alpha feature, in combination with <strong>tolerationSeconds</strong>, allows a pod to specify how long it should stay bound to a node that has one or both of these problems.</p><p>For example, an application with a lot of local state might want to stay bound to node for a long time in the event of network partition, in the hope that the partition will recover and thus the pod eviction can be avoided. The toleration the pod would use in that case would look like</p><p><strong>tolerations:</strong></p><p><strong>- key: &quot;node.alpha.kubernetes.io/unreachable&quot;</strong></p><p><strong>operator: &quot;Exists&quot;</strong></p><p><strong>effect: &quot;NoExecute&quot;</strong></p><p><strong>tolerationSeconds: 6000</strong></p><p>Note that Kubernetes automatically adds a toleration for <strong>node.kubernetes.io/not-ready</strong> with <strong>tolerationSeconds=300</strong> unless the pod configuration provided by the user already has a toleration for <strong>node.kubernetes.io/not-ready</strong>. Likewise it adds a toleration for <strong>node.alpha.kubernetes.io/unreachable</strong> with <strong>tolerationSeconds=300</strong> unless the pod configuration provided by the user already has a toleration for <strong>node.alpha.kubernetes.io/unreachable</strong>.</p><p>These automatically-added tolerations ensure that the default pod behavior of remaining bound for 5 minutes after one of these problems is detected is maintained. The two default tolerations are added by the <a href="https://git.k8s.io/kubernetes/plugin/pkg/admission/defaulttolerationseconds">DefaultTolerationSeconds admission controller</a>.</p><p><a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/">DaemonSet</a> pods are created with <strong>NoExecute</strong> tolerations for the following taints with no <strong>tolerationSeconds</strong>:</p><ul><li><strong>node.alpha.kubernetes.io/unreachable</strong></li><li><strong>node.kubernetes.io/not-ready</strong></li></ul><p>This ensures that DaemonSet pods are never evicted due to these problems, which matches the behavior when this feature is disabled.</p><h4>Taint Nodes by Condition</h4><p>Version 1.8 introduces an alpha feature that causes the node controller to create taints corresponding to Node conditions. When this feature is enabled (you can do this by including <strong>TaintNodesByCondition=true</strong> in the <strong>--feature-gates</strong> command line flag to the scheduler, such as <strong>--feature-gates=FooBar=true,TaintNodesByCondition=true</strong>), the scheduler does not check Node conditions; instead the scheduler checks taints. This assures that Node conditions don&#x27;t affect what&#x27;s scheduled onto the Node. The user can choose to ignore some of the Node&#x27;s problems (represented as Node conditions) by adding appropriate Pod tolerations.</p><p>To make sure that turning on this feature doesn&#x27;t break DaemonSets, starting in version 1.8, the DaemonSet controller automatically adds the following <strong>NoSchedule</strong> tolerations to all daemons:</p><ul><li><strong>node.kubernetes.io/memory-pressure</strong></li><li><strong>node.kubernetes.io/disk-pressure</strong></li><li><strong>node.kubernetes.io/out-of-disk</strong> (only for critical pods)</li></ul><p>The above settings ensure backward compatibility, but we understand they may not fit all user&#x27;s needs, which is why cluster admin may choose to add arbitrary tolerations to DaemonSets.</p><h3>Secrets</h3><p>Objects of type <strong>secret</strong> are intended to hold sensitive information, such as passwords, OAuth tokens, and ssh keys. Putting this information in a <strong>secret</strong> is safer and more flexible than putting it verbatim in a <strong>pod</strong> definition or in a docker image. See <a href="https://git.k8s.io/community/contributors/design-proposals/auth/secrets.md">Secrets design document</a> for more information.</p><ul><li><a href="https://kubernetes.io/docs/concepts/configuration/secret/#overview-of-secrets"><strong>Overview of Secrets</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/configuration/secret/#built-in-secrets"><strong>Built-in Secrets</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/configuration/secret/#service-accounts-automatically-create-and-attach-secrets-with-api-credentials"><strong>Service Accounts Automatically Create and Attach Secrets with API Credentials</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/configuration/secret/#creating-your-own-secrets"><strong>Creating your own Secrets</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-using-kubectl-create-secret"><strong>Creating a Secret Using kubectl create secret</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually"><strong>Creating a Secret Manually</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/secret/#decoding-a-secret"><strong>Decoding a Secret</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets"><strong>Using Secrets</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets-as-files-from-a-pod"><strong>Using Secrets as Files from a Pod</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets-as-environment-variables"><strong>Using Secrets as Environment Variables</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/secret/#using-imagepullsecrets"><strong>Using imagePullSecrets</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/configuration/secret/#arranging-for-imagepullsecrets-to-be-automatically-attached"><strong>Arranging for imagePullSecrets to be Automatically Attached</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/secret/#automatic-mounting-of-manually-created-secrets"><strong>Automatic Mounting of Manually Created Secrets</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/configuration/secret/#details"><strong>Details</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/configuration/secret/#restrictions"><strong>Restrictions</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/secret/#secret-and-pod-lifetime-interaction"><strong>Secret and Pod Lifetime interaction</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/configuration/secret/#use-cases"><strong>Use cases</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/configuration/secret/#use-case-pod-with-ssh-keys"><strong>Use-Case: Pod with ssh keys</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/secret/#use-case-pods-with-prod--test-credentials"><strong>Use-Case: Pods with prod / test credentials</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/secret/#use-case-dotfiles-in-secret-volume"><strong>Use-case: Dotfiles in secret volume</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/secret/#use-case-secret-visible-to-one-container-in-a-pod"><strong>Use-case: Secret visible to one container in a pod</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/configuration/secret/#best-practices"><strong>Best practices</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/configuration/secret/#clients-that-use-the-secrets-api"><strong>Clients that use the secrets API</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/configuration/secret/#security-properties"><strong>Security Properties</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/configuration/secret/#protections"><strong>Protections</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/secret/#risks"><strong>Risks</strong></a></li></ul></li></ul><h4>Overview of Secrets</h4><p>A Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key. Such information might otherwise be put in a Pod specification or in an image; putting it in a Secret object allows for more control over how it is used, and reduces the risk of accidental exposure.</p><p>Users can create secrets, and the system also creates some secrets.</p><p>To use a secret, a pod needs to reference the secret. A secret can be used with a pod in two ways: as files in a <a href="https://kubernetes.io/docs/concepts/storage/volumes/">volume</a> mounted on one or more of its containers, or used by kubelet when pulling images for the pod.</p><h5><strong>Built-in Secrets</strong></h5><h6><strong>Service Accounts Automatically Create and Attach Secrets with API Credentials</strong></h6><p>Kubernetes automatically creates secrets which contain credentials for accessing the API and it automatically modifies your pods to use this type of secret.</p><p>The automatic creation and use of API credentials can be disabled or overridden if desired. However, if all you need to do is securely access the apiserver, this is the recommended workflow.</p><p>See the <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/">Service Account</a> documentation for more information on how Service Accounts work.</p><h5><strong>Creating your own Secrets</strong></h5><h6><strong>Creating a Secret Using kubectl create secret</strong></h6><p>Say that some pods need to access a database. The username and password that the pods should use is in the files <strong>./username.txt</strong> and <strong>./password.txt</strong> on your local machine.</p><p><strong><em># Create files needed for rest of example.</em></strong></p><p><strong>$ echo -n &quot;admin&quot; &gt; ./username.txt</strong></p><p><strong>$ echo -n &quot;1f2d1e2e67df&quot; &gt; ./password.txt</strong></p><p>The <strong>kubectl create secret</strong> command packages these files into a Secret and creates the object on the Apiserver.</p><p><strong>$ kubectl create secret generic db-user-pass --from-file=./username.txt --from-file=./password.txt</strong></p><p><strong>secret &quot;db-user-pass&quot; created</strong></p><p>You can check that the secret was created like this:</p><p><strong>$ kubectl get secrets</strong></p><p><strong>NAME TYPE DATA AGE</strong></p><p><strong>db-user-pass Opaque 2 51s</strong></p><p><strong>$ kubectl describe secrets/db-user-pass</strong></p><p><strong>Name: db-user-pass</strong></p><p><strong>Namespace: default</strong></p><p><strong>Labels: <code>&lt;none&gt;</code></strong></p><p><strong>Annotations: <code>&lt;none&gt;</code></strong></p><p><strong>Type: Opaque</strong></p><p><strong>Data</strong></p><p><strong>====</strong></p><p><strong>password.txt: 12 bytes</strong></p><p><strong>username.txt: 5 bytes</strong></p><p>Note that neither <strong>get</strong> nor <strong>describe</strong> shows the contents of the file by default. This is to protect the secret from being exposed accidentally to someone looking or from being stored in a terminal log.</p><p>See <a href="https://kubernetes.io/docs/concepts/configuration/secret/#decoding-a-secret">decoding a secret</a> for how to see the contents.</p><h6><strong>Creating a Secret Manually</strong></h6><p>You can also create a secret object in a file first, in json or yaml format, and then create that object.</p><p>Each item must be base64 encoded:</p><p><strong>$ echo -n &quot;admin&quot; | base64</strong></p><p><strong>YWRtaW4=</strong></p><p><strong>$ echo -n &quot;1f2d1e2e67df&quot; | base64</strong></p><p><strong>MWYyZDFlMmU2N2Rm</strong></p><p>Now write a secret object that looks like this:</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Secret</strong></p><p><strong>metadata:</strong></p><p><strong>name: mysecret</strong></p><p><strong>type: Opaque</strong></p><p><strong>data:</strong></p><p><strong>username: YWRtaW4=</strong></p><p><strong>password: MWYyZDFlMmU2N2Rm</strong></p><p>The data field is a map. Its keys must consist of alphanumeric characters, &#x27;-&#x27;, &#x27;<!-- -->_<!-- -->&#x27; or &#x27;.&#x27;. The values are arbitrary data, encoded using base64.</p><p>Create the secret using <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#create"><strong>kubectl create</strong></a>:</p><p><strong>$ kubectl create -f ./secret.yaml</strong></p><p><strong>secret &quot;mysecret&quot; created</strong></p><p><strong>Encoding Note:</strong> The serialized JSON and YAML values of secret data are encoded as base64 strings. Newlines are not valid within these strings and must be omitted. When using the <strong>base64</strong> utility on Darwin/OS X users should avoid using the <strong>-b</strong> option to split long lines. Conversely Linux users should add the option <strong>-w 0</strong> to <strong>base64</strong> commands or the pipeline <strong>base64 | tr -d \&#x27;<!-- -->\<!-- -->n\&#x27;</strong> if <strong>-w</strong>option is not available.</p><h6><strong>Decoding a Secret</strong></h6><p>Secrets can be retrieved via the <strong>kubectl get secret</strong> command. For example, to retrieve the secret created in the previous section:</p><p><strong>$ kubectl get secret mysecret -o yaml</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>data:</strong></p><p><strong>username: YWRtaW4=</strong></p><p><strong>password: MWYyZDFlMmU2N2Rm</strong></p><p><strong>kind: Secret</strong></p><p><strong>metadata:</strong></p><p><strong>creationTimestamp: 2016-01-22T18:41:56Z</strong></p><p><strong>name: mysecret</strong></p><p><strong>namespace: default</strong></p><p><strong>resourceVersion: &quot;164619&quot;</strong></p><p><strong>selfLink: /api/v1/namespaces/default/secrets/mysecret</strong></p><p><strong>uid: cfee02d6-c137-11e5-8d73-42010af00002</strong></p><p><strong>type: Opaque</strong></p><p>Decode the password field:</p><p><strong>$ echo &quot;MWYyZDFlMmU2N2Rm&quot; | base64 --decode</strong></p><p><strong>1f2d1e2e67df</strong></p><h5><strong>Using Secrets</strong></h5><p>Secrets can be mounted as data volumes or be exposed as environment variables to be used by a container in a pod. They can also be used by other parts of the system, without being directly exposed to the pod. For example, they can hold credentials that other parts of the system should use to interact with external systems on your behalf.</p><h6><strong>Using Secrets as Files from a Pod</strong></h6><p>To consume a Secret in a volume in a Pod:</p><ol><li>Create a secret or use an existing one. Multiple pods can reference the same secret.</li><li>Modify your Pod definition to add a volume under <strong>spec.volumes[]</strong>. Name the volume anything, and have a <strong>spec.volumes[].secret.secretName</strong> field equal to the name of the secret object.</li><li>Add a <strong>spec.containers[].volumeMounts[]</strong> to each container that needs the secret. Specify <strong>spec.containers[].volumeMounts[].readOnly = true</strong> and <strong>spec.containers[].volumeMounts[].mountPath</strong> to an unused directory name where you would like the secrets to appear.</li><li>Modify your image and/or command line so that the program looks for files in that directory. Each key in the secret <strong>data</strong> map becomes the filename under <strong>mountPath</strong>.</li></ol><p>This is an example of a pod that mounts a secret in a volume:</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: mypod</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: mypod</strong></p><p><strong>image: redis</strong></p><p><strong>volumeMounts:</strong></p><p><strong>- name: foo</strong></p><p><strong>mountPath: &quot;/etc/foo&quot;</strong></p><p><strong>readOnly: true</strong></p><p><strong>volumes:</strong></p><p><strong>- name: foo</strong></p><p><strong>secret:</strong></p><p><strong>secretName: mysecret</strong></p><p>Each secret you want to use needs to be referred to in <strong>spec.volumes</strong>.</p><p>If there are multiple containers in the pod, then each container needs its own <strong>volumeMounts</strong> block, but only one <strong>spec.volumes</strong> is needed per secret.</p><p>You can package many files into one secret, or use many secrets, whichever is convenient.</p><p><strong>Projection of secret keys to specific paths</strong></p><p>We can also control the paths within the volume where Secret keys are projected. You can use <strong>spec.volumes[].secret.items</strong> field to change target path of each key:</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: mypod</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: mypod</strong></p><p><strong>image: redis</strong></p><p><strong>volumeMounts:</strong></p><p><strong>- name: foo</strong></p><p><strong>mountPath: &quot;/etc/foo&quot;</strong></p><p><strong>readOnly: true</strong></p><p><strong>volumes:</strong></p><p><strong>- name: foo</strong></p><p><strong>secret:</strong></p><p><strong>secretName: mysecret</strong></p><p><strong>items:</strong></p><p><strong>- key: username</strong></p><p><strong>path: my-group/my-username</strong></p><p>What will happen:</p><ul><li><strong>username</strong> secret is stored under <strong>/etc/foo/my-group/my-username</strong> file instead of <strong>/etc/foo/username</strong>.</li><li><strong>password</strong> secret is not projected</li></ul><p>If <strong>spec.volumes[].secret.items</strong> is used, only keys specified in <strong>items</strong> are projected. To consume all keys from the secret, all of them must be listed in the <strong>items</strong> field. All listed keys must exist in the corresponding secret. Otherwise, the volume is not created.</p><p><strong>Secret files permissions</strong></p><p>You can also specify the permission mode bits files part of a secret will have. If you don&#x27;t specify any, <strong>0644</strong> is used by default. You can specify a default mode for the whole secret volume and override per key if needed.</p><p>For example, you can specify a default mode like this:</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: mypod</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: mypod</strong></p><p><strong>image: redis</strong></p><p><strong>volumeMounts:</strong></p><p><strong>- name: foo</strong></p><p><strong>mountPath: &quot;/etc/foo&quot;</strong></p><p><strong>volumes:</strong></p><p><strong>- name: foo</strong></p><p><strong>secret:</strong></p><p><strong>secretName: mysecret</strong></p><p><strong>defaultMode: 256</strong></p><p>Then, the secret will be mounted on <strong>/etc/foo</strong> and all the files created by the secret volume mount will have permission <strong>0400</strong>.</p><p>Note that the JSON spec doesn&#x27;t support octal notation, so use the value 256 for 0400 permissions. If you use yaml instead of json for the pod, you can use octal notation to specify permissions in a more natural way.</p><p>You can also use mapping, as in the previous example, and specify different permission for different files like this:</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: mypod</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: mypod</strong></p><p><strong>image: redis</strong></p><p><strong>volumeMounts:</strong></p><p><strong>- name: foo</strong></p><p><strong>mountPath: &quot;/etc/foo&quot;</strong></p><p><strong>volumes:</strong></p><p><strong>- name: foo</strong></p><p><strong>secret:</strong></p><p><strong>secretName: mysecret</strong></p><p><strong>items:</strong></p><p><strong>- key: username</strong></p><p><strong>path: my-group/my-username</strong></p><p><strong>mode: 511</strong></p><p>In this case, the file resulting in <strong>/etc/foo/my-group/my-username</strong> will have permission value of <strong>0777</strong>. Owing to JSON limitations, you must specify the mode in decimal notation.</p><p>Note that this permission value might be displayed in decimal notation if you read it later.</p><p><strong>Consuming Secret Values from Volumes</strong></p><p>Inside the container that mounts a secret volume, the secret keys appear as files and the secret values are base-64 decoded and stored inside these files. This is the result of commands executed inside the container from the example above:</p><p><strong>$ ls /etc/foo/</strong></p><p><strong>username</strong></p><p><strong>password</strong></p><p><strong>$ cat /etc/foo/username</strong></p><p><strong>admin</strong></p><p><strong>$ cat /etc/foo/password</strong></p><p><strong>1f2d1e2e67df</strong></p><p>The program in a container is responsible for reading the secrets from the files.</p><p><strong>Mounted Secrets are updated automatically</strong></p><p>When a secret being already consumed in a volume is updated, projected keys are eventually updated as well. Kubelet is checking whether the mounted secret is fresh on every periodic sync. However, it is using its local ttl-based cache for getting the current value of the secret. As a result, the total delay from the moment when the secret is updated to the moment when new keys are projected to the pod can be as long as kubelet sync period + ttl of secrets cache in kubelet.</p><p><strong>Note:</strong> A container using a Secret as a <a href="https://kubernetes.io/docs/concepts/storage/volumes#using-subpath">subPath</a> volume mount will not receive Secret updates.</p><h6><strong>Using Secrets as Environment Variables</strong></h6><p>To use a secret in an environment variable in a pod:</p><ol><li>Create a secret or use an existing one. Multiple pods can reference the same secret.</li><li>Modify your Pod definition in each container that you wish to consume the value of a secret key to add an environment variable for each secret key you wish to consume. The environment variable that consumes the secret key should populate the secret&#x27;s name and key in <strong>env[].valueFrom.secretKeyRef</strong>.</li><li>Modify your image and/or command line so that the program looks for values in the specified environment variables</li></ol><p>This is an example of a pod that uses secrets from environment variables:</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: secret-env-pod</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: mycontainer</strong></p><p><strong>image: redis</strong></p><p><strong>env:</strong></p><p><strong>- name: SECRET_USERNAME</strong></p><p><strong>valueFrom:</strong></p><p><strong>secretKeyRef:</strong></p><p><strong>name: mysecret</strong></p><p><strong>key: username</strong></p><p><strong>- name: SECRET_PASSWORD</strong></p><p><strong>valueFrom:</strong></p><p><strong>secretKeyRef:</strong></p><p><strong>name: mysecret</strong></p><p><strong>key: password</strong></p><p><strong>restartPolicy: Never</strong></p><p><strong>Consuming Secret Values from Environment Variables</strong></p><p>Inside a container that consumes a secret in an environment variables, the secret keys appear as normal environment variables containing the base-64 decoded values of the secret data. This is the result of commands executed inside the container from the example above:</p><p><strong>$ echo $SECRET_USERNAME</strong></p><p><strong>admin</strong></p><p><strong>$ echo $SECRET_PASSWORD</strong></p><p><strong>1f2d1e2e67df</strong></p><h6><strong>Using imagePullSecrets</strong></h6><p>An imagePullSecret is a way to pass a secret that contains a Docker (or other) image registry password to the Kubelet so it can pull a private image on behalf of your Pod.</p><p><strong>Manually specifying an imagePullSecret</strong></p><p>Use of imagePullSecrets is described in the <a href="https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod">images documentation</a></p><h5><strong>Arranging for imagePullSecrets to be Automatically Attached</strong></h5><p>You can manually create an imagePullSecret, and reference it from a serviceAccount. Any pods created with that serviceAccount or that default to use that serviceAccount, will get their imagePullSecret field set to that of the service account. See <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account">Add ImagePullSecrets to a service account</a> for a detailed explanation of that process.</p><h5><strong>Automatic Mounting of Manually Created Secrets</strong></h5><p>Manually created secrets (e.g. one containing a token for accessing a github account) can be automatically attached to pods based on their service account. See <a href="https://kubernetes.io/docs/tasks/inject-data-application/podpreset/">Injecting Information into Pods Using a PodPreset</a> for a detailed explanation of that process.</p><h4>Details</h4><h5><strong>Restrictions</strong></h5><p>Secret volume sources are validated to ensure that the specified object reference actually points to an object of type <strong>Secret</strong>. Therefore, a secret needs to be created before any pods that depend on it.</p><p>Secret API objects reside in a namespace. They can only be referenced by pods in that same namespace.</p><p>Individual secrets are limited to 1MB in size. This is to discourage creation of very large secrets which would exhaust apiserver and kubelet memory. However, creation of many smaller secrets could also exhaust memory. More comprehensive limits on memory usage due to secrets is a planned feature.</p><p>Kubelet only supports use of secrets for Pods it gets from the API server. This includes any pods created using kubectl, or indirectly via a replication controller. It does not include pods created via the kubelets <strong>--manifest-url</strong> flag, its <strong>--config</strong> flag, or its REST API (these are not common ways to create pods.)</p><p>Secrets must be created before they are consumed in pods as environment variables unless they are marked as optional. References to Secrets that do not exist will prevent the pod from starting.</p><p>References via <strong>secretKeyRef</strong> to keys that do not exist in a named Secret will prevent the pod from starting.</p><p>Secrets used to populate environment variables via <strong>envFrom</strong> that have keys that are considered invalid environment variable names will have those keys skipped. The pod will be allowed to start. There will be an event whose reason is <strong>InvalidVariableNames</strong> and the message will contain the list of invalid keys that were skipped. The example shows a pod which refers to the default/mysecret that contains 2 invalid keys, 1badkey and 2alsobad.</p><p><strong>$ kubectl get events</strong></p><p><strong>LASTSEEN FIRSTSEEN COUNT NAME KIND SUBOBJECT TYPE REASON</strong></p><p><strong>0s 0s 1 dapi-test-pod Pod Warning InvalidEnvironmentVariableNames kubelet, 127.0.0.1 Keys <!-- -->[1badkey, 2alsobad]<!-- --> from the EnvFrom secret default/mysecret were skipped since they are considered invalid environment variable names.</strong></p><h5><strong>Secret and Pod Lifetime interaction</strong></h5><p>When a pod is created via the API, there is no check whether a referenced secret exists. Once a pod is scheduled, the kubelet will try to fetch the secret value. If the secret cannot be fetched because it does not exist or because of a temporary lack of connection to the API server, kubelet will periodically retry. It will report an event about the pod explaining the reason it is not started yet. Once the secret is fetched, the kubelet will create and mount a volume containing it. None of the pod&#x27;s containers will start until all the pod&#x27;s volumes are mounted.</p><h4>Use cases</h4><h5><strong>Use-Case: Pod with ssh keys</strong></h5><p>Create a secret containing some ssh keys:</p><p><strong>$ kubectl create secret generic ssh-key-secret --from-file=ssh-privatekey=/path/to/.ssh/id_rsa --from-file=ssh-publickey=/path/to/.ssh/id_rsa.pub</strong></p><p><strong>Security Note:</strong> think carefully before sending your own ssh keys: other users of the cluster may have access to the secret. Use a service account which you want to be accessible to all the users with whom you share the Kubernetes cluster, and can revoke if they are compromised.</p><p>Now we can create a pod which references the secret with the ssh key and consumes it in a volume:</p><p><strong>kind: Pod</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>metadata:</strong></p><p><strong>name: secret-test-pod</strong></p><p><strong>labels:</strong></p><p><strong>name: secret-test</strong></p><p><strong>spec:</strong></p><p><strong>volumes:</strong></p><p><strong>- name: secret-volume</strong></p><p><strong>secret:</strong></p><p><strong>secretName: ssh-key-secret</strong></p><p><strong>containers:</strong></p><p><strong>- name: ssh-test-container</strong></p><p><strong>image: mySshImage</strong></p><p><strong>volumeMounts:</strong></p><p><strong>- name: secret-volume</strong></p><p><strong>readOnly: true</strong></p><p><strong>mountPath: &quot;/etc/secret-volume&quot;</strong></p><p>When the container&#x27;s command runs, the pieces of the key will be available in:</p><p><strong>/etc/secret-volume/ssh-publickey</strong></p><p><strong>/etc/secret-volume/ssh-privatekey</strong></p><p>The container is then free to use the secret data to establish an ssh connection.</p><h5><strong>Use-Case: Pods with prod / test credentials</strong></h5><p>This example illustrates a pod which consumes a secret containing prod credentials and another pod which consumes a secret with test environment credentials.</p><p>Make the secrets:</p><p><strong>$ kubectl create secret generic prod-db-secret --from-literal=username=produser --from-literal=password=Y4nys7f11</strong></p><p><strong>secret &quot;prod-db-secret&quot; created</strong></p><p><strong>$ kubectl create secret generic test-db-secret --from-literal=username=testuser --from-literal=password=iluvtests</strong></p><p><strong>secret &quot;test-db-secret&quot; created</strong></p><p>Now make the pods:</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: List</strong></p><p><strong>items:</strong></p><p><strong>- kind: Pod</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>metadata:</strong></p><p><strong>name: prod-db-client-pod</strong></p><p><strong>labels:</strong></p><p><strong>name: prod-db-client</strong></p><p><strong>spec:</strong></p><p><strong>volumes:</strong></p><p><strong>- name: secret-volume</strong></p><p><strong>secret:</strong></p><p><strong>secretName: prod-db-secret</strong></p><p><strong>containers:</strong></p><p><strong>- name: db-client-container</strong></p><p><strong>image: myClientImage</strong></p><p><strong>volumeMounts:</strong></p><p><strong>- name: secret-volume</strong></p><p><strong>readOnly: true</strong></p><p><strong>mountPath: &quot;/etc/secret-volume&quot;</strong></p><p><strong>- kind: Pod</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>metadata:</strong></p><p><strong>name: test-db-client-pod</strong></p><p><strong>labels:</strong></p><p><strong>name: test-db-client</strong></p><p><strong>spec:</strong></p><p><strong>volumes:</strong></p><p><strong>- name: secret-volume</strong></p><p><strong>secret:</strong></p><p><strong>secretName: test-db-secret</strong></p><p><strong>containers:</strong></p><p><strong>- name: db-client-container</strong></p><p><strong>image: myClientImage</strong></p><p><strong>volumeMounts:</strong></p><p><strong>- name: secret-volume</strong></p><p><strong>readOnly: true</strong></p><p><strong>mountPath: &quot;/etc/secret-volume&quot;</strong></p><p>Both containers will have the following files present on their filesystems with the values for each container&#x27;s environment:</p><p><strong>/etc/secret-volume/username</strong></p><p><strong>/etc/secret-volume/password</strong></p><p>Note how the specs for the two pods differ only in one field; this facilitates creating pods with different capabilities from a common pod config template.</p><p>You could further simplify the base pod specification by using two Service Accounts: one called, say, <strong>prod-user</strong> with the <strong>prod-db-secret</strong>, and one called, say, <strong>test-user</strong> with the <strong>test-db-secret</strong>. Then, the pod spec can be shortened to, for example:</p><p><strong>kind: Pod</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>metadata:</strong></p><p><strong>name: prod-db-client-pod</strong></p><p><strong>labels:</strong></p><p><strong>name: prod-db-client</strong></p><p><strong>spec:</strong></p><p><strong>serviceAccount: prod-db-client</strong></p><p><strong>containers:</strong></p><p><strong>- name: db-client-container</strong></p><p><strong>image: myClientImage</strong></p><h5><strong>Use-case: Dotfiles in secret volume</strong></h5><p>In order to make piece of data &#x27;hidden&#x27; (i.e., in a file whose name begins with a dot character), simply make that key begin with a dot. For example, when the following secret is mounted into a volume:</p><p><strong>kind: Secret</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>metadata:</strong></p><p><strong>name: dotfile-secret</strong></p><p><strong>data:</strong></p><p><strong>.secret-file: dmFsdWUtMg0KDQo=</strong></p><p><strong>---</strong></p><p><strong>kind: Pod</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>metadata:</strong></p><p><strong>name: secret-dotfiles-pod</strong></p><p><strong>spec:</strong></p><p><strong>volumes:</strong></p><p><strong>- name: secret-volume</strong></p><p><strong>secret:</strong></p><p><strong>secretName: dotfile-secret</strong></p><p><strong>containers:</strong></p><p><strong>- name: dotfile-test-container</strong></p><p><strong>image: k8s.gcr.io/busybox</strong></p><p><strong>command:</strong></p><p><strong>- ls</strong></p><p><strong>- &quot;-l&quot;</strong></p><p><strong>- &quot;/etc/secret-volume&quot;</strong></p><p><strong>volumeMounts:</strong></p><p><strong>- name: secret-volume</strong></p><p><strong>readOnly: true</strong></p><p><strong>mountPath: &quot;/etc/secret-volume&quot;</strong></p><p>The <strong>secret-volume</strong> will contain a single file, called <strong>.secret-file</strong>, and the <strong>dotfile-test-container</strong> will have this file present at the path <strong>/etc/secret-volume/.secret-file</strong>.</p><p><strong>NOTE</strong></p><p>Files beginning with dot characters are hidden from the output of <strong>ls -l</strong>; you must use <strong>ls -la</strong> to see them when listing directory contents.</p><h5><strong>Use-case: Secret visible to one container in a pod</strong></h5><p>Consider a program that needs to handle HTTP requests, do some complex business logic, and then sign some messages with an HMAC. Because it has complex application logic, there might be an unnoticed remote file reading exploit in the server, which could expose the private key to an attacker.</p><p>This could be divided into two processes in two containers: a frontend container which handles user interaction and business logic, but which cannot see the private key; and a signer container that can see the private key, and responds to simple signing requests from the frontend (e.g. over localhost networking).</p><p>With this partitioned approach, an attacker now has to trick the application server into doing something rather arbitrary, which may be harder than getting it to read a file.</p><h4>Best practices</h4><h5><strong>Clients that use the secrets API</strong></h5><p>When deploying applications that interact with the secrets API, access should be limited using <a href="https://kubernetes.io/docs/admin/authorization/">authorization policies</a> such as <a href="https://kubernetes.io/docs/admin/authorization/rbac/">RBAC</a>.</p><p>Secrets often hold values that span a spectrum of importance, many of which can cause escalations within Kubernetes (e.g. service account tokens) and to external systems. Even if an individual app can reason about the power of the secrets it expects to interact with, other apps within the same namespace can render those assumptions invalid.</p><p>For these reasons <strong>watch</strong> and <strong>list</strong> requests for secrets within a namespace are extremely powerful capabilities and should be avoided, since listing secrets allows the clients to inspect the values of all secrets that are in that namespace. The ability to <strong>watch</strong> and <strong>list</strong> all secrets in a cluster should be reserved for only the most privileged, system-level components.</p><p>Applications that need to access the secrets API should perform <strong>get</strong> requests on the secrets they need. This lets administrators restrict access to all secrets while <a href="https://kubernetes.io/docs/admin/authorization/rbac/#referring-to-resources">white-listing access to individual instances</a> that the app needs.</p><p>For improved performance over a looping <strong>get</strong>, clients can design resources that reference a secret then <strong>watch</strong> the resource, re-requesting the secret when the reference changes. Additionally, a <a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/bulk_watch.md">&quot;bulk watch&quot; API</a> to let clients <strong>watch</strong> individual resources has also been proposed, and will likely be available in future releases of Kubernetes.</p><h4>Security Properties</h4><h5><strong>Protections</strong></h5><p>Because <strong>secret</strong> objects can be created independently of the <strong>pods</strong> that use them, there is less risk of the secret being exposed during the workflow of creating, viewing, and editing pods. The system can also take additional precautions with <strong>secret</strong> objects, such as avoiding writing them to disk where possible.</p><p>A secret is only sent to a node if a pod on that node requires it. It is not written to disk. It is stored in a tmpfs. It is deleted once the pod that depends on it is deleted.</p><p>On most Kubernetes-project-maintained distributions, communication between user to the apiserver, and from apiserver to the kubelets, is protected by SSL/TLS. Secrets are protected when transmitted over these channels.</p><p>Secret data on nodes is stored in tmpfs volumes and thus does not come to rest on the node.</p><p>There may be secrets for several pods on the same node. However, only the secrets that a pod requests are potentially visible within its containers. Therefore, one Pod does not have access to the secrets of another pod.</p><p>There may be several containers in a pod. However, each container in a pod has to request the secret volume in its <strong>volumeMounts</strong> for it to be visible within the container. This can be used to construct useful <a href="https://kubernetes.io/docs/concepts/configuration/secret/#use-case-secret-visible-to-one-container-in-a-pod">security partitions at the Pod level</a>.</p><h5><strong>Risks</strong></h5><ul><li>In the API server secret data is stored as plaintext in etcd; therefore:<ul><li>Administrators should limit access to etcd to admin users</li><li>Secret data in the API server is at rest on the disk that etcd uses; admins may want to wipe/shred disks used by etcd when no longer in use</li></ul></li><li>If you configure the secret through a manifest (JSON or YAML) file which has the secret data encoded as base64, sharing this file or checking it in to a source repository means the secret is compromised. Base64 encoding is not an encryption method and is considered the same as plain text.</li><li>Applications still need to protect the value of secret after reading it from the volume, such as not accidentally logging it or transmitting it to an untrusted party.</li><li>A user who can create a pod that uses a secret can also see the value of that secret. Even if apiserver policy does not allow that user to read the secret object, the user could run a pod which exposes the secret.</li><li>If multiple replicas of etcd are run, then the secrets will be shared between them. By default, etcd does not secure peer-to-peer communication with SSL/TLS, though this can be configured.</li><li>Currently, anyone with root on any node can read any secret from the apiserver, by impersonating the kubelet. It is a planned feature to only send secrets to nodes that actually require them, to restrict the impact of a root exploit on a single node.</li></ul><p><strong>Note:</strong> As of 1.7 <a href="https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/">encryption of secret data at rest is supported</a>.</p><h3>Organizing Cluster Access Using kubeconfig Files</h3><p>Use kubeconfig files to organize information about clusters, users, namespaces, and authentication mechanisms. The <strong>kubectl</strong> command-line tool uses kubeconfig files to find the information it needs to choose a cluster and communicate with the API server of a cluster.</p><p><strong>Note:</strong> A file that is used to configure access to clusters is called a kubeconfig file. This is a generic way of referring to configuration files. It does not mean that there is a file named <strong>kubeconfig</strong>.</p><p>By default, <strong>kubectl</strong> looks for a file named <strong>config</strong> in the <strong>$HOME/.kube</strong> directory. You can specify other kubeconfig files by setting the <strong>KUBECONFIG</strong> environment variable or by setting the <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/"><strong>--kubeconfig</strong></a> flag.</p><p>For step-by-step instructions on creating and specifying kubeconfig files, see <a href="https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters">Configure Access to Multiple Clusters</a>.</p><ul><li><a href="https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/#supporting-multiple-clusters-users-and-authentication-mechanisms"><strong>Supporting multiple clusters, users, and authentication mechanisms</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/#context"><strong>Context</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/#the-kubeconfig-environment-variable"><strong>The KUBECONFIG environment variable</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/#merging-kubeconfig-files"><strong>Merging kubeconfig files</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/#file-references"><strong>File references</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Supporting multiple clusters, users, and authentication mechanisms</h4><p>Suppose you have several clusters, and your users and components authenticate in a variety of ways. For example:</p><ul><li>A running kubelet might authenticate using certificates.</li><li>A user might authenticate using tokens.</li><li>Administrators might have sets of certificates that they provide to individual users.</li></ul><p>With kubeconfig files, you can organize your clusters, users, and namespaces. You can also define contexts to quickly and easily switch between clusters and namespaces.</p><h4>Context</h4><p>A context element in a kubeconfig file is used to group access parameters under a convenient name. Each context has three parameters: cluster, namespace, and user. By default, the <strong>kubectl</strong>command-line tool uses parameters from the current context to communicate with the cluster.</p><p>To choose the current context:</p><p><strong>kubectl config use-context</strong></p><h4>The KUBECONFIG environment variable</h4><p>The <strong>KUBECONFIG</strong> environment variable holds a list of kubeconfig files. For Linux and Mac, the list is colon-delimited. For Windows, the list is semicolon-delimited. The <strong>KUBECONFIG</strong> environment variable is not required. If the <strong>KUBECONFIG</strong> environment variable doesn&#x27;t exist, <strong>kubectl</strong> uses the default kubeconfig file, <strong>$HOME/.kube/config</strong>.</p><p>If the <strong>KUBECONFIG</strong> environment variable does exist, <strong>kubectl</strong> uses an effective configuration that is the result of merging the files listed in the <strong>KUBECONFIG</strong> environment variable.</p><h4>Merging kubeconfig files</h4><p>To see your configuration, enter this command:</p><p><strong>kubectl config view</strong></p><p>As described previously, the output might be from a single kubeconfig file, or it might be the result of merging several kubeconfig files.</p><p>Here are the rules that <strong>kubectl</strong> uses when it merges kubeconfig files:</p><ol><li>If the <strong>--kubeconfig</strong> flag is set, use only the specified file. Do not merge. Only one instance of this flag is allowed.</li></ol><p>Otherwise, if the <strong>KUBECONFIG</strong> environment variable is set, use it as a list of files that should be merged. Merge the files listed in the <strong>KUBECONFIG</strong> environment variable according to these rules:</p><ul><li><ul><li>Ignore empty filenames.</li><li>Produce errors for files with content that cannot be deserialized.</li><li>The first file to set a particular value or map key wins.</li><li>Never change the value or map key. Example: Preserve the context of the first file to set <strong>current-context</strong>. Example: If two files specify a <strong>red-user</strong>, use only values from the first file&#x27;s <strong>red-user</strong>. Even if the second file has non-conflicting entries under <strong>red-user</strong>, discard them.</li></ul></li></ul><p>For an example of setting the <strong>KUBECONFIG</strong> environment variable, see <a href="https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/#set-the-kubeconfig-environment-variable">Setting the KUBECONFIG environment variable</a>.</p><p>Otherwise, use the default kubeconfig file, <strong>$HOME/.kube/config</strong>, with no merging.</p><ol><li>Determine the context to use based on the first hit in this chain:<ul><li>Use the <strong>--context</strong> command-line flag if it exits.</li><li>Use the <strong>current-context</strong> from the merged kubeconfig files.</li></ul></li></ol><p>An empty context is allowed at this point.</p><ol><li>Determine the cluster and user. At this point, there might or might not be a context. Determine the cluster and user based on the first hit in this chain, which is run twice: once for user and once for cluster:<ul><li>Use a command-line flag if it exists: <strong>--user</strong> or <strong>--cluster</strong>.</li><li>If the context is non-empty, take the user or cluster from the context.</li></ul></li></ol><p>The user and cluster can be empty at this point.</p><ol><li>Determine the actual cluster information to use. At this point, there might or might not be cluster information. Build each piece of the cluster information based on this chain; the first hit wins:<ul><li>Use command line flags if they exist: <strong>--server</strong>, <strong>--certificate-authority</strong>, <strong>--insecure-skip-tls-verify</strong>.</li><li>If any cluster information attributes exist from the merged kubeconfig files, use them.</li><li>If there is no server location, fail.</li></ul></li><li>Determine the actual user information to use. Build user information using the same rules as cluster information, except allow only one authentication technique per user:<ul><li>Use command line flags if they exist: <strong>--client-certificate</strong>, <strong>--client-key</strong>, <strong>--username</strong>, <strong>--password</strong>, <strong>--token</strong>.</li><li>Use the <strong>user</strong> fields from the merged kubeconfig files.</li><li>If there are two conflicting techniques, fail.</li></ul></li><li>For any information still missing, use default values and potentially prompt for authentication information.</li></ol><h4>File references</h4><p>File and path references in a kubeconfig file are relative to the location of the kubeconfig file. File references on the command line are relative to the current working directory. In <strong>$HOME/.kube/config</strong>, relative paths are stored relatively, and absolute paths are stored absolutely.</p><h4>What&#x27;s next</h4><ul><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/">Configure Access to Multiple Clusters</a></li><li><a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/">kubectl config</a></li></ul><h3>Pod Priority and Preemption</h3><p><strong>FEATURE STATE:</strong> <strong>Kubernetes v1.10</strong> <a href="https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/">alpha</a></p><p><a href="https://kubernetes.io/docs/user-guide/pods">Pods</a> in Kubernetes 1.8 and later can have priority. Priority indicates the importance of a Pod relative to other Pods. When a Pod cannot be scheduled, the scheduler tries to preempt (evict) lower priority Pods to make scheduling of the pending Pod possible. In Kubernetes 1.9 and later, Priority also affects scheduling order of Pods and out-of-resource eviction ordering on the Node.</p><ul><li><a href="https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#how-to-use-priority-and-preemption"><strong>How to use priority and preemption</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#enabling-priority-and-preemption"><strong>Enabling priority and preemption</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#priorityclass"><strong>PriorityClass</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#example-priorityclass"><strong>Example PriorityClass</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#pod-priority"><strong>Pod priority</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#effect-of-pod-priority-on-scheduling-order"><strong>Effect of Pod priority on scheduling order</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#preemption"><strong>Preemption</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#user-exposed-information"><strong>User exposed information</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#limitations-of-preemption"><strong>Limitations of preemption</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#graceful-termination-of-preemption-victims"><strong>Graceful termination of preemption victims</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#poddisruptionbudget-is-supported-but-not-guaranteed"><strong>PodDisruptionBudget is supported, but not guaranteed!</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#inter-pod-affinity-on-lower-priority-pods"><strong>Inter-Pod affinity on lower-priority Pods</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#cross-node-preemption"><strong>Cross node preemption</strong></a></li></ul></li></ul></li></ul><h4>How to use priority and preemption</h4><p>To use priority and preemption in Kubernetes 1.8 and later, follow these steps:</p><ol><li>Enable the feature.</li><li>Add one or more PriorityClasses.</li><li>Create Pods with <strong>priorityClassName</strong> set to one of the added PriorityClasses. Of course you do not need to create the Pods directly; normally you would add <strong>priorityClassName</strong> to the Pod template of a collection object like a Deployment.</li></ol><p>The following sections provide more information about these steps.</p><h4>Enabling priority and preemption</h4><p>Pod priority and preemption is disabled by default in Kubernetes 1.8. To enable the feature, set this command-line flag for the API server, scheduler and kubelet:</p><p><strong>--feature-gates=PodPriority=true</strong></p><p>Also enable scheduling.k8s.io/v1alpha1 API and Priority <a href="https://kubernetes.io/docs/admin/admission-controllers/">admission controller</a> in API server:</p><p><strong>--runtime-config=scheduling.k8s.io/v1alpha1=true --enable-admission-plugins=Controller-Foo,Controller-Bar,<!-- -->.<!-- -->..,Priority</strong></p><p>After the feature is enabled, you can create <a href="https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#priorityclass">PriorityClasses</a> and create Pods with <a href="https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#pod-priority"><strong>priorityClassName</strong></a> set.</p><p>If you try the feature and then decide to disable it, you must remove the PodPriority command-line flag or set it to false, and then restart the API server and scheduler. After the feature is disabled, the existing Pods keep their priority fields, but preemption is disabled, and priority fields are ignored, and you cannot set <strong>priorityClassName</strong> in new Pods.</p><h4>PriorityClass</h4><p>A PriorityClass is a non-namespaced object that defines a mapping from a priority class name to the integer value of the priority. The name is specified in the <strong>name</strong> field of the PriorityClass object&#x27;s metadata. The value is specified in the required <strong>value</strong> field. The higher the value, the higher the priority.</p><p>A PriorityClass object can have any 32-bit integer value smaller than or equal to 1 billion. Larger numbers are reserved for critical system Pods that should not normally be preempted or evicted. A cluster admin should create one PriorityClass object for each such mapping that they want.</p><p>PriorityClass also has two optional fields: <strong>globalDefault</strong> and <strong>description</strong>. The <strong>globalDefault</strong>field indicates that the value of this PriorityClass should be used for Pods without a <strong>priorityClassName</strong>. Only one PriorityClass with <strong>globalDefault</strong> set to true can exist in the system. If there is no PriorityClass with <strong>globalDefault</strong> set, the priority of Pods with no <strong>priorityClassName</strong> is zero.</p><p>The <strong>description</strong> field is an arbitrary string. It is meant to tell users of the cluster when they should use this PriorityClass.</p><p><strong>Note 1</strong>: If you upgrade your existing cluster and enable this feature, the priority of your existing Pods will be considered to be zero.</p><p><strong>Note 2</strong>: Addition of a PriorityClass with <strong>globalDefault</strong> set to true does not change the priorities of existing Pods. The value of such a PriorityClass is used only for Pods created after the PriorityClass is added.</p><p><strong>Note 3</strong>: If you delete a PriorityClass, existing Pods that use the name of the deleted priority class remain unchanged, but you are not able to create more Pods that use the name of the deleted PriorityClass.</p><h5><strong>Example PriorityClass</strong></h5><p><strong>apiVersion: scheduling.k8s.io/v1alpha1</strong></p><p><strong>kind: PriorityClass</strong></p><p><strong>metadata:</strong></p><p><strong>name: high-priority</strong></p><p><strong>value: 1000000</strong></p><p><strong>globalDefault: false</strong></p><p><strong>description: &quot;This priority class should be used for XYZ service pods only.&quot;</strong></p><h4>Pod priority</h4><p>After you have one or more PriorityClasses, you can create Pods that specify one of those PriorityClass names in their specifications. The priority admission controller uses the <strong>priorityClassName</strong> field and populates the integer value of the priority. If the priority class is not found, the Pod is rejected.</p><p>The following YAML is an example of a Pod configuration that uses the PriorityClass created in the preceding example. The priority admission controller checks the specification and resolves the priority of the Pod to 1000000.</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: nginx</strong></p><p><strong>labels:</strong></p><p><strong>env: test</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: nginx</strong></p><p><strong>image: nginx</strong></p><p><strong>imagePullPolicy: IfNotPresent</strong></p><p><strong>priorityClassName: high-priority</strong></p><h5><strong>Effect of Pod priority on scheduling order</strong></h5><p>In Kubernetes 1.9 and later, when Pod priority is enabled, scheduler orders pending Pods by their priority and a pending Pod is placed ahead of other pending Pods with lower priority in the scheduling queue. As a result, the higher priority Pod may by scheduled sooner that Pods with lower priority if its scheduling requirements are met. If such Pod cannot be scheduled, scheduler will continue and tries to schedule other lower priority Pods.</p><h4>Preemption</h4><p>When Pods are created, they go to a queue and wait to be scheduled. The scheduler picks a Pod from the queue and tries to schedule it on a Node. If no Node is found that satisfies all the specified requirements of the Pod, preemption logic is triggered for the pending Pod. Let&#x27;s call the pending Pod P. Preemption logic tries to find a Node where removal of one or more Pods with lower priority than P would enable P to be scheduled on that Node. If such a Node is found, one or more lower priority Pods get deleted from the Node. After the Pods are gone, P can be scheduled on the Node.</p><h5><strong>User exposed information</strong></h5><p>When Pod P preempts one or more Pods on Node N, <strong>nominatedNodeName</strong> field of Pod P&#x27;s status is set to the name of Node N. This field helps scheduler track resources reserved for Pod P and also gives users information about preemptions in their clusters.</p><p>Please note that Pod P is not necessarily scheduled to the &quot;nominated Node&quot;. After victim Pods are preempted, they get their graceful termination period. If another node becomes available while scheduler is waiting for the victim Pods to terminate, scheduler will use the other node to schedule Pod P. As a result <strong>nominatedNodeName</strong> and <strong>nodeName</strong> of Pod spec are not always the same. Also, if scheduler preempts Pods on Node N, but then a higher priority Pod than Pod P arrives, scheduler may give Node N to the new higher priority Pod. In such a case, scheduler clears <strong>nominatedNodeName</strong> of Pod P. By doing this, scheduler makes Pod P eligible to preempt Pods on another Node.</p><h5><strong>Limitations of preemption</strong></h5><h6><strong>Graceful termination of preemption victims</strong></h6><p>When Pods are preempted, the victims get their <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods">graceful termination period</a>. They have that much time to finish their work and exit. If they don&#x27;t, they are killed. This graceful termination period creates a time gap between the point that the scheduler preempts Pods and the time when the pending Pod (P) can be scheduled on the Node (N). In the meantime, the scheduler keeps scheduling other pending Pods. As victims exit or get terminated, the scheduler tries to schedule Pods in the pending queue. Therefore, there is usually a time gap between the point that scheduler preempts victims and the time that Pod P is scheduled. In order to minimize this gap, one can set graceful termination period of lower priority Pods to zero or a small number.</p><h6><strong>PodDisruptionBudget is supported, but not guaranteed!</strong></h6><p>A <a href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/">Pod Disruption Budget (PDB)</a> allows application owners to limit the number Pods of a replicated application that are down simultaneously from voluntary disruptions. Kubernetes 1.9 supports PDB when preempting Pods, but respecting PDB is best effort. The Scheduler tries to find victims whose PDB are not violated by preemption, but if no such victims are found, preemption will still happen, and lower priority Pods will be removed despite their PDBs being violated.</p><h6><strong>Inter-Pod affinity on lower-priority Pods</strong></h6><p>A Node is considered for preemption only when the answer to this question is yes: &quot;If all the Pods with lower priority than the pending Pod are removed from the Node, can the pending Pod be scheduled on the Node?&quot;</p><p><strong>Note:</strong> Preemption does not necessarily remove all lower-priority Pods. If the pending Pod can be scheduled by removing fewer than all lower-priority Pods, then only a portion of the lower-priority Pods are removed. Even so, the answer to the preceding question must be yes. If the answer is no, the Node is not considered for preemption.</p><p>If a pending Pod has inter-pod affinity to one or more of the lower-priority Pods on the Node, the inter-Pod affinity rule cannot be satisfied in the absence of those lower-priority Pods. In this case, the scheduler does not preempt any Pods on the Node. Instead, it looks for another Node. The scheduler might find a suitable Node or it might not. There is no guarantee that the pending Pod can be scheduled.</p><p>Our recommended solution for this problem is to create inter-Pod affinity only towards equal or higher priority Pods.</p><h6><strong>Cross node preemption</strong></h6><p>Suppose a Node N is being considered for preemption so that a pending Pod P can be scheduled on N. P might become feasible on N only if a Pod on another Node is preempted. Here&#x27;s an example:</p><ul><li>Pod P is being considered for Node N.</li><li>Pod Q is running on another Node in the same Zone as Node N.</li><li>Pod P has Zone-wide anti-affinity with Pod Q (<strong>topologyKey: failure-domain.beta.kubernetes.io/zone</strong>).</li><li>There are no other cases of anti-affinity between Pod P and other Pods in the Zone.</li><li>In order to schedule Pod P on Node N, Pod Q can be preempted, but scheduler does not perform cross-node preemption. So, Pod P will be deemed unschedulable on Node N.</li></ul><p>If Pod Q were removed from its Node, the Pod anti-affinity violation would be gone, and Pod P could possibly be scheduled on Node N.</p><p>We may consider adding cross Node preemption in future versions if we find an algorithm with reasonable performance. We cannot promise anything at this point, and cross Node preemption will not be considered a blocker for Beta or GA.</p><h2>Services, Load Balancing, and Networking</h2><h3>Services</h3><p>Kubernetes <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/"><strong>Pods</strong></a> are mortal. They are born and when they die, they are not resurrected.<a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/"><strong>ReplicationControllers</strong></a> in particular create and destroy <strong>Pods</strong> dynamically (e.g. when scaling up or down or when doing <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#rolling-update">rolling updates</a>). While each <strong>Pod</strong> gets its own IP address, even those IP addresses cannot be relied upon to be stable over time. This leads to a problem: if some set of <strong>Pods</strong>(let&#x27;s call them backends) provides functionality to other <strong>Pods</strong> (let&#x27;s call them frontends) inside the Kubernetes cluster, how do those frontends find out and keep track of which backends are in that set?</p><p>Enter <strong>Services</strong>.</p><p>A Kubernetes <strong>Service</strong> is an abstraction which defines a logical set of <strong>Pods</strong> and a policy by which to access them - sometimes called a micro-service. The set of <strong>Pods</strong> targeted by a <strong>Service</strong> is (usually) determined by a <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors"><strong>Label Selector</strong></a> (see below for why you might want a <strong>Service</strong> without a selector).</p><p>As an example, consider an image-processing backend which is running with 3 replicas. Those replicas are fungible - frontends do not care which backend they use. While the actual <strong>Pods</strong> that compose the backend set may change, the frontend clients should not need to be aware of that or keep track of the list of backends themselves. The <strong>Service</strong> abstraction enables this decoupling.</p><p>For Kubernetes-native applications, Kubernetes offers a simple <strong>Endpoints</strong> API that is updated whenever the set of <strong>Pods</strong> in a <strong>Service</strong> changes. For non-native applications, Kubernetes offers a virtual-IP-based bridge to Services which redirects to the backend <strong>Pods</strong>.</p><ul><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#defining-a-service"><strong>Defining a service</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#services-without-selectors"><strong>Services without selectors</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies"><strong>Virtual IPs and service proxies</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#proxy-mode-userspace"><strong>Proxy-mode: userspace</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#proxy-mode-iptables"><strong>Proxy-mode: iptables</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#proxy-mode-ipvs"><strong>Proxy-mode: ipvs</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#multi-port-services"><strong>Multi-Port Services</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#choosing-your-own-ip-address"><strong>Choosing your own IP address</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#why-not-use-round-robin-dns"><strong>Why not use round-robin DNS?</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#discovering-services"><strong>Discovering services</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#environment-variables"><strong>Environment variables</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#dns"><strong>DNS</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#headless-services"><strong>Headless services</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#with-selectors"><strong>With selectors</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#without-selectors"><strong>Without selectors</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services---service-types"><strong>Publishing services - service types</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport"><strong>Type NodePort</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer"><strong>Type LoadBalancer</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer"><strong>Internal load balancer</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#ssl-support-on-aws"><strong>SSL support on AWS</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#proxy-protocol-support-on-aws"><strong>PROXY protocol support on AWS</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#elb-access-logs-on-aws"><strong>ELB Access Logs on AWS</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#connection-draining-on-aws"><strong>Connection Draining on AWS</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#other-elb-annotations"><strong>Other ELB annotations</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#network-load-balancer-support-on-aws-alpha"><strong>Network Load Balancer support on AWS [alpha]</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#external-ips"><strong>External IPs</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#shortcomings"><strong>Shortcomings</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#future-work"><strong>Future work</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#the-gory-details-of-virtual-ips"><strong>The gory details of virtual IPs</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#avoiding-collisions"><strong>Avoiding collisions</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#ips-and-vips"><strong>IPs and VIPs</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#userspace"><strong>Userspace</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#iptables"><strong>Iptables</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#ipvs"><strong>Ipvs</strong></a></li></ul></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#api-object"><strong>API Object</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#for-more-information"><strong>For More Information</strong></a></li></ul><h4>Defining a service</h4><p>A <strong>Service</strong> in Kubernetes is a REST object, similar to a <strong>Pod</strong>. Like all of the REST objects, a <strong>Service</strong> definition can be POSTed to the apiserver to create a new instance. For example, suppose you have a set of <strong>Pods</strong> that each expose port 9376 and carry a label <strong>&quot;app=MyApp&quot;</strong>.</p><p><strong>kind: Service</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>metadata:</strong></p><p><strong>name: my-service</strong></p><p><strong>spec:</strong></p><p><strong>selector:</strong></p><p><strong>app: MyApp</strong></p><p><strong>ports:</strong></p><p><strong>- protocol: TCP</strong></p><p><strong>port: 80</strong></p><p><strong>targetPort: 9376</strong></p><p>This specification will create a new <strong>Service</strong> object named &quot;my-service&quot; which targets TCP port 9376 on any <strong>Pod</strong> with the <strong>&quot;app=MyApp&quot;</strong> label. This <strong>Service</strong> will also be assigned an IP address (sometimes called the &quot;cluster IP&quot;), which is used by the service proxies (see below). The <strong>Service</strong>&#x27;s selector will be evaluated continuously and the results will be POSTed to an <strong>Endpoints</strong> object also named &quot;my-service&quot;.</p><p>Note that a <strong>Service</strong> can map an incoming port to any <strong>targetPort</strong>. By default the <strong>targetPort</strong>will be set to the same value as the <strong>port</strong> field. Perhaps more interesting is that <strong>targetPort</strong> can be a string, referring to the name of a port in the backend <strong>Pods</strong>. The actual port number assigned to that name can be different in each backend <strong>Pod</strong>. This offers a lot of flexibility for deploying and evolving your <strong>Services</strong>. For example, you can change the port number that pods expose in the next version of your backend software, without breaking clients.</p><p>Kubernetes <strong>Services</strong> support <strong>TCP</strong> and <strong>UDP</strong> for protocols. The default is <strong>TCP</strong>.</p><h5><strong>Services without selectors</strong></h5><p>Services generally abstract access to Kubernetes <strong>Pods</strong>, but they can also abstract other kinds of backends. For example:</p><ul><li>You want to have an external database cluster in production, but in test you use your own databases.</li><li>You want to point your service to a service in another <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/"><strong>Namespace</strong></a> or on another cluster.</li><li>You are migrating your workload to Kubernetes and some of your backends run outside of Kubernetes.</li></ul><p>In any of these scenarios you can define a service without a selector:</p><p><strong>kind: Service</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>metadata:</strong></p><p><strong>name: my-service</strong></p><p><strong>spec:</strong></p><p><strong>ports:</strong></p><p><strong>- protocol: TCP</strong></p><p><strong>port: 80</strong></p><p><strong>targetPort: 9376</strong></p><p>Because this service has no selector, the corresponding <strong>Endpoints</strong> object will not be created. You can manually map the service to your own specific endpoints:</p><p><strong>kind: Endpoints</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>metadata:</strong></p><p><strong>name: my-service</strong></p><p><strong>subsets:</strong></p><p><strong>- addresses:</strong></p><p><strong>- ip: 1.2.3.4</strong></p><p><strong>ports:</strong></p><p><strong>- port: 9376</strong></p><p><strong>NOTE:</strong> Endpoint IPs may not be loopback (127.0.0.0/8), link-local (169.254.0.0/16), or link-local multicast (224.0.0.0/24).</p><p>Accessing a <strong>Service</strong> without a selector works the same as if it had a selector. The traffic will be routed to endpoints defined by the user (<strong>1.2.3.4:9376</strong> in this example).</p><p>An ExternalName service is a special case of service that does not have selectors. It does not define any ports or Endpoints. Rather, it serves as a way to return an alias to an external service residing outside the cluster.</p><p><strong>kind: Service</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>metadata:</strong></p><p><strong>name: my-service</strong></p><p><strong>namespace: prod</strong></p><p><strong>spec:</strong></p><p><strong>type: ExternalName</strong></p><p><strong>externalName: my.database.example.com</strong></p><p>When looking up the host <strong>my-service.prod.svc.CLUSTER</strong>, the cluster DNS service will return a <strong>CNAME</strong> record with the value <strong>my.database.example.com</strong>. Accessing such a service works in the same way as others, with the only difference that the redirection happens at the DNS level and no proxying or forwarding occurs. Should you later decide to move your database into your cluster, you can start its pods, add appropriate selectors or endpoints and change the service <strong>type</strong>.</p><h4>Virtual IPs and service proxies</h4><p>Every node in a Kubernetes cluster runs a <strong>kube-proxy</strong>. <strong>kube-proxy</strong> is responsible for implementing a form of virtual IP for <strong>Services</strong> of type other than <strong>ExternalName</strong>. In Kubernetes v1.0, <strong>Services</strong> are a &quot;layer 4&quot; (TCP/UDP over IP) construct, the proxy was purely in userspace. In Kubernetes v1.1, the <strong>Ingress</strong> API was added (beta) to represent &quot;layer 7&quot;(HTTP) services, iptables proxy was added too, and become the default operating mode since Kubernetes v1.2. In Kubernetes v1.8.0-beta.0, ipvs proxy was added.</p><h5><strong>Proxy-mode: userspace</strong></h5><p>In this mode, kube-proxy watches the Kubernetes master for the addition and removal of <strong>Service</strong>and <strong>Endpoints</strong> objects. For each <strong>Service</strong> it opens a port (randomly chosen) on the local node. Any connections to this &quot;proxy port&quot; will be proxied to one of the <strong>Service</strong>&#x27;s backend <strong>Pods</strong> (as reported in <strong>Endpoints</strong>). Which backend <strong>Pod</strong> to use is decided based on the <strong>SessionAffinity</strong> of the <strong>Service</strong>. Lastly, it installs iptables rules which capture traffic to the <strong>Service</strong>&#x27;s <strong>clusterIP</strong>(which is virtual) and <strong>Port</strong> and redirects that traffic to the proxy port which proxies the backend <strong>Pod</strong>. By default, the choice of backend is round robin.</p><p>Note that in the above diagram, <strong>clusterIP</strong> is shown as <strong>ServiceIP</strong>.</p><h5><strong>Proxy-mode: iptables</strong></h5><p>In this mode, kube-proxy watches the Kubernetes master for the addition and removal of <strong>Service</strong>and <strong>Endpoints</strong> objects. For each <strong>Service</strong>, it installs iptables rules which capture traffic to the <strong>Service</strong>&#x27;s <strong>clusterIP</strong> (which is virtual) and <strong>Port</strong> and redirects that traffic to one of the <strong>Service</strong>&#x27;s backend sets. For each <strong>Endpoints</strong> object, it installs iptables rules which select a backend <strong>Pod</strong>. By default, the choice of backend is random.</p><p>Obviously, iptables need not switch back between userspace and kernelspace, it should be faster and more reliable than the userspace proxy. However, unlike the userspace proxier, the iptables proxier cannot automatically retry another <strong>Pod</strong> if the one it initially selects does not respond, so it depends on having working <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#defining-readiness-probes">readiness probes</a>.</p><p>Note that in the above diagram, <strong>clusterIP</strong> is shown as <strong>ServiceIP</strong>.</p><h5><strong>Proxy-mode: ipvs</strong></h5><p><strong>FEATURE STATE:</strong> <strong>Kubernetes v1.9</strong> <a href="https://kubernetes.io/docs/concepts/services-networking/service/">beta</a></p><p>In this mode, kube-proxy watches Kubernetes Services and Endpoints, calls <strong>netlink</strong> interface to create ipvs rules accordingly and syncs ipvs rules with Kubernetes Services and Endpoints periodically, to make sure ipvs status is consistent with the expectation. When Service is accessed, traffic will be redirected to one of the backend Pods.</p><p>Similar to iptables, Ipvs is based on netfilter hook function, but uses hash table as the underlying data structure and works in the kernel space. That means ipvs redirects traffic much faster, and has much better performance when syncing proxy rules. Furthermore, ipvs provides more options for load balancing algorithm, such as:</p><ul><li><strong>rr</strong>: round-robin</li><li><strong>lc</strong>: least connection</li><li><strong>dh</strong>: destination hashing</li><li><strong>sh</strong>: source hashing</li><li><strong>sed</strong>: shortest expected delay</li><li><strong>nq</strong>: never queue</li></ul><p><strong>Note:</strong> ipvs mode assumes IPVS kernel modules are installed on the node before running kube-proxy. When kube-proxy starts with ipvs proxy mode, kube-proxy would validate if IPVS modules are installed on the node, if it&#x27;s not installed kube-proxy will fall back to iptables proxy mode.</p><p>In any of these proxy model, any traffic bound for the Service&#x27;s IP:Port is proxied to an appropriate backend without the clients knowing anything about Kubernetes or Services or Pods. Client-IP based session affinity can be selected by setting <strong>service.spec.sessionAffinity</strong> to &quot;ClientIP&quot; (the default is &quot;None&quot;), and you can set the max session sticky time by setting the field <strong>service.spec.sessionAffinityConfig.clientIP.timeoutSeconds</strong> if you have already set <strong>service.spec.sessionAffinity</strong> to &quot;ClientIP&quot; (the default is &quot;10800&quot;).</p><h4>Multi-Port Services</h4><p>Many <strong>Services</strong> need to expose more than one port. For this case, Kubernetes supports multiple port definitions on a <strong>Service</strong> object. When using multiple ports you must give all of your ports names, so that endpoints can be disambiguated. For example:</p><p><strong>kind: Service</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>metadata:</strong></p><p><strong>name: my-service</strong></p><p><strong>spec:</strong></p><p><strong>selector:</strong></p><p><strong>app: MyApp</strong></p><p><strong>ports:</strong></p><p><strong>- name: http</strong></p><p><strong>protocol: TCP</strong></p><p><strong>port: 80</strong></p><p><strong>targetPort: 9376</strong></p><p><strong>- name: https</strong></p><p><strong>protocol: TCP</strong></p><p><strong>port: 443</strong></p><p><strong>targetPort: 9377</strong></p><h4>Choosing your own IP address</h4><p>You can specify your own cluster IP address as part of a <strong>Service</strong> creation request. To do this, set the <strong>spec.clusterIP</strong> field. For example, if you already have an existing DNS entry that you wish to replace, or legacy systems that are configured for a specific IP address and difficult to re-configure. The IP address that a user chooses must be a valid IP address and within the <strong>service-cluster-ip-range</strong> CIDR range that is specified by flag to the API server. If the IP address value is invalid, the apiserver returns a 422 HTTP status code to indicate that the value is invalid.</p><h5><strong>Why not use round-robin DNS?</strong></h5><p>A question that pops up every now and then is why we do all this stuff with virtual IPs rather than just use standard round-robin DNS. There are a few reasons:</p><ul><li>There is a long history of DNS libraries not respecting DNS TTLs and caching the results of name lookups.</li><li>Many apps do DNS lookups once and cache the results.</li><li>Even if apps and libraries did proper re-resolution, the load of every client re-resolving DNS over and over would be difficult to manage.</li></ul><p>We try to discourage users from doing things that hurt themselves. That said, if enough people ask for this, we may implement it as an alternative.</p><h4>Discovering services</h4><p>Kubernetes supports 2 primary modes of finding a <strong>Service</strong> - environment variables and DNS.</p><h5><strong>Environment variables</strong></h5><p>When a <strong>Pod</strong> is run on a <strong>Node</strong>, the kubelet adds a set of environment variables for each active <strong>Service</strong>. It supports both <a href="https://docs.docker.com/userguide/dockerlinks/">Docker links compatible</a> variables (see <a href="http://releases.k8s.io/master/pkg/kubelet/envvars/envvars.go#L49">makeLinkVariables</a>) and simpler <strong>{SVCNAME}<!-- -->_<!-- -->SERVICE_HOST</strong> and <strong>{SVCNAME}<!-- -->_<!-- -->SERVICE_PORT</strong> variables, where the Service name is upper-cased and dashes are converted to underscores.</p><p>For example, the Service <strong>&quot;redis-master&quot;</strong> which exposes TCP port 6379 and has been allocated cluster IP address 10.0.0.11 produces the following environment variables:</p><p><strong>REDIS_MASTER_SERVICE_HOST=10.0.0.11</strong></p><p><strong>REDIS_MASTER_SERVICE_PORT=6379</strong></p><p><strong>REDIS_MASTER_PORT=tcp://10.0.0.11:6379</strong></p><p><strong>REDIS_MASTER_PORT_6379_TCP=tcp://10.0.0.11:6379</strong></p><p><strong>REDIS_MASTER_PORT_6379_TCP_PROTO=tcp</strong></p><p><strong>REDIS_MASTER_PORT_6379_TCP_PORT=6379</strong></p><p><strong>REDIS_MASTER_PORT_6379_TCP_ADDR=10.0.0.11</strong></p><p>This does imply an ordering requirement - any <strong>Service</strong> that a <strong>Pod</strong> wants to access must be created before the <strong>Pod</strong> itself, or else the environment variables will not be populated. DNS does not have this restriction.</p><h5><strong>DNS</strong></h5><p>An optional (though strongly recommended) <a href="https://kubernetes.io/docs/concepts/cluster-administration/addons/">cluster add-on</a> is a DNS server. The DNS server watches the Kubernetes API for new <strong>Services</strong> and creates a set of DNS records for each. If DNS has been enabled throughout the cluster then all <strong>Pods</strong> should be able to do name resolution of <strong>Services</strong> automatically.</p><p>For example, if you have a <strong>Service</strong> called <strong>&quot;my-service&quot;</strong> in Kubernetes <strong>Namespace</strong> <strong>&quot;my-ns&quot;</strong> a DNS record for <strong>&quot;my-service.my-ns&quot;</strong> is created. <strong>Pods</strong> which exist in the <strong>&quot;my-ns&quot;</strong> <strong>Namespace</strong>should be able to find it by simply doing a name lookup for <strong>&quot;my-service&quot;</strong>. <strong>Pods</strong> which exist in other <strong>Namespaces</strong> must qualify the name as <strong>&quot;my-service.my-ns&quot;</strong>. The result of these name lookups is the cluster IP.</p><p>Kubernetes also supports DNS SRV (service) records for named ports. If the <strong>&quot;my-service.my-ns&quot;</strong> <strong>Service</strong> has a port named <strong>&quot;http&quot;</strong> with protocol <strong>TCP</strong>, you can do a DNS SRV query for <strong>&quot;<!-- -->_<!-- -->http.<!-- -->_<!-- -->tcp.my-service.my-ns&quot;</strong> to discover the port number for <strong>&quot;http&quot;</strong>.</p><p>The Kubernetes DNS server is the only way to access services of type <strong>ExternalName</strong>. More information is available in the <a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/">DNS Pods and Services</a>.</p><h4>Headless services</h4><p>Sometimes you don&#x27;t need or want load-balancing and a single service IP. In this case, you can create &quot;headless&quot; services by specifying <strong>&quot;None&quot;</strong> for the cluster IP (<strong>spec.clusterIP</strong>).</p><p>This option allows developers to reduce coupling to the Kubernetes system by allowing them freedom to do discovery their own way. Applications can still use a self-registration pattern and adapters for other discovery systems could easily be built upon this API.</p><p>For such <strong>Services</strong>, a cluster IP is not allocated, kube-proxy does not handle these services, and there is no load balancing or proxying done by the platform for them. How DNS is automatically configured depends on whether the service has selectors defined.</p><h5><strong>With selectors</strong></h5><p>For headless services that define selectors, the endpoints controller creates <strong>Endpoints</strong> records in the API, and modifies the DNS configuration to return A records (addresses) that point directly to the <strong>Pods</strong> backing the <strong>Service</strong>.</p><h5><strong>Without selectors</strong></h5><p>For headless services that do not define selectors, the endpoints controller does not create <strong>Endpoints</strong> records. However, the DNS system looks for and configures either:</p><ul><li>CNAME records for <strong>ExternalName</strong>-type services.</li><li>A records for any <strong>Endpoints</strong> that share a name with the service, for all other types.</li></ul><h4>Publishing services - service types</h4><p>For some parts of your application (e.g. frontends) you may want to expose a Service onto an external (outside of your cluster) IP address.</p><p>Kubernetes <strong>ServiceTypes</strong> allow you to specify what kind of service you want. The default is <strong>ClusterIP</strong>.</p><p><strong>Type</strong> values and their behaviors are:</p><ul><li><strong>ClusterIP</strong>: Exposes the service on a cluster-internal IP. Choosing this value makes the service only reachable from within the cluster. This is the default <strong>ServiceType</strong>.</li><li><strong>NodePort</strong>: Exposes the service on each Node&#x27;s IP at a static port (the <strong>NodePort</strong>). A <strong>ClusterIP</strong>service, to which the <strong>NodePort</strong> service will route, is automatically created. You&#x27;ll be able to contact the <strong>NodePort</strong> service, from outside the cluster, by requesting <strong><code>&lt;NodeIP&gt;:&lt;NodePort&gt;</code></strong>.</li><li><strong>LoadBalancer</strong>: Exposes the service externally using a cloud provider&#x27;s load balancer. <strong>NodePort</strong>and <strong>ClusterIP</strong> services, to which the external load balancer will route, are automatically created.</li><li><strong>ExternalName</strong>: Maps the service to the contents of the <strong>externalName</strong> field (e.g. <strong>foo.bar.example.com</strong>), by returning a <strong>CNAME</strong> record with its value. No proxying of any kind is set up. This requires version 1.7 or higher of <strong>kube-dns</strong>.</li></ul><h5><strong>Type NodePort</strong></h5><p>If you set the <strong>type</strong> field to <strong>&quot;NodePort&quot;</strong>, the Kubernetes master will allocate a port from a flag-configured range (default: 30000-32767), and each Node will proxy that port (the same port number on every Node) into your <strong>Service</strong>. That port will be reported in your <strong>Service</strong>&#x27;s <strong>spec.ports<!-- -->[*]<!-- -->.nodePort</strong> field.</p><p>If you want to specify particular IP(s) to proxy the port, you can set the <strong>--nodeport-addresses</strong> flag in kube-proxy to particular IP block(s) (which is supported since Kubernetes v1.10). A comma-delimited list of IP blocks (e.g. 10.0.0.0/8, 1.2.3.4/32) is used to filter addresses local to this node. For example, if you start kube-proxy with flag <strong>--nodeport-addresses=127.0.0.0/8</strong>, kube-proxy will select only the loopback interface for NodePort Services. The <strong>--nodeport-addresses</strong> is defaulted to empty (<strong>[]</strong>), which means select all available interfaces and is in compliance with current NodePort behaviors.</p><p>If you want a specific port number, you can specify a value in the <strong>nodePort</strong> field, and the system will allocate you that port or else the API transaction will fail (i.e. you need to take care about possible port collisions yourself). The value you specify must be in the configured range for node ports.</p><p>This gives developers the freedom to set up their own load balancers, to configure environments that are not fully supported by Kubernetes, or even to just expose one or more nodes&#x27; IPs directly.</p><p>Note that this Service will be visible as both <strong><code>&lt;NodeIP&gt;</code>:spec.ports<!-- -->[*]<!-- -->.nodePort</strong> and <strong>spec.clusterIP:spec.ports<!-- -->[*]<!-- -->.port</strong>. (If the <strong>--nodeport-addresses</strong> flag in kube-proxy is set,would be filtered NodeIP(s).)</p><h5><strong>Type LoadBalancer</strong></h5><p>On cloud providers which support external load balancers, setting the <strong>type</strong> field to <strong>&quot;LoadBalancer&quot;</strong> will provision a load balancer for your <strong>Service</strong>. The actual creation of the load balancer happens asynchronously, and information about the provisioned balancer will be published in the <strong>Service</strong>&#x27;s <strong>status.loadBalancer</strong> field. For example:</p><p><strong>kind: Service</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>metadata:</strong></p><p><strong>name: my-service</strong></p><p><strong>spec:</strong></p><p><strong>selector:</strong></p><p><strong>app: MyApp</strong></p><p><strong>ports:</strong></p><p><strong>- protocol: TCP</strong></p><p><strong>port: 80</strong></p><p><strong>targetPort: 9376</strong></p><p><strong>clusterIP: 10.0.171.239</strong></p><p><strong>loadBalancerIP: 78.11.24.19</strong></p><p><strong>type: LoadBalancer</strong></p><p><strong>status:</strong></p><p><strong>loadBalancer:</strong></p><p><strong>ingress:</strong></p><p><strong>- ip: 146.148.47.155</strong></p><p>Traffic from the external load balancer will be directed at the backend <strong>Pods</strong>, though exactly how that works depends on the cloud provider. Some cloud providers allow the <strong>loadBalancerIP</strong> to be specified. In those cases, the load-balancer will be created with the user-specified <strong>loadBalancerIP</strong>. If the <strong>loadBalancerIP</strong> field is not specified, an ephemeral IP will be assigned to the loadBalancer. If the <strong>loadBalancerIP</strong> is specified, but the cloud provider does not support the feature, the field will be ignored.</p><p><strong>Special notes for Azure</strong>: To use user-specified public type <strong>loadBalancerIP</strong>, a static type public IP address resource needs to be created first, and it should be in the same resource group of the cluster. Specify the assigned IP address as loadBalancerIP. Verify you have securityGroupName in the cloud provider configuration file.</p><h6><strong>Internal load balancer</strong></h6><p>In a mixed environment it is sometimes necessary to route traffic from services inside the same VPC.</p><p>In a split-horizon DNS environment you would need two services to be able to route both external and internal traffic to your endpoints.</p><p>This can be achieved by adding the following annotations to the service based on cloud provider.</p><ul><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#tabset-0">Default</a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#tabset-1">GCP</a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#tabset-2">AWS</a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#tabset-3">Azure</a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#tabset-4">OpenStack</a></li></ul><p>Select one of the tabs.</p><h6><strong>SSL support on AWS</strong></h6><p>For partial SSL support on clusters running on AWS, starting with 1.3 three annotations can be added to a <strong>LoadBalancer</strong> service:</p><p><strong>metadata:</strong></p><p><strong>name: my-service</strong></p><p><strong>annotations:</strong></p><p><strong>service.beta.kubernetes.io/aws-load-balancer-ssl-cert: arn:aws:acm:us-east-1:123456789012:certificate/12345678-1234-1234-1234-123456789012</strong></p><p>The first specifies the ARN of the certificate to use. It can be either a certificate from a third party issuer that was uploaded to IAM or one created within AWS Certificate Manager.</p><p><strong>metadata:</strong></p><p><strong>name: my-service</strong></p><p><strong>annotations:</strong></p><p><strong>service.beta.kubernetes.io/aws-load-balancer-backend-protocol: (https|http|ssl|tcp)</strong></p><p>The second annotation specifies which protocol a pod speaks. For HTTPS and SSL, the ELB will expect the pod to authenticate itself over the encrypted connection.</p><p>HTTP and HTTPS will select layer 7 proxying: the ELB will terminate the connection with the user, parse headers and inject the <strong>X-Forwarded-For</strong> header with the user&#x27;s IP address (pods will only see the IP address of the ELB at the other end of its connection) when forwarding requests.</p><p>TCP and SSL will select layer 4 proxying: the ELB will forward traffic without modifying the headers.</p><p>In a mixed-use environment where some ports are secured and others are left unencrypted, the following annotations may be used:</p><p><strong>metadata:</strong></p><p><strong>name: my-service</strong></p><p><strong>annotations:</strong></p><p><strong>service.beta.kubernetes.io/aws-load-balancer-backend-protocol: http</strong></p><p><strong>service.beta.kubernetes.io/aws-load-balancer-ssl-ports: &quot;443,8443&quot;</strong></p><p>In the above example, if the service contained three ports, <strong>80</strong>, <strong>443</strong>, and <strong>8443</strong>, then <strong>443</strong> and <strong>8443</strong>would use the SSL certificate, but <strong>80</strong> would just be proxied HTTP.</p><p>Beginning in 1.9, services can use <a href="http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-security-policy-table.html">predefined AWS SSL policies</a> for any HTTPS or SSL listeners. To see which policies are available for use, run the awscli command:</p><p><strong>aws elb describe-load-balancer-policies --query \&#x27;PolicyDescriptions[].PolicyName\&#x27;</strong></p><p>Any one of those policies can then be specified using the &quot;<strong>service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy</strong>&quot; annotation, for example:</p><p><strong>metadata:</strong></p><p><strong>name: my-service</strong></p><p><strong>annotations:</strong></p><p><strong>service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy: &quot;ELBSecurityPolicy-TLS-1-2-2017-01&quot;</strong></p><h6><strong>PROXY protocol support on AWS</strong></h6><p>To enable <a href="https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt">PROXY protocol</a> support for clusters running on AWS, you can use the following service annotation:</p><p><strong>metadata:</strong></p><p><strong>name: my-service</strong></p><p><strong>annotations:</strong></p><p><strong>service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: &quot;<!-- -->*<!-- -->&quot;</strong></p><p>Since version 1.3.0 the use of this annotation applies to all ports proxied by the ELB and cannot be configured otherwise.</p><h6><strong>ELB Access Logs on AWS</strong></h6><p>There are several annotations to manage access logs for ELB services on AWS.</p><p>The annotation <strong>service.beta.kubernetes.io/aws-load-balancer-access-log-enabled</strong>controls whether access logs are enabled.</p><p>The annotation <strong>service.beta.kubernetes.io/aws-load-balancer-access-log-emit-interval</strong>controls the interval in minutes for publishing the access logs. You can specify an interval of either 5 or 60.</p><p>The annotation <strong>service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name</strong> controls the name of the Amazon S3 bucket where load balancer access logs are stored.</p><p>The annotation <strong>service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-prefix</strong> specifies the logical hierarchy you created for your Amazon S3 bucket.</p><p><strong>metadata:</strong></p><p><strong>name: my-service</strong></p><p><strong>annotations:</strong></p><p><strong>service.beta.kubernetes.io/aws-load-balancer-access-log-enabled: &quot;true&quot;</strong></p><p><strong><em># Specifies whether access logs are enabled for the load balancer</em></strong></p><p><strong>service.beta.kubernetes.io/aws-load-balancer-access-log-emit-interval: &quot;60&quot;</strong></p><p><strong><em># The interval for publishing the access logs. You can specify an interval of either 5 or 60 (minutes).</em></strong></p><p><strong>service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name: &quot;my-bucket&quot;</strong></p><p><strong><em># The name of the Amazon S3 bucket where the access logs are stored</em></strong></p><p><strong>service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-prefix: &quot;my-bucket-prefix/prod&quot;</strong></p><p><strong><em># The logical hierarchy you created for your Amazon S3 bucket, for example <code>my-bucket-prefix/prod</code></em></strong></p><h6><strong>Connection Draining on AWS</strong></h6><p>Connection draining for Classic ELBs can be managed with the annotation <strong>service.beta.kubernetes.io/aws-load-balancer-connection-draining-enabled</strong> set to the value of <strong>&quot;true&quot;</strong>. The annotation <strong>service.beta.kubernetes.io/aws-load-balancer-connection-draining-timeout</strong> can also be used to set maximum time, in seconds, to keep the existing connections open before deregistering the instances.</p><p><strong>metadata:</strong></p><p><strong>name: my-service</strong></p><p><strong>annotations:</strong></p><p><strong>service.beta.kubernetes.io/aws-load-balancer-connection-draining-enabled: &quot;true&quot;</strong></p><p><strong>service.beta.kubernetes.io/aws-load-balancer-connection-draining-timeout: &quot;60&quot;</strong></p><h6><strong>Other ELB annotations</strong></h6><p>There are other annotations to manage Classic Elastic Load Balancers that are described below.</p><p><strong>metadata:</strong></p><p><strong>name: my-service</strong></p><p><strong>annotations:</strong></p><p><strong>service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: &quot;60&quot;</strong></p><p><strong><em># The time, in seconds, that the connection is allowed to be idle (no data has been sent over the connection) before it is closed by the load balancer</em></strong></p><p><strong>service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: &quot;true&quot;</strong></p><p><strong><em># Specifies whether cross-zone load balancing is enabled for the load balancer</em></strong></p><p><strong>service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags: &quot;environment=prod,owner=devops&quot;</strong></p><p><strong><em># A comma-separated list of key-value pairs which will be recorded as</em></strong></p><p><strong><em># additional tags in the ELB.</em></strong></p><p><strong>service.beta.kubernetes.io/aws-load-balancer-healthcheck-healthy-threshold: &quot;&quot;</strong></p><p><strong><em># The number of successive successful health checks required for a backend to</em></strong></p><p><strong><em># be considered healthy for traffic. Defaults to 2, must be between 2 and 10</em></strong></p><p><strong>service.beta.kubernetes.io/aws-load-balancer-healthcheck-unhealthy-threshold: &quot;3&quot;</strong></p><p><strong><em># The number of unsuccessful health checks required for a backend to be</em></strong></p><p><strong><em># considered unhealthy for traffic. Defaults to 6, must be between 2 and 10</em></strong></p><p><strong>service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval: &quot;20&quot;</strong></p><p><strong><em># The approximate interval, in seconds, between health checks of an</em></strong></p><p><strong><em># individual instance. Defaults to 10, must be between 5 and 300</em></strong></p><p><strong>service.beta.kubernetes.io/aws-load-balancer-healthcheck-timeout: &quot;5&quot;</strong></p><p><strong><em># The amount of time, in seconds, during which no response means a failed</em></strong></p><p><strong><em># health check. This value must be less than the service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval</em></strong></p><p><strong><em># value. Defaults to 5, must be between 2 and 60</em></strong></p><p><strong>service.beta.kubernetes.io/aws-load-balancer-extra-security-groups: &quot;sg-53fae93f,sg-42efd82e&quot;</strong></p><p><strong><em># A list of additional security groups to be added to ELB</em></strong></p><h6><strong>Network Load Balancer support on AWS <!-- -->[alpha]</strong></h6><p><strong>Warning:</strong> This is an alpha feature and not recommended for production clusters yet.</p><p>Starting in version 1.9.0, Kubernetes supports Network Load Balancer (NLB). To use a Network Load Balancer on AWS, use the annotation <strong>service.beta.kubernetes.io/aws-load-balancer-type</strong>with the value set to <strong>nlb</strong>.</p><p><strong>metadata:</strong></p><p><strong>name: my-service</strong></p><p><strong>annotations:</strong></p><p><strong>service.beta.kubernetes.io/aws-load-balancer-type: &quot;nlb&quot;</strong></p><p>Unlike Classic Elastic Load Balancers, Network Load Balancers (NLBs) forward the client&#x27;s IP through to the node. If a service&#x27;s <strong>spec.externalTrafficPolicy</strong> is set to <strong>Cluster</strong>, the client&#x27;s IP address will not be propagated to the end pods.</p><p>By setting <strong>spec.externalTrafficPolicy</strong> to <strong>Local</strong>, client IP addresses will be propagated to the end pods, but this could result in uneven distribution of traffic. Nodes without any pods for a particular LoadBalancer service will fail the NLB Target Group&#x27;s health check on the auto-assigned <strong>spec.healthCheckNodePort</strong> and not receive any traffic.</p><p>In order to achieve even traffic, either use a DaemonSet, or specify a <a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity-beta-feature">pod anti-affinity</a> to not locate pods on the same node.</p><p>NLB can also be used with the <a href="https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer">internal load balancer</a> annotation.</p><p>In order for client traffic to reach instances behind an NLB, the Node security groups are modified with the following IP rules:</p><p>  Rule             Protocol   Port(s)                                                                                IpRange(s)                                                     IpRange Description</p><hr/><p>  Health Check     TCP        NodePort(s) (<strong>spec.healthCheckNodePort</strong>for <strong>spec.externalTrafficPolicy = Local</strong>)   VPC CIDR                                                       kubernetes.io/rule/nlb/health=<code style="background-color:lightgray">&lt;loadBalancerName&gt;</code>
Client Traffic   TCP        NodePort(s)                                                                            <strong>spec.loadBalancerSourceRanges</strong>(defaults to <strong>0.0.0.0/0</strong>)   kubernetes.io/rule/nlb/client=<code style="background-color:lightgray">&lt;loadBalancerName&gt;</code>
MTU Discovery    ICMP       3,4                                                                                    <strong>spec.loadBalancerSourceRanges</strong>(defaults to <strong>0.0.0.0/0</strong>)   kubernetes.io/rule/nlb/mtu=<code style="background-color:lightgray">&lt;loadBalancerName&gt;</code></p><p>Be aware that if <strong>spec.loadBalancerSourceRanges</strong> is not set, Kubernetes will allow traffic from <strong>0.0.0.0/0</strong> to the Node Security Group(s). If nodes have public IP addresses, be aware that non-NLB traffic can also reach all instances in those modified security groups.</p><p>In order to limit which client IP&#x27;s can access the Network Load Balancer, specify <strong>loadBalancerSourceRanges</strong>.</p><p><strong>spec:</strong></p><p><strong>loadBalancerSourceRanges:</strong></p><p><strong>- &quot;143.231.0.0/16&quot;</strong></p><p><strong>Note:</strong> NLB only works with certain instance classes, see the <a href="http://docs.aws.amazon.com/elasticloadbalancing/latest/network/target-group-register-targets.html#register-deregister-targets">AWS documentation</a> for supported instance types.</p><h5><strong>External IPs</strong></h5><p>If there are external IPs that route to one or more cluster nodes, Kubernetes services can be exposed on those <strong>externalIPs</strong>. Traffic that ingresses into the cluster with the external IP (as destination IP), on the service port, will be routed to one of the service endpoints. <strong>externalIPs</strong> are not managed by Kubernetes and are the responsibility of the cluster administrator.</p><p>In the <strong>ServiceSpec</strong>, <strong>externalIPs</strong> can be specified along with any of the <strong>ServiceTypes</strong>. In the example below, &quot;<strong>my-service</strong>&quot; can be accessed by clients on &quot;<strong>80.11.12.10:80</strong>&quot;&quot; (<strong>externalIP:port</strong>)</p><p><strong>kind: Service</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>metadata:</strong></p><p><strong>name: my-service</strong></p><p><strong>spec:</strong></p><p><strong>selector:</strong></p><p><strong>app: MyApp</strong></p><p><strong>ports:</strong></p><p><strong>- name: http</strong></p><p><strong>protocol: TCP</strong></p><p><strong>port: 80</strong></p><p><strong>targetPort: 9376</strong></p><p><strong>externalIPs:</strong></p><p><strong>- 80.11.12.10</strong></p><h4>Shortcomings</h4><p>Using the userspace proxy for VIPs will work at small to medium scale, but will not scale to very large clusters with thousands of Services. See <a href="http://issue.k8s.io/1107">the original design proposal for portals</a> for more details.</p><p>Using the userspace proxy obscures the source-IP of a packet accessing a <strong>Service</strong>. This makes some kinds of firewalling impossible. The iptables proxier does not obscure in-cluster source IPs, but it does still impact clients coming through a load-balancer or node-port.</p><p>The <strong>Type</strong> field is designed as nested functionality - each level adds to the previous. This is not strictly required on all cloud providers (e.g. Google Compute Engine does not need to allocate a <strong>NodePort</strong> to make <strong>LoadBalancer</strong> work, but AWS does) but the current API requires it.</p><h4>Future work</h4><p>In the future we envision that the proxy policy can become more nuanced than simple round robin balancing, for example master-elected or sharded. We also envision that some <strong>Services</strong> will have &quot;real&quot; load balancers, in which case the VIP will simply transport the packets there.</p><p>We intend to improve our support for L7 (HTTP) <strong>Services</strong>.</p><p>We intend to have more flexible ingress modes for <strong>Services</strong> which encompass the current <strong>ClusterIP</strong>, <strong>NodePort</strong>, and <strong>LoadBalancer</strong> modes and more.</p><h4>The gory details of virtual IPs</h4><p>The previous information should be sufficient for many people who just want to use <strong>Services</strong>. However, there is a lot going on behind the scenes that may be worth understanding.</p><h5><strong>Avoiding collisions</strong></h5><p>One of the primary philosophies of Kubernetes is that users should not be exposed to situations that could cause their actions to fail through no fault of their own. In this situation, we are looking at network ports - users should not have to choose a port number if that choice might collide with another user. That is an isolation failure.</p><p>In order to allow users to choose a port number for their <strong>Services</strong>, we must ensure that no two <strong>Services</strong> can collide. We do that by allocating each <strong>Service</strong> its own IP address.</p><p>To ensure each service receives a unique IP, an internal allocator atomically updates a global allocation map in etcd prior to creating each service. The map object must exist in the registry for services to get IPs, otherwise creations will fail with a message indicating an IP could not be allocated. A background controller is responsible for creating that map (to migrate from older versions of Kubernetes that used in memory locking) as well as checking for invalid assignments due to administrator intervention and cleaning up any IPs that were allocated but which no service currently uses.</p><h5><strong>IPs and VIPs</strong></h5><p>Unlike <strong>Pod</strong> IP addresses, which actually route to a fixed destination, <strong>Service</strong> IPs are not actually answered by a single host. Instead, we use <strong>iptables</strong> (packet processing logic in Linux) to define virtual IP addresses which are transparently redirected as needed. When clients connect to the VIP, their traffic is automatically transported to an appropriate endpoint. The environment variables and DNS for <strong>Services</strong> are actually populated in terms of the <strong>Service</strong>&#x27;s VIP and port.</p><p>We support three proxy modes - userspace, iptables and ipvs which operate slightly differently.</p><h6><strong>Userspace</strong></h6><p>As an example, consider the image processing application described above. When the backend <strong>Service</strong> is created, the Kubernetes master assigns a virtual IP address, for example 10.0.0.1. Assuming the <strong>Service</strong> port is 1234, the <strong>Service</strong> is observed by all of the <strong>kube-proxy</strong> instances in the cluster. When a proxy sees a new <strong>Service</strong>, it opens a new random port, establishes an iptables redirect from the VIP to this new port, and starts accepting connections on it.</p><p>When a client connects to the VIP the iptables rule kicks in, and redirects the packets to the <strong>Service proxy</strong>&#x27;s own port. The <strong>Service proxy</strong> chooses a backend, and starts proxying traffic from the client to the backend.</p><p>This means that <strong>Service</strong> owners can choose any port they want without risk of collision. Clients can simply connect to an IP and port, without being aware of which <strong>Pods</strong> they are actually accessing.</p><h6><strong>Iptables</strong></h6><p>Again, consider the image processing application described above. When the backend <strong>Service</strong> is created, the Kubernetes master assigns a virtual IP address, for example 10.0.0.1. Assuming the <strong>Service</strong> port is 1234, the <strong>Service</strong> is observed by all of the <strong>kube-proxy</strong> instances in the cluster. When a proxy sees a new <strong>Service</strong>, it installs a series of iptables rules which redirect from the VIP to per-<strong>Service</strong> rules. The per-<strong>Service</strong> rules link to per-<strong>Endpoint</strong> rules which redirect (Destination NAT) to the backends.</p><p>When a client connects to the VIP the iptables rule kicks in. A backend is chosen (either based on session affinity or randomly) and packets are redirected to the backend. Unlike the userspace proxy, packets are never copied to userspace, the kube-proxy does not have to be running for the VIP to work, and the client IP is not altered.</p><p>This same basic flow executes when traffic comes in through a node-port or through a load-balancer, though in those cases the client IP does get altered.</p><h6><strong>Ipvs</strong></h6><p>Iptables operations slow down dramatically in large scale cluster e.g 10,000 Services. IPVS is designed for load balancing and based on in-kernel hash tables. So we can achieve performance consistency in large number of services from IPVS-based kube-proxy. Meanwhile, IPVS-based kube-proxy has more sophisticated load balancing algorithms (least conns, locality, weighted, persistence).</p><h4>API Object</h4><p>Service is a top-level resource in the Kubernetes REST API. More details about the API object can be found at: <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#service-v1-core">Service API object</a>.</p><h4>For More Information</h4><p>Read <a href="https://kubernetes.io/docs/tasks/access-application-cluster/connecting-frontend-backend/">Connecting a Front End to a Back End Using a Service</a>.</p><h3>DNS for Services and Pods</h3><p>This page provides an overview of DNS support by Kubernetes.</p><ul><li><a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#introduction"><strong>Introduction</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#what-things-get-dns-names"><strong>What things get DNS names?</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#services"><strong>Services</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#a-records"><strong>A records</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#srv-records"><strong>SRV records</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pods"><strong>Pods</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#a-records-1"><strong>A Records</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pods-hostname-and-subdomain-fields"><strong>Pod&#x27;s hostname and subdomain fields</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pods-dns-policy"><strong>Pod&#x27;s DNS Policy</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pods-dns-config"><strong>Pod&#x27;s DNS Config</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Introduction</h4><p>Kubernetes DNS schedules a DNS Pod and Service on the cluster, and configures the kubelets to tell individual containers to use the DNS Service&#x27;s IP to resolve DNS names.</p><h5><strong>What things get DNS names?</strong></h5><p>Every Service defined in the cluster (including the DNS server itself) is assigned a DNS name. By default, a client Pod&#x27;s DNS search list will include the Pod&#x27;s own namespace and the cluster&#x27;s default domain. This is best illustrated by example:</p><p>Assume a Service named <strong>foo</strong> in the Kubernetes namespace <strong>bar</strong>. A Pod running in namespace <strong>bar</strong> can look up this service by simply doing a DNS query for <strong>foo</strong>. A Pod running in namespace <strong>quux</strong> can look up this service by doing a DNS query for <strong>foo.bar</strong>.</p><p>The following sections detail the supported record types and layout that is supported. Any other layout or names or queries that happen to work are considered implementation details and are subject to change without warning. For more up-to-date specification, see <a href="https://github.com/kubernetes/dns/blob/master/docs/specification.md">Kubernetes DNS-Based Service Discovery</a>.</p><h4>Services</h4><h5><strong>A records</strong></h5><p>&quot;Normal&quot; (not headless) Services are assigned a DNS A record for a name of the form <strong>my-svc.my-namespace.svc.cluster.local</strong>. This resolves to the cluster IP of the Service.</p><p>&quot;Headless&quot; (without a cluster IP) Services are also assigned a DNS A record for a name of the form <strong>my-svc.my-namespace.svc.cluster.local</strong>. Unlike normal Services, this resolves to the set of IPs of the pods selected by the Service. Clients are expected to consume the set or else use standard round-robin selection from the set.</p><h5><strong>SRV records</strong></h5><p>SRV Records are created for named ports that are part of normal or <a href="https://kubernetes.io/docs/concepts/services-networking/service/#headless-services">Headless Services</a>. For each named port, the SRV record would have the form <strong>_<!-- -->my-port-name.<!-- -->_<!-- -->my-port-protocol.my-svc.my-namespace.svc.cluster.local</strong>. For a regular service, this resolves to the port number and the CNAME: <strong>my-svc.my-namespace.svc.cluster.local</strong>. For a headless service, this resolves to multiple answers, one for each pod that is backing the service, and contains the port number and a CNAME of the pod of the form <strong>auto-generated-name.my-svc.my-namespace.svc.cluster.local</strong>.</p><h4>Pods</h4><h5><strong>A Records</strong></h5><p>When enabled, pods are assigned a DNS A record in the form of &quot;<strong>pod-ip-address.my-namespace.pod.cluster.local</strong>&quot;.</p><p>For example, a pod with IP <strong>1.2.3.4</strong> in the namespace <strong>default</strong> with a DNS name of <strong>cluster.local</strong> would have an entry: <strong>1-2-3-4.default.pod.cluster.local</strong>.</p><h5><strong>Pod&#x27;s hostname and subdomain fields</strong></h5><p>Currently when a pod is created, its hostname is the Pod&#x27;s <strong>metadata.name</strong> value.</p><p>The Pod spec has an optional <strong>hostname</strong> field, which can be used to specify the Pod&#x27;s hostname. When specified, it takes precedence over the Pod&#x27;s name to be the hostname of the pod. For example, given a Pod with <strong>hostname</strong> set to &quot;<strong>my-host</strong>&quot;, the Pod will have its hostname set to &quot;<strong>my-host</strong>&quot;.</p><p>The Pod spec also has an optional <strong>subdomain</strong> field which can be used to specify its subdomain. For example, a Pod with <strong>hostname</strong> set to &quot;<strong>foo</strong>&quot;, and <strong>subdomain</strong> set to &quot;<strong>bar</strong>&quot;, in namespace &quot;<strong>my-namespace</strong>&quot;, will have the fully qualified domain name (FQDN) &quot;<strong>foo.bar.my-namespace.svc.cluster.local</strong>&quot;.</p><p>Example:</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Service</strong></p><p><strong>metadata:</strong></p><p><strong>name: default-subdomain</strong></p><p><strong>spec:</strong></p><p><strong>selector:</strong></p><p><strong>name: busybox</strong></p><p><strong>clusterIP: None</strong></p><p><strong>ports:</strong></p><p><strong>- name: foo <em># Actually, no port is needed.</em></strong></p><p><strong>port: 1234</strong></p><p><strong>targetPort: 1234</strong></p><p><strong>---</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: busybox1</strong></p><p><strong>labels:</strong></p><p><strong>name: busybox</strong></p><p><strong>spec:</strong></p><p><strong>hostname: busybox-1</strong></p><p><strong>subdomain: default-subdomain</strong></p><p><strong>containers:</strong></p><p><strong>- image: busybox</strong></p><p><strong>command:</strong></p><p><strong>- sleep</strong></p><p><strong>- &quot;3600&quot;</strong></p><p><strong>name: busybox</strong></p><p><strong>---</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: busybox2</strong></p><p><strong>labels:</strong></p><p><strong>name: busybox</strong></p><p><strong>spec:</strong></p><p><strong>hostname: busybox-2</strong></p><p><strong>subdomain: default-subdomain</strong></p><p><strong>containers:</strong></p><p><strong>- image: busybox</strong></p><p><strong>command:</strong></p><p><strong>- sleep</strong></p><p><strong>- &quot;3600&quot;</strong></p><p><strong>name: busybox</strong></p><p>If there exists a headless service in the same namespace as the pod and with the same name as the subdomain, the cluster&#x27;s KubeDNS Server also returns an A record for the Pod&#x27;s fully qualified hostname. For example, given a Pod with the hostname set to &quot;<strong>busybox-1</strong>&quot; and the subdomain set to &quot;<strong>default-subdomain</strong>&quot;, and a headless Service named &quot;<strong>default-subdomain</strong>&quot; in the same namespace, the pod will see its own FQDN as &quot;<strong>busybox-1.default-subdomain.my-namespace.svc.cluster.local</strong>&quot;. DNS serves an A record at that name, pointing to the Pod&#x27;s IP. Both pods &quot;<strong>busybox1</strong>&quot; and &quot;<strong>busybox2</strong>&quot; can have their distinct A records.</p><p>The Endpoints object can specify the <strong>hostname</strong> for any endpoint addresses, along with its IP.</p><h5><strong>Pod&#x27;s DNS Policy</strong></h5><p>DNS policies can be set on a per-pod basis. Currently Kubernetes supports the following pod-specific DNS policies. These policies are specified in the <strong>dnsPolicy</strong> field of a Pod Spec.</p><ul><li>&quot;<strong>Default</strong>&quot;: The Pod inherits the name resolution configuration from the node that the pods run on. See <a href="https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/#inheriting-dns-from-the-node">related discussion</a> for more details.</li><li>&quot;<strong>ClusterFirst</strong>&quot;: Any DNS query that does not match the configured cluster domain suffix, such as &quot;<strong><a href="http://www.kubernetes.io">www.kubernetes.io</a></strong>&quot;, is forwarded to the upstream nameserver inherited from the node. Cluster administrators may have extra stub-domain and upstream DNS servers configured. See <a href="https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/#impacts-on-pods">related discussion</a> for details on how DNS queries are handled in those cases.</li><li>&quot;<strong>ClusterFirstWithHostNet</strong>&quot;: For Pods running with hostNetwork, you should explicitly set its DNS policy &quot;<strong>ClusterFirstWithHostNet</strong>&quot;.</li><li>&quot;<strong>None</strong>&quot;: A new option value introduced in Kubernetes v1.9 (Beta in v1.10). It allows a Pod to ignore DNS settings from the Kubernetes environment. All DNS settings are supposed to be provided using the <strong>dnsConfig</strong> field in the Pod Spec. See <a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#dns-config">DNS config</a> subsection below.</li></ul><p><strong>NOTE:</strong> &quot;Default&quot; is not the default DNS policy. If <strong>dnsPolicy</strong> is not explicitly specified, then &quot;ClusterFirst&quot; is used.</p><p>The example below shows a Pod with its DNS policy set to &quot;<strong>ClusterFirstWithHostNet</strong>&quot; because it has <strong>hostNetwork</strong> set to <strong>true</strong>.</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: busybox</strong></p><p><strong>namespace: default</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- image: busybox</strong></p><p><strong>command:</strong></p><p><strong>- sleep</strong></p><p><strong>- &quot;3600&quot;</strong></p><p><strong>imagePullPolicy: IfNotPresent</strong></p><p><strong>name: busybox</strong></p><p><strong>restartPolicy: Always</strong></p><p><strong>hostNetwork: true</strong></p><p><strong>dnsPolicy: ClusterFirstWithHostNet</strong></p><h5><strong>Pod&#x27;s DNS Config</strong></h5><p>Kubernetes v1.9 introduces an Alpha feature (Beta in v1.10) that allows users more control on the DNS settings for a Pod. This feature is enabled by default in v1.10. To enable this feature in v1.9, the cluster administrator needs to enable the <strong>CustomPodDNS</strong> feature gate on the apiserver and the kubelet, for example, &quot;<strong>--feature-gates=CustomPodDNS=true,<!-- -->.<!-- -->..</strong>&quot;. When the feature gate is enabled, users can set the <strong>dnsPolicy</strong> field of a Pod to &quot;<strong>None</strong>&quot; and they can add a new field <strong>dnsConfig</strong> to a Pod Spec.</p><p>The <strong>dnsConfig</strong> field is optional and it can work with any <strong>dnsPolicy</strong> settings. However, when a Pod&#x27;s <strong>dnsPolicy</strong> is set to &quot;<strong>None</strong>&quot;, the <strong>dnsConfig</strong> field has to be specified.</p><p>Below are the properties a user can specify in the <strong>dnsConfig</strong> field:</p><ul><li><strong>nameservers</strong>: a list of IP addresses that will be used as DNS servers for the Pod. There can be at most 3 IP addresses specified. When the Pod&#x27;s <strong>dnsPolicy</strong> is set to &quot;<strong>None</strong>&quot;, the list must contain at least one IP address, otherwise this property is optional. The servers listed will be combined to the base nameservers generated from the specified DNS policy with duplicate addresses removed.</li><li><strong>searches</strong>: a list of DNS search domains for hostname lookup in the Pod. This property is optional. When specified, the provided list will be merged into the base search domain names generated from the chosen DNS policy. Duplicate domain names are removed. Kubernetes allows for at most 6 search domains.</li><li><strong>options</strong>: an optional list of objects where each object may have a <strong>name</strong> property (required) and a <strong>value</strong> property (optional). The contents in this property will be merged to the options generated from the specified DNS policy. Duplicate entries are removed.</li></ul><p>The following is an example Pod with custom DNS settings:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>custom-dns.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kuberne">https://raw.githubusercontent.com/kuberne</a>      |
| tes/website/master/docs/concepts/services-networking/custom-dns.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>namespace: default</strong>                                                |
|                                                                       |
| <strong>name: dns-example</strong>                                                 |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: test</strong>                                                      |
|                                                                       |
| <strong>image: nginx</strong>                                                      |
|                                                                       |
| <strong>dnsPolicy: &quot;None&quot;</strong>                                               |
|                                                                       |
| <strong>dnsConfig:</strong>                                                        |
|                                                                       |
| <strong>nameservers:</strong>                                                      |
|                                                                       |
| <strong>- 1.2.3.4</strong>                                                         |
|                                                                       |
| <strong>searches:</strong>                                                         |
|                                                                       |
| <strong>- ns1.svc.cluster.local</strong>                                           |
|                                                                       |
| <strong>- my.dns.search.suffix</strong>                                            |
|                                                                       |
| <strong>options:</strong>                                                          |
|                                                                       |
| <strong>- name: ndots</strong>                                                     |
|                                                                       |
| <strong>value: &quot;2&quot;</strong>                                                      |
|                                                                       |
| <strong>- name: edns0</strong>                                                     |
+-----------------------------------------------------------------------+</p><p>When the Pod above is created, the container <strong>test</strong> gets the following contents in its <strong>/etc/resolv.conf</strong> file:</p><p><strong>nameserver 1.2.3.4</strong></p><p><strong>search ns1.svc.cluster.local my.dns.search.suffix</strong></p><p><strong>options ndots:2 edns0</strong></p><h4>What&#x27;s next</h4><p>For guidance on administering DNS configurations, check <a href="https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/">Configure DNS Service</a></p><h3>Connecting Applications with Services</h3><ul><li><a href="https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/#the-kubernetes-model-for-connecting-containers"><strong>The Kubernetes model for connecting containers</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/#exposing-pods-to-the-cluster"><strong>Exposing pods to the cluster</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/#creating-a-service"><strong>Creating a Service</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/#accessing-the-service"><strong>Accessing the Service</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/#environment-variables"><strong>Environment Variables</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/#dns"><strong>DNS</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/#securing-the-service"><strong>Securing the Service</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/#exposing-the-service"><strong>Exposing the Service</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/#further-reading"><strong>Further reading</strong></a></li></ul><h4>The Kubernetes model for connecting containers</h4><p>Now that you have a continuously running, replicated application you can expose it on a network. Before discussing the Kubernetes approach to networking, it is worthwhile to contrast it with the &quot;normal&quot; way networking works with Docker.</p><p>By default, Docker uses host-private networking, so containers can talk to other containers only if they are on the same machine. In order for Docker containers to communicate across nodes, there must be allocated ports on the machine&#x27;s own IP address, which are then forwarded or proxied to the containers. This obviously means that containers must either coordinate which ports they use very carefully or ports must be allocated dynamically.</p><p>Coordinating ports across multiple developers is very difficult to do at scale and exposes users to cluster-level issues outside of their control. Kubernetes assumes that pods can communicate with other pods, regardless of which host they land on. We give every pod its own cluster-private-IP address so you do not need to explicitly create links between pods or mapping container ports to host ports. This means that containers within a Pod can all reach each other&#x27;s ports on localhost, and all pods in a cluster can see each other without NAT. The rest of this document will elaborate on how you can run reliable services on such a networking model.</p><p>This guide uses a simple nginx server to demonstrate proof of concept. The same principles are embodied in a more complete <a href="http://blog.kubernetes.io/2015/07/strong-simple-ssl-for-kubernetes.html">Jenkins CI application</a>.</p><h4>Exposing pods to the cluster</h4><p>We did this in a previous example, but let&#x27;s do it once again and focus on the networking perspective. Create an nginx pod, and note that it has a container port specification:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>run-my-nginx.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernete">https://raw.githubusercontent.com/kubernete</a>  |
| s/website/master/docs/concepts/services-networking/run-my-nginx.yaml) |
+=======================================================================+
| <strong>apiVersion: apps/v1</strong>                                               |
|                                                                       |
| <strong>kind: Deployment</strong>                                                  |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: my-nginx</strong>                                                    |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>matchLabels:</strong>                                                      |
|                                                                       |
| <strong>run: my-nginx</strong>                                                     |
|                                                                       |
| <strong>replicas: 2</strong>                                                       |
|                                                                       |
| <strong>template:</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>run: my-nginx</strong>                                                     |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: my-nginx</strong>                                                  |
|                                                                       |
| <strong>image: nginx</strong>                                                      |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- containerPort: 80</strong>                                               |
+-----------------------------------------------------------------------+</p><p>This makes it accessible from any node in your cluster. Check the nodes the pod is running on:</p><p><strong>$ kubectl create -f ./run-my-nginx.yaml</strong></p><p><strong>$ kubectl get pods -l run=my-nginx -o wide</strong></p><p><strong>NAME READY STATUS RESTARTS AGE IP NODE</strong></p><p><strong>my-nginx-3800858182-jr4a2 1/1 Running 0 13s 10.244.3.4 kubernetes-minion-905m</strong></p><p><strong>my-nginx-3800858182-kna2y 1/1 Running 0 13s 10.244.2.5 kubernetes-minion-ljyd</strong></p><p>Check your pods&#x27; IPs:</p><p><strong>$ kubectl get pods -l run=my-nginx -o yaml | grep podIP</strong></p><p><strong>podIP: 10.244.3.4</strong></p><p><strong>podIP: 10.244.2.5</strong></p><p>You should be able to ssh into any node in your cluster and curl both IPs. Note that the containers are not using port 80 on the node, nor are there any special NAT rules to route traffic to the pod. This means you can run multiple nginx pods on the same node all using the same containerPort and access them from any other pod or node in your cluster using IP. Like Docker, ports can still be published to the host node&#x27;s interfaces, but the need for this is radically diminished because of the networking model.</p><p>You can read more about <a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-achieve-this">how we achieve this</a> if you&#x27;re curious.</p><h4>Creating a Service</h4><p>So we have pods running nginx in a flat, cluster wide, address space. In theory, you could talk to these pods directly, but what happens when a node dies? The pods die with it, and the Deployment will create new ones, with different IPs. This is the problem a Service solves.</p><p>A Kubernetes Service is an abstraction which defines a logical set of Pods running somewhere in your cluster, that all provide the same functionality. When created, each Service is assigned a unique IP address (also called clusterIP). This address is tied to the lifespan of the Service, and will not change while the Service is alive. Pods can be configured to talk to the Service, and know that communication to the Service will be automatically load-balanced out to some pod that is a member of the Service.</p><p>You can create a Service for your 2 nginx replicas with <strong>kubectl expose</strong>:</p><p><strong>$ kubectl expose deployment/my-nginx</strong></p><p><strong>service &quot;my-nginx&quot; exposed</strong></p><p>This is equivalent to <strong>kubectl create -f</strong> the following yaml:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>nginx-svc.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubern">https://raw.githubusercontent.com/kubern</a>        |
| etes/website/master/docs/concepts/services-networking/nginx-svc.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Service</strong>                                                     |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: my-nginx</strong>                                                    |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>run: my-nginx</strong>                                                     |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- port: 80</strong>                                                        |
|                                                                       |
| <strong>protocol: TCP</strong>                                                     |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>run: my-nginx</strong>                                                     |
+-----------------------------------------------------------------------+</p><p>This specification will create a Service which targets TCP port 80 on any Pod with the <strong>run: my-nginx</strong> label, and expose it on an abstracted Service port (<strong>targetPort</strong>: is the port the container accepts traffic on, <strong>port</strong>: is the abstracted Service port, which can be any port other pods use to access the Service). View <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#service-v1-core">service API object</a> to see the list of supported fields in service definition. Check your Service:</p><p><strong>$ kubectl get svc my-nginx</strong></p><p><strong>NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE</strong></p><p><strong>my-nginx 10.0.162.149 <code>&lt;none&gt;</code> 80/TCP 21s</strong></p><p>As mentioned previously, a Service is backed by a group of pods. These pods are exposed through <strong>endpoints</strong>. The Service&#x27;s selector will be evaluated continuously and the results will be POSTed to an Endpoints object also named <strong>my-nginx</strong>. When a pod dies, it is automatically removed from the endpoints, and new pods matching the Service&#x27;s selector will automatically get added to the endpoints. Check the endpoints, and note that the IPs are the same as the pods created in the first step:</p><p><strong>$ kubectl describe svc my-nginx</strong></p><p><strong>Name: my-nginx</strong></p><p><strong>Namespace: default</strong></p><p><strong>Labels: run=my-nginx</strong></p><p><strong>Annotations: <code>&lt;none&gt;</code></strong></p><p><strong>Selector: run=my-nginx</strong></p><p><strong>Type: ClusterIP</strong></p><p><strong>IP: 10.0.162.149</strong></p><p><strong>Port: <code>&lt;unset&gt;</code> 80/TCP</strong></p><p><strong>Endpoints: 10.244.2.5:80,10.244.3.4:80</strong></p><p><strong>Session Affinity: None</strong></p><p><strong>Events: <code>&lt;none&gt;</code></strong></p><p><strong>$ kubectl get ep my-nginx</strong></p><p><strong>NAME ENDPOINTS AGE</strong></p><p><strong>my-nginx 10.244.2.5:80,10.244.3.4:80 1m</strong></p><p>You should now be able to curl the nginx Service on <strong><code>&lt;CLUSTER-IP&gt;:&lt;PORT&gt;</code></strong> from any node in your cluster. Note that the Service IP is completely virtual, it never hits the wire, if you&#x27;re curious about how this works you can read more about the <a href="https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies">service proxy</a>.</p><h4>Accessing the Service</h4><p>Kubernetes supports 2 primary modes of finding a Service - environment variables and DNS. The former works out of the box while the latter requires the <a href="http://releases.k8s.io/master/cluster/addons/dns/README.md">kube-dns cluster addon</a>.</p><h5><strong>Environment Variables</strong></h5><p>When a Pod runs on a Node, the kubelet adds a set of environment variables for each active Service. This introduces an ordering problem. To see why, inspect the environment of your running nginx pods (your pod name will be different):</p><p><strong>$ kubectl exec my-nginx-3800858182-jr4a2 -- printenv | grep SERVICE</strong></p><p><strong>KUBERNETES_SERVICE_HOST=10.0.0.1</strong></p><p><strong>KUBERNETES_SERVICE_PORT=443</strong></p><p><strong>KUBERNETES_SERVICE_PORT_HTTPS=443</strong></p><p>Note there&#x27;s no mention of your Service. This is because you created the replicas before the Service. Another disadvantage of doing this is that the scheduler might put both pods on the same machine, which will take your entire Service down if it dies. We can do this the right way by killing the 2 pods and waiting for the Deployment to recreate them. This time around the Service exists before the replicas. This will give you scheduler-level Service spreading of your pods (provided all your nodes have equal capacity), as well as the right environment variables:</p><p><strong>$ kubectl scale deployment my-nginx --replicas=0; kubectl scale deployment my-nginx --replicas=2;</strong></p><p><strong>$ kubectl get pods -l run=my-nginx -o wide</strong></p><p><strong>NAME READY STATUS RESTARTS AGE IP NODE</strong></p><p><strong>my-nginx-3800858182-e9ihh 1/1 Running 0 5s 10.244.2.7 kubernetes-minion-ljyd</strong></p><p><strong>my-nginx-3800858182-j4rm4 1/1 Running 0 5s 10.244.3.8 kubernetes-minion-905m</strong></p><p>You may notice that the pods have different names, since they are killed and recreated.</p><p><strong>$ kubectl exec my-nginx-3800858182-e9ihh -- printenv | grep SERVICE</strong></p><p><strong>KUBERNETES_SERVICE_PORT=443</strong></p><p><strong>MY_NGINX_SERVICE_HOST=10.0.162.149</strong></p><p><strong>KUBERNETES_SERVICE_HOST=10.0.0.1</strong></p><p><strong>MY_NGINX_SERVICE_PORT=80</strong></p><p><strong>KUBERNETES_SERVICE_PORT_HTTPS=443</strong></p><h5><strong>DNS</strong></h5><p>Kubernetes offers a DNS cluster addon Service that uses skydns to automatically assign dns names to other Services. You can check if it&#x27;s running on your cluster:</p><p><strong>$ kubectl get services kube-dns --namespace=kube-system</strong></p><p><strong>NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE</strong></p><p><strong>kube-dns 10.0.0.10 <code>&lt;none&gt;</code> 53/UDP,53/TCP 8m</strong></p><p>If it isn&#x27;t running, you can <a href="http://releases.k8s.io/master/cluster/addons/dns/README.md#how-do-i-configure-it">enable it</a>. The rest of this section will assume you have a Service with a long lived IP (my-nginx), and a dns server that has assigned a name to that IP (the kube-dns cluster addon), so you can talk to the Service from any pod in your cluster using standard methods (e.g. gethostbyname). Let&#x27;s run another curl application to test this:</p><p><strong>$ kubectl run curl --image=radial/busyboxplus:curl -i --tty</strong></p><p><strong>Waiting for pod default/curl-131556218-9fnch to be running, status is Pending, pod ready: false</strong></p><p><strong>Hit enter for command prompt</strong></p><p>Then, hit enter and run <strong>nslookup my-nginx</strong>:</p><p><strong>[ root@curl-131556218-9fnch:/ ]<!-- -->$ nslookup my-nginx</strong></p><p><strong>Server: 10.0.0.10</strong></p><p><strong>Address 1: 10.0.0.10</strong></p><p><strong>Name: my-nginx</strong></p><p><strong>Address 1: 10.0.162.149</strong></p><h4>Securing the Service</h4><p>Till now we have only accessed the nginx server from within the cluster. Before exposing the Service to the internet, you want to make sure the communication channel is secure. For this, you will need:</p><ul><li>Self signed certificates for https (unless you already have an identity certificate)</li><li>An nginx server configured to use the certificates</li><li>A <a href="https://kubernetes.io/docs/concepts/configuration/secret/">secret</a> that makes the certificates accessible to pods</li></ul><p>You can acquire all these from the <a href="https://github.com/kubernetes/examples/tree/master/staging/https-nginx/">nginx https example</a>. This requires having go and make tools installed. If you don&#x27;t want to install those, then follow the manual steps later. In short:</p><p><strong>$ make keys secret KEY=/tmp/nginx.key CERT=/tmp/nginx.crt SECRET=/tmp/secret.json</strong></p><p><strong>$ kubectl create -f /tmp/secret.json</strong></p><p><strong>secret &quot;nginxsecret&quot; created</strong></p><p><strong>$ kubectl get secrets</strong></p><p><strong>NAME TYPE DATA AGE</strong></p><p><strong>default-token-il9rc kubernetes.io/service-account-token 1 1d</strong></p><p><strong>nginxsecret Opaque 2 1m</strong></p><p>Following are the manual steps to follow in case you run into problems running make (on windows for example):</p><p><strong><em>#create a public private key pair</em></strong></p><p><strong>openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /d/tmp/nginx.key -out /d/tmp/nginx.crt -subj &quot;/CN=my-nginx/O=my-nginx&quot;</strong></p><p><strong><em>#convert the keys to base64 encoding</em></strong></p><p><strong>cat /d/tmp/nginx.crt | base64</strong></p><p><strong>cat /d/tmp/nginx.key | base64</strong></p><p>Use the output from the previous commands to create a yaml file as follows. The base64 encoded value should all be on a single line.</p><p><strong>apiVersion: &quot;v1&quot;</strong></p><p><strong>kind: &quot;Secret&quot;</strong></p><p><strong>metadata:</strong></p><p><strong>name: &quot;nginxsecret&quot;</strong></p><p><strong>namespace: &quot;default&quot;</strong></p><p><strong>data:</strong></p><p><strong>nginx.crt: &quot;LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURIekNDQWdlZ0F3SUJBZ0lKQUp5M3lQK0pzMlpJTUEwR0NTcUdTSWIzRFFFQkJRVUFNQ1l4RVRBUEJnTlYKQkFNVENHNW5hVzU0YzNaak1SRXdEd1lEVlFRS0V3aHVaMmx1ZUhOMll6QWVGdzB4TnpFd01qWXdOekEzTVRKYQpGdzB4T0RFd01qWXdOekEzTVRKYU1DWXhFVEFQQmdOVkJBTVRDRzVuYVc1NGMzWmpNUkV3RHdZRFZRUUtFd2h1CloybHVlSE4yWXpDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBSjFxSU1SOVdWM0IKMlZIQlRMRmtobDRONXljMEJxYUhIQktMSnJMcy8vdzZhU3hRS29GbHlJSU94NGUrMlN5ajBFcndCLzlYTnBwbQppeW1CL3JkRldkOXg5UWhBQUxCZkVaTmNiV3NsTVFVcnhBZW50VWt1dk1vLzgvMHRpbGhjc3paenJEYVJ4NEo5Ci82UVRtVVI3a0ZTWUpOWTVQZkR3cGc3dlVvaDZmZ1Voam92VG42eHNVR0M2QURVODBpNXFlZWhNeVI1N2lmU2YKNHZpaXdIY3hnL3lZR1JBRS9mRTRqakxCdmdONjc2SU90S01rZXV3R0ljNDFhd05tNnNTSzRqYUNGeGpYSnZaZQp2by9kTlEybHhHWCtKT2l3SEhXbXNhdGp4WTRaNVk3R1ZoK0QrWnYvcW1mMFgvbVY0Rmo1NzV3ajFMWVBocWtsCmdhSXZYRyt4U1FVQ0F3RUFBYU5RTUU0d0hRWURWUjBPQkJZRUZPNG9OWkI3YXc1OUlsYkROMzhIYkduYnhFVjcKTUI4R0ExVWRJd1FZTUJhQUZPNG9OWkI3YXc1OUlsYkROMzhIYkduYnhFVjdNQXdHQTFVZEV3UUZNQU1CQWY4dwpEUVlKS29aSWh2Y05BUUVGQlFBRGdnRUJBRVhTMW9FU0lFaXdyMDhWcVA0K2NwTHI3TW5FMTducDBvMm14alFvCjRGb0RvRjdRZnZqeE04Tzd2TjB0clcxb2pGSW0vWDE4ZnZaL3k4ZzVaWG40Vm8zc3hKVmRBcStNZC9jTStzUGEKNmJjTkNUekZqeFpUV0UrKzE5NS9zb2dmOUZ3VDVDK3U2Q3B5N0M3MTZvUXRUakViV05VdEt4cXI0Nk1OZWNCMApwRFhWZmdWQTRadkR4NFo3S2RiZDY5eXM3OVFHYmg5ZW1PZ05NZFlsSUswSGt0ejF5WU4vbVpmK3FqTkJqbWZjCkNnMnlwbGQ0Wi8rUUNQZjl3SkoybFIrY2FnT0R4elBWcGxNSEcybzgvTHFDdnh6elZPUDUxeXdLZEtxaUMwSVEKQ0I5T2wwWW5scE9UNEh1b2hSUzBPOStlMm9KdFZsNUIyczRpbDlhZ3RTVXFxUlU9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K&quot;</strong></p><p><strong>nginx.key: &quot;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&quot;</strong></p><p>Now create the secrets using the file:</p><p><strong>$ kubectl create -f nginxsecrets.yaml</strong></p><p><strong>$ kubectl get secrets</strong></p><p><strong>NAME TYPE DATA AGE</strong></p><p><strong>default-token-il9rc kubernetes.io/service-account-token 1 1d</strong></p><p><strong>nginxsecret Opaque 2 1m</strong></p><p>Now modify your nginx replicas to start an https server using the certificate in the secret, and the Service, to expose both ports (80 and 443):</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>ngin                                                               |
| x-secure-app.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/we">https://raw.githubusercontent.com/kubernetes/we</a> |
| bsite/master/docs/concepts/services-networking/nginx-secure-app.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Service</strong>                                                     |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: my-nginx</strong>                                                    |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>run: my-nginx</strong>                                                     |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>type: NodePort</strong>                                                    |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- port: 8080</strong>                                                      |
|                                                                       |
| <strong>targetPort: 80</strong>                                                    |
|                                                                       |
| <strong>protocol: TCP</strong>                                                     |
|                                                                       |
| <strong>name: http</strong>                                                        |
|                                                                       |
| <strong>- port: 443</strong>                                                       |
|                                                                       |
| <strong>protocol: TCP</strong>                                                     |
|                                                                       |
| <strong>name: https</strong>                                                       |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>run: my-nginx</strong>                                                     |
|                                                                       |
| <strong>---</strong>                                                             |
|                                                                       |
| <strong>apiVersion: apps/v1</strong>                                               |
|                                                                       |
| <strong>kind: Deployment</strong>                                                  |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: my-nginx</strong>                                                    |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>matchLabels:</strong>                                                      |
|                                                                       |
| <strong>run: my-nginx</strong>                                                     |
|                                                                       |
| <strong>replicas: 1</strong>                                                       |
|                                                                       |
| <strong>template:</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>run: my-nginx</strong>                                                     |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>volumes:</strong>                                                          |
|                                                                       |
| <strong>- name: secret-volume</strong>                                             |
|                                                                       |
| <strong>secret:</strong>                                                           |
|                                                                       |
| <strong>secretName: nginxsecret</strong>                                           |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: nginxhttps</strong>                                                |
|                                                                       |
| <strong>image: bprashanth/nginxhttps:1.0</strong>                                  |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- containerPort: 443</strong>                                              |
|                                                                       |
| <strong>- containerPort: 80</strong>                                               |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- mountPath: /etc/nginx/ssl</strong>                                       |
|                                                                       |
| <strong>name: secret-volume</strong>                                               |
+-----------------------------------------------------------------------+</p><p>Noteworthy points about the nginx-secure-app manifest:</p><ul><li>It contains both Deployment and Service specification in the same file.</li><li>The <a href="https://github.com/kubernetes/examples/tree/master/staging/https-nginx/default.conf">nginx server</a> serves http traffic on port 80 and https traffic on 443, and nginx Service exposes both ports.</li><li>Each container has access to the keys through a volume mounted at /etc/nginx/ssl. This is setup before the nginx server is started.</li></ul><p><strong>$ kubectl delete deployments,svc my-nginx; kubectl create -f ./nginx-secure-app.yaml</strong></p><p>At this point you can reach the nginx server from any node.</p><p><strong>$ kubectl get pods -o yaml | grep -i podip</strong></p><p><strong>podIP: 10.244.3.5</strong></p><p><strong>node $ curl -k <a href="https://10.244.3.5">https://10.244.3.5</a></strong></p><p><strong>.<!-- -->..</strong></p><p><strong><code>&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;</code></strong></p><p>Note how we supplied the <strong>-k</strong> parameter to curl in the last step, this is because we don&#x27;t know anything about the pods running nginx at certificate generation time, so we have to tell curl to ignore the CName mismatch. By creating a Service we linked the CName used in the certificate with the actual DNS name used by pods during Service lookup. Let&#x27;s test this from a pod (the same secret is being reused for simplicity, the pod only needs nginx.crt to access the Service):</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>curlpod.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kube">https://raw.githubusercontent.com/kube</a>            |
| rnetes/website/master/docs/concepts/services-networking/curlpod.yaml) |
+=======================================================================+
| <strong>apiVersion: apps/v1</strong>                                               |
|                                                                       |
| <strong>kind: Deployment</strong>                                                  |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: curl-deployment</strong>                                             |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>matchLabels:</strong>                                                      |
|                                                                       |
| <strong>app: curlpod</strong>                                                      |
|                                                                       |
| <strong>replicas: 1</strong>                                                       |
|                                                                       |
| <strong>template:</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: curlpod</strong>                                                      |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>volumes:</strong>                                                          |
|                                                                       |
| <strong>- name: secret-volume</strong>                                             |
|                                                                       |
| <strong>secret:</strong>                                                           |
|                                                                       |
| <strong>secretName: nginxsecret</strong>                                           |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: curlpod</strong>                                                   |
|                                                                       |
| <strong>command:</strong>                                                          |
|                                                                       |
| <strong>- sh</strong>                                                              |
|                                                                       |
| <strong>- -c</strong>                                                              |
|                                                                       |
| <strong>- while true; do sleep 1; done</strong>                                    |
|                                                                       |
| <strong>image: radial/busyboxplus:curl</strong>                                    |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- mountPath: /etc/nginx/ssl</strong>                                       |
|                                                                       |
| <strong>name: secret-volume</strong>                                               |
+-----------------------------------------------------------------------+</p><p><strong>$ kubectl create -f ./curlpod.yaml</strong></p><p><strong>$ kubectl get pods -l app=curlpod</strong></p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>curl-deployment-1515033274-1410r 1/1 Running 0 1m</strong></p><p><strong>$ kubectl exec curl-deployment-1515033274-1410r -- curl https://my-nginx --cacert /etc/nginx/ssl/nginx.crt</strong></p><p><strong>.<!-- -->..</strong></p><p><strong><code>&lt;title&gt;Welcome to nginx!&lt;/title&gt;</code></strong></p><p><strong>.<!-- -->..</strong></p><h4>Exposing the Service</h4><p>For some parts of your applications you may want to expose a Service onto an external IP address. Kubernetes supports two ways of doing this: NodePorts and LoadBalancers. The Service created in the last section already used <strong>NodePort</strong>, so your nginx https replica is ready to serve traffic on the internet if your node has a public IP.</p><p><strong>$ kubectl get svc my-nginx -o yaml | grep nodePort -C 5</strong></p><p><strong>uid: 07191fb3-f61a-11e5-8ae5-42010af00002</strong></p><p><strong>spec:</strong></p><p><strong>clusterIP: 10.0.162.149</strong></p><p><strong>ports:</strong></p><p><strong>- name: http</strong></p><p><strong>nodePort: 31704</strong></p><p><strong>port: 8080</strong></p><p><strong>protocol: TCP</strong></p><p><strong>targetPort: 80</strong></p><p><strong>- name: https</strong></p><p><strong>nodePort: 32453</strong></p><p><strong>port: 443</strong></p><p><strong>protocol: TCP</strong></p><p><strong>targetPort: 443</strong></p><p><strong>selector:</strong></p><p><strong>run: my-nginx</strong></p><p><strong>$ kubectl get nodes -o yaml | grep ExternalIP -C 1</strong></p><p><strong>- address: 104.197.41.11</strong></p><p><strong>type: ExternalIP</strong></p><p><strong>allocatable:</strong></p><p><strong>--</strong></p><p><strong>- address: 23.251.152.56</strong></p><p><strong>type: ExternalIP</strong></p><p><strong>allocatable:</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>$ curl https://<code>&lt;EXTERNAL-IP&gt;:&lt;NODE-PORT&gt;</code> -k</strong></p><p><strong>.<!-- -->..</strong></p><p><strong><code>&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;</code></strong></p><p>Let&#x27;s now recreate the Service to use a cloud load balancer, just change the <strong>Type</strong> of <strong>my-nginx</strong>Service from <strong>NodePort</strong> to <strong>LoadBalancer</strong>:</p><p><strong>$ kubectl edit svc my-nginx</strong></p><p><strong>$ kubectl get svc my-nginx</strong></p><p><strong>NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE</strong></p><p><strong>my-nginx 10.0.162.149 162.222.184.144 80/TCP,81/TCP,82/TCP 21s</strong></p><p><strong>$ curl https://<code>&lt;EXTERNAL-IP&gt;</code> -k</strong></p><p><strong>.<!-- -->..</strong></p><p><strong><code>&lt;title&gt;Welcome to nginx!&lt;/title&gt;</code></strong></p><p>The IP address in the <strong>EXTERNAL-IP</strong> column is the one that is available on the public internet. The <strong>CLUSTER-IP</strong> is only available inside your cluster/private cloud network.</p><p>Note that on AWS, type <strong>LoadBalancer</strong> creates an ELB, which uses a (long) hostname, not an IP. It&#x27;s too long to fit in the standard <strong>kubectl get svc</strong> output, in fact, so you&#x27;ll need to do <strong>kubectl describe service my-nginx</strong> to see it. You&#x27;ll see something like this:</p><p><strong>$ kubectl describe service my-nginx</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>LoadBalancer Ingress: a320587ffd19711e5a37606cf4a74574-1142138393.us-east-1.elb.amazonaws.com</strong></p><p><strong>.<!-- -->..</strong></p><h4>Further reading</h4><p>Kubernetes also supports Federated Services, which can span multiple clusters and cloud providers, to provide increased availability, better fault tolerance and greater scalability for your services. See the <a href="https://kubernetes.io/docs/concepts/cluster-administration/federation-service-discovery/">Federated Services User Guide</a> for further information.</p><h3>Ingress</h3><p>An API object that manages external access to the services in a cluster, typically HTTP.</p><p>Ingress can provide load balancing, SSL termination and name-based virtual hosting.</p><ul><li><a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#what-is-ingress"><strong>What is Ingress?</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#prerequisites"><strong>Prerequisites</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#the-ingress-resource"><strong>The Ingress Resource</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-controllers"><strong>Ingress controllers</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#types-of-ingress"><strong>Types of Ingress</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#single-service-ingress"><strong>Single Service Ingress</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#simple-fanout"><strong>Simple fanout</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#name-based-virtual-hosting"><strong>Name based virtual hosting</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#tls"><strong>TLS</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#loadbalancing"><strong>Loadbalancing</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#updating-an-ingress"><strong>Updating an Ingress</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#failing-across-availability-zones"><strong>Failing across availability zones</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#future-work"><strong>Future Work</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#alternatives"><strong>Alternatives</strong></a></li></ul><p><strong>Terminology</strong></p><p>Throughout this doc you will see a few terms that are sometimes used interchangeably elsewhere, that might cause confusion. This section attempts to clarify them.</p><ul><li>Node: A single virtual or physical machine in a Kubernetes cluster.</li><li>Cluster: A group of nodes firewalled from the internet, that are the primary compute resources managed by Kubernetes.</li><li>Edge router: A router that enforces the firewall policy for your cluster. This could be a gateway managed by a cloud provider or a physical piece of hardware.</li><li>Cluster network: A set of links, logical or physical, that facilitate communication within a cluster according to the <a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/">Kubernetes networking model</a>. Examples of a Cluster network include Overlays such as <a href="https://github.com/coreos/flannel#flannel">flannel</a> or SDNs such as <a href="https://www.openvswitch.org/">OVS</a>.</li><li>Service: A Kubernetes <a href="https://kubernetes.io/docs/concepts/services-networking/service/">Service</a> that identifies a set of pods using label selectors. Unless mentioned otherwise, Services are assumed to have virtual IPs only routable within the cluster network.</li></ul><h4>What is Ingress?</h4><p>Typically, services and pods have IPs only routable by the cluster network. All traffic that ends up at an edge router is either dropped or forwarded elsewhere. Conceptually, this might look like:</p><p><strong>internet</strong></p><p><strong>|</strong></p><p><strong>------------</strong></p><p><strong>[ Services ]</strong></p><p>An Ingress is a collection of rules that allow inbound connections to reach the cluster services.</p><p><strong>internet</strong></p><p><strong>|</strong></p><p><strong>[ Ingress ]</strong></p><p><strong>--|-----|--</strong></p><p><strong>[ Services ]</strong></p><p>It can be configured to give services externally-reachable URLs, load balance traffic, terminate SSL, offer name based virtual hosting, and more. Users request ingress by POSTing the Ingress resource to the API server. An <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-controllers">Ingress controller</a> is responsible for fulfilling the Ingress, usually with a loadbalancer, though it may also configure your edge router or additional frontends to help handle the traffic in an HA manner.</p><h4>Prerequisites</h4><p>Before you start using the Ingress resource, there are a few things you should understand. The Ingress is a beta resource, not available in any Kubernetes release prior to 1.1. You need an Ingress controller to satisfy an Ingress, simply creating the resource will have no effect.</p><p>GCE/Google Kubernetes Engine deploys an ingress controller on the master. You can deploy any number of custom ingress controllers in a pod. You must annotate each ingress with the appropriate class, as indicated <a href="https://git.k8s.io/ingress#running-multiple-ingress-controllers">here</a> and <a href="https://git.k8s.io/ingress-gce/BETA_LIMITATIONS.md#disabling-glbc">here</a>.</p><p>Make sure you review the <a href="https://github.com/kubernetes/ingress-gce/blob/master/BETA_LIMITATIONS.md#glbc-beta-limitations">beta limitations</a> of this controller. In environments other than GCE/Google Kubernetes Engine, you need to <a href="https://git.k8s.io/ingress-nginx/README.md">deploy a controller</a> as a pod.</p><h4>The Ingress Resource</h4><p>A minimal Ingress might look like:</p><p><strong>apiVersion: extensions/v1beta1</strong></p><p><strong>kind: Ingress</strong></p><p><strong>metadata:</strong></p><p><strong>name: test-ingress</strong></p><p><strong>annotations:</strong></p><p><strong>nginx.ingress.kubernetes.io/rewrite-target: /</strong></p><p><strong>spec:</strong></p><p><strong>rules:</strong></p><p><strong>- http:</strong></p><p><strong>paths:</strong></p><p><strong>- path: /testpath</strong></p><p><strong>backend:</strong></p><p><strong>serviceName: test</strong></p><p><strong>servicePort: 80</strong></p><p>POSTing this to the API server will have no effect if you have not configured an <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-controllers"><em>Ingress controller</em></a>.</p><p><strong>Lines 1-6</strong>: As with all other Kubernetes config, an Ingress needs <strong>apiVersion</strong>, <strong>kind</strong>, and <strong>metadata</strong>fields. For general information about working with config files, see <a href="https://kubernetes.io/docs/tasks/run-application/run-stateless-application-deployment/">deploying applications</a>, <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/">configuring containers</a>, <a href="https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/">managing resources</a> and <a href="https://github.com/kubernetes/ingress-nginx/blob/master/docs/examples/rewrite/README.md">ingress configuration rewrite</a>.</p><p><strong>Lines 7-9</strong>: Ingress <a href="https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status">spec</a> has all the information needed to configure a loadbalancer or proxy server. Most importantly, it contains a list of rules matched against all incoming requests. Currently the Ingress resource only supports http rules.</p><p><strong>Lines 10-11</strong>: Each http rule contains the following information: A host (e.g.: foo.bar.com, defaults to <!-- -->*<!-- --> in this example), a list of paths (e.g.: /testpath) each of which has an associated backend (test:80). Both the host and path must match the content of an incoming request before the loadbalancer directs traffic to the backend.</p><p><strong>Lines 12-14</strong>: A backend is a service:port combination as described in the <a href="https://kubernetes.io/docs/concepts/services-networking/service/">services doc</a>. Ingress traffic is typically sent directly to the endpoints matching a backend.</p><p><strong>Global Parameters</strong>: For the sake of simplicity the example Ingress has no global parameters, see the <a href="https://releases.k8s.io/master/staging/src/k8s.io/api/extensions/v1beta1/types.go">API reference</a> for a full definition of the resource. One can specify a global default backend in the absence of which requests that don&#x27;t match a path in the spec are sent to the default backend of the Ingress controller.</p><h4>Ingress controllers</h4><p>In order for the Ingress resource to work, the cluster must have an Ingress controller running. This is unlike other types of controllers, which typically run as part of the <strong>kube-controller-manager</strong>binary, and which are typically started automatically as part of cluster creation. You need to choose the ingress controller implementation that is the best fit for your cluster, or implement one. We currently support and maintain <a href="https://git.k8s.io/ingress-gce/README.md">GCE</a> and <a href="https://git.k8s.io/ingress-nginx/README.md">nginx</a> controllers.</p><h4>Before you begin</h4><p>The following document describes a set of cross platform features exposed through the Ingress resource. Ideally, all Ingress controllers should fulfill this specification, but we&#x27;re not there yet. We currently support and maintain <a href="https://git.k8s.io/ingress-gce/README.md">GCE</a> and <a href="https://git.k8s.io/ingress-nginx/README.md">nginx</a> controllers. <strong>Make sure you review controller specific docs so you understand the caveats of each one</strong>.</p><h4>Types of Ingress</h4><h5><strong>Single Service Ingress</strong></h5><p>There are existing Kubernetes concepts that allow you to expose a single service (see <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#alternatives">alternatives</a>), however you can do so through an Ingress as well, by specifying a default backend with no rules.</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>ingress.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kube">https://raw.githubusercontent.com/kube</a>            |
| rnetes/website/master/docs/concepts/services-networking/ingress.yaml) |
+=======================================================================+
| <strong>apiVersion: extensions/v1beta1</strong>                                    |
|                                                                       |
| <strong>kind: Ingress</strong>                                                     |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: test-ingress</strong>                                                |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>backend:</strong>                                                          |
|                                                                       |
| <strong>serviceName: testsvc</strong>                                              |
|                                                                       |
| <strong>servicePort: 80</strong>                                                   |
+-----------------------------------------------------------------------+</p><p>If you create it using <strong>kubectl create -f</strong> you should see:</p><p><strong>$ kubectl get ing</strong></p><p><strong>NAME RULE BACKEND ADDRESS</strong></p><p><strong>test-ingress - testsvc:80 107.178.254.228</strong></p><p>Where <strong>107.178.254.228</strong> is the IP allocated by the Ingress controller to satisfy this Ingress. The <strong>RULE</strong> column shows that all traffic sent to the IP is directed to the Kubernetes Service listed under <strong>BACKEND</strong>.</p><h5><strong>Simple fanout</strong></h5><p>As described previously, pods within kubernetes have IPs only visible on the cluster network, so we need something at the edge accepting ingress traffic and proxying it to the right endpoints. This component is usually a highly available loadbalancer. An Ingress allows you to keep the number of loadbalancers down to a minimum, for example, a setup like:</p><p><strong>foo.bar.com -&gt; 178.91.123.132 -&gt; / foo s1:80</strong></p><p><strong>/ bar s2:80</strong></p><p>would require an Ingress such as:</p><p><strong>apiVersion: extensions/v1beta1</strong></p><p><strong>kind: Ingress</strong></p><p><strong>metadata:</strong></p><p><strong>name: test</strong></p><p><strong>annotations:</strong></p><p><strong>ingress.kubernetes.io/rewrite-target: /</strong></p><p><strong>spec:</strong></p><p><strong>rules:</strong></p><p><strong>- host: foo.bar.com</strong></p><p><strong>http:</strong></p><p><strong>paths:</strong></p><p><strong>- path: /foo</strong></p><p><strong>backend:</strong></p><p><strong>serviceName: s1</strong></p><p><strong>servicePort: 80</strong></p><p><strong>- path: /bar</strong></p><p><strong>backend:</strong></p><p><strong>serviceName: s2</strong></p><p><strong>servicePort: 80</strong></p><p>When you create the Ingress with <strong>kubectl create -f</strong>:</p><p><strong>$ kubectl get ing</strong></p><p><strong>NAME RULE BACKEND ADDRESS</strong></p><p><strong>test -</strong></p><p><strong>foo.bar.com</strong></p><p><strong>/foo s1:80</strong></p><p><strong>/bar s2:80</strong></p><p>The Ingress controller will provision an implementation specific loadbalancer that satisfies the Ingress, as long as the services (s1, s2) exist. When it has done so, you will see the address of the loadbalancer under the last column of the Ingress.</p><h5><strong>Name based virtual hosting</strong></h5><p>Name-based virtual hosts use multiple host names for the same IP address.</p><p><strong>foo.bar.com --| |-&gt; foo.bar.com s1:80</strong></p><p><strong>| 178.91.123.132 |</strong></p><p><strong>bar.foo.com --| |-&gt; bar.foo.com s2:80</strong></p><p>The following Ingress tells the backing loadbalancer to route requests based on the <a href="https://tools.ietf.org/html/rfc7230#section-5.4">Host header</a>.</p><p><strong>apiVersion: extensions/v1beta1</strong></p><p><strong>kind: Ingress</strong></p><p><strong>metadata:</strong></p><p><strong>name: test</strong></p><p><strong>spec:</strong></p><p><strong>rules:</strong></p><p><strong>- host: foo.bar.com</strong></p><p><strong>http:</strong></p><p><strong>paths:</strong></p><p><strong>- backend:</strong></p><p><strong>serviceName: s1</strong></p><p><strong>servicePort: 80</strong></p><p><strong>- host: bar.foo.com</strong></p><p><strong>http:</strong></p><p><strong>paths:</strong></p><p><strong>- backend:</strong></p><p><strong>serviceName: s2</strong></p><p><strong>servicePort: 80</strong></p><p><strong>Default Backends</strong>: An Ingress with no rules, like the one shown in the previous section, sends all traffic to a single default backend. You can use the same technique to tell a loadbalancer where to find your website&#x27;s 404 page, by specifying a set of rules and a default backend. Traffic is routed to your default backend if none of the Hosts in your Ingress match the Host in the request header, and/or none of the paths match the URL of the request.</p><h5><strong>TLS</strong></h5><p>You can secure an Ingress by specifying a <a href="https://kubernetes.io/docs/user-guide/secrets">secret</a> that contains a TLS private key and certificate. Currently the Ingress only supports a single TLS port, 443, and assumes TLS termination. If the TLS configuration section in an Ingress specifies different hosts, they will be multiplexed on the same port according to the hostname specified through the SNI TLS extension (provided the Ingress controller supports SNI). The TLS secret must contain keys named <strong>tls.crt</strong> and <strong>tls.key</strong> that contain the certificate and private key to use for TLS, e.g.:</p><p><strong>apiVersion: v1</strong></p><p><strong>data:</strong></p><p><strong>tls.crt: base64 encoded cert</strong></p><p><strong>tls.key: base64 encoded key</strong></p><p><strong>kind: Secret</strong></p><p><strong>metadata:</strong></p><p><strong>name: testsecret</strong></p><p><strong>namespace: default</strong></p><p><strong>type: Opaque</strong></p><p>Referencing this secret in an Ingress will tell the Ingress controller to secure the channel from the client to the loadbalancer using TLS:</p><p><strong>apiVersion: extensions/v1beta1</strong></p><p><strong>kind: Ingress</strong></p><p><strong>metadata:</strong></p><p><strong>name: no-rules-map</strong></p><p><strong>spec:</strong></p><p><strong>tls:</strong></p><p><strong>- secretName: testsecret</strong></p><p><strong>backend:</strong></p><p><strong>serviceName: s1</strong></p><p><strong>servicePort: 80</strong></p><p>Note that there is a gap between TLS features supported by various Ingress controllers. Please refer to documentation on <a href="https://git.k8s.io/ingress-nginx/README.md#https">nginx</a>, <a href="https://git.k8s.io/ingress-gce/README.md#frontend-https">GCE</a>, or any other platform specific Ingress controller to understand how TLS works in your environment.</p><h5><strong>Loadbalancing</strong></h5><p>An Ingress controller is bootstrapped with some load balancing policy settings that it applies to all Ingress, such as the load balancing algorithm, backend weight scheme, and others. More advanced load balancing concepts (e.g.: persistent sessions, dynamic weights) are not yet exposed through the Ingress. You can still get these features through the <a href="https://github.com/kubernetes/ingress-nginx/blob/master/docs/catalog.md">service loadbalancer</a>. With time, we plan to distill load balancing patterns that are applicable cross platform into the Ingress resource.</p><p>It&#x27;s also worth noting that even though health checks are not exposed directly through the Ingress, there exist parallel concepts in Kubernetes such as <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/">readiness probes</a> which allow you to achieve the same end result. Please review the controller specific docs to see how they handle health checks (<a href="https://git.k8s.io/ingress-nginx/README.md">nginx</a>, <a href="https://git.k8s.io/ingress-gce/README.md#health-checks">GCE</a>).</p><h4>Updating an Ingress</h4><p>Say you&#x27;d like to add a new Host to an existing Ingress, you can update it by editing the resource:</p><p><strong>$ kubectl get ing</strong></p><p><strong>NAME RULE BACKEND ADDRESS</strong></p><p><strong>test - 178.91.123.132</strong></p><p><strong>foo.bar.com</strong></p><p><strong>/foo s1:80</strong></p><p><strong>$ kubectl edit ing test</strong></p><p>This should pop up an editor with the existing yaml, modify it to include the new Host:</p><p><strong>spec:</strong></p><p><strong>rules:</strong></p><p><strong>- host: foo.bar.com</strong></p><p><strong>http:</strong></p><p><strong>paths:</strong></p><p><strong>- backend:</strong></p><p><strong>serviceName: s1</strong></p><p><strong>servicePort: 80</strong></p><p><strong>path: /foo</strong></p><p><strong>- host: bar.baz.com</strong></p><p><strong>http:</strong></p><p><strong>paths:</strong></p><p><strong>- backend:</strong></p><p><strong>serviceName: s2</strong></p><p><strong>servicePort: 80</strong></p><p><strong>path: /foo</strong></p><p><strong>..</strong></p><p>Saving the yaml will update the resource in the API server, which should tell the Ingress controller to reconfigure the loadbalancer.</p><p><strong>$ kubectl get ing</strong></p><p><strong>NAME RULE BACKEND ADDRESS</strong></p><p><strong>test - 178.91.123.132</strong></p><p><strong>foo.bar.com</strong></p><p><strong>/foo s1:80</strong></p><p><strong>bar.baz.com</strong></p><p><strong>/foo s2:80</strong></p><p>You can achieve the same by invoking <strong>kubectl replace -f</strong> on a modified Ingress yaml file.</p><h4>Failing across availability zones</h4><p>Techniques for spreading traffic across failure domains differs between cloud providers. Please check the documentation of the relevant Ingress controller for details. Please refer to the federation <a href="https://kubernetes.io/docs/concepts/cluster-administration/federation/">doc</a> for details on deploying Ingress in a federated cluster.</p><h4>Future Work</h4><ul><li>Various modes of HTTPS/TLS support (e.g.: SNI, re-encryption)</li><li>Requesting an IP or Hostname via claims</li><li>Combining L4 and L7 Ingress</li><li>More Ingress controllers</li></ul><p>Please track the <a href="https://github.com/kubernetes/kubernetes/pull/12827">L7 and Ingress proposal</a> for more details on the evolution of the resource, and the <a href="https://github.com/kubernetes/ingress/tree/master">Ingress repository</a> for more details on the evolution of various Ingress controllers.</p><h4>Alternatives</h4><p>You can expose a Service in multiple ways that don&#x27;t directly involve the Ingress resource:</p><ul><li>Use <a href="https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer">Service.Type=LoadBalancer</a></li><li>Use <a href="https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport">Service.Type=NodePort</a></li><li>Use a <a href="https://git.k8s.io/contrib/for-demos/proxy-to-service">Port Proxy</a></li></ul><h3>Network Policies</h3><ul><li><a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/#prerequisites"><strong>Prerequisites</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/#isolated-and-non-isolated-pods"><strong>Isolated and Non-isolated Pods</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/#the-networkpolicy-resource"><strong>The NetworkPolicy Resource</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-policies"><strong>Default policies</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-deny-all-ingress-traffic"><strong>Default deny all ingress traffic</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-allow-all-ingress-traffic"><strong>Default allow all ingress traffic</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-deny-all-egress-traffic"><strong>Default deny all egress traffic</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-allow-all-egress-traffic"><strong>Default allow all egress traffic</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-deny-all-ingress-and-all-egress-traffic"><strong>Default deny all ingress and all egress traffic</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/#whats-next"><strong>What&#x27;s next?</strong></a></li></ul><p>A network policy is a specification of how groups of pods are allowed to communicate with each other and other network endpoints.</p><p><strong>NetworkPolicy</strong> resources use labels to select pods and define rules which specify what traffic is allowed to the selected pods.</p><h4>Prerequisites</h4><p>Network policies are implemented by the network plugin, so you must be using a networking solution which supports <strong>NetworkPolicy</strong> - simply creating the resource without a controller to implement it will have no effect.</p><h4>Isolated and Non-isolated Pods</h4><p>By default, pods are non-isolated; they accept traffic from any source.</p><p>Pods become isolated by having a NetworkPolicy that selects them. Once there is any NetworkPolicy in a namespace selecting a particular pod, that pod will reject any connections that are not allowed by any NetworkPolicy. (Other pods in the namespace that are not selected by any NetworkPolicy will continue to accept all traffic.)</p><h4>The NetworkPolicy Resource</h4><p>See the <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#networkpolicy-v1-networking">NetworkPolicy</a> for a full definition of the resource.</p><p>An example <strong>NetworkPolicy</strong> might look like this:</p><p><strong>apiVersion: networking.k8s.io/v1</strong></p><p><strong>kind: NetworkPolicy</strong></p><p><strong>metadata:</strong></p><p><strong>name: test-network-policy</strong></p><p><strong>namespace: default</strong></p><p><strong>spec:</strong></p><p><strong>podSelector:</strong></p><p><strong>matchLabels:</strong></p><p><strong>role: db</strong></p><p><strong>policyTypes:</strong></p><p><strong>- Ingress</strong></p><p><strong>- Egress</strong></p><p><strong>ingress:</strong></p><p><strong>- from:</strong></p><p><strong>- ipBlock:</strong></p><p><strong>cidr: 172.17.0.0/16</strong></p><p><strong>except:</strong></p><p><strong>- 172.17.1.0/24</strong></p><p><strong>- namespaceSelector:</strong></p><p><strong>matchLabels:</strong></p><p><strong>project: myproject</strong></p><p><strong>- podSelector:</strong></p><p><strong>matchLabels:</strong></p><p><strong>role: frontend</strong></p><p><strong>ports:</strong></p><p><strong>- protocol: TCP</strong></p><p><strong>port: 6379</strong></p><p><strong>egress:</strong></p><p><strong>- to:</strong></p><p><strong>- ipBlock:</strong></p><p><strong>cidr: 10.0.0.0/24</strong></p><p><strong>ports:</strong></p><p><strong>- protocol: TCP</strong></p><p><strong>port: 5978</strong></p><p>POSTing this to the API server will have no effect unless your chosen networking solution supports network policy.</p><p><strong>Mandatory Fields</strong>: As with all other Kubernetes config, a <strong>NetworkPolicy</strong> needs <strong>apiVersion</strong>, <strong>kind</strong>, and <strong>metadata</strong> fields. For general information about working with config files, see <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/">Configure Containers Using a ConfigMap</a>, and <a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/overview/">Object Management</a>.</p><p><strong>spec</strong>: <strong>NetworkPolicy</strong> <a href="https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status">spec</a> has all the information needed to define a particular network policy in the given namespace.</p><p><strong>podSelector</strong>: Each <strong>NetworkPolicy</strong> includes a <strong>podSelector</strong> which selects the grouping of pods to which the policy applies. The example policy selects pods with the label &quot;role=db&quot;. An empty <strong>podSelector</strong> selects all pods in the namespace.</p><p><strong>policyTypes</strong>: Each <strong>NetworkPolicy</strong> includes a <strong>policyTypes</strong> list which may include either <strong>Ingress</strong>, <strong>Egress</strong>, or both. The <strong>policyTypes</strong> field indicates whether or not the given policy applies to ingress traffic to selected pod, egress traffic from selected pods, or both. If no <strong>policyTypes</strong> are specified on a NetworkPolicy then by default <strong>Ingress</strong> will always be set and <strong>Egress</strong> will be set if the NetworkPolicy has any egress rules.</p><p><strong>ingress</strong>: Each <strong>NetworkPolicy</strong> may include a list of whitelist <strong>ingress</strong> rules. Each rule allows traffic which matches both the <strong>from</strong> and <strong>ports</strong> sections. The example policy contains a single rule, which matches traffic on a single port, from one of three sources, the first specified via an <strong>ipBlock</strong>, the second via a <strong>namespaceSelector</strong> and the third via a <strong>podSelector</strong>.</p><p><strong>egress</strong>: Each <strong>NetworkPolicy</strong> may include a list of whitelist <strong>egress</strong> rules. Each rule allows traffic which matches both the <strong>to</strong> and <strong>ports</strong> sections. The example policy contains a single rule, which matches traffic on a single port to any destination in <strong>10.0.0.0/24</strong>.</p><p>So, the example NetworkPolicy:</p><ol><li>isolates &quot;role=db&quot; pods in the &quot;default&quot; namespace for both ingress and egress traffic (if they weren&#x27;t already isolated)</li><li>allows connections to TCP port 6379 of &quot;role=db&quot; pods in the &quot;default&quot; namespace from any pod in the &quot;default&quot; namespace with the label &quot;role=frontend&quot;</li><li>allows connections to TCP port 6379 of &quot;role=db&quot; pods in the &quot;default&quot; namespace from any pod in a namespace with the label &quot;project=myproject&quot;</li><li>allows connections to TCP port 6379 of &quot;role=db&quot; pods in the &quot;default&quot; namespace from IP addresses that are in CIDR 172.17.0.0/16 and not in 172.17.1.0/24</li><li>allows connections from any pod in the &quot;default&quot; namespace with the label &quot;role=db&quot; to CIDR 10.0.0.0/24 on TCP port 5978</li></ol><p>See the <a href="https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/">Declare Network Policy</a> walkthrough for further examples.</p><h4>Default policies</h4><p>By default, if no policies exist in a namespace, then all ingress and egress traffic is allowed to and from pods in that namespace. The following examples let you change the default behavior in that namespace.</p><h5><strong>Default deny all ingress traffic</strong></h5><p>You can create a &quot;default&quot; isolation policy for a namespace by creating a NetworkPolicy that selects all pods but does not allow any ingress traffic to those pods.</p><p><strong>apiVersion: networking.k8s.io/v1</strong></p><p><strong>kind: NetworkPolicy</strong></p><p><strong>metadata:</strong></p><p><strong>name: default-deny</strong></p><p><strong>spec:</strong></p><p><strong>podSelector: {}</strong></p><p><strong>policyTypes:</strong></p><p><strong>- Ingress</strong></p><p>This ensures that even pods that aren&#x27;t selected by any other NetworkPolicy will still be isolated. This policy does not change the default egress isolation behavior.</p><h5><strong>Default allow all ingress traffic</strong></h5><p>If you want to allow all traffic to all pods in a namespace (even if policies are added that cause some pods to be treated as &quot;isolated&quot;), you can create a policy that explicitly allows all traffic in that namespace.</p><p><strong>apiVersion: networking.k8s.io/v1</strong></p><p><strong>kind: NetworkPolicy</strong></p><p><strong>metadata:</strong></p><p><strong>name: allow-all</strong></p><p><strong>spec:</strong></p><p><strong>podSelector: {}</strong></p><p><strong>ingress:</strong></p><p><strong>- {}</strong></p><h5><strong>Default deny all egress traffic</strong></h5><p>You can create a &quot;default&quot; egress isolation policy for a namespace by creating a NetworkPolicy that selects all pods but does not allow any egress traffic from those pods.</p><p><strong>apiVersion: networking.k8s.io/v1</strong></p><p><strong>kind: NetworkPolicy</strong></p><p><strong>metadata:</strong></p><p><strong>name: default-deny</strong></p><p><strong>spec:</strong></p><p><strong>podSelector: {}</strong></p><p><strong>policyTypes:</strong></p><p><strong>- Egress</strong></p><p>This ensures that even pods that aren&#x27;t selected by any other NetworkPolicy will not be allowed egress traffic. This policy does not change the default ingress isolation behavior.</p><h5><strong>Default allow all egress traffic</strong></h5><p>If you want to allow all traffic from all pods in a namespace (even if policies are added that cause some pods to be treated as &quot;isolated&quot;), you can create a policy that explicitly allows all egress traffic in that namespace.</p><p><strong>apiVersion: networking.k8s.io/v1</strong></p><p><strong>kind: NetworkPolicy</strong></p><p><strong>metadata:</strong></p><p><strong>name: allow-all</strong></p><p><strong>spec:</strong></p><p><strong>podSelector: {}</strong></p><p><strong>egress:</strong></p><p><strong>- {}</strong></p><p><strong>policyTypes:</strong></p><p><strong>- Egress</strong></p><h5><strong>Default deny all ingress and all egress traffic</strong></h5><p>You can create a &quot;default&quot; policy for a namespace which prevents all ingress AND egress traffic by creating the following NetworkPolicy in that namespace.</p><p><strong>apiVersion: networking.k8s.io/v1</strong></p><p><strong>kind: NetworkPolicy</strong></p><p><strong>metadata:</strong></p><p><strong>name: default-deny</strong></p><p><strong>spec:</strong></p><p><strong>podSelector: {}</strong></p><p><strong>policyTypes:</strong></p><p><strong>- Ingress</strong></p><p><strong>- Egress</strong></p><p>This ensures that even pods that aren&#x27;t selected by any other NetworkPolicy will not be allowed ingress or egress traffic.</p><h4>What&#x27;s next?</h4><ul><li>See the <a href="https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/">Declare Network Policy</a> walkthrough for further examples.</li><li>See more <a href="https://github.com/ahmetb/kubernetes-network-policy-recipes">Recipes</a> for common scenarios enabled by the NetworkPolicy resource.</li></ul><h3>Adding entries to Pod /etc/hosts with HostAliases</h3><ul><li><a href="https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/#default-hosts-file-content"><strong>Default Hosts File Content</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/#adding-additional-entries-with-hostaliases"><strong>Adding Additional Entries with HostAliases</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/#limitations"><strong>Limitations</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/#why-does-kubelet-manage-the-hosts-file"><strong>Why Does Kubelet Manage the Hosts File?</strong></a></li></ul><p>Adding entries to a Pod&#x27;s /etc/hosts file provides Pod-level override of hostname resolution when DNS and other options are not applicable. In 1.7, users can add these custom entries with the HostAliases field in PodSpec.</p><p>Modification not using HostAliases is not suggested because the file is managed by Kubelet and can be overwritten on during Pod creation/restart.</p><h4>Default Hosts File Content</h4><p>Lets start an Nginx Pod which is assigned an Pod IP:</p><p><strong>$ kubectl run nginx --image nginx --generator=run-pod/v1</strong></p><p><strong>pod &quot;nginx&quot; created</strong></p><p><strong>$ kubectl get pods --output=wide</strong></p><p><strong>NAME READY STATUS RESTARTS AGE IP NODE</strong></p><p><strong>nginx 1/1 Running 0 13s 10.200.0.4 worker0</strong></p><p>The hosts file content would look like this:</p><p><strong>$ kubectl exec nginx -- cat /etc/hosts</strong></p><p><strong><em># Kubernetes-managed hosts file.</em></strong></p><p><strong>127.0.0.1 localhost</strong></p><p><strong>::1 localhost ip6-localhost ip6-loopback</strong></p><p><strong>fe00::0 ip6-localnet</strong></p><p><strong>fe00::0 ip6-mcastprefix</strong></p><p><strong>fe00::1 ip6-allnodes</strong></p><p><strong>fe00::2 ip6-allrouters</strong></p><p><strong>10.200.0.4 nginx</strong></p><p>by default, the hosts file only includes ipv4 and ipv6 boilerplates like <strong>localhost</strong> and its own hostname.</p><h4>Adding Additional Entries with HostAliases</h4><p>In addition to the default boilerplate, we can add additional entries to the hosts file to resolve <strong>foo.local</strong>, <strong>bar.local</strong> to <strong>127.0.0.1</strong> and <strong>foo.remote</strong>, <strong>bar.remote</strong> to <strong>10.1.2.3</strong>, we can by adding HostAliases to the Pod under <strong>.spec.hostAliases</strong>:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>ho                                                                 |
| staliases-pod.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/w">https://raw.githubusercontent.com/kubernetes/w</a> |
| ebsite/master/docs/concepts/services-networking/hostaliases-pod.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: hostaliases-pod</strong>                                             |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>restartPolicy: Never</strong>                                              |
|                                                                       |
| <strong>hostAliases:</strong>                                                      |
|                                                                       |
| <strong>- ip: &quot;127.0.0.1&quot;</strong>                                               |
|                                                                       |
| <strong>hostnames:</strong>                                                        |
|                                                                       |
| <strong>- &quot;foo.local&quot;</strong>                                                   |
|                                                                       |
| <strong>- &quot;bar.local&quot;</strong>                                                   |
|                                                                       |
| <strong>- ip: &quot;10.1.2.3&quot;</strong>                                                |
|                                                                       |
| <strong>hostnames:</strong>                                                        |
|                                                                       |
| <strong>- &quot;foo.remote&quot;</strong>                                                  |
|                                                                       |
| <strong>- &quot;bar.remote&quot;</strong>                                                  |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: cat-hosts</strong>                                                 |
|                                                                       |
| <strong>image: busybox</strong>                                                    |
|                                                                       |
| <strong>command:</strong>                                                          |
|                                                                       |
| <strong>- cat</strong>                                                             |
|                                                                       |
| <strong>args:</strong>                                                             |
|                                                                       |
| <strong>- &quot;/etc/hosts&quot;</strong>                                                  |
+-----------------------------------------------------------------------+</p><p>This Pod can be started with the following commands:</p><p><strong>$ kubectl apply -f hostaliases-pod.yaml</strong></p><p><strong>pod &quot;hostaliases-pod&quot; created</strong></p><p><strong>$ kubectl get pod -o=wide</strong></p><p><strong>NAME READY STATUS RESTARTS AGE IP NODE</strong></p><p><strong>hostaliases-pod 0/1 Completed 0 6s 10.244.135.10 node3</strong></p><p>The hosts file content would look like this:</p><p><strong>$ kubectl logs hostaliases-pod</strong></p><p><strong><em># Kubernetes-managed hosts file.</em></strong></p><p><strong>127.0.0.1 localhost</strong></p><p><strong>::1 localhost ip6-localhost ip6-loopback</strong></p><p><strong>fe00::0 ip6-localnet</strong></p><p><strong>fe00::0 ip6-mcastprefix</strong></p><p><strong>fe00::1 ip6-allnodes</strong></p><p><strong>fe00::2 ip6-allrouters</strong></p><p><strong>10.244.135.10 hostaliases-pod</strong></p><p><strong>127.0.0.1 foo.local</strong></p><p><strong>127.0.0.1 bar.local</strong></p><p><strong>10.1.2.3 foo.remote</strong></p><p><strong>10.1.2.3 bar.remote</strong></p><p>With the additional entries specified at the bottom.</p><h4>Limitations</h4><p>HostAlias is only supported in 1.7+.</p><p>HostAlias support in 1.7 is limited to non-hostNetwork Pods because kubelet only manages the hosts file for non-hostNetwork Pods.</p><p>In 1.8, HostAlias is supported for all Pods regardless of network configuration.</p><h4>Why Does Kubelet Manage the Hosts File?</h4><p>Kubelet <a href="https://github.com/kubernetes/kubernetes/issues/14633">manages</a> the hosts file for each container of the Pod to prevent Docker from <a href="https://github.com/moby/moby/issues/17190">modifying</a> the file after the containers have already been started.</p><p>Because of the managed-nature of the file, any user-written content will be overwritten whenever the hosts file is remounted by Kubelet in the event of a container restart or a Pod reschedule. Thus, it is not suggested to modify the contents of the file.</p><h2>Storage</h2><h3>Volumes</h3><p>On-disk files in a container are ephemeral, which presents some problems for non-trivial applications when running in containers. First, when a container crashes, kubelet will restart it, but the files will be lost - the container starts with a clean state. Second, when running containers together in a <strong>Pod</strong> it is often necessary to share files between those containers. The Kubernetes <strong>Volume</strong> abstraction solves both of these problems.</p><p>Familiarity with <a href="https://kubernetes.io/docs/user-guide/pods">pods</a> is suggested.</p><ul><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#background"><strong>Background</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes"><strong>Types of Volumes</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#awselasticblockstore"><strong>awsElasticBlockStore</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#creating-an-ebs-volume"><strong>Creating an EBS volume</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#aws-ebs-example-configuration"><strong>AWS EBS Example configuration</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#azuredisk"><strong>azureDisk</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#azurefile"><strong>azureFile</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#cephfs"><strong>cephfs</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#configmap"><strong>configMap</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#downwardapi"><strong>downwardAPI</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#emptydir"><strong>emptyDir</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#example-pod"><strong>Example pod</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#fc-fibre-channel"><strong>fc (fibre channel)</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#flocker"><strong>flocker</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#gcepersistentdisk"><strong>gcePersistentDisk</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#creating-a-pd"><strong>Creating a PD</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#example-pod-1"><strong>Example pod</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#gitrepo"><strong>gitRepo</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#glusterfs"><strong>glusterfs</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#hostpath"><strong>hostPath</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#example-pod-2"><strong>Example pod</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#iscsi"><strong>iscsi</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#local"><strong>local</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#nfs"><strong>nfs</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#persistentvolumeclaim"><strong>persistentVolumeClaim</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#projected"><strong>projected</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#example-pod-with-a-secret-a-downward-api-and-a-configmap"><strong>Example pod with a secret, a downward API, and a configmap.</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#example-pod-with-multiple-secrets-with-a-non-default-permission-mode-set"><strong>Example pod with multiple secrets with a non-default permission mode set.</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#portworxvolume"><strong>portworxVolume</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#quobyte"><strong>quobyte</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#rbd"><strong>rbd</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#scaleio"><strong>scaleIO</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#secret"><strong>secret</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#storageos"><strong>storageOS</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#vspherevolume"><strong>vsphereVolume</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#creating-a-vmdk-volume"><strong>Creating a VMDK volume</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#vsphere-vmdk-example-configuration"><strong>vSphere VMDK Example configuration</strong></a></li></ul></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#using-subpath"><strong>Using subPath</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#resources"><strong>Resources</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#out-of-tree-volume-plugins"><strong>Out-of-Tree Volume Plugins</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#csi"><strong>CSI</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#flexvolume"><strong>FlexVolume</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#mount-propagation"><strong>Mount propagation</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#configuration"><strong>Configuration</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Background</h4><p>Docker also has a concept of <a href="https://docs.docker.com/engine/admin/volumes/">volumes</a>, though it is somewhat looser and less managed. In Docker, a volume is simply a directory on disk or in another container. Lifetimes are not managed and until very recently there were only local-disk-backed volumes. Docker now provides volume drivers, but the functionality is very limited for now (e.g. as of Docker 1.7 only one volume driver is allowed per container and there is no way to pass parameters to volumes).</p><p>A Kubernetes volume, on the other hand, has an explicit lifetime - the same as the pod that encloses it. Consequently, a volume outlives any containers that run within the Pod, and data is preserved across Container restarts. Of course, when a Pod ceases to exist, the volume will cease to exist, too. Perhaps more importantly than this, Kubernetes supports many types of volumes, and a Pod can use any number of them simultaneously.</p><p>At its core, a volume is just a directory, possibly with some data in it, which is accessible to the containers in a pod. How that directory comes to be, the medium that backs it, and the contents of it are determined by the particular volume type used.</p><p>To use a volume, a pod specifies what volumes to provide for the pod (the <strong>spec.volumes</strong> field) and where to mount those into containers (the <strong>spec.containers.volumeMounts</strong> field).</p><p>A process in a container sees a filesystem view composed from their Docker image and volumes. The <a href="https://docs.docker.com/userguide/dockerimages/">Docker image</a> is at the root of the filesystem hierarchy, and any volumes are mounted at the specified paths within the image. Volumes can not mount onto other volumes or have hard links to other volumes. Each container in the Pod must independently specify where to mount each volume.</p><h4>Types of Volumes</h4><p>Kubernetes supports several types of Volumes:</p><ul><li><strong>awsElasticBlockStore</strong></li><li><strong>azureDisk</strong></li><li><strong>azureFile</strong></li><li><strong>cephfs</strong></li><li><strong>configMap</strong></li><li><strong>csi</strong></li><li><strong>downwardAPI</strong></li><li><strong>emptyDir</strong></li><li><strong>fc</strong> (fibre channel)</li><li><strong>flocker</strong></li><li><strong>gcePersistentDisk</strong></li><li><strong>gitRepo</strong></li><li><strong>glusterfs</strong></li><li><strong>hostPath</strong></li><li><strong>iscsi</strong></li><li><strong>local</strong></li><li><strong>nfs</strong></li><li><strong>persistentVolumeClaim</strong></li><li><strong>projected</strong></li><li><strong>portworxVolume</strong></li><li><strong>quobyte</strong></li><li><strong>rbd</strong></li><li><strong>scaleIO</strong></li><li><strong>secret</strong></li><li><strong>storageos</strong></li><li><strong>vsphereVolume</strong></li></ul><p>We welcome additional contributions.</p><h5><strong>awsElasticBlockStore</strong></h5><p>An <strong>awsElasticBlockStore</strong> volume mounts an Amazon Web Services (AWS) <a href="http://aws.amazon.com/ebs/">EBS Volume</a> into your pod. Unlike <strong>emptyDir</strong>, which is erased when a Pod is removed, the contents of an EBS volume are preserved and the volume is merely unmounted. This means that an EBS volume can be pre-populated with data, and that data can be &quot;handed off&quot; between pods.</p><p><strong>Important:</strong> You must create an EBS volume using <strong>aws ec2 create-volume</strong> or the AWS API before you can use it.</p><p>There are some restrictions when using an <strong>awsElasticBlockStore</strong> volume:</p><ul><li>the nodes on which pods are running must be AWS EC2 instances</li><li>those instances need to be in the same region and availability-zone as the EBS volume</li><li>EBS only supports a single EC2 instance mounting a volume</li></ul><h6><strong>Creating an EBS volume</strong></h6><p>Before you can use an EBS volume with a pod, you need to create it.</p><p><strong>aws ec2 create-volume --availability-zone=eu-west-1a --size=10 --volume-type=gp2</strong></p><p>Make sure the zone matches the zone you brought up your cluster in. (And also check that the size and EBS volume type are suitable for your use!)</p><h6><strong>AWS EBS Example configuration</strong></h6><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: test-ebs</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- image: k8s.gcr.io/test-webserver</strong></p><p><strong>name: test-container</strong></p><p><strong>volumeMounts:</strong></p><p><strong>- mountPath: /test-ebs</strong></p><p><strong>name: test-volume</strong></p><p><strong>volumes:</strong></p><p><strong>- name: test-volume</strong></p><p><strong><em># This AWS EBS volume must already exist.</em></strong></p><p><strong>awsElasticBlockStore:</strong></p><p><strong>volumeID: <code>&lt;volume-id&gt;</code></strong></p><p><strong>fsType: ext4</strong></p><h5><strong>azureDisk</strong></h5><p>A <strong>azureDisk</strong> is used to mount a Microsoft Azure <a href="https://azure.microsoft.com/en-us/documentation/articles/virtual-machines-linux-about-disks-vhds/">Data Disk</a> into a Pod.</p><p>More details can be found <a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/azure_disk/README.md">here</a>.</p><h5><strong>azureFile</strong></h5><p>A <strong>azureFile</strong> is used to mount a Microsoft Azure File Volume (SMB 2.1 and 3.0) into a Pod.</p><p>More details can be found <a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/azure_file/README.md">here</a>.</p><h5><strong>cephfs</strong></h5><p>A <strong>cephfs</strong> volume allows an existing CephFS volume to be mounted into your pod. Unlike <strong>emptyDir</strong>, which is erased when a Pod is removed, the contents of a <strong>cephfs</strong> volume are preserved and the volume is merely unmounted. This means that a CephFS volume can be pre-populated with data, and that data can be &quot;handed off&quot; between pods. CephFS can be mounted by multiple writers simultaneously.</p><p><strong>Important:</strong> You must have your own Ceph server running with the share exported before you can use it.</p><p>See the <a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/cephfs/">CephFS example</a> for more details.</p><h5><strong>configMap</strong></h5><p>The <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/"><strong>configMap</strong></a> resource provides a way to inject configuration data into Pods. The data stored in a <strong>ConfigMap</strong> object can be referenced in a volume of type <strong>configMap</strong> and then consumed by containerized applications running in a Pod.</p><p>When referencing a <strong>configMap</strong> object, you can simply provide its name in the volume to reference it. You can also customize the path to use for a specific entry in the ConfigMap. For example, to mount the <strong>log-config</strong> ConfigMap onto a Pod called <strong>configmap-pod</strong>, you might use the YAML below:</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: configmap-pod</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: test</strong></p><p><strong>image: busybox</strong></p><p><strong>volumeMounts:</strong></p><p><strong>- name: config-vol</strong></p><p><strong>mountPath: /etc/config</strong></p><p><strong>volumes:</strong></p><p><strong>- name: config-vol</strong></p><p><strong>configMap:</strong></p><p><strong>name: log-config</strong></p><p><strong>items:</strong></p><p><strong>- key: log_level</strong></p><p><strong>path: log_level</strong></p><p>The <strong>log-config</strong> ConfigMap is mounted as a volume, and all contents stored in its <strong>log_level</strong>entry are mounted into the Pod at path &quot;<strong>/etc/config/log_level</strong>&quot;. Note that this path is derived from the volume&#x27;s <strong>mountPath</strong> and the <strong>path</strong> keyed with <strong>log_level</strong>.</p><p><strong>Note:</strong> A container using a ConfigMap as a <a href="https://kubernetes.io/docs/concepts/storage/volumes/#using-subpath">subPath</a> volume mount will not receive ConfigMap updates.</p><h5><strong>downwardAPI</strong></h5><p>A <strong>downwardAPI</strong> volume is used to make downward API data available to applications. It mounts a directory and writes the requested data in plain text files.</p><p><strong>Note:</strong> A container using Downward API as a <a href="https://kubernetes.io/docs/concepts/storage/volumes/#using-subpath">subPath</a> volume mount will not receive Downward API updates.</p><p>See the <a href="https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/"><strong>downwardAPI</strong> volume example</a> for more details.</p><h5><strong>emptyDir</strong></h5><p>An <strong>emptyDir</strong> volume is first created when a Pod is assigned to a Node, and exists as long as that Pod is running on that node. As the name says, it is initially empty. Containers in the pod can all read and write the same files in the <strong>emptyDir</strong> volume, though that volume can be mounted at the same or different paths in each container. When a Pod is removed from a node for any reason, the data in the <strong>emptyDir</strong> is deleted forever.</p><p><strong>Note:</strong> a container crashing does NOT remove a pod from a node, so the data in an <strong>emptyDir</strong>volume is safe across container crashes.</p><p>Some uses for an <strong>emptyDir</strong> are:</p><ul><li>scratch space, such as for a disk-based merge sort</li><li>checkpointing a long computation for recovery from crashes</li><li>holding files that a content-manager container fetches while a webserver container serves the data</li></ul><p>By default, <strong>emptyDir</strong> volumes are stored on whatever medium is backing the node - that might be disk or SSD or network storage, depending on your environment. However, you can set the <strong>emptyDir.medium</strong> field to <strong>&quot;Memory&quot;</strong> to tell Kubernetes to mount a tmpfs (RAM-backed filesystem) for you instead. While tmpfs is very fast, be aware that unlike disks, tmpfs is cleared on node reboot and any files you write will count against your container&#x27;s memory limit.</p><h6><strong>Example pod</strong></h6><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: test-pd</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- image: k8s.gcr.io/test-webserver</strong></p><p><strong>name: test-container</strong></p><p><strong>volumeMounts:</strong></p><p><strong>- mountPath: /cache</strong></p><p><strong>name: cache-volume</strong></p><p><strong>volumes:</strong></p><p><strong>- name: cache-volume</strong></p><p><strong>emptyDir: {}</strong></p><h5><strong>fc (fibre channel)</strong></h5><p>An <strong>fc</strong> volume allows an existing fibre channel volume to be mounted in a pod. You can specify single or multiple target World Wide Names using the parameter <strong>targetWWNs</strong> in your volume configuration. If multiple WWNs are specified, targetWWNs expect that those WWNs are from multi-path connections.</p><p><strong>Important:</strong> You must configure FC SAN Zoning to allocate and mask those LUNs (volumes) to the target WWNs beforehand so that Kubernetes hosts can access them.</p><p>See the <a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/fibre_channel">FC example</a> for more details.</p><h5><strong>flocker</strong></h5><p><a href="https://github.com/ClusterHQ/flocker">Flocker</a> is an open-source clustered container data volume manager. It provides management and orchestration of data volumes backed by a variety of storage backends.</p><p>A <strong>flocker</strong> volume allows a Flocker dataset to be mounted into a pod. If the dataset does not already exist in Flocker, it needs to be first created with the Flocker CLI or by using the Flocker API. If the dataset already exists it will be reattached by Flocker to the node that the pod is scheduled. This means data can be &quot;handed off&quot; between pods as required.</p><p><strong>Important:</strong> You must have your own Flocker installation running before you can use it.</p><p>See the <a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/flocker">Flocker example</a> for more details.</p><h5><strong>gcePersistentDisk</strong></h5><p>A <strong>gcePersistentDisk</strong> volume mounts a Google Compute Engine (GCE) <a href="http://cloud.google.com/compute/docs/disks">Persistent Disk</a> into your pod. Unlike <strong>emptyDir</strong>, which is erased when a Pod is removed, the contents of a PD are preserved and the volume is merely unmounted. This means that a PD can be pre-populated with data, and that data can be &quot;handed off&quot; between pods.</p><p><strong>Important:</strong> You must create a PD using <strong>gcloud</strong> or the GCE API or UI before you can use it.</p><p>There are some restrictions when using a <strong>gcePersistentDisk</strong>:</p><ul><li>the nodes on which pods are running must be GCE VMs</li><li>those VMs need to be in the same GCE project and zone as the PD</li></ul><p>A feature of PD is that they can be mounted as read-only by multiple consumers simultaneously. This means that you can pre-populate a PD with your dataset and then serve it in parallel from as many pods as you need. Unfortunately, PDs can only be mounted by a single consumer in read-write mode - no simultaneous writers allowed.</p><p>Using a PD on a pod controlled by a ReplicationController will fail unless the PD is read-only or the replica count is 0 or 1.</p><h6><strong>Creating a PD</strong></h6><p>Before you can use a GCE PD with a pod, you need to create it.</p><p><strong>gcloud compute disks create --size=500GB --zone=us-central1-a my-data-disk</strong></p><h6><strong>Example pod</strong></h6><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: test-pd</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- image: k8s.gcr.io/test-webserver</strong></p><p><strong>name: test-container</strong></p><p><strong>volumeMounts:</strong></p><p><strong>- mountPath: /test-pd</strong></p><p><strong>name: test-volume</strong></p><p><strong>volumes:</strong></p><p><strong>- name: test-volume</strong></p><p><strong><em># This GCE PD must already exist.</em></strong></p><p><strong>gcePersistentDisk:</strong></p><p><strong>pdName: my-data-disk</strong></p><p><strong>fsType: ext4</strong></p><h5><strong>gitRepo</strong></h5><p>A <strong>gitRepo</strong> volume is an example of what can be done as a volume plugin. It mounts an empty directory and clones a git repository into it for your pod to use. In the future, such volumes may be moved to an even more decoupled model, rather than extending the Kubernetes API for every such use case.</p><p>Here is an example for gitRepo volume:</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: server</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- image: nginx</strong></p><p><strong>name: nginx</strong></p><p><strong>volumeMounts:</strong></p><p><strong>- mountPath: /mypath</strong></p><p><strong>name: git-volume</strong></p><p><strong>volumes:</strong></p><p><strong>- name: git-volume</strong></p><p><strong>gitRepo:</strong></p><p><strong>repository: &quot;git@somewhere:me/my-git-repository.git&quot;</strong></p><p><strong>revision: &quot;22f1d8406d464b0c0874075539c1f2e96c253775&quot;</strong></p><h5><strong>glusterfs</strong></h5><p>A <strong>glusterfs</strong> volume allows a <a href="http://www.gluster.org/">Glusterfs</a> (an open source networked filesystem) volume to be mounted into your pod. Unlike <strong>emptyDir</strong>, which is erased when a Pod is removed, the contents of a <strong>glusterfs</strong> volume are preserved and the volume is merely unmounted. This means that a glusterfs volume can be pre-populated with data, and that data can be &quot;handed off&quot; between pods. GlusterFS can be mounted by multiple writers simultaneously.</p><p><strong>Important:</strong> You must have your own GlusterFS installation running before you can use it.</p><p>See the <a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/glusterfs">GlusterFS example</a> for more details.</p><h5><strong>hostPath</strong></h5><p>A <strong>hostPath</strong> volume mounts a file or directory from the host node&#x27;s filesystem into your pod. This is not something that most Pods will need, but it offers a powerful escape hatch for some applications.</p><p>For example, some uses for a <strong>hostPath</strong> are:</p><ul><li>running a container that needs access to Docker internals; use a <strong>hostPath</strong> of <strong>/var/lib/docker</strong></li><li>running cAdvisor in a container; use a <strong>hostPath</strong> of <strong>/sys</strong></li><li>allowing a pod to specify whether a given <strong>hostPath</strong> should exist prior to the pod running, whether it should be created, and what it should exist as</li></ul><p>In addition to the required <strong>path</strong> property, user can optionally specify a <strong>type</strong> for a <strong>hostPath</strong>volume.</p><p>The supported values for field <strong>type</strong> are:</p><p>  Value                   Behavior</p><hr/><p>                          Empty string (default) is for backward compatibility, which means that no checks will be performed before mounting the hostPath volume.
<strong>DirectoryOrCreate</strong>   If nothing exists at the given path, an empty directory will be created there as needed with permission set to 0755, having the same group and ownership with Kubelet.
<strong>Directory</strong>           A directory must exist at the given path
<strong>FileOrCreate</strong>        If nothing exists at the given path, an empty file will be created there as needed with permission set to 0644, having the same group and ownership with Kubelet.
<strong>File</strong>                A file must exist at the given path
<strong>Socket</strong>              A UNIX socket must exist at the given path
<strong>CharDevice</strong>          A character device must exist at the given path
<strong>BlockDevice</strong>         A block device must exist at the given path</p><p>Watch out when using this type of volume, because:</p><ul><li>pods with identical configuration (such as created from a podTemplate) may behave differently on different nodes due to different files on the nodes</li><li>when Kubernetes adds resource-aware scheduling, as is planned, it will not be able to account for resources used by a <strong>hostPath</strong></li><li>the files or directories created on the underlying hosts are only writable by root. You either need to run your process as root in a <a href="https://kubernetes.io/docs/user-guide/security-context">privileged container</a> or modify the file permissions on the host to be able to write to a <strong>hostPath</strong> volume</li></ul><h6><strong>Example pod</strong></h6><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: test-pd</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- image: k8s.gcr.io/test-webserver</strong></p><p><strong>name: test-container</strong></p><p><strong>volumeMounts:</strong></p><p><strong>- mountPath: /test-pd</strong></p><p><strong>name: test-volume</strong></p><p><strong>volumes:</strong></p><p><strong>- name: test-volume</strong></p><p><strong>hostPath:</strong></p><p><strong><em># directory location on host</em></strong></p><p><strong>path: /data</strong></p><p><strong><em># this field is optional</em></strong></p><p><strong>type: Directory</strong></p><h5><strong>iscsi</strong></h5><p>An <strong>iscsi</strong> volume allows an existing iSCSI (SCSI over IP) volume to be mounted into your pod. Unlike <strong>emptyDir</strong>, which is erased when a Pod is removed, the contents of an <strong>iscsi</strong> volume are preserved and the volume is merely unmounted. This means that an iscsi volume can be pre-populated with data, and that data can be &quot;handed off&quot; between pods.</p><p><strong>Important:</strong> You must have your own iSCSI server running with the volume created before you can use it.</p><p>A feature of iSCSI is that it can be mounted as read-only by multiple consumers simultaneously. This means that you can pre-populate a volume with your dataset and then serve it in parallel from as many pods as you need. Unfortunately, iSCSI volumes can only be mounted by a single consumer in read-write mode - no simultaneous writers allowed.</p><p>See the <a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/iscsi">iSCSI example</a> for more details.</p><h5><strong>local</strong></h5><p><strong>FEATURE STATE:</strong> <strong>Kubernetes v1.10</strong> <a href="https://kubernetes.io/docs/concepts/storage/volumes/">beta</a></p><p><strong>Note:</strong> The alpha PersistentVolume NodeAffinity annotation has been deprecated and will be removed in a future release. Existing PersistentVolumes using this annotation must be updated by the user to use the new PersistentVolume <strong>NodeAffinity</strong> field.</p><p>A <strong>local</strong> volume represents a mounted local storage device such as a disk, partition or directory.</p><p>Local volumes can only be used as a statically created PersistentVolume. Dynamic provisioning is not supported yet.</p><p>Compared to <strong>hostPath</strong> volumes, local volumes can be used in a durable and portable manner without manually scheduling pods to nodes, as the system is aware of the volume&#x27;s node constraints by looking at the node affinity on the PersistentVolume.</p><p>However, local volumes are still subject to the availability of the underlying node and are not suitable for all applications. If a node becomes unhealthy, then the local volume will also become inaccessible, and a pod using it will not be able to run. Applications using local volumes must be able to tolerate this reduced availability, as well as potential data loss, depending on the durability characteristics of the underlying disk.</p><p>The following is an example PersistentVolume spec using a <strong>local</strong> volume and <strong>nodeAffinity</strong>:</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: PersistentVolume</strong></p><p><strong>metadata:</strong></p><p><strong>name: example-pv</strong></p><p><strong>spec:</strong></p><p><strong>capacity:</strong></p><p><strong>storage: 100Gi</strong></p><p><strong><em># volumeMode field requires BlockVolume Alpha feature gate to be enabled.</em></strong></p><p><strong>volumeMode: Filesystem</strong></p><p><strong>accessModes:</strong></p><p><strong>- ReadWriteOnce</strong></p><p><strong>persistentVolumeReclaimPolicy: Delete</strong></p><p><strong>storageClassName: local-storage</strong></p><p><strong>local:</strong></p><p><strong>path: /mnt/disks/ssd1</strong></p><p><strong>nodeAffinity:</strong></p><p><strong>required:</strong></p><p><strong>nodeSelectorTerms:</strong></p><p><strong>- matchExpressions:</strong></p><p><strong>- key: kubernetes.io/hostname</strong></p><p><strong>operator: In</strong></p><p><strong>values:</strong></p><p><strong>- example-node</strong></p><p>PersistentVolume <strong>nodeAffinity</strong> is required when using local volumes. It enables the Kubernetes scheduler to correctly schedule pods using local volumes to the correct node.</p><p>PersistentVolume <strong>volumeMode</strong> can now be set to &quot;Block&quot; (instead of the default value &quot;Filesystem&quot;) to expose the local volume as a raw block device. The <strong>volumeMode</strong> field requires <strong>BlockVolume</strong>Alpha feature gate to be enabled.</p><p>When using local volumes, it is recommended to create a StorageClass with <strong>volumeBindingMode</strong>set to <strong>WaitForFirstConsumer</strong>. See the <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#local">example</a>. Delaying volume binding ensures that the PersistentVolumeClaim binding decision will also be evaluated with any other node constraints the pod may have, such as node resource requirements, node selectors, pod affinity, and pod anti-affinity.</p><p>An external static provisioner can be run separately for improved management of the local volume lifecycle. Note that this provisioner does not support dynamic provisioning yet. For an example on how to run an external local provisioner, see the <a href="https://github.com/kubernetes-incubator/external-storage/tree/master/local-volume">local volume provisioner user guide</a>.</p><p><strong>Note:</strong> The local PersistentVolume requires manual cleanup and deletion by the user if the external static provisioner is not used to manage the volume lifecycle.</p><h5><strong>nfs</strong></h5><p>An <strong>nfs</strong> volume allows an existing NFS (Network File System) share to be mounted into your pod. Unlike <strong>emptyDir</strong>, which is erased when a Pod is removed, the contents of an <strong>nfs</strong> volume are preserved and the volume is merely unmounted. This means that an NFS volume can be pre-populated with data, and that data can be &quot;handed off&quot; between pods. NFS can be mounted by multiple writers simultaneously.</p><p><strong>Important:</strong> You must have your own NFS server running with the share exported before you can use it.</p><p>See the <a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/nfs">NFS example</a> for more details.</p><h5><strong>persistentVolumeClaim</strong></h5><p>A <strong>persistentVolumeClaim</strong> volume is used to mount a <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">PersistentVolume</a> into a pod. PersistentVolumes are a way for users to &quot;claim&quot; durable storage (such as a GCE PersistentDisk or an iSCSI volume) without knowing the details of the particular cloud environment.</p><p>See the <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">PersistentVolumes example</a> for more details.</p><h5><strong>projected</strong></h5><p>A <strong>projected</strong> volume maps several existing volume sources into the same directory.</p><p>Currently, the following types of volume sources can be projected:</p><ul><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#secret"><strong>secret</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#downwardapi"><strong>downwardAPI</strong></a></li><li><strong>configMap</strong></li></ul><p>All sources are required to be in the same namespace as the pod. For more details, see the <a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/all-in-one-volume.md">all-in-one volume design document</a>.</p><h6><strong>Example pod with a secret, a downward API, and a configmap.</strong></h6><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: volume-test</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: container-test</strong></p><p><strong>image: busybox</strong></p><p><strong>volumeMounts:</strong></p><p><strong>- name: all-in-one</strong></p><p><strong>mountPath: &quot;/projected-volume&quot;</strong></p><p><strong>readOnly: true</strong></p><p><strong>volumes:</strong></p><p><strong>- name: all-in-one</strong></p><p><strong>projected:</strong></p><p><strong>sources:</strong></p><p><strong>- secret:</strong></p><p><strong>name: mysecret</strong></p><p><strong>items:</strong></p><p><strong>- key: username</strong></p><p><strong>path: my-group/my-username</strong></p><p><strong>- downwardAPI:</strong></p><p><strong>items:</strong></p><p><strong>- path: &quot;labels&quot;</strong></p><p><strong>fieldRef:</strong></p><p><strong>fieldPath: metadata.labels</strong></p><p><strong>- path: &quot;cpu_limit&quot;</strong></p><p><strong>resourceFieldRef:</strong></p><p><strong>containerName: container-test</strong></p><p><strong>resource: limits.cpu</strong></p><p><strong>- configMap:</strong></p><p><strong>name: myconfigmap</strong></p><p><strong>items:</strong></p><p><strong>- key: config</strong></p><p><strong>path: my-group/my-config</strong></p><h6><strong>Example pod with multiple secrets with a non-default permission mode set.</strong></h6><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: volume-test</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: container-test</strong></p><p><strong>image: busybox</strong></p><p><strong>volumeMounts:</strong></p><p><strong>- name: all-in-one</strong></p><p><strong>mountPath: &quot;/projected-volume&quot;</strong></p><p><strong>readOnly: true</strong></p><p><strong>volumes:</strong></p><p><strong>- name: all-in-one</strong></p><p><strong>projected:</strong></p><p><strong>sources:</strong></p><p><strong>- secret:</strong></p><p><strong>name: mysecret</strong></p><p><strong>items:</strong></p><p><strong>- key: username</strong></p><p><strong>path: my-group/my-username</strong></p><p><strong>- secret:</strong></p><p><strong>name: mysecret2</strong></p><p><strong>items:</strong></p><p><strong>- key: password</strong></p><p><strong>path: my-group/my-password</strong></p><p><strong>mode: 511</strong></p><p>Each projected volume source is listed in the spec under <strong>sources</strong>. The parameters are nearly the same with two exceptions:</p><ul><li>For secrets, the <strong>secretName</strong> field has been changed to <strong>name</strong> to be consistent with ConfigMap naming.</li><li>The <strong>defaultMode</strong> can only be specified at the projected level and not for each volume source. However, as illustrated above, you can explicitly set the <strong>mode</strong> for each individual projection.</li></ul><p><strong>Note:</strong> A container using a projected volume source as a <a href="https://kubernetes.io/docs/concepts/storage/volumes/#using-subpath">subPath</a> volume mount will not receive updates for those volume sources.</p><h5><strong>portworxVolume</strong></h5><p>A <strong>portworxVolume</strong> is an elastic block storage layer that runs hyperconverged with Kubernetes. Portworx fingerprints storage in a server, tiers based on capabilities, and aggregates capacity across multiple servers. Portworx runs in-guest in virtual machines or on bare metal Linux nodes.</p><p>A <strong>portworxVolume</strong> can be dynamically created through Kubernetes or it can also be pre-provisioned and referenced inside a Kubernetes pod. Here is an example pod referencing a pre-provisioned PortworxVolume:</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: test-portworx-volume-pod</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- image: k8s.gcr.io/test-webserver</strong></p><p><strong>name: test-container</strong></p><p><strong>volumeMounts:</strong></p><p><strong>- mountPath: /mnt</strong></p><p><strong>name: pxvol</strong></p><p><strong>volumes:</strong></p><p><strong>- name: pxvol</strong></p><p><strong><em># This Portworx volume must already exist.</em></strong></p><p><strong>portworxVolume:</strong></p><p><strong>volumeID: &quot;pxvol&quot;</strong></p><p><strong>fsType: &quot;<code>&lt;fs-type&gt;</code>&quot;</strong></p><p><strong>Important:</strong> Make sure you have an existing PortworxVolume with name <strong>pxvol</strong> before using it in the pod.</p><p>More details and examples can be found <a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/portworx/README.md">here</a>.</p><h5><strong>quobyte</strong></h5><p>A <strong>quobyte</strong> volume allows an existing <a href="http://www.quobyte.com/">Quobyte</a> volume to be mounted into your pod.</p><p><strong>Important:</strong> You must have your own Quobyte setup running with the volumes created before you can use it.</p><p>See the <a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/quobyte">Quobyte example</a> for more details.</p><h5><strong>rbd</strong></h5><p>An <strong>rbd</strong> volume allows a <a href="http://ceph.com/docs/master/rbd/rbd/">Rados Block Device</a> volume to be mounted into your pod. Unlike <strong>emptyDir</strong>, which is erased when a Pod is removed, the contents of a <strong>rbd</strong> volume are preserved and the volume is merely unmounted. This means that a RBD volume can be pre-populated with data, and that data can be &quot;handed off&quot; between pods.</p><p><strong>Important:</strong> You must have your own Ceph installation running before you can use RBD.</p><p>A feature of RBD is that it can be mounted as read-only by multiple consumers simultaneously. This means that you can pre-populate a volume with your dataset and then serve it in parallel from as many pods as you need. Unfortunately, RBD volumes can only be mounted by a single consumer in read-write mode - no simultaneous writers allowed.</p><p>See the <a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/rbd">RBD example</a> for more details.</p><h5><strong>scaleIO</strong></h5><p>ScaleIO is a software-based storage platform that can use existing hardware to create clusters of scalable shared block networked storage. The <strong>scaleIO</strong> volume plugin allows deployed pods to access existing ScaleIO volumes (or it can dynamically provision new volumes for persistent volume claims, see <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#scaleio">ScaleIO Persistent Volumes</a>).</p><p><strong>Important:</strong> You must have an existing ScaleIO cluster already setup and running with the volumes created before you can use them.</p><p>The following is an example pod configuration with ScaleIO:</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: pod-0</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- image: k8s.gcr.io/test-webserver</strong></p><p><strong>name: pod-0</strong></p><p><strong>volumeMounts:</strong></p><p><strong>- mountPath: /test-pd</strong></p><p><strong>name: vol-0</strong></p><p><strong>volumes:</strong></p><p><strong>- name: vol-0</strong></p><p><strong>scaleIO:</strong></p><p><strong>gateway: https://localhost:443/api</strong></p><p><strong>system: scaleio</strong></p><p><strong>protectionDomain: sd0</strong></p><p><strong>storagePool: sp1</strong></p><p><strong>volumeName: vol-0</strong></p><p><strong>secretRef:</strong></p><p><strong>name: sio-secret</strong></p><p><strong>fsType: xfs</strong></p><p>For further detail, please the see the <a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/scaleio">ScaleIO examples</a>.</p><h5><strong>secret</strong></h5><p>A <strong>secret</strong> volume is used to pass sensitive information, such as passwords, to pods. You can store secrets in the Kubernetes API and mount them as files for use by pods without coupling to Kubernetes directly. <strong>secret</strong> volumes are backed by tmpfs (a RAM-backed filesystem) so they are never written to non-volatile storage.</p><p><strong>Important:</strong> You must create a secret in the Kubernetes API before you can use it.</p><p><strong>Note:</strong> A container using a Secret as a <a href="https://kubernetes.io/docs/concepts/storage/volumes/#using-subpath">subPath</a> volume mount will not receive Secret updates.</p><p>Secrets are described in more detail <a href="https://kubernetes.io/docs/user-guide/secrets">here</a>.</p><h5><strong>storageOS</strong></h5><p>A <strong>storageos</strong> volume allows an existing <a href="https://www.storageos.com/">StorageOS</a> volume to be mounted into your pod.</p><p>StorageOS runs as a container within your Kubernetes environment, making local or attached storage accessible from any node within the Kubernetes cluster. Data can be replicated to protect against node failure. Thin provisioning and compression can improve utilization and reduce cost.</p><p>At its core, StorageOS provides block storage to containers, accessible via a file system.</p><p>The StorageOS container requires 64-bit Linux and has no additional dependencies. A free developer license is available.</p><p><strong>Important:</strong> You must run the StorageOS container on each node that wants to access StorageOS volumes or that will contribute storage capacity to the pool. For installation instructions, consult the <a href="https://docs.storageos.com/">StorageOS documentation</a>.</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>labels:</strong></p><p><strong>name: redis</strong></p><p><strong>role: master</strong></p><p><strong>name: test-storageos-redis</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: master</strong></p><p><strong>image: kubernetes/redis:v1</strong></p><p><strong>env:</strong></p><p><strong>- name: MASTER</strong></p><p><strong>value: &quot;true&quot;</strong></p><p><strong>ports:</strong></p><p><strong>- containerPort: 6379</strong></p><p><strong>volumeMounts:</strong></p><p><strong>- mountPath: /redis-master-data</strong></p><p><strong>name: redis-data</strong></p><p><strong>volumes:</strong></p><p><strong>- name: redis-data</strong></p><p><strong>storageos:</strong></p><p><strong><em># The <code>redis-vol01</code> volume must already exist within StorageOS in the <code>default</code> namespace.</em></strong></p><p><strong>volumeName: redis-vol01</strong></p><p><strong>fsType: ext4</strong></p><p>For more information including Dynamic Provisioning and Persistent Volume Claims, please see the<a href="https://github.com/kubernetes/kubernetes/tree/master/examples/volumes/storageos">StorageOS examples</a>.</p><h5><strong>vsphereVolume</strong></h5><p><strong>Prerequisite:</strong> Kubernetes with vSphere Cloud Provider configured. For cloudprovider configuration please refer <a href="https://vmware.github.io/vsphere-storage-for-kubernetes/documentation/">vSphere getting started guide</a>.</p><p>A <strong>vsphereVolume</strong> is used to mount a vSphere VMDK Volume into your Pod. The contents of a volume are preserved when it is unmounted. It supports both VMFS and VSAN datastore.</p><p><strong>Important:</strong> You must create VMDK using one of the following method before using with POD.</p><h6><strong>Creating a VMDK volume</strong></h6><p>Choose one of the following methods to create a VMDK.</p><ul><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#tabset-0">Create using vmkfstools</a></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#tabset-1">Create using vmware-vdiskmanager</a></li></ul><p>First ssh into ESX, then use the following command to create a VMDK:</p><p><strong>vmkfstools -c 2G /vmfs/volumes/DatastoreName/volumes/myDisk.vmdk</strong></p><h6><strong>vSphere VMDK Example configuration</strong></h6><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: test-vmdk</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- image: k8s.gcr.io/test-webserver</strong></p><p><strong>name: test-container</strong></p><p><strong>volumeMounts:</strong></p><p><strong>- mountPath: /test-vmdk</strong></p><p><strong>name: test-volume</strong></p><p><strong>volumes:</strong></p><p><strong>- name: test-volume</strong></p><p><strong><em># This VMDK volume must already exist.</em></strong></p><p><strong>vsphereVolume:</strong></p><p><strong>volumePath: &quot;<!-- -->[DatastoreName]<!-- --> volumes/myDisk&quot;</strong></p><p><strong>fsType: ext4</strong></p><p>More examples can be found <a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/vsphere">here</a>.</p><h4>Using subPath</h4><p>Sometimes, it is useful to share one volume for multiple uses in a single pod. The <strong>volumeMounts.subPath</strong> property can be used to specify a sub-path inside the referenced volume instead of its root.</p><p>Here is an example of a pod with a LAMP stack (Linux Apache Mysql PHP) using a single, shared volume. The HTML contents are mapped to its <strong>html</strong> folder, and the databases will be stored in its <strong>mysql</strong> folder:</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: my-lamp-site</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: mysql</strong></p><p><strong>image: mysql</strong></p><p><strong>env:</strong></p><p><strong>- name: MYSQL_ROOT_PASSWORD</strong></p><p><strong>value: &quot;rootpasswd&quot;</strong></p><p><strong>volumeMounts:</strong></p><p><strong>- mountPath: /var/lib/mysql</strong></p><p><strong>name: site-data</strong></p><p><strong>subPath: mysql</strong></p><p><strong>- name: php</strong></p><p><strong>image: php:7.0-apache</strong></p><p><strong>volumeMounts:</strong></p><p><strong>- mountPath: /var/www/html</strong></p><p><strong>name: site-data</strong></p><p><strong>subPath: html</strong></p><p><strong>volumes:</strong></p><p><strong>- name: site-data</strong></p><p><strong>persistentVolumeClaim:</strong></p><p><strong>claimName: my-lamp-site-data</strong></p><h4>Resources</h4><p>The storage media (Disk, SSD, etc.) of an <strong>emptyDir</strong> volume is determined by the medium of the filesystem holding the kubelet root dir (typically <strong>/var/lib/kubelet</strong>). There is no limit on how much space an <strong>emptyDir</strong> or <strong>hostPath</strong> volume can consume, and no isolation between containers or between pods.</p><p>In the future, we expect that <strong>emptyDir</strong> and <strong>hostPath</strong> volumes will be able to request a certain amount of space using a <a href="https://kubernetes.io/docs/user-guide/compute-resources">resource</a> specification, and to select the type of media to use, for clusters that have several media types.</p><h4>Out-of-Tree Volume Plugins</h4><p>The Out-of-tree volume plugins include the Container Storage Interface (<strong>CSI</strong>) and <strong>FlexVolume</strong>. They enable storage vendors to create custom storage plugins without adding them to the Kubernetes repository.</p><p>Before the introduction of <strong>CSI</strong> and <strong>FlexVolume</strong>, all volume plugins (like volume types listed above) were &quot;in-tree&quot; meaning they were built, linked, compiled, and shipped with the core Kubernetes binaries and extend the core Kubernetes API. This meant that adding a new storage system to Kubernetes (a volume plugin) required checking code into the core Kubernetes code repository.</p><p>Both <strong>CSI</strong> and <strong>FlexVolume</strong> allow volume plugins to be developed independent of the Kubernetes code base, and deployed (installed) on Kubernetes clusters as extensions.</p><p>For storage vendors looking to create an out-of-tree volume plugin, please refer to <a href="https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md">this FAQ</a>.</p><h5><strong>CSI</strong></h5><p><strong>FEATURE STATE:</strong> <strong>Kubernetes v1.10</strong> <a href="https://kubernetes.io/docs/concepts/storage/volumes/">beta</a></p><p><a href="https://github.com/container-storage-interface/spec/blob/master/spec.md">Container Storage Interface</a> (CSI) defines a standard interface for container orchestration systems (like Kubernetes) to expose arbitrary storage systems to their container workloads.</p><p>Please read the <a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md">CSI design proposal</a> for more information.</p><p>CSI support was introduced as alpha in Kubernetes v1.9 and moved to beta in Kubernets v1.10.</p><p>Once a CSI compatible volume driver is deployed on a Kubernetes cluster, users may use the <strong>csi</strong>volume type to attach, mount, etc. the volumes exposed by the CSI driver.</p><p>The <strong>csi</strong> volume type does not support direct reference from pod and may only be referenced in a pod via a <strong>PersistentVolumeClaim</strong> object.</p><p>The following fields are available to storage administrators to configure a CSI persistent volume:</p><ul><li><strong>driver</strong>: A string value that specifies the name of the volume driver to use. This value must corespond to the value returned in the <strong>GetPluginInfoResponse</strong> by the CSI driver as defined in the <a href="https://github.com/container-storage-interface/spec/blob/master/spec.md#getplugininfo">CSI spec</a>. It is used by Kubernetes to identify which CSI driver to call out to, and by CSI driver components to identify which PV objects belong to the CSI driver.</li><li><strong>volumeHandle</strong>: A string value that uniquely identifies the volume. This value must correspond to the value returned in the <strong>volume.id</strong> field of the <strong>CreateVolumeResponse</strong> by the CSI driver as defined in the <a href="https://github.com/container-storage-interface/spec/blob/master/spec.md#createvolume">CSI spec</a>. The value is passed as <strong>volume_id</strong> on all calls to the CSI volume driver when referencing the volume.</li><li><strong>readOnly</strong>: An optional boolean value indicating whether the volume is to be &quot;ControllerPublished&quot; (attached) as read only. Default is false. This value is passed to the CSI driver via the <strong>readonly</strong>field in the <strong>ControllerPublishVolumeRequest</strong>.</li><li><strong>fsType</strong>: If the PV&#x27;s <strong>VolumeMode</strong> is <strong>Filesystem</strong> then this field may be used to specify the filesystem that should be used to mount the volume. If the volume has not been formated and formating is supported, this value will be used to format the volume. If a value is not specified, <strong>ext4</strong> is assumed. This value is passed to the CSI driver via the <strong>VolumeCapability</strong> field of <strong>ControllerPublishVolumeRequest</strong>, <strong>NodeStageVolumeRequest</strong>, and <strong>NodePublishVolumeRequest</strong>.</li><li><strong>volumeAttributes</strong>: A map of string to string that specifies static properties of a volume. This map must corespond to the map returned in the <strong>volume.attributes</strong> field of the <strong>CreateVolumeResponse</strong> by the CSI driver as defined in the <a href="https://github.com/container-storage-interface/spec/blob/master/spec.md#createvolume">CSI spec</a>. The map is passed to the CSI driver via the <strong>volume_attributes</strong> field in the <strong>ControllerPublishVolumeRequest</strong>, <strong>NodeStageVolumeRequest</strong>, and <strong>NodePublishVolumeRequest</strong>.</li><li><strong>controllerPublishSecretRef</strong>: A reference to the secret object containing sensitive information to pass to the CSI driver to complete the CSI <strong>ControllerPublishVolume</strong> and <strong>ControllerUnpublishVolume</strong> calls. This field is optional, and may be empty if no secret is required. If the secret object contains more than one secret, all secrets are passed.</li><li><strong>nodeStageSecretRef</strong>: A reference to the secret object containing sensitive information to pass to the CSI driver to complete the CSI <strong>NodeStageVolume</strong> call. This field is optional, and may be empty if no secret is required. If the secret object contains more than one secret, all secrets are passed.</li><li><strong>nodePublishSecretRef</strong>: A reference to the secret object containing sensitive information to pass to the CSI driver to complete the CSI <strong>NodePublishVolume</strong> call. This field is optional, and may be empty if no secret is required. If the secret object contains more than one secret, all secrets are passed.</li></ul><h5><strong>FlexVolume</strong></h5><p><strong>FlexVolume</strong> is an out-of-tree plugin interface that has existed in Kubernetes since version 1.2 (before CSI). It uses an exec-based model to interface with drivers. FlexVolume driver binaries must be installed in a pre-defined volume plugin path on each node (and in some cases master).</p><p>Pods interact with FlexVolume drivers through the <strong>flexVolume</strong> in-tree plugin. More details can be found <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/flexvolume.md">here</a>.</p><h4>Mount propagation</h4><p><strong>FEATURE STATE:</strong> <strong>Kubernetes v1.10</strong> <a href="https://kubernetes.io/docs/concepts/storage/volumes/">beta</a></p><p>Mount propagation allows for sharing volumes mounted by a Container to other Containers in the same Pod, or even to other Pods on the same node.</p><p>If the &quot;<strong>MountPropagation</strong>&quot; feature is disabled, volume mounts in pods are not propagated. That is, Containers run with <strong>private</strong> mount propagation as described in the <a href="https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt">Linux kernel documentation</a>.</p><p>Mount propagation of a volume is controlled by <strong>mountPropagation</strong> field in Container.volumeMounts. Its values are:</p><ul><li><strong>HostToContainer</strong> - This volume mount will receive all subsequent mounts that are mounted to this volume or any of its subdirectories. This is the default mode.</li></ul><p>In other words, if the host mounts anything inside the volume mount, the Container will see it mounted there.</p><p>Similarly, if any pod with <strong>Bidirectional</strong> mount propagation to the same volume mounts anything there, the Container with <strong>HostToContainer</strong> mount propagation will see it.</p><p>This mode is equal to <strong>rslave</strong> mount propagation as described in the <a href="https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt">Linux kernel documentation</a></p><ul><li><strong>Bidirectional</strong> - This volume mount behaves the same the <strong>HostToContainer</strong> mount. In addition, all volume mounts created by the Container will be propagated back to the host and to all Containers of all Pods that use the same volume.</li></ul><p>A typical use case for this mode is a Pod with a <strong>FlexVolume</strong> driver or a Pod that needs to mount something on the host using a <strong>hostPath</strong> volume.</p><p>This mode is equal to <strong>rshared</strong> mount propagation as described in the <a href="https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt">Linux kernel documentation</a></p><p><strong>Caution:</strong> <strong>Bidirectional</strong> mount propagation can be dangerous. It can damage the host operating system and therefore it is allowed only in privileged Containers. Familiarity with Linux kernel behavior is strongly recommended. In addition, any volume mounts created by Containers in Pods must be destroyed (unmounted) by the Containers on termination.</p><h5><strong>Configuration</strong></h5><p>Before mount propagation can work properly on some deployments (CoreOS, RedHat/Centos, Ubuntu) mount share must be configured correctly in Docker as shown below.</p><p>Edit your Docker&#x27;s <strong>systemd</strong> service file. Set <strong>MountFlags</strong> as follows:</p><p><strong>MountFlags=shared</strong></p><p>Or, remove <strong>MountFlags=slave</strong> if present. Then restart the Docker daemon:</p><p><strong>$ sudo systemctl daemon-reload</strong></p><p><strong>$ sudo systemctl restart docker</strong></p><h4>What&#x27;s next</h4><ul><li>Follow an example of <a href="https://kubernetes.io/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/">deploying WordPress and MySQL with Persistent Volumes</a>.</li></ul><h3>Persistent Volumes</h3><p>This document describes the current state of <strong>PersistentVolumes</strong> in Kubernetes. Familiarity with <a href="https://kubernetes.io/docs/concepts/storage/volumes/">volumes</a> is suggested.</p><ul><li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#introduction"><strong>Introduction</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#lifecycle-of-a-volume-and-claim"><strong>Lifecycle of a volume and claim</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#provisioning"><strong>Provisioning</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#static"><strong>Static</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#dynamic"><strong>Dynamic</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#binding"><strong>Binding</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#using"><strong>Using</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#storage-object-in-use-protection"><strong>Storage Object in Use Protection</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#reclaiming"><strong>Reclaiming</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#retain"><strong>Retain</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#delete"><strong>Delete</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#recycle"><strong>Recycle</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#expanding-persistent-volumes-claims"><strong>Expanding Persistent Volumes Claims</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#types-of-persistent-volumes"><strong>Types of Persistent Volumes</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes"><strong>Persistent Volumes</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#capacity"><strong>Capacity</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#volume-mode"><strong>Volume Mode</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes"><strong>Access Modes</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#class"><strong>Class</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#reclaim-policy"><strong>Reclaim Policy</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#mount-options"><strong>Mount Options</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#phase"><strong>Phase</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims"><strong>PersistentVolumeClaims</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes-1"><strong>Access Modes</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#volume-modes"><strong>Volume Modes</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#resources"><strong>Resources</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#selector"><strong>Selector</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#class-1"><strong>Class</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#claims-as-volumes"><strong>Claims As Volumes</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#a-note-on-namespaces"><strong>A Note on Namespaces</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#raw-block-volume-support"><strong>Raw Block Volume Support</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes-using-a-raw-block-volume"><strong>Persistent Volumes using a Raw Block Volume</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volume-claim-requesting-a-raw-block-volume"><strong>Persistent Volume Claim requesting a Raw Block Volume</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#pod-specification-adding-raw-block-device-path-in-container"><strong>Pod specification adding Raw Block Device path in container</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#binding-block-volumes"><strong>Binding Block Volumes</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#writing-portable-configuration"><strong>Writing Portable Configuration</strong></a></li></ul><h4>Introduction</h4><p>Managing storage is a distinct problem from managing compute. The <strong>PersistentVolume</strong>subsystem provides an API for users and administrators that abstracts details of how storage is provided from how it is consumed. To do this we introduce two new API resources:<strong>PersistentVolume</strong> and <strong>PersistentVolumeClaim</strong>.</p><p>A <strong>PersistentVolume</strong> (PV) is a piece of storage in the cluster that has been provisioned by an administrator. It is a resource in the cluster just like a node is a cluster resource. PVs are volume plugins like Volumes, but have a lifecycle independent of any individual pod that uses the PV. This API object captures the details of the implementation of the storage, be that NFS, iSCSI, or a cloud-provider-specific storage system.</p><p>A <strong>PersistentVolumeClaim</strong> (PVC) is a request for storage by a user. It is similar to a pod. Pods consume node resources and PVCs consume PV resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific size and access modes (e.g., can be mounted once read/write or many times read-only).</p><p>While <strong>PersistentVolumeClaims</strong> allow a user to consume abstract storage resources, it is common that users need <strong>PersistentVolumes</strong> with varying properties, such as performance, for different problems. Cluster administrators need to be able to offer a variety of <strong>PersistentVolumes</strong> that differ in more ways than just size and access modes, without exposing users to the details of how those volumes are implemented. For these needs there is the <strong>StorageClass</strong> resource.</p><p>Please see the <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/">detailed walkthrough with working examples</a>.</p><h4>Lifecycle of a volume and claim</h4><p>PVs are resources in the cluster. PVCs are requests for those resources and also act as claim checks to the resource. The interaction between PVs and PVCs follows this lifecycle:</p><h5><strong>Provisioning</strong></h5><p>There are two ways PVs may be provisioned: statically or dynamically.</p><h6><strong>Static</strong></h6><p>A cluster administrator creates a number of PVs. They carry the details of the real storage which is available for use by cluster users. They exist in the Kubernetes API and are available for consumption.</p><h6><strong>Dynamic</strong></h6><p>When none of the static PVs the administrator created matches a user&#x27;s <strong>PersistentVolumeClaim</strong>, the cluster may try to dynamically provision a volume specially for the PVC. This provisioning is based on <strong>StorageClasses</strong>: the PVC must request a <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/">storage class</a> and the administrator must have created and configured that class in order for dynamic provisioning to occur. Claims that request the class <strong>&quot;&quot;</strong> effectively disable dynamic provisioning for themselves.</p><p>To enable dynamic storage provisioning based on storage class, the cluster administrator needs to enable the <strong>DefaultStorageClass</strong> <a href="https://kubernetes.io/docs/admin/admission-controllers/#defaultstorageclass">admission controller</a> on the API server. This can be done, for example, by ensuring that <strong>DefaultStorageClass</strong> is among the comma-delimited, ordered list of values for the <strong>--enable-admission-plugins</strong> flag of the API server component. For more information on API server command line flags, please check <a href="https://kubernetes.io/docs/admin/kube-apiserver/">kube-apiserver</a> documentation.</p><h5><strong>Binding</strong></h5><p>A user creates, or has already created in the case of dynamic provisioning, a <strong>PersistentVolumeClaim</strong> with a specific amount of storage requested and with certain access modes. A control loop in the master watches for new PVCs, finds a matching PV (if possible), and binds them together. If a PV was dynamically provisioned for a new PVC, the loop will always bind that PV to the PVC. Otherwise, the user will always get at least what they asked for, but the volume may be in excess of what was requested. Once bound, <strong>PersistentVolumeClaim</strong> binds are exclusive, regardless of how they were bound. A PVC to PV binding is a one-to-one mapping.</p><p>Claims will remain unbound indefinitely if a matching volume does not exist. Claims will be bound as matching volumes become available. For example, a cluster provisioned with many 50Gi PVs would not match a PVC requesting 100Gi. The PVC can be bound when a 100Gi PV is added to the cluster.</p><h5><strong>Using</strong></h5><p>Pods use claims as volumes. The cluster inspects the claim to find the bound volume and mounts that volume for a pod. For volumes which support multiple access modes, the user specifies which mode desired when using their claim as a volume in a pod.</p><p>Once a user has a claim and that claim is bound, the bound PV belongs to the user for as long as they need it. Users schedule Pods and access their claimed PVs by including a <strong>persistentVolumeClaim</strong> in their Pod&#x27;s volumes block. <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#claims-as-volumes">See below for syntax details</a>.</p><h5><strong>Storage Object in Use Protection</strong></h5><p><strong>FEATURE STATE:</strong> <strong>Kubernetes v1.10</strong> <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">beta</a></p><p>The purpose of the Storage Object in Use Protection feature is to ensure that Persistent Volume Claims (PVCs) in active use by a pod and Persistent Volume (PVs) that are bound to PVCs are not removed from the system as this may result in data loss.</p><p><strong>Note:</strong> PVC is in active use by a pod when the pod status is <strong>Pending</strong> and the pod is assigned to a node or the pod status is <strong>Running</strong>.</p><p>When the <a href="https://kubernetes.io/docs/tasks/administer-cluster/storage-object-in-use-protection/">Storage Object in Use Protection beta feature</a> is enabled, if a user deletes a PVC in active use by a pod, the PVC is not removed immediately. PVC removal is postponed until the PVC is no longer actively used by any pods, and also if admin deletes a PV that is bound to a PVC, the PV is not removed immediately. PV removal is postponed until the PV is not bound to a PVC any more.</p><p>You can see that a PVC is protected when the PVC&#x27;s status is <strong>Terminating</strong> and the <strong>Finalizers</strong>list includes <strong>kubernetes.io/pvc-protection</strong>:</p><p><strong>kubectl describe pvc hostpath</strong></p><p><strong>Name: hostpath</strong></p><p><strong>Namespace: default</strong></p><p><strong>StorageClass: example-hostpath</strong></p><p><strong>Status: Terminating</strong></p><p><strong>Volume:</strong></p><p><strong>Labels: <code>&lt;none&gt;</code></strong></p><p><strong>Annotations: volume.beta.kubernetes.io/storage-class=example-hostpath</strong></p><p><strong>volume.beta.kubernetes.io/storage-provisioner=example.com/hostpath</strong></p><p><strong>Finalizers: <!-- -->[kubernetes.io/pvc-protection]</strong></p><p><strong>.<!-- -->..</strong></p><p>You can see that a PV is protected when the PV&#x27;s status is <strong>Terminating</strong> and the <strong>Finalizers</strong> list includes <strong>kubernetes.io/pv-protection</strong> too:</p><p><strong>kubectl describe pv task-pv-volume</strong></p><p><strong>Name: task-pv-volume</strong></p><p><strong>Labels: type=local</strong></p><p><strong>Annotations: <code>&lt;none&gt;</code></strong></p><p><strong>Finalizers: <!-- -->[kubernetes.io/pv-protection]</strong></p><p><strong>StorageClass: standard</strong></p><p><strong>Status: Available</strong></p><p><strong>Claim:</strong></p><p><strong>Reclaim Policy: Delete</strong></p><p><strong>Access Modes: RWO</strong></p><p><strong>Capacity: 1Gi</strong></p><p><strong>Message:</strong></p><p><strong>Source:</strong></p><p><strong>Type: HostPath (bare host directory volume)</strong></p><p><strong>Path: /tmp/data</strong></p><p><strong>HostPathType:</strong></p><p><strong>Events: <code>&lt;none&gt;</code></strong></p><h5><strong>Reclaiming</strong></h5><p>When a user is done with their volume, they can delete the PVC objects from the API which allows reclamation of the resource. The reclaim policy for a <strong>PersistentVolume</strong> tells the cluster what to do with the volume after it has been released of its claim. Currently, volumes can either be Retained, Recycled or Deleted.</p><h6><strong>Retain</strong></h6><p>The <strong>Retain</strong> reclaim policy allows for manual reclamation of the resource. When the <strong>PersistentVolumeClaim</strong> is deleted, the <strong>PersistentVolume</strong> still exists and the volume is considered &quot;released&quot;. But it is not yet available for another claim because the previous claimant&#x27;s data remains on the volume. An administrator can manually reclaim the volume with the following steps.</p><ol><li>Delete the <strong>PersistentVolume</strong>. The associated storage asset in external infrastructure (such as an AWS EBS, GCE PD, Azure Disk, or Cinder volume) still exists after the PV is deleted.</li><li>Manually clean up the data on the associated storage asset accordingly.</li><li>Manually delete the associated storage asset, or if you want to reuse the same storage asset, create a new <strong>PersistentVolume</strong> with the storage asset definition.</li></ol><h6><strong>Delete</strong></h6><p>For volume plugins that support the <strong>Delete</strong> reclaim policy, deletion removes both the <strong>PersistentVolume</strong> object from Kubernetes, as well as the associated storage asset in the external infrastructure, such as an AWS EBS, GCE PD, Azure Disk, or Cinder volume. Volumes that were dynamically provisioned inherit the <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#reclaim-policy">reclaim policy of their <strong>StorageClass</strong></a>, which defaults to <strong>Delete</strong>. The administrator should configure the <strong>StorageClass</strong> according to users&#x27; expectations, otherwise the PV must be edited or patched after it is created. See <a href="https://kubernetes.io/docs/tasks/administer-cluster/change-pv-reclaim-policy/">Change the Reclaim Policy of a PersistentVolume</a>.</p><h6><strong>Recycle</strong></h6><p><strong>Warning:</strong> The <strong>Recycle</strong> reclaim policy is deprecated. Instead, the recommended approach is to use dynamic provisioning.</p><p>If supported by the underlying volume plugin, the <strong>Recycle</strong> reclaim policy performs a basic scrub (<strong>rm -rf /thevolume/<!-- -->*</strong>) on the volume and makes it available again for a new claim.</p><p>However, an administrator can configure a custom recycler pod template using the Kubernetes controller manager command line arguments as described <a href="https://kubernetes.io/docs/admin/kube-controller-manager/">here</a>. The custom recycler pod template must contain a <strong>volumes</strong> specification, as shown in the example below:</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: pv-recycler</strong></p><p><strong>namespace: default</strong></p><p><strong>spec:</strong></p><p><strong>restartPolicy: Never</strong></p><p><strong>volumes:</strong></p><p><strong>- name: vol</strong></p><p><strong>hostPath:</strong></p><p><strong>path: /any/path/it/will/be/replaced</strong></p><p><strong>containers:</strong></p><p><strong>- name: pv-recycler</strong></p><p><strong>image: &quot;k8s.gcr.io/busybox&quot;</strong></p><p><strong>command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;test -e /scrub &amp;&amp; rm -rf /scrub/..?<!-- -->*<!-- --> /scrub/.<!-- -->[!.]<!-- -->*<!-- --> /scrub/<!-- -->*<!-- --> &amp;&amp; test -z <!-- -->\<!-- -->&quot;$(ls -A /scrub)<!-- -->\<!-- -->&quot; || exit 1&quot;]</strong></p><p><strong>volumeMounts:</strong></p><p><strong>- name: vol</strong></p><p><strong>mountPath: /scrub</strong></p><p>However, the particular path specified in the custom recycler pod template in the <strong>volumes</strong> part is replaced with the particular path of the volume that is being recycled.</p><h5><strong>Expanding Persistent Volumes Claims</strong></h5><p>Kubernetes 1.8 added Alpha support for expanding persistent volumes. In v1.9, the following volume types support expanding Persistent volume claims:</p><ul><li>gcePersistentDisk</li><li>awsElasticBlockStore</li><li>Cinder</li><li>glusterfs</li><li>rbd</li></ul><p>Administrator can allow expanding persistent volume claims by setting <strong>ExpandPersistentVolumes</strong>feature gate to true. Administrator should also enable <a href="https://kubernetes.io/docs/admin/admission-controllers/#persistentvolumeclaimresize"><strong>PersistentVolumeClaimResize</strong> admission plugin</a> to perform additional validations of volumes that can be resized.</p><p>Once <strong>PersistentVolumeClaimResize</strong> admission plug-in has been turned on, resizing will only be allowed for storage classes whose <strong>allowVolumeExpansion</strong> field is set to true.</p><p><strong>kind: StorageClass</strong></p><p><strong>apiVersion: storage.k8s.io/v1</strong></p><p><strong>metadata:</strong></p><p><strong>name: gluster-vol-default</strong></p><p><strong>provisioner: kubernetes.io/glusterfs</strong></p><p><strong>parameters:</strong></p><p><strong>resturl: &quot;<a href="http://192.168.10.100:8080%22">http://192.168.10.100:8080&quot;</a></strong></p><p><strong>restuser: &quot;&quot;</strong></p><p><strong>secretNamespace: &quot;&quot;</strong></p><p><strong>secretName: &quot;&quot;</strong></p><p><strong>allowVolumeExpansion: true</strong></p><p>Once both feature gate and the aforementioned admission plug-in are turned on, a user can request larger volume for their <strong>PersistentVolumeClaim</strong> by simply editing the claim and requesting a larger size. This in turn will trigger expansion of the volume that is backing the underlying <strong>PersistentVolume</strong>.</p><p>Under no circumstances will a new <strong>PersistentVolume</strong> be created to satisfy the claim. Kubernetes will instead attempt to resize the existing volume.</p><p>For expanding volumes containing a file system, file system resizing is only performed when a new Pod is started using the <strong>PersistentVolumeClaim</strong> in ReadWrite mode. In other words, if a volume being expanded is used in a pod or deployment, you will need to delete and recreate the pod for file system resizing to take place. Also, file system resizing is only supported for following file system types:</p><ul><li>XFS</li><li>Ext3, Ext4</li></ul><p><strong>Note:</strong> Expanding EBS volumes is a time consuming operation. Also, there is a per-volume quota of one modification every 6 hours.</p><h4>Types of Persistent Volumes</h4><p><strong>PersistentVolume</strong> types are implemented as plugins. Kubernetes currently supports the following plugins:</p><ul><li>GCEPersistentDisk</li><li>AWSElasticBlockStore</li><li>AzureFile</li><li>AzureDisk</li><li>FC (Fibre Channel)<!-- -->*<!-- -->*</li><li>FlexVolume</li><li>Flocker</li><li>NFS</li><li>iSCSI</li><li>RBD (Ceph Block Device)</li><li>CephFS</li><li>Cinder (OpenStack block storage)</li><li>Glusterfs</li><li>VsphereVolume</li><li>Quobyte Volumes</li><li>HostPath (Single node testing only -- local storage is not supported in any way and WILL NOT WORK in a multi-node cluster)</li><li>VMware Photon</li><li>Portworx Volumes</li><li>ScaleIO Volumes</li><li>StorageOS</li></ul><p>Raw Block Support exists for these plugins only.</p><h4>Persistent Volumes</h4><p>Each PV contains a spec and status, which is the specification and status of the volume.</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: PersistentVolume</strong></p><p><strong>metadata:</strong></p><p><strong>name: pv0003</strong></p><p><strong>spec:</strong></p><p><strong>capacity:</strong></p><p><strong>storage: 5Gi</strong></p><p><strong>volumeMode: Filesystem</strong></p><p><strong>accessModes:</strong></p><p><strong>- ReadWriteOnce</strong></p><p><strong>persistentVolumeReclaimPolicy: Recycle</strong></p><p><strong>storageClassName: slow</strong></p><p><strong>mountOptions:</strong></p><p><strong>- hard</strong></p><p><strong>- nfsvers=4.1</strong></p><p><strong>nfs:</strong></p><p><strong>path: /tmp</strong></p><p><strong>server: 172.17.0.2</strong></p><h5><strong>Capacity</strong></h5><p>Generally, a PV will have a specific storage capacity. This is set using the PV&#x27;s <strong>capacity</strong> attribute. See the Kubernetes <a href="https://git.k8s.io/community/contributors/design-proposals/scheduling/resources.md">Resource Model</a> to understand the units expected by <strong>capacity</strong>.</p><p>Currently, storage size is the only resource that can be set or requested. Future attributes may include IOPS, throughput, etc.</p><h5><strong>Volume Mode</strong></h5><p>Prior to v1.9, the default behavior for all volume plugins was to create a filesystem on the persistent volume. With v1.9, the user can specify a <strong>volumeMode</strong> which will now support raw block devices in addition to file systems. Valid values for <strong>volumeMode</strong> are &quot;Filesystem&quot; or &quot;Block&quot;. If left unspecified, <strong>volumeMode</strong> defaults to &quot;Filesystem&quot; internally. This is an optional API parameter.</p><p><strong>Note:</strong> This feature is alpha in v1.9 and may change in the future.</p><h5><strong>Access Modes</strong></h5><p>A <strong>PersistentVolume</strong> can be mounted on a host in any way supported by the resource provider. As shown in the table below, providers will have different capabilities and each PV&#x27;s access modes are set to the specific modes supported by that particular volume. For example, NFS can support multiple read/write clients, but a specific NFS PV might be exported on the server as read-only. Each PV gets its own set of access modes describing that specific PV&#x27;s capabilities.</p><p>The access modes are:</p><ul><li>ReadWriteOnce -- the volume can be mounted as read-write by a single node</li><li>ReadOnlyMany -- the volume can be mounted read-only by many nodes</li><li>ReadWriteMany -- the volume can be mounted as read-write by many nodes</li></ul><p>In the CLI, the access modes are abbreviated to:</p><ul><li>RWO - ReadWriteOnce</li><li>ROX - ReadOnlyMany</li><li>RWX - ReadWriteMany</li></ul><p><strong>Important!</strong> A volume can only be mounted using one access mode at a time, even if it supports many. For example, a GCEPersistentDisk can be mounted as ReadWriteOnce by a single node or ReadOnlyMany by many nodes, but not at the same time.</p><p>  Volume Plugin          ReadWriteOnce   ReadOnlyMany   ReadWriteMany</p><hr/><p>  AWSElasticBlockStore   ✓               -             -
AzureFile              ✓               ✓              ✓
AzureDisk              ✓               -             -
CephFS                 ✓               ✓              ✓
Cinder                 ✓               -             -
FC                     ✓               ✓              -
FlexVolume             ✓               ✓              -
Flocker                ✓               -             -
GCEPersistentDisk      ✓               ✓              -
Glusterfs              ✓               ✓              ✓
HostPath               ✓               -             -
iSCSI                  ✓               ✓              -
PhotonPersistentDisk   ✓               -             -
Quobyte                ✓               ✓              ✓
NFS                    ✓               ✓              ✓
RBD                    ✓               ✓              -
VsphereVolume          ✓               -             - (works when pods are collocated)
PortworxVolume         ✓               -             ✓
ScaleIO                ✓               ✓              -
StorageOS              ✓               -             -</p><h5><strong>Class</strong></h5><p>A PV can have a class, which is specified by setting the <strong>storageClassName</strong> attribute to the name of a <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/">StorageClass</a>. A PV of a particular class can only be bound to PVCs requesting that class. A PV with no <strong>storageClassName</strong> has no class and can only be bound to PVCs that request no particular class.</p><p>In the past, the annotation <strong>volume.beta.kubernetes.io/storage-class</strong> was used instead of the <strong>storageClassName</strong> attribute. This annotation is still working, however it will become fully deprecated in a future Kubernetes release.</p><h5><strong>Reclaim Policy</strong></h5><p>Current reclaim policies are:</p><ul><li>Retain -- manual reclamation</li><li>Recycle -- basic scrub (<strong>rm -rf /thevolume/<!-- -->*</strong>)</li><li>Delete -- associated storage asset such as AWS EBS, GCE PD, Azure Disk, or OpenStack Cinder volume is deleted</li></ul><p>Currently, only NFS and HostPath support recycling. AWS EBS, GCE PD, Azure Disk, and Cinder volumes support deletion.</p><h5><strong>Mount Options</strong></h5><p>A Kubernetes administrator can specify additional mount options for when a Persistent Volume is mounted on a node.</p><p><strong>Note:</strong> Not all Persistent volume types support mount options.</p><p>The following volume types support mount options:</p><ul><li>GCEPersistentDisk</li><li>AWSElasticBlockStore</li><li>AzureFile</li><li>AzureDisk</li><li>NFS</li><li>iSCSI</li><li>RBD (Ceph Block Device)</li><li>CephFS</li><li>Cinder (OpenStack block storage)</li><li>Glusterfs</li><li>VsphereVolume</li><li>Quobyte Volumes</li><li>VMware Photon</li></ul><p>Mount options are not validated, so mount will simply fail if one is invalid.</p><p>In the past, the annotation <strong>volume.beta.kubernetes.io/mount-options</strong> was used instead of the <strong>mountOptions</strong> attribute. This annotation is still working, however it will become fully deprecated in a future Kubernetes release.</p><h5><strong>Phase</strong></h5><p>A volume will be in one of the following phases:</p><ul><li>Available -- a free resource that is not yet bound to a claim</li><li>Bound -- the volume is bound to a claim</li><li>Released -- the claim has been deleted, but the resource is not yet reclaimed by the cluster</li><li>Failed -- the volume has failed its automatic reclamation</li></ul><p>The CLI will show the name of the PVC bound to the PV.</p><h4>PersistentVolumeClaims</h4><p>Each PVC contains a spec and status, which is the specification and status of the claim.</p><p><strong>kind: PersistentVolumeClaim</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>metadata:</strong></p><p><strong>name: myclaim</strong></p><p><strong>spec:</strong></p><p><strong>accessModes:</strong></p><p><strong>- ReadWriteOnce</strong></p><p><strong>volumeMode: Filesystem</strong></p><p><strong>resources:</strong></p><p><strong>requests:</strong></p><p><strong>storage: 8Gi</strong></p><p><strong>storageClassName: slow</strong></p><p><strong>selector:</strong></p><p><strong>matchLabels:</strong></p><p><strong>release: &quot;stable&quot;</strong></p><p><strong>matchExpressions:</strong></p><p><strong>- {key: environment, operator: In, values: <!-- -->[dev]<!-- -->}</strong></p><h5><strong>Access Modes</strong></h5><p>Claims use the same conventions as volumes when requesting storage with specific access modes.</p><h5><strong>Volume Modes</strong></h5><p>Claims use the same convention as volumes to indicates the consumption of the volume as either a filesystem or block device.</p><h5><strong>Resources</strong></h5><p>Claims, like pods, can request specific quantities of a resource. In this case, the request is for storage. The same <a href="https://git.k8s.io/community/contributors/design-proposals/scheduling/resources.md">resource model</a> applies to both volumes and claims.</p><h5><strong>Selector</strong></h5><p>Claims can specify a <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors">label selector</a> to further filter the set of volumes. Only the volumes whose labels match the selector can be bound to the claim. The selector can consist of two fields:</p><ul><li><strong>matchLabels</strong> - the volume must have a label with this value</li><li><strong>matchExpressions</strong> - a list of requirements made by specifying key, list of values, and operator that relates the key and values. Valid operators include In, NotIn, Exists, and DoesNotExist.</li></ul><p>All of the requirements, from both <strong>matchLabels</strong> and <strong>matchExpressions</strong> are ANDed together -- they must all be satisfied in order to match.</p><h5><strong>Class</strong></h5><p>A claim can request a particular class by specifying the name of a <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/">StorageClass</a> using the attribute <strong>storageClassName</strong>. Only PVs of the requested class, ones with the same <strong>storageClassName</strong> as the PVC, can be bound to the PVC.</p><p>PVCs don&#x27;t necessarily have to request a class. A PVC with its <strong>storageClassName</strong> set equal to <strong>&quot;&quot;</strong>is always interpreted to be requesting a PV with no class, so it can only be bound to PVs with no class (no annotation or one set equal to <strong>&quot;&quot;</strong>). A PVC with no <strong>storageClassName</strong> is not quite the same and is treated differently by the cluster depending on whether the <a href="https://kubernetes.io/docs/admin/admission-controllers/#defaultstorageclass"><strong>DefaultStorageClass</strong>admission plugin</a> is turned on.</p><ul><li>If the admission plugin is turned on, the administrator may specify a default <strong>StorageClass</strong>. All PVCs that have no <strong>storageClassName</strong> can be bound only to PVs of that default. Specifying a default <strong>StorageClass</strong> is done by setting the annotation <strong>storageclass.kubernetes.io/is-default-class</strong> equal to &quot;true&quot; in a <strong>StorageClass</strong> object. If the administrator does not specify a default, the cluster responds to PVC creation as if the admission plugin were turned off. If more than one default is specified, the admission plugin forbids the creation of all PVCs.</li><li>If the admission plugin is turned off, there is no notion of a default <strong>StorageClass</strong>. All PVCs that have no <strong>storageClassName</strong> can be bound only to PVs that have no class. In this case, the PVCs that have no <strong>storageClassName</strong> are treated the same way as PVCs that have their <strong>storageClassName</strong> set to <strong>&quot;&quot;</strong>.</li></ul><p>Depending on installation method, a default StorageClass may be deployed to Kubernetes cluster by addon manager during installation.</p><p>When a PVC specifies a <strong>selector</strong> in addition to requesting a <strong>StorageClass</strong>, the requirements are ANDed together: only a PV of the requested class and with the requested labels may be bound to the PVC.</p><p><strong>Note:</strong> Currently, a PVC with a non-empty <strong>selector</strong> can&#x27;t have a PV dynamically provisioned for it.</p><p>In the past, the annotation <strong>volume.beta.kubernetes.io/storage-class</strong> was used instead of <strong>storageClassName</strong> attribute. This annotation is still working, however it won&#x27;t be supported in a future Kubernetes release.</p><h4>Claims As Volumes</h4><p>Pods access storage by using the claim as a volume. Claims must exist in the same namespace as the pod using the claim. The cluster finds the claim in the pod&#x27;s namespace and uses it to get the <strong>PersistentVolume</strong> backing the claim. The volume is then mounted to the host and into the pod.</p><p><strong>kind: Pod</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>metadata:</strong></p><p><strong>name: mypod</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: myfrontend</strong></p><p><strong>image: dockerfile/nginx</strong></p><p><strong>volumeMounts:</strong></p><p><strong>- mountPath: &quot;/var/www/html&quot;</strong></p><p><strong>name: mypd</strong></p><p><strong>volumes:</strong></p><p><strong>- name: mypd</strong></p><p><strong>persistentVolumeClaim:</strong></p><p><strong>claimName: myclaim</strong></p><h5><strong>A Note on Namespaces</strong></h5><p><strong>PersistentVolumes</strong> binds are exclusive, and since <strong>PersistentVolumeClaims</strong> are namespaced objects, mounting claims with &quot;Many&quot; modes (<strong>ROX</strong>, <strong>RWX</strong>) is only possible within one namespace.</p><h4>Raw Block Volume Support</h4><p>Static provisioning support for Raw Block Volumes is included as an alpha feature for v1.9. With this change are some new API fields that need to be used to facilitate this functionality. Kubernetes v1.10 supports only Fibre Channel and Local Volume plugins for this feature.</p><h5><strong>Persistent Volumes using a Raw Block Volume</strong></h5><p><strong>apiVersion: v1</strong></p><p><strong>kind: PersistentVolume</strong></p><p><strong>metadata:</strong></p><p><strong>name: block-pv</strong></p><p><strong>spec:</strong></p><p><strong>capacity:</strong></p><p><strong>storage: 10Gi</strong></p><p><strong>accessModes:</strong></p><p><strong>- ReadWriteOnce</strong></p><p><strong>volumeMode: Block</strong></p><p><strong>persistentVolumeReclaimPolicy: Retain</strong></p><p><strong>fc:</strong></p><p><strong>targetWWNs: <!-- -->[&quot;50060e801049cfd1&quot;]</strong></p><p><strong>lun: 0</strong></p><p><strong>readOnly: false</strong></p><h5><strong>Persistent Volume Claim requesting a Raw Block Volume</strong></h5><p><strong>apiVersion: v1</strong></p><p><strong>kind: PersistentVolumeClaim</strong></p><p><strong>metadata:</strong></p><p><strong>name: block-pvc</strong></p><p><strong>spec:</strong></p><p><strong>accessModes:</strong></p><p><strong>- ReadWriteOnce</strong></p><p><strong>volumeMode: Block</strong></p><p><strong>resources:</strong></p><p><strong>requests:</strong></p><p><strong>storage: 10Gi</strong></p><h5><strong>Pod specification adding Raw Block Device path in container</strong></h5><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: pod-with-block-volume</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: fc-container</strong></p><p><strong>image: fedora:26</strong></p><p><strong>command: <!-- -->[&quot;/bin/sh&quot;, &quot;-c&quot;]</strong></p><p><strong>args: <!-- -->[ &quot;tail -f /dev/null&quot; ]</strong></p><p><strong>volumeDevices:</strong></p><p><strong>- name: data</strong></p><p><strong>devicePath: /dev/xvda</strong></p><p><strong>volumes:</strong></p><p><strong>- name: data</strong></p><p><strong>persistentVolumeClaim:</strong></p><p><strong>claimName: block-pvc</strong></p><p><strong>Note:</strong> When adding a raw block device for a Pod, we specify the device path in the container instead of a mount path.</p><h5><strong>Binding Block Volumes</strong></h5><p>If a user requests a raw block volume by indicating this using the <strong>volumeMode</strong> field in the <strong>PersistentVolumeClaim</strong> spec, the binding rules differ slightly from previous releases that didn&#x27;t consider this mode as part of the spec. Listed is a table of possible combinations the user and admin might specify for requesting a raw block device. The table indicates if the volume will be bound or not given the combinations: Volume binding matrix for statically provisioned volumes:</p><p>  PV volumeMode   PVC volumeMode   Result</p><hr/><p>  unspecified     unspecified      BIND
unspecified     Block            NO BIND
unspecified     Filesystem       BIND
Block           unspecified      NO BIND
Block           Block            BIND
Block           Filesystem       NO BIND
Filesystem      Filesystem       BIND
Filesystem      Block            NO BIND
Filesystem      unspecified      BIND</p><p><strong>Note:</strong> Only statically provisioned volumes are supported for alpha release. Administrators should take care to consider these values when working with raw block devices.</p><h4>Writing Portable Configuration</h4><p>If you&#x27;re writing configuration templates or examples that run on a wide range of clusters and need persistent storage, we recommend that you use the following pattern:</p><ul><li>Do include PersistentVolumeClaim objects in your bundle of config (alongside Deployments, ConfigMaps, etc).</li><li>Do not include PersistentVolume objects in the config, since the user instantiating the config may not have permission to create PersistentVolumes.</li><li>Give the user the option of providing a storage class name when instantiating the template.<ul><li>If the user provides a storage class name, put that value into the <strong>persistentVolumeClaim.storageClassName</strong> field. This will cause the PVC to match the right storage class if the cluster has StorageClasses enabled by the admin.</li><li>If the user does not provide a storage class name, leave the<strong>persistentVolumeClaim.storageClassName</strong> field as nil.<ul><li>This will cause a PV to be automatically provisioned for the user with the default StorageClass in the cluster. Many cluster environments have a default StorageClass installed, or administrators can create their own default StorageClass.</li></ul></li></ul></li><li>In your tooling, do watch for PVCs that are not getting bound after some time and surface this to the user, as this may indicate that the cluster has no dynamic storage support (in which case the user should create a matching PV) or the cluster has no storage system (in which case the user cannot deploy config requiring PVCs).</li></ul><h3>Storage Classes</h3><p>This document describes the concept of <strong>StorageClass</strong> in Kubernetes. Familiarity with <a href="https://kubernetes.io/docs/concepts/storage/volumes/">volumes</a> and<a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes">persistent volumes</a> is suggested.</p><ul><li><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#introduction"><strong>Introduction</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource"><strong>The StorageClass Resource</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner"><strong>Provisioner</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy"><strong>Reclaim Policy</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options"><strong>Mount Options</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#parameters"><strong>Parameters</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#aws"><strong>AWS</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#gce"><strong>GCE</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#glusterfs"><strong>Glusterfs</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#openstack-cinder"><strong>OpenStack Cinder</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#vsphere"><strong>vSphere</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#ceph-rbd"><strong>Ceph RBD</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#quobyte"><strong>Quobyte</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#azure-disk"><strong>Azure Disk</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#azure-unmanaged-disk-storage-class"><strong>Azure Unmanaged Disk Storage Class</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#new-azure-disk-storage-class-starting-from-v172"><strong>New Azure Disk Storage Class (starting from v1.7.2)</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#azure-file"><strong>Azure File</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#portworx-volume"><strong>Portworx Volume</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#scaleio"><strong>ScaleIO</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#storageos"><strong>StorageOS</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#local"><strong>Local</strong></a></li></ul></li></ul><h4>Introduction</h4><p>A <strong>StorageClass</strong> provides a way for administrators to describe the &quot;classes&quot; of storage they offer. Different classes might map to quality-of-service levels, or to backup policies, or to arbitrary policies determined by the cluster administrators. Kubernetes itself is unopinionated about what classes represent. This concept is sometimes called &quot;profiles&quot; in other storage systems.</p><h4>The StorageClass Resource</h4><p>Each <strong>StorageClass</strong> contains the fields <strong>provisioner</strong>, <strong>parameters</strong>, and <strong>reclaimPolicy</strong>, which are used when a <strong>PersistentVolume</strong> belonging to the class needs to be dynamically provisioned.</p><p>The name of a <strong>StorageClass</strong> object is significant, and is how users can request a particular class. Administrators set the name and other parameters of a class when first creating <strong>StorageClass</strong>objects, and the objects cannot be updated once they are created.</p><p>Administrators can specify a default <strong>StorageClass</strong> just for PVCs that don&#x27;t request any particular class to bind to: see the <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#persistentvolumeclaims"><strong>PersistentVolumeClaim</strong> section</a> for details.</p><p><strong>kind: StorageClass</strong></p><p><strong>apiVersion: storage.k8s.io/v1</strong></p><p><strong>metadata:</strong></p><p><strong>name: standard</strong></p><p><strong>provisioner: kubernetes.io/aws-ebs</strong></p><p><strong>parameters:</strong></p><p><strong>type: gp2</strong></p><p><strong>reclaimPolicy: Retain</strong></p><p><strong>mountOptions:</strong></p><p><strong>- debug</strong></p><h5><strong>Provisioner</strong></h5><p>Storage classes have a provisioner that determines what volume plugin is used for provisioning PVs. This field must be specified.</p><p>  Volume Plugin          Internal Provisioner   Config Example</p><hr/><p>  AWSElasticBlockStore   ✓                      <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#aws">AWS</a>
AzureFile              ✓                      <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#azure-file">Azure File</a>
AzureDisk              ✓                      <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#azure-disk">Azure Disk</a>
CephFS                 -                     -
Cinder                 ✓                      <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#openstack-cinder">OpenStack Cinder</a>
FC                     -                     -
FlexVolume             -                     -
Flocker                ✓                      -
GCEPersistentDisk      ✓                      <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#gce">GCE</a>
Glusterfs              ✓                      <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#glusterfs">Glusterfs</a>
iSCSI                  -                     -
PhotonPersistentDisk   ✓                      -
Quobyte                ✓                      <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#quobyte">Quobyte</a>
NFS                    -                     -
RBD                    ✓                      <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#ceph-rbd">Ceph RBD</a>
VsphereVolume          ✓                      <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#vsphere">vSphere</a>
PortworxVolume         ✓                      <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#portworx-volume">Portworx Volume</a>
ScaleIO                ✓                      <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#scaleio">ScaleIO</a>
StorageOS              ✓                      <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#storageos">StorageOS</a>
Local                  -                     <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#local">Local</a></p><p>You are not restricted to specifying the &quot;internal&quot; provisioners listed here (whose names are prefixed with &quot;kubernetes.io&quot; and shipped alongside Kubernetes). You can also run and specify external provisioners, which are independent programs that follow a <a href="https://git.k8s.io/community/contributors/design-proposals/storage/volume-provisioning.md">specification</a> defined by Kubernetes. Authors of external provisioners have full discretion over where their code lives, how the provisioner is shipped, how it needs to be run, what volume plugin it uses (including Flex), etc. The repository <a href="https://github.com/kubernetes-incubator/external-storage">kubernetes-incubator/external-storage</a> houses a library for writing external provisioners that implements the bulk of the specification plus various community-maintained external provisioners.</p><p>For example, NFS doesn&#x27;t provide an internal provisioner, but an external provisioner can be used. Some external provisioners are listed under the repository <a href="https://github.com/kubernetes-incubator/external-storage">kubernetes-incubator/external-storage</a>. There are also cases when 3rd party storage vendors provide their own external provisioner.</p><h5><strong>Reclaim Policy</strong></h5><p>Persistent Volumes that are dynamically created by a storage class will have the reclaim policy specified in the <strong>reclaimPolicy</strong> field of the class, which can be either <strong>Delete</strong> or <strong>Retain</strong>. If no <strong>reclaimPolicy</strong> is specified when a <strong>StorageClass</strong> object is created, it will default to <strong>Delete</strong>.</p><p>Persistent Volumes that are created manually and managed via a storage class will have whatever reclaim policy they were assigned at creation.</p><h5><strong>Mount Options</strong></h5><p>Persistent Volumes that are dynamically created by a storage class will have the mount options specified in the <strong>mountOptions</strong> field of the class.</p><p>If the volume plugin does not support mount options but mount options are specified, provisioning will fail. Mount options are not validated on neither the class nor PV, so mount of the PV will simply fail if one is invalid.</p><h4>Parameters</h4><p>Storage classes have parameters that describe volumes belonging to the storage class. Different parameters may be accepted depending on the <strong>provisioner</strong>. For example, the value <strong>io1</strong>, for the parameter <strong>type</strong>, and the parameter <strong>iopsPerGB</strong> are specific to EBS. When a parameter is omitted, some default is used.</p><h5><strong>AWS</strong></h5><p><strong>kind: StorageClass</strong></p><p><strong>apiVersion: storage.k8s.io/v1</strong></p><p><strong>metadata:</strong></p><p><strong>name: slow</strong></p><p><strong>provisioner: kubernetes.io/aws-ebs</strong></p><p><strong>parameters:</strong></p><p><strong>type: io1</strong></p><p><strong>zones: us-east-1d, us-east-1c</strong></p><p><strong>iopsPerGB: &quot;10&quot;</strong></p><ul><li><strong>type</strong>: <strong>io1</strong>, <strong>gp2</strong>, <strong>sc1</strong>, <strong>st1</strong>. See <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html">AWS docs</a> for details. Default: <strong>gp2</strong>.</li><li><strong>zone</strong>: AWS zone. If neither <strong>zone</strong> nor <strong>zones</strong> is specified, volumes are generally round-robin-ed across all active zones where Kubernetes cluster has a node. <strong>zone</strong> and <strong>zones</strong> parameters must not be used at the same time.</li><li><strong>zones</strong>: A comma separated list of AWS zone(s). If neither <strong>zone</strong> nor <strong>zones</strong> is specified, volumes are generally round-robin-ed across all active zones where Kubernetes cluster has a node. <strong>zone</strong>and <strong>zones</strong> parameters must not be used at the same time.</li><li><strong>iopsPerGB</strong>: only for <strong>io1</strong> volumes. I/O operations per second per GiB. AWS volume plugin multiplies this with size of requested volume to compute IOPS of the volume and caps it at 20 000 IOPS (maximum supported by AWS, see <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html">AWS docs</a>. A string is expected here, i.e. <strong>&quot;10&quot;</strong>, not <strong>10</strong>.</li><li><strong>encrypted</strong>: denotes whether the EBS volume should be encrypted or not. Valid values are <strong>&quot;true&quot;</strong> or <strong>&quot;false&quot;</strong>. A string is expected here, i.e. <strong>&quot;true&quot;</strong>, not <strong>true</strong>.</li><li><strong>kmsKeyId</strong>: optional. The full Amazon Resource Name of the key to use when encrypting the volume. If none is supplied but <strong>encrypted</strong> is true, a key is generated by AWS. See AWS docs for valid ARN value.</li></ul><h5><strong>GCE</strong></h5><p><strong>kind: StorageClass</strong></p><p><strong>apiVersion: storage.k8s.io/v1</strong></p><p><strong>metadata:</strong></p><p><strong>name: slow</strong></p><p><strong>provisioner: kubernetes.io/gce-pd</strong></p><p><strong>parameters:</strong></p><p><strong>type: pd-standard</strong></p><p><strong>zones: us-central1-a, us-central1-b</strong></p><ul><li><strong>type</strong>: <strong>pd-standard</strong> or <strong>pd-ssd</strong>. Default: <strong>pd-standard</strong></li><li><strong>zone</strong>: GCE zone. If neither <strong>zone</strong> nor <strong>zones</strong> is specified, volumes are generally round-robin-ed across all active zones where Kubernetes cluster has a node. <strong>zone</strong> and <strong>zones</strong> parameters must not be used at the same time.</li><li><strong>zones</strong>: A comma separated list of GCE zone(s). If neither <strong>zone</strong> nor <strong>zones</strong> is specified, volumes are generally round-robin-ed across all active zones where Kubernetes cluster has a node. <strong>zone</strong>and <strong>zones</strong> parameters must not be used at the same time.</li></ul><h5><strong>Glusterfs</strong></h5><p><strong>apiVersion: storage.k8s.io/v1</strong></p><p><strong>kind: StorageClass</strong></p><p><strong>metadata:</strong></p><p><strong>name: slow</strong></p><p><strong>provisioner: kubernetes.io/glusterfs</strong></p><p><strong>parameters:</strong></p><p><strong>resturl: &quot;<a href="http://127.0.0.1:8081%22">http://127.0.0.1:8081&quot;</a></strong></p><p><strong>clusterid: &quot;630372ccdc720a92c681fb928f27b53f&quot;</strong></p><p><strong>restauthenabled: &quot;true&quot;</strong></p><p><strong>restuser: &quot;admin&quot;</strong></p><p><strong>secretNamespace: &quot;default&quot;</strong></p><p><strong>secretName: &quot;heketi-secret&quot;</strong></p><p><strong>gidMin: &quot;40000&quot;</strong></p><p><strong>gidMax: &quot;50000&quot;</strong></p><p><strong>volumetype: &quot;replicate:3&quot;</strong></p><ul><li><strong>resturl</strong>: Gluster REST service/Heketi service url which provision gluster volumes on demand. The general format should be <strong>IPaddress:Port</strong> and this is a mandatory parameter for GlusterFS dynamic provisioner. If Heketi service is exposed as a routable service in openshift/kubernetes setup, this can have a format similar to <strong><a href="http://heketi-storage-project.cloudapps.mystorage.com">http://heketi-storage-project.cloudapps.mystorage.com</a></strong> where the fqdn is a resolvable Heketi service url.</li><li><strong>restauthenabled</strong> : Gluster REST service authentication boolean that enables authentication to the REST server. If this value is <strong>&quot;true&quot;</strong>, <strong>restuser</strong> and <strong>restuserkey</strong> or <strong>secretNamespace</strong> + <strong>secretName</strong> have to be filled. This option is deprecated, authentication is enabled when any of <strong>restuser</strong>, <strong>restuserkey</strong>, <strong>secretName</strong> or <strong>secretNamespace</strong> is specified.</li><li><strong>restuser</strong> : Gluster REST service/Heketi user who has access to create volumes in the Gluster Trusted Pool.</li><li><strong>restuserkey</strong> : Gluster REST service/Heketi user&#x27;s password which will be used for authentication to the REST server. This parameter is deprecated in favor of <strong>secretNamespace</strong> + <strong>secretName</strong>.</li><li><strong>secretNamespace</strong>, <strong>secretName</strong> : Identification of Secret instance that contains user password to use when talking to Gluster REST service. These parameters are optional, empty password will be used when both <strong>secretNamespace</strong> and <strong>secretName</strong> are omitted. The provided secret must have type <strong>&quot;kubernetes.io/glusterfs&quot;</strong>, e.g. created in this way:</li><li><strong>kubectl create secret generic heketi-secret <!-- -->\</strong></li><li><strong>--type=&quot;kubernetes.io/glusterfs&quot; --from-literal=key=\&#x27;opensesame\&#x27; <!-- -->\</strong></li><li><strong>--namespace=default</strong></li></ul><p>Example of a secret can be found in <a href="https://github.com/kubernetes/examples/tree/master/staging/persistent-volume-provisioning/glusterfs/glusterfs-secret.yaml">glusterfs-provisioning-secret.yaml</a>.</p><ul><li><strong>clusterid</strong>: <strong>630372ccdc720a92c681fb928f27b53f</strong> is the ID of the cluster which will be used by Heketi when provisioning the volume. It can also be a list of clusterids, for example: <strong>&quot;8452344e2becec931ece4e33c4674e4e,42982310de6c63381718ccfa6d8cf397&quot;</strong>. This is an optional parameter.</li><li><strong>gidMin</strong>, <strong>gidMax</strong> : The minimum and maximum value of GID range for the storage class. A unique value (GID) in this range ( gidMin-gidMax ) will be used for dynamically provisioned volumes. These are optional values. If not specified, the volume will be provisioned with a value between 2000-2147483647 which are defaults for gidMin and gidMax respectively.</li><li><strong>volumetype</strong> : The volume type and its parameters can be configured with this optional value. If the volume type is not mentioned, it&#x27;s up to the provisioner to decide the volume type. For example: &#x27;Replica volume&#x27;: <strong>volumetype: replicate:3</strong> where &#x27;3&#x27; is replica count. &#x27;Disperse/EC volume&#x27;: <strong>volumetype: disperse:4:2</strong> where &#x27;4&#x27; is data and &#x27;2&#x27; is the redundancy count. &#x27;Distribute volume&#x27;: <strong>volumetype: none</strong></li></ul><p>For available volume types and administration options, refer to the <a href="https://access.redhat.com/documentation/en-US/Red_Hat_Storage/3.1/html/Administration_Guide/part-Overview.html">Administration Guide</a>.</p><p>For further reference information, see <a href="https://github.com/heketi/heketi/wiki/Setting-up-the-topology">How to configure Heketi</a>.</p><p>When persistent volumes are dynamically provisioned, the Gluster plugin automatically creates an endpoint and a headless service in the name <strong>gluster-dynamic-<code>&lt;claimname&gt;</code></strong>. The dynamic endpoint and service are automatically deleted when the persistent volume claim is deleted.</p><h5><strong>OpenStack Cinder</strong></h5><p><strong>kind: StorageClass</strong></p><p><strong>apiVersion: storage.k8s.io/v1</strong></p><p><strong>metadata:</strong></p><p><strong>name: gold</strong></p><p><strong>provisioner: kubernetes.io/cinder</strong></p><p><strong>parameters:</strong></p><p><strong>type: fast</strong></p><p><strong>availability: nova</strong></p><ul><li><strong>type</strong>: <a href="https://docs.openstack.org/user-guide/dashboard-manage-volumes.html">VolumeType</a> created in Cinder. Default is empty.</li><li><strong>availability</strong>: Availability Zone. If not specified, volumes are generally round-robin-ed across all active zones where Kubernetes cluster has a node.</li></ul><h5><strong>vSphere</strong></h5><ol><li>Create a StorageClass with a user specified disk format.</li><li><strong>kind: StorageClass</strong></li><li><strong>apiVersion: storage.k8s.io/v1</strong></li><li><strong>metadata:</strong></li><li><strong>name: fast</strong></li><li><strong>provisioner: kubernetes.io/vsphere-volume</strong></li><li><strong>parameters:</strong></li><li><strong>diskformat: zeroedthick</strong></li></ol><p><strong>diskformat</strong>: <strong>thin</strong>, <strong>zeroedthick</strong> and <strong>eagerzeroedthick</strong>. Default: <strong>&quot;thin&quot;</strong>.</p><ol><li>Create a StorageClass with a disk format on a user specified datastore.</li><li><strong>kind: StorageClass</strong></li><li><strong>apiVersion: storage.k8s.io/v1</strong></li><li><strong>metadata:</strong></li><li><strong>name: fast</strong></li><li><strong>provisioner: kubernetes.io/vsphere-volume</strong></li><li><strong>parameters:</strong></li><li><strong>diskformat: zeroedthick</strong></li><li><strong>datastore: VSANDatastore</strong></li></ol><p><strong>datastore</strong>: The user can also specify the datastore in the StorageClass. The volume will be created on the datastore specified in the storage class, which in this case is <strong>VSANDatastore</strong>. This field is optional. If the datastore is not specified, then the volume will be created on the datastore specified in the vSphere config file used to initialize the vSphere Cloud Provider.</p><ol><li>Storage Policy Management inside kubernetes<ul><li>Using existing vCenter SPBM policy</li></ul></li></ol><p>One of the most important features of vSphere for Storage Management is policy based Management. Storage Policy Based Management (SPBM) is a storage policy framework that provides a single unified control plane across a broad range of data services and storage solutions. SPBM enables vSphere administrators to overcome upfront storage provisioning challenges, such as capacity planning, differentiated service levels and managing capacity headroom.</p><p>The SPBM policies can be specified in the StorageClass using the <strong>storagePolicyName</strong>parameter.</p><ul><li><ul><li>Virtual SAN policy support inside Kubernetes</li></ul></li></ul><p>Vsphere Infrastructure (VI) Admins will have the ability to specify custom Virtual SAN Storage Capabilities during dynamic volume provisioning. You can now define storage requirements, such as performance and availability, in the form of storage capabilities during dynamic volume provisioning. The storage capability requirements are converted into a Virtual SAN policy which are then pushed down to the Virtual SAN layer when a persistent volume (virtual disk) is being created. The virtual disk is distributed across the Virtual SAN datastore to meet the requirements.</p><p>You can see <a href="https://vmware.github.io/vsphere-storage-for-kubernetes/documentation/policy-based-mgmt.html">Storage Policy Based Management for dynamic provisioning of volumes</a> for more details on how to use storage policies for persistent volumes management.</p><p>There are few <a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/vsphere">vSphere examples</a> which you try out for persistent volume management inside Kubernetes for vSphere.</p><h5><strong>Ceph RBD</strong></h5><p><strong>kind: StorageClass</strong></p><p><strong>apiVersion: storage.k8s.io/v1</strong></p><p><strong>metadata:</strong></p><p><strong>name: fast</strong></p><p><strong>provisioner: kubernetes.io/rbd</strong></p><p><strong>parameters:</strong></p><p><strong>monitors: 10.16.153.105:6789</strong></p><p><strong>adminId: kube</strong></p><p><strong>adminSecretName: ceph-secret</strong></p><p><strong>adminSecretNamespace: kube-system</strong></p><p><strong>pool: kube</strong></p><p><strong>userId: kube</strong></p><p><strong>userSecretName: ceph-secret-user</strong></p><p><strong>fsType: ext4</strong></p><p><strong>imageFormat: &quot;2&quot;</strong></p><p><strong>imageFeatures: &quot;layering&quot;</strong></p><ul><li><strong>monitors</strong>: Ceph monitors, comma delimited. This parameter is required.</li><li><strong>adminId</strong>: Ceph client ID that is capable of creating images in the pool. Default is &quot;admin&quot;.</li><li><strong>adminSecretNamespace</strong>: The namespace for <strong>adminSecret</strong>. Default is &quot;default&quot;.</li><li><strong>adminSecret</strong>: Secret Name for <strong>adminId</strong>. This parameter is required. The provided secret must have type &quot;kubernetes.io/rbd&quot;.</li><li><strong>pool</strong>: Ceph RBD pool. Default is &quot;rbd&quot;.</li><li><strong>userId</strong>: Ceph client ID that is used to map the RBD image. Default is the same as <strong>adminId</strong>.</li><li><strong>userSecretName</strong>: The name of Ceph Secret for <strong>userId</strong> to map RBD image. It must exist in the same namespace as PVCs. This parameter is required. The provided secret must have type &quot;kubernetes.io/rbd&quot;, e.g. created in this way:</li><li><strong>kubectl create secret generic ceph-secret --type=&quot;kubernetes.io/rbd&quot; <!-- -->\</strong></li><li><strong>--from-literal=key=\&#x27;QVFEQ1pMdFhPUnQrSmhBQUFYaERWNHJsZ3BsMmNjcDR6RFZST0E9PQ==\&#x27; <!-- -->\</strong></li><li><strong>--namespace=kube-system</strong></li><li><strong>fsType</strong>: fsType that is supported by kubernetes. Default: <strong>&quot;ext4&quot;</strong>.</li><li><strong>imageFormat</strong>: Ceph RBD image format, &quot;1&quot; or &quot;2&quot;. Default is &quot;1&quot;.</li><li><strong>imageFeatures</strong>: This parameter is optional and should only be used if you set <strong>imageFormat</strong> to &quot;2&quot;. Currently supported features are <strong>layering</strong> only. Default is &quot;&quot;, and no features are turned on.</li></ul><h5><strong>Quobyte</strong></h5><p><strong>apiVersion: storage.k8s.io/v1</strong></p><p><strong>kind: StorageClass</strong></p><p><strong>metadata:</strong></p><p><strong>name: slow</strong></p><p><strong>provisioner: kubernetes.io/quobyte</strong></p><p><strong>parameters:</strong></p><p><strong>quobyteAPIServer: &quot;<a href="http://138.68.74.142:7860%22">http://138.68.74.142:7860&quot;</a></strong></p><p><strong>registry: &quot;138.68.74.142:7861&quot;</strong></p><p><strong>adminSecretName: &quot;quobyte-admin-secret&quot;</strong></p><p><strong>adminSecretNamespace: &quot;kube-system&quot;</strong></p><p><strong>user: &quot;root&quot;</strong></p><p><strong>group: &quot;root&quot;</strong></p><p><strong>quobyteConfig: &quot;BASE&quot;</strong></p><p><strong>quobyteTenant: &quot;DEFAULT&quot;</strong></p><ul><li><strong>quobyteAPIServer</strong>: API Server of Quobyte in the format <strong>&quot;http(s)://api-server:7860&quot;</strong></li><li><strong>registry</strong>: Quobyte registry to use to mount the volume. You can specify the registry as **<code>&lt;host&gt;:&lt;port&gt;</code> pair or if you want to specify multiple registries you just have to put a comma between them e.q. <code>&lt;host1&gt;:&lt;port&gt;,&lt;host2&gt;:&lt;port&gt;,&lt;host3&gt;:&lt;port&gt;</code>. The host can be an IP address or if you have a working DNS you can also provide the DNS names.</li><li><strong>adminSecretNamespace</strong>: The namespace for <strong>adminSecretName</strong>. Default is &quot;default&quot;.</li><li><strong>adminSecretName</strong>: secret that holds information about the Quobyte user and the password to authenticate against the API server. The provided secret must have type &quot;kubernetes.io/quobyte&quot;, e.g. created in this way:</li><li><strong>kubectl create secret generic quobyte-admin-secret <!-- -->\</strong></li><li><strong>--type=&quot;kubernetes.io/quobyte&quot; --from-literal=key=\&#x27;opensesame\&#x27; <!-- -->\</strong></li><li><strong>--namespace=kube-system</strong></li><li><strong>user</strong>: maps all access to this user. Default is &quot;root&quot;.</li><li><strong>group</strong>: maps all access to this group. Default is &quot;nfsnobody&quot;.</li><li><strong>quobyteConfig</strong>: use the specified configuration to create the volume. You can create a new configuration or modify an existing one with the Web console or the quobyte CLI. Default is &quot;BASE&quot;.</li><li><strong>quobyteTenant</strong>: use the specified tenant ID to create/delete the volume. This Quobyte tenant has to be already present in Quobyte. Default is &quot;DEFAULT&quot;.</li></ul><h5><strong>Azure Disk</strong></h5><h6><strong>Azure Unmanaged Disk Storage Class</strong></h6><p><strong>kind: StorageClass</strong></p><p><strong>apiVersion: storage.k8s.io/v1</strong></p><p><strong>metadata:</strong></p><p><strong>name: slow</strong></p><p><strong>provisioner: kubernetes.io/azure-disk</strong></p><p><strong>parameters:</strong></p><p><strong>skuName: Standard_LRS</strong></p><p><strong>location: eastus</strong></p><p><strong>storageAccount: azure_storage_account_name</strong></p><ul><li><strong>skuName</strong>: Azure storage account Sku tier. Default is empty.</li><li><strong>location</strong>: Azure storage account location. Default is empty.</li><li><strong>storageAccount</strong>: Azure storage account name. If a storage account is provided, it must reside in the same resource group as the cluster, and <strong>location</strong> is ignored. If a storage account is not provided, a new storage account will be created in the same resource group as the cluster.</li></ul><h6><strong>New Azure Disk Storage Class (starting from v1.7.2)</strong></h6><p><strong>kind: StorageClass</strong></p><p><strong>apiVersion: storage.k8s.io/v1</strong></p><p><strong>metadata:</strong></p><p><strong>name: slow</strong></p><p><strong>provisioner: kubernetes.io/azure-disk</strong></p><p><strong>parameters:</strong></p><p><strong>storageaccounttype: Standard_LRS</strong></p><p><strong>kind: Shared</strong></p><ul><li><strong>storageaccounttype</strong>: Azure storage account Sku tier. Default is empty.</li><li><strong>kind</strong>: Possible values are <strong>shared</strong> (default), <strong>dedicated</strong>, and <strong>managed</strong>. When <strong>kind</strong> is <strong>shared</strong>, all unmanaged disks are created in a few shared storage accounts in the same resource group as the cluster. When <strong>kind</strong> is <strong>dedicated</strong>, a new dedicated storage account will be created for the new unmanaged disk in the same resource group as the cluster. When <strong>kind</strong>is <strong>managed</strong>, all managed disks are created in the same resource group as the cluster.</li><li>Premium VM can attach both Standard_LRS and Premium_LRS disks, while Standard VM can only attach Standard_LRS disks.</li><li>Managed VM can only attach managed disks and unmanaged VM can only attach unmanaged disks.</li></ul><h5><strong>Azure File</strong></h5><p><strong>kind: StorageClass</strong></p><p><strong>apiVersion: storage.k8s.io/v1</strong></p><p><strong>metadata:</strong></p><p><strong>name: azurefile</strong></p><p><strong>provisioner: kubernetes.io/azure-file</strong></p><p><strong>parameters:</strong></p><p><strong>skuName: Standard_LRS</strong></p><p><strong>location: eastus</strong></p><p><strong>storageAccount: azure_storage_account_name</strong></p><ul><li><strong>skuName</strong>: Azure storage account Sku tier. Default is empty.</li><li><strong>location</strong>: Azure storage account location. Default is empty.</li><li><strong>storageAccount</strong>: Azure storage account name. Default is empty. If a storage account is not provided, all storage accounts associated with the resource group are searched to find one that matches <strong>skuName</strong> and <strong>location</strong>. If a storage account is provided, it must reside in the same resource group as the cluster, and <strong>skuName</strong> and <strong>location</strong> are ignored.</li></ul><p>During provision, a secret is created for mounting credentials. If the cluster has enabled both <a href="https://kubernetes.io/docs/admin/authorization/rbac/">RBAC</a>and <a href="https://kubernetes.io/docs/admin/authorization/rbac/#controller-roles">Controller Roles</a>, add the <strong>create</strong> permission of resource <strong>secret</strong> for clusterrole <strong>system:controller:persistent-volume-binder</strong>.</p><h5><strong>Portworx Volume</strong></h5><p><strong>kind: StorageClass</strong></p><p><strong>apiVersion: storage.k8s.io/v1</strong></p><p><strong>metadata:</strong></p><p><strong>name: portworx-io-priority-high</strong></p><p><strong>provisioner: kubernetes.io/portworx-volume</strong></p><p><strong>parameters:</strong></p><p><strong>repl: &quot;1&quot;</strong></p><p><strong>snap_interval: &quot;70&quot;</strong></p><p><strong>io_priority: &quot;high&quot;</strong></p><ul><li><strong>fs</strong>: filesystem to be laid out: <!-- -->[none/xfs/ext4]<!-- --> (default: <strong>ext4</strong>).</li><li><strong>block_size</strong>: block size in Kbytes (default: <strong>32</strong>).</li><li><strong>repl</strong>: number of synchronous replicas to be provided in the form of replication factor <!-- -->[1..3]<!-- --> (default: <strong>1</strong>) A string is expected here i.e. <strong>&quot;1&quot;</strong> and not <strong>1</strong>.</li><li><strong>io_priority</strong>: determines whether the volume will be created from higher performance or a lower priority storage <!-- -->[high/medium/low]<!-- --> (default: <strong>low</strong>).</li><li><strong>snap_interval</strong>: clock/time interval in minutes for when to trigger snapshots. Snapshots are incremental based on difference with the prior snapshot, 0 disables snaps (default: <strong>0</strong>). A string is expected here i.e. <strong>&quot;70&quot;</strong> and not <strong>70</strong>.</li><li><strong>aggregation_level</strong>: specifies the number of chunks the volume would be distributed into, 0 indicates a non-aggregated volume (default: <strong>0</strong>). A string is expected here i.e. <strong>&quot;0&quot;</strong> and not <strong>0</strong></li><li><strong>ephemeral</strong>: specifies whether the volume should be cleaned-up after unmount or should be persistent. <strong>emptyDir</strong> use case can set this value to true and <strong>persistent volumes</strong> use case such as for databases like Cassandra should set to false, <!-- -->[true/false]<!-- --> (default <strong>false</strong>). A string is expected here i.e. <strong>&quot;true&quot;</strong> and not <strong>true</strong>.</li></ul><h5><strong>ScaleIO</strong></h5><p><strong>kind: StorageClass</strong></p><p><strong>apiVersion: storage.k8s.io/v1</strong></p><p><strong>metadata:</strong></p><p><strong>name: slow</strong></p><p><strong>provisioner: kubernetes.io/scaleio</strong></p><p><strong>parameters:</strong></p><p><strong>gateway: <a href="https://192.168.99.200:443/api">https://192.168.99.200:443/api</a></strong></p><p><strong>system: scaleio</strong></p><p><strong>protectionDomain: pd0</strong></p><p><strong>storagePool: sp1</strong></p><p><strong>storageMode: ThinProvisioned</strong></p><p><strong>secretRef: sio-secret</strong></p><p><strong>readOnly: false</strong></p><p><strong>fsType: xfs</strong></p><ul><li><strong>provisioner</strong>: attribute is set to <strong>kubernetes.io/scaleio</strong></li><li><strong>gateway</strong>: address to a ScaleIO API gateway (required)</li><li><strong>system</strong>: the name of the ScaleIO system (required)</li><li><strong>protectionDomain</strong>: the name of the ScaleIO protection domain (required)</li><li><strong>storagePool</strong>: the name of the volume storage pool (required)</li><li><strong>storageMode</strong>: the storage provision mode: <strong>ThinProvisioned</strong> (default) or <strong>ThickProvisioned</strong></li><li><strong>secretRef</strong>: reference to a configured Secret object (required)</li><li><strong>readOnly</strong>: specifies the access mode to the mounted volume (default false)</li><li><strong>fsType</strong>: the file system to use for the volume (default ext4)</li></ul><p>The ScaleIO Kubernetes volume plugin requires a configured Secret object. The secret must be created with type <strong>kubernetes.io/scaleio</strong> and use the same namespace value as that of the PVC where it is referenced as shown in the following command:</p><p><strong>kubectl create secret generic sio-secret --type=&quot;kubernetes.io/scaleio&quot; <!-- -->\</strong></p><p><strong>--from-literal=username=sioadmin --from-literal=password=d2NABDNjMA== <!-- -->\</strong></p><p><strong>--namespace=default</strong></p><h5><strong>StorageOS</strong></h5><p><strong>kind: StorageClass</strong></p><p><strong>apiVersion: storage.k8s.io/v1</strong></p><p><strong>metadata:</strong></p><p><strong>name: fast</strong></p><p><strong>provisioner: kubernetes.io/storageos</strong></p><p><strong>parameters:</strong></p><p><strong>pool: default</strong></p><p><strong>description: Kubernetes volume</strong></p><p><strong>fsType: ext4</strong></p><p><strong>adminSecretNamespace: default</strong></p><p><strong>adminSecretName: storageos-secret</strong></p><ul><li><strong>pool</strong>: The name of the StorageOS distributed capacity pool to provision the volume from. Uses the <strong>default</strong> pool which is normally present if not specified.</li><li><strong>description</strong>: The description to assign to volumes that were created dynamically. All volume descriptions will be the same for the storage class, but different storage classes can be used to allow descriptions for different use cases. Defaults to <strong>Kubernetes volume</strong>.</li><li><strong>fsType</strong>: The default filesystem type to request. Note that user-defined rules within StorageOS may override this value. Defaults to <strong>ext4</strong>.</li><li><strong>adminSecretNamespace</strong>: The namespace where the API configuration secret is located. Required if adminSecretName set.</li><li><strong>adminSecretName</strong>: The name of the secret to use for obtaining the StorageOS API credentials. If not specified, default values will be attempted.</li></ul><p>The StorageOS Kubernetes volume plugin can use a Secret object to specify an endpoint and credentials to access the StorageOS API. This is only required when the defaults have been changed. The secret must be created with type <strong>kubernetes.io/storageos</strong> as shown in the following command:</p><p><strong>kubectl create secret generic storageos-secret <!-- -->\</strong></p><p><strong>--type=&quot;kubernetes.io/storageos&quot; <!-- -->\</strong></p><p><strong>--from-literal=apiAddress=tcp://localhost:5705 <!-- -->\</strong></p><p><strong>--from-literal=apiUsername=storageos <!-- -->\</strong></p><p><strong>--from-literal=apiPassword=storageos <!-- -->\</strong></p><p><strong>--namespace=default</strong></p><p>Secrets used for dynamically provisioned volumes may be created in any namespace and referenced with the <strong>adminSecretNamespace</strong> parameter. Secrets used by pre-provisioned volumes must be created in the same namespace as the PVC that references it.</p><h5><strong>Local</strong></h5><p><strong>FEATURE STATE:</strong> <strong>Kubernetes v1.10</strong> <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/">beta</a></p><p><strong>kind: StorageClass</strong></p><p><strong>apiVersion: storage.k8s.io/v1</strong></p><p><strong>metadata:</strong></p><p><strong>name: local-storage</strong></p><p><strong>provisioner: kubernetes.io/no-provisioner</strong></p><p><strong>volumeBindingMode: WaitForFirstConsumer</strong></p><p>Local volumes do not support dynamic provisioning yet, however a StorageClass should still be created to delay volume binding until pod scheduling. This is specified by the <strong>WaitForFirstConsumer</strong> volume binding mode.</p><p>Delaying volume binding allows the scheduler to consider all of a pod&#x27;s scheduling constraints when choosing an appropriate PersistentVolume for a PersistentVolumeClaim.</p><h3>Dynamic Volume Provisioning</h3><p>Dynamic volume provisioning allows storage volumes to be created on-demand. Without dynamic provisioning, cluster administrators have to manually make calls to their cloud or storage provider to create new storage volumes, and then create <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/"><strong>PersistentVolume</strong> objects</a> to represent them in Kubernetes. The dynamic provisioning feature eliminates the need for cluster administrators to pre-provision storage. Instead, it automatically provisions storage when it is requested by users.</p><ul><li><a href="https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/#background"><strong>Background</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/#enabling-dynamic-provisioning"><strong>Enabling Dynamic Provisioning</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/#using-dynamic-provisioning"><strong>Using Dynamic Provisioning</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/#defaulting-behavior"><strong>Defaulting Behavior</strong></a></li></ul><h4>Background</h4><p>The implementation of dynamic volume provisioning is based on the API object <strong>StorageClass</strong> from the API group <strong>storage.k8s.io</strong>. A cluster administrator can define as many <strong>StorageClass</strong> objects as needed, each specifying a volume plugin (aka provisioner) that provisions a volume and the set of parameters to pass to that provisioner when provisioning. A cluster administrator can define and expose multiple flavors of storage (from the same or different storage systems) within a cluster, each with a custom set of parameters. This design also ensures that end users don&#x27;t have to worry about the complexity and nuances of how storage is provisioned, but still have the ability to select from multiple storage options.</p><p>More information on storage classes can be found <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#storageclasses">here</a>.</p><h4>Enabling Dynamic Provisioning</h4><p>To enable dynamic provisioning, a cluster administrator needs to pre-create one or more StorageClass objects for users. StorageClass objects define which provisioner should be used and what parameters should be passed to that provisioner when dynamic provisioning is invoked. The following manifest creates a storage class &quot;slow&quot; which provisions standard disk-like persistent disks.</p><p><strong>apiVersion: storage.k8s.io/v1</strong></p><p><strong>kind: StorageClass</strong></p><p><strong>metadata:</strong></p><p><strong>name: slow</strong></p><p><strong>provisioner: kubernetes.io/gce-pd</strong></p><p><strong>parameters:</strong></p><p><strong>type: pd-standard</strong></p><p>The following manifest creates a storage class &quot;fast&quot; which provisions SSD-like persistent disks.</p><p><strong>apiVersion: storage.k8s.io/v1</strong></p><p><strong>kind: StorageClass</strong></p><p><strong>metadata:</strong></p><p><strong>name: fast</strong></p><p><strong>provisioner: kubernetes.io/gce-pd</strong></p><p><strong>parameters:</strong></p><p><strong>type: pd-ssd</strong></p><h4>Using Dynamic Provisioning</h4><p>Users request dynamically provisioned storage by including a storage class in their <strong>PersistentVolumeClaim</strong>. Before Kubernetes v1.6, this was done via the <strong>volume.beta.kubernetes.io/storage-class</strong> annotation. However, this annotation is deprecated since v1.6. Users now can and should instead use the <strong>storageClassName</strong> field of the <strong>PersistentVolumeClaim</strong> object. The value of this field must match the name of a <strong>StorageClass</strong>configured by the administrator (see <a href="https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/#enabling-dynamic-provisioning">below</a>).</p><p>To select the &quot;fast&quot; storage class, for example, a user would create the following <strong>PersistentVolumeClaim</strong>:</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: PersistentVolumeClaim</strong></p><p><strong>metadata:</strong></p><p><strong>name: claim1</strong></p><p><strong>spec:</strong></p><p><strong>accessModes:</strong></p><p><strong>- ReadWriteOnce</strong></p><p><strong>storageClassName: fast</strong></p><p><strong>resources:</strong></p><p><strong>requests:</strong></p><p><strong>storage: 30Gi</strong></p><p>This claim results in an SSD-like Persistent Disk being automatically provisioned. When the claim is deleted, the volume is destroyed.</p><h4>Defaulting Behavior</h4><p>Dynamic provisioning can be enabled on a cluster such that all claims are dynamically provisioned if no storage class is specified. A cluster administrator can enable this behavior by:</p><ul><li>Marking one <strong>StorageClass</strong> object as default;</li><li>Making sure that the <a href="https://kubernetes.io/docs/admin/admission-controllers/#defaultstorageclass"><strong>DefaultStorageClass</strong> admission controller</a> is enabled on the API server.</li></ul><p>An administrator can mark a specific <strong>StorageClass</strong> as default by adding the <strong>storageclass.kubernetes.io/is-default-class</strong> annotation to it. When a default <strong>StorageClass</strong> exists in a cluster and a user creates a <strong>PersistentVolumeClaim</strong> with <strong>storageClassName</strong> unspecified, the <strong>DefaultStorageClass</strong> admission controller automatically adds the <strong>storageClassName</strong> field pointing to the default storage class.</p><p>Note that there can be at most one default storage class on a cluster, or a <strong>PersistentVolumeClaim</strong>without <strong>storageClassName</strong> explicitly specified cannot be created.</p><h2>Cluster Administration</h2><h3>Cluster Administration Overview</h3><p>The cluster administration overview is for anyone creating or administering a Kubernetes cluster. It assumes some familiarity with core Kubernetes <a href="https://kubernetes.io/docs/concepts/">concepts</a>.</p><ul><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/cluster-administration-overview/#planning-a-cluster"><strong>Planning a cluster</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/cluster-administration-overview/#managing-a-cluster"><strong>Managing a cluster</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/cluster-administration-overview/#securing-a-cluster"><strong>Securing a cluster</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/cluster-administration-overview/#securing-the-kubelet"><strong>Securing the kubelet</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/cluster-administration-overview/#optional-cluster-services"><strong>Optional Cluster Services</strong></a></li></ul><h4>Planning a cluster</h4><p>See the guides in <a href="https://kubernetes.io/docs/setup/pick-right-solution/">Picking the Right Solution</a> for examples of how to plan, set up, and configure Kubernetes clusters. The solutions listed in this article are called distros.</p><p>Before choosing a guide, here are some considerations:</p><ul><li>Do you just want to try out Kubernetes on your computer, or do you want to build a high-availability, multi-node cluster? Choose distros best suited for your needs.</li><li><strong>If you are designing for high-availability</strong>, learn about configuring <a href="https://kubernetes.io/docs/concepts/cluster-administration/federation/">clusters in multiple zones</a>.</li><li>Will you be using <strong>a hosted Kubernetes cluster</strong>, such as <a href="https://cloud.google.com/kubernetes-engine/">Google Kubernetes Engine</a>, or <strong>hosting your own cluster</strong>?</li><li>Will your cluster be <strong>on-premises</strong>, or <strong>in the cloud (IaaS)</strong>? Kubernetes does not directly support hybrid clusters. Instead, you can set up multiple clusters.</li><li><strong>If you are configuring Kubernetes on-premises</strong>, consider which <a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/">networking model</a> fits best.</li><li>Will you be running Kubernetes on <strong>&quot;bare metal&quot; hardware</strong> or on <strong>virtual machines (VMs)</strong>?</li><li>Do you <strong>just want to run a cluster</strong>, or do you expect to do <strong>active development of Kubernetes project code</strong>? If the latter, choose an actively-developed distro. Some distros only use binary releases, but offer a greater variety of choices.</li><li>Familiarize yourself with the <a href="https://kubernetes.io/docs/admin/cluster-components/">components</a> needed to run a cluster.</li></ul><p>Note: Not all distros are actively maintained. Choose distros which have been tested with a recent version of Kubernetes.</p><p>If you are using a guide involving Salt, see <a href="https://kubernetes.io/docs/admin/salt/">Configuring Kubernetes with Salt</a>.</p><h4>Managing a cluster</h4><ul><li><a href="https://kubernetes.io/docs/tasks/administer-cluster/cluster-management/">Managing a cluster</a> describes several topics related to the lifecycle of a cluster: creating a new cluster, upgrading your cluster&#x27;s master and worker nodes, performing node maintenance (e.g. kernel upgrades), and upgrading the Kubernetes API version of a running cluster.</li><li>Learn how to <a href="https://kubernetes.io/docs/concepts/nodes/node/">manage nodes</a>.</li><li>Learn how to set up and manage the <a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/">resource quota</a> for shared clusters.</li></ul><h4>Securing a cluster</h4><ul><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/certificates/">Certificates</a> describes the steps to generate certificates using different tool chains.</li><li><a href="https://kubernetes.io/docs/concepts/containers/container-environment-variables/">Kubernetes Container Environment</a> describes the environment for Kubelet managed containers on a Kubernetes node.</li><li><a href="https://kubernetes.io/docs/admin/accessing-the-api/">Controlling Access to the Kubernetes API</a> describes how to set up permissions for users and service accounts.</li><li><a href="https://kubernetes.io/docs/admin/authentication/">Authenticating</a> explains authentication in Kubernetes, including the various authentication options.</li><li><a href="https://kubernetes.io/docs/admin/authorization/">Authorization</a> is separate from authentication, and controls how HTTP calls are handled.</li><li><a href="https://kubernetes.io/docs/admin/admission-controllers/">Using Admission Controllers</a> explains plug-ins which intercepts requests to the Kubernetes API server after authentication and authorization.</li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/sysctl-cluster/">Using Sysctls in a Kubernetes Cluster</a> describes to an administrator how to use the <strong>sysctl</strong>command-line tool to set kernel parameters .</li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/">Auditing</a> describes how to interact with Kubernetes&#x27; audit logs.</li></ul><h5><strong>Securing the kubelet</strong></h5><ul><li><a href="https://kubernetes.io/docs/concepts/architecture/master-node-communication/">Master-Node communication</a></li><li><a href="https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/">TLS bootstrapping</a></li><li><a href="https://kubernetes.io/docs/admin/kubelet-authentication-authorization/">Kubelet authentication/authorization</a></li></ul><h4>Optional Cluster Services</h4><ul><li><a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/">DNS Integration with SkyDNS</a> describes how to resolve a DNS name directly to a Kubernetes service.</li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/logging/">Logging and Monitoring Cluster Activity</a> explains how logging in Kubernetes works and how to implement it.</li></ul><h3>Certificates</h3><ul><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/certificates/#creating-certificates"><strong>Creating Certificates</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/certificates/#easyrsa"><strong>easyrsa</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/certificates/#openssl"><strong>openssl</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/certificates/#cfssl"><strong>cfssl</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/certificates/#distributing-self-signed-ca-certificate"><strong>Distributing Self-Signed CA Certificate</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/certificates/#certificates-api"><strong>Certificates API</strong></a></li></ul><h4>Creating Certificates</h4><p>When using client certificate authentication, you can generate certificates manually through <strong>easyrsa</strong>, <strong>openssl</strong> or <strong>cfssl</strong>.</p><h5><strong>easyrsa</strong></h5><p><strong>easyrsa</strong> can manually generate certificates for your cluster.</p><ol><li>Download, unpack, and initialize the patched version of easyrsa3.</li><li><strong>curl -LO <a href="https://storage.googleapis.com/kubernetes-release/easy-rsa/easy-rsa.tar.gz">https://storage.googleapis.com/kubernetes-release/easy-rsa/easy-rsa.tar.gz</a></strong></li><li><strong>tar xzf easy-rsa.tar.gz</strong></li><li><strong>cd easy-rsa-master/easyrsa3</strong></li><li><strong>./easyrsa init-pki</strong></li><li>Generate a CA. (<strong>--batch</strong> set automatic mode. <strong>--req-cn</strong> default CN to use.)</li><li><strong>./easyrsa --batch &quot;--req-cn=${MASTER_IP}@<code>date +%s</code>&quot; build-ca nopass</strong></li><li>Generate server certificate and key. The argument <strong>--subject-alt-name</strong> sets the possible IPs and DNS names the API server will be accessed with. The <strong>MASTER_CLUSTER_IP</strong> is usually the first IP from the service CIDR that is specified as the <strong>--service-cluster-ip-range</strong> argument for both the API server and the controller manager component. The argument <strong>--days</strong> is used to set the number of days after which the certificate expires. The sample below also assume that you are using <strong>cluster.local</strong> as the default DNS domain name.</li><li><strong>./easyrsa --subject-alt-name=&quot;IP:${MASTER_IP},&quot;<!-- -->\</strong></li><li><strong>&quot;IP:${MASTER_CLUSTER_IP},&quot;<!-- -->\</strong></li><li><strong>&quot;DNS:kubernetes,&quot;<!-- -->\</strong></li><li><strong>&quot;DNS:kubernetes.default,&quot;<!-- -->\</strong></li><li><strong>&quot;DNS:kubernetes.default.svc,&quot;<!-- -->\</strong></li><li><strong>&quot;DNS:kubernetes.default.svc.cluster,&quot;<!-- -->\</strong></li><li><strong>&quot;DNS:kubernetes.default.svc.cluster.local&quot; <!-- -->\</strong></li><li><strong>--days=10000 <!-- -->\</strong></li><li><strong>build-server-full server nopass</strong></li><li>Copy <strong>pki/ca.crt</strong>, <strong>pki/issued/server.crt</strong>, and <strong>pki/private/server.key</strong> to your directory.</li><li>Fill in and add the following parameters into the API server start parameters:</li><li><strong>--client-ca-file=/yourdirectory/ca.crt</strong></li><li><strong>--tls-cert-file=/yourdirectory/server.crt</strong></li><li><strong>--tls-private-key-file=/yourdirectory/server.key</strong></li></ol><h5><strong>openssl</strong></h5><p><strong>openssl</strong> can manually generate certificates for your cluster.</p><ol><li>Generate a ca.key with 2048bit:</li><li><strong>openssl genrsa -out ca.key 2048</strong></li><li>According to the ca.key generate a ca.crt (use -days to set the certificate effective time):</li><li><strong>openssl req -x509 -new -nodes -key ca.key -subj &quot;/CN=${MASTER_IP}&quot; -days 10000 -out ca.crt</strong></li><li>Generate a server.key with 2048bit:</li><li><strong>openssl genrsa -out server.key 2048</strong></li><li>Create a config file for generating a Certificate Signing Request (CSR). Be sure to substitute the values marked with angle brackets (e.g. <strong><code>&lt;MASTER_IP&gt;</code></strong>) with real values before saving this to a file (e.g. <strong>csr.conf</strong>). Note that the value for <strong>MASTER_CLUSTER_IP</strong> is the service cluster IP for the API server as described in previous subsection. The sample below also assume that you are using <strong>cluster.local</strong> as the default DNS domain name.</li><li><strong>[ req ]</strong></li><li><strong>default_bits = 2048</strong></li><li><strong>prompt = no</strong></li><li><strong>default_md = sha256</strong></li><li><strong>req_extensions = req_ext</strong></li><li><strong>distinguished_name = dn</strong></li><li></li><li><strong>[ dn ]</strong></li><li><strong>C = <code>&lt;country&gt;</code></strong></li><li><strong>ST = <code>&lt;state&gt;</code></strong></li><li><strong>L = <code>&lt;city&gt;</code></strong></li><li><strong>O = <code>&lt;organization&gt;</code></strong></li><li><strong>OU = <code>&lt;organization unit&gt;</code></strong></li><li><strong>CN = <code>&lt;MASTER_IP&gt;</code></strong></li><li></li><li><strong>[ req_ext ]</strong></li><li><strong>subjectAltName = @alt_names</strong></li><li></li><li><strong>[ alt_names ]</strong></li><li><strong>DNS.1 = kubernetes</strong></li><li><strong>DNS.2 = kubernetes.default</strong></li><li><strong>DNS.3 = kubernetes.default.svc</strong></li><li><strong>DNS.4 = kubernetes.default.svc.cluster</strong></li><li><strong>DNS.5 = kubernetes.default.svc.cluster.local</strong></li><li><strong>IP.1 = <code>&lt;MASTER_IP&gt;</code></strong></li><li><strong>IP.2 = <code>&lt;MASTER_CLUSTER_IP&gt;</code></strong></li><li></li><li><strong>[ v3_ext ]</strong></li><li><strong>authorityKeyIdentifier=keyid,issuer:always</strong></li><li><strong>basicConstraints=CA:FALSE</strong></li><li><strong>keyUsage=keyEncipherment,dataEncipherment</strong></li><li><strong>extendedKeyUsage=serverAuth,clientAuth</strong></li><li><strong>subjectAltName=@alt_names</strong></li><li>Generate the certificate signing request based on the config file:</li><li><strong>openssl req -new -key server.key -out server.csr -config csr.conf</strong></li><li>Generate the server certificate using the ca.key, ca.crt and server.csr:</li><li><strong>openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key <!-- -->\</strong></li><li><strong>-CAcreateserial -out server.crt -days 10000 <!-- -->\</strong></li><li><strong>-extensions v3_ext -extfile csr.conf</strong></li><li>View the certificate:</li><li><strong>openssl x509 -noout -text -in ./server.crt</strong></li></ol><p>Finally, add the same parameters into the API server start parameters.</p><h5><strong>cfssl</strong></h5><p><strong>cfssl</strong> is another tool for certificate generation.</p><ol><li>Download, unpack and prepare the command line tools as shown below. Note that you may need to adapt the sample commands based on the hardware architecture and cfssl version you are using.</li><li><strong>curl -LO <a href="https://pkg.cfssl.org/R1.2/cfssl_linux-amd64">https://pkg.cfssl.org/R1.2/cfssl_linux-amd64</a> -o cfssl</strong></li><li><strong>chmod +x cfssl</strong></li><li><strong>curl -LO <a href="https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64">https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64</a> -o cfssljson</strong></li><li><strong>chmod +x cfssljson</strong></li><li><strong>curl -LO <a href="https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64">https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64</a> -o cfssl-certinfo</strong></li><li><strong>chmod +x cfssl-certinfo</strong></li><li>Create a directory to hold the artifacts and initialize cfssl:</li><li><strong>mkdir cert</strong></li><li><strong>cd cert</strong></li><li><strong>../cfssl print-defaults config &gt; config.json</strong></li><li><strong>../cfssl print-defaults csr &gt; csr.json</strong></li><li>Create a JSON config file for generating the CA file, for example, <strong>ca-config.json</strong>:</li><li><strong>{</strong></li><li><strong>&quot;signing&quot;: {</strong></li><li><strong>&quot;default&quot;: {</strong></li><li><strong>&quot;expiry&quot;: &quot;8760h&quot;</strong></li><li><strong>},</strong></li><li><strong>&quot;profiles&quot;: {</strong></li><li><strong>&quot;kubernetes&quot;: {</strong></li><li><strong>&quot;usages&quot;: [</strong></li><li><strong>&quot;signing&quot;,</strong></li><li><strong>&quot;key encipherment&quot;,</strong></li><li><strong>&quot;server auth&quot;,</strong></li><li><strong>&quot;client auth&quot;</strong></li><li><strong>],</strong></li><li><strong>&quot;expiry&quot;: &quot;8760h&quot;</strong></li><li><strong>}</strong></li><li><strong>}</strong></li><li><strong>}</strong></li><li><strong>}</strong></li><li>Create a JSON config file for CA certificate signing request (CSR), for example, <strong>ca-csr.json</strong>. Be sure the replace the values marked with angle brackets with real values you want to use.</li><li><strong>{</strong></li><li><strong>&quot;CN&quot;: &quot;kubernetes&quot;,</strong></li><li><strong>&quot;key&quot;: {</strong></li><li><strong>&quot;algo&quot;: &quot;rsa&quot;,</strong></li><li><strong>&quot;size&quot;: 2048</strong></li><li><strong>},</strong></li><li><strong>&quot;names&quot;:[{</strong></li><li><strong>&quot;C&quot;: &quot;<code>&lt;country&gt;</code>&quot;,</strong></li><li><strong>&quot;ST&quot;: &quot;<code>&lt;state&gt;</code>&quot;,</strong></li><li><strong>&quot;L&quot;: &quot;<code>&lt;city&gt;</code>&quot;,</strong></li><li><strong>&quot;O&quot;: &quot;<code>&lt;organization&gt;</code>&quot;,</strong></li><li><strong>&quot;OU&quot;: &quot;<code>&lt;organization unit&gt;</code>&quot;</strong></li><li><strong>}]</strong></li><li><strong>}</strong></li><li>Generate CA key (<strong>ca-key.pem</strong>) and certificate (<strong>ca.pem</strong>):</li><li><strong>../cfssl gencert -initca ca-csr.json | ../cfssljson -bare ca</strong></li><li>Create a JSON config file for generating keys and certificates for the API server as shown below. Be sure to replace the values in angle brackets with real values you want to use. The <strong>MASTER_CLUSTER_IP</strong> is the service cluster IP for the API server as described in previous subsection. The sample below also assume that you are using <strong>cluster.local</strong> as the default DNS domain name.</li><li><strong>{</strong></li><li><strong>&quot;CN&quot;: &quot;kubernetes&quot;,</strong></li><li><strong>&quot;hosts&quot;: [</strong></li><li><strong>&quot;127.0.0.1&quot;,</strong></li><li><strong>&quot;<code>&lt;MASTER_IP&gt;</code>&quot;,</strong></li><li><strong>&quot;<code>&lt;MASTER_CLUSTER_IP&gt;</code>&quot;,</strong></li><li><strong>&quot;kubernetes&quot;,</strong></li><li><strong>&quot;kubernetes.default&quot;,</strong></li><li><strong>&quot;kubernetes.default.svc&quot;,</strong></li><li><strong>&quot;kubernetes.default.svc.cluster&quot;,</strong></li><li><strong>&quot;kubernetes.default.svc.cluster.local&quot;</strong></li><li><strong>],</strong></li><li><strong>&quot;key&quot;: {</strong></li><li><strong>&quot;algo&quot;: &quot;rsa&quot;,</strong></li><li><strong>&quot;size&quot;: 2048</strong></li><li><strong>},</strong></li><li><strong>&quot;names&quot;: [{</strong></li><li><strong>&quot;C&quot;: &quot;<code>&lt;country&gt;</code>&quot;,</strong></li><li><strong>&quot;ST&quot;: &quot;<code>&lt;state&gt;</code>&quot;,</strong></li><li><strong>&quot;L&quot;: &quot;<code>&lt;city&gt;</code>&quot;,</strong></li><li><strong>&quot;O&quot;: &quot;<code>&lt;organization&gt;</code>&quot;,</strong></li><li><strong>&quot;OU&quot;: &quot;<code>&lt;organization unit&gt;</code>&quot;</strong></li><li><strong>}]</strong></li><li><strong>}</strong></li><li>Generate the key and certificate for the API server, which are by default saved into file <strong>server-key.pem</strong> and <strong>server.pem</strong> respectively:</li><li><strong>../cfssl gencert -ca=ca.pem -ca-key=ca-key.pem <!-- -->\</strong></li><li><strong>--config=ca-config.json -profile=kubernetes <!-- -->\</strong></li><li><strong>server-csr.json | ../cfssljson -bare server</strong></li></ol><h4>Distributing Self-Signed CA Certificate</h4><p>A client node may refuse to recognize a self-signed CA certificate as valid. For a non-production deployment, or for a deployment that runs behind a company firewall, you can distribute a self-signed CA certificate to all clients and refresh the local list for valid certificates.</p><p>On each client, perform the following operations:</p><p><strong>$ sudo cp ca.crt /usr/local/share/ca-certificates/kubernetes.crt</strong></p><p><strong>$ sudo update-ca-certificates</strong></p><p><strong>Updating certificates in /etc/ssl/certs<!-- -->.<!-- -->..</strong></p><p><strong>1 added, 0 removed; done.</strong></p><p><strong>Running hooks in /etc/ca-certificates/update.d<!-- -->.<!-- -->...</strong></p><p><strong>done.</strong></p><h4>Certificates API</h4><p>You can use the <strong>certificates.k8s.io</strong> API to provision x509 certificates to use for authentication as documented <a href="https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster">here</a>.</p><h3>Cloud Providers</h3><p>This page explains how to manage Kubernetes running on a specific cloud provider.</p><ul><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/cloud-providers/#aws"><strong>AWS</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/cloud-providers/#load-balancers"><strong>Load Balancers</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/cloud-providers/#openstack"><strong>OpenStack</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/cloud-providers/#cloudconf"><strong>cloud.conf</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/cloud-providers/#typical-configuration"><strong>Typical configuration</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/cloud-providers/#global"><strong>Global</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/cloud-providers/#load-balancer"><strong>Load Balancer</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/cloud-providers/#block-storage"><strong>Block Storage</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/cloud-providers/#metadata"><strong>Metadata</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/cloud-providers/#router"><strong>Router</strong></a></li></ul></li></ul></li></ul></li></ul><h4>AWS</h4><p>This section describes all the possible configurations which can be used when running Kubernetes on Amazon Web Services.</p><h5><strong>Load Balancers</strong></h5><p>You can setup <a href="https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/">external load balancers</a> to use specific features in AWS by configuring the annotations as shown below.</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Service</strong></p><p><strong>metadata:</strong></p><p><strong>name: example</strong></p><p><strong>namespace: kube-system</strong></p><p><strong>labels:</strong></p><p><strong>run: example</strong></p><p><strong>annotations:</strong></p><p><strong>service.beta.kubernetes.io/aws-load-balancer-ssl-cert: arn:aws:acm:xx-xxxx-x:xxxxxxxxx:xxxxxxx/xxxxx-xxxx-xxxx-xxxx-xxxxxxxxx <em>#replace this value</em></strong></p><p><strong>service.beta.kubernetes.io/aws-load-balancer-backend-protocol: http</strong></p><p><strong>spec:</strong></p><p><strong>type: LoadBalancer</strong></p><p><strong>ports:</strong></p><p><strong>- port: 443</strong></p><p><strong>targetPort: 5556</strong></p><p><strong>protocol: TCP</strong></p><p><strong>selector:</strong></p><p><strong>app: example</strong></p><p>Different settings can be applied to a load balancer service in AWS using annotations. The following describes the annotations supported on AWS ELBs:</p><ul><li><strong>service.beta.kubernetes.io/aws-load-balancer-access-log-emit-interval</strong>: Used to specify access log emit interval.</li><li><strong>service.beta.kubernetes.io/aws-load-balancer-access-log-enabled</strong>: Used on the service to enable or disable access logs.</li><li><strong>service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name</strong>: Used to specify access log s3 bucket name.</li><li><strong>service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-prefix</strong>: Used to specify access log s3 bucket prefix.</li><li><strong>service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags</strong>: Used on the service to specify a comma-separated list of key-value pairs which will be recorded as additional tags in the ELB. For example: <strong>&quot;Key1=Val1,Key2=Val2,KeyNoVal1=,KeyNoVal2&quot;</strong>.</li><li><strong>service.beta.kubernetes.io/aws-load-balancer-backend-protocol</strong>: Used on the service to specify the protocol spoken by the backend (pod) behind a listener. If <strong>http</strong> (default) or <strong>https</strong>, an HTTPS listener that terminates the connection and parses headers is created. If set to <strong>ssl</strong> or <strong>tcp</strong>, a &quot;raw&quot; SSL listener is used. If set to <strong>http</strong> and <strong>aws-load-balancer-ssl-cert</strong> is not used then a HTTP listener is used.</li><li><strong>service.beta.kubernetes.io/aws-load-balancer-ssl-cert</strong>: Used on the service to request a secure listener. Value is a valid certificate ARN. For more, see <a href="http://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/elb-listener-config.html">ELB Listener Config</a> CertARN is an IAM or CM certificate ARN, e.g. <strong>arn:aws:acm:us-east-1:123456789012:certificate/12345678-1234-1234-1234-123456789012</strong>.</li><li><strong>service.beta.kubernetes.io/aws-load-balancer-connection-draining-enabled</strong>: Used on the service to enable or disable connection draining.</li><li><strong>service.beta.kubernetes.io/aws-load-balancer-connection-draining-timeout</strong>: Used on the service to specify a connection draining timeout.</li><li><strong>service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout</strong>: Used on the service to specify the idle connection timeout.</li><li><strong>service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled</strong>: Used on the service to enable or disable cross-zone load balancing.</li><li><strong>service.beta.kubernetes.io/aws-load-balancer-extra-security-groups</strong>: Used on the service to specify additional security groups to be added to ELB created</li><li><strong>service.beta.kubernetes.io/aws-load-balancer-internal</strong>: Used on the service to indicate that we want an internal ELB.</li><li><strong>service.beta.kubernetes.io/aws-load-balancer-proxy-protocol</strong>: Used on the service to enable the proxy protocol on an ELB. Right now we only accept the value <strong>*</strong> which means enable the proxy protocol on all ELB backends. In the future we could adjust this to allow setting the proxy protocol only on certain backends.</li><li><strong>service.beta.kubernetes.io/aws-load-balancer-ssl-ports</strong>: Used on the service to specify a comma-separated list of ports that will use SSL/HTTPS listeners. Defaults to <strong>*</strong> (all)</li></ul><p>The information for the annotations for AWS is taken from the comments on <a href="https://github.com/kubernetes/kubernetes/blob/master/pkg/cloudprovider/providers/aws/aws.go">aws.go</a></p><h4>OpenStack</h4><p>This section describes all the possible configurations which can be used when using OpenStack with Kubernetes. The OpenStack cloud provider implementation for Kubernetes supports the use of these OpenStack services from the underlying cloud, where available:</p><p>  Service                    API Version(s)   Required</p><hr/><p>  Block Storage (Cinder)     V1†, V2, V3      No
Compute (Nova)             V2               No
Identity (Keystone)        V2‡, V3          Yes
Load Balancing (Neutron)   V1§, V2          No
Load Balancing (Octavia)   V2               No</p><p>† Block Storage V1 API support is deprecated, Block Storage V3 API support was added in Kubernetes 1.9.</p><p>‡ Identity V2 API support is deprecated and will be removed from the provider in a future release. As of the &quot;Queens&quot; release, OpenStack will no longer expose the Identity V2 API.</p><p>§ Load Balancing V1 API support was removed in Kubernetes 1.9.</p><p>Service discovery is achieved by listing the service catalog managed by OpenStack Identity (Keystone) using the <strong>auth-url</strong> provided in the provider configuration. The provider will gracefully degrade in functionality when OpenStack services other than Keystone are not available and simply disclaim support for impacted features. Certain features are also enabled or disabled based on the list of extensions published by Neutron in the underlying cloud.</p><h5><strong>cloud.conf</strong></h5><p>Kubernetes knows how to interact with OpenStack via the file cloud.conf. It is the file that will provide Kubernetes with credentials and location for the OpenStack auth endpoint. You can create a cloud.conf file by specifying the following details in it</p><h6><strong>Typical configuration</strong></h6><p>This is an example of a typical configuration that touches the values that most often need to be set. It points the provider at the OpenStack cloud&#x27;s Keystone endpoint, provides details for how to authenticate with it, and configures the load balancer:</p><p><strong>[Global]</strong></p><p><strong>username=user</strong></p><p><strong>password=pass</strong></p><p><strong>auth-url=https://<code>&lt;keystone_ip&gt;</code>/identity/v3</strong></p><p><strong>tenant-id=c869168a828847f39f7f06edd7305637</strong></p><p><strong>domain-id=2a73b8f597c04551a0fdc8e95544be8a</strong></p><p><strong>[LoadBalancer]</strong></p><p><strong>subnet-id=6937f8fa-858d-4bc9-a3a5-18d2c957166a</strong></p><p><strong>Global</strong></p><p>These configuration options for the OpenStack provider pertain to its global configuration and should appear in the <strong>[Global]</strong> section of the <strong>cloud.conf</strong> file:</p><ul><li><strong>auth-url</strong> (Required): The URL of the keystone API used to authenticate. On OpenStack control panels, this can be found at Access and Security &gt; API Access &gt; Credentials.</li><li><strong>username</strong> (Required): Refers to the username of a valid user set in keystone.</li><li><strong>password</strong> (Required): Refers to the password of a valid user set in keystone.</li><li><strong>tenant-id</strong> (Required): Used to specify the id of the project where you want to create your resources.</li><li><strong>tenant-name</strong> (Optional): Used to specify the name of the project where you want to create your resources.</li><li><strong>trust-id</strong> (Optional): Used to specify the identifier of the trust to use for authorization. A trust represents a user&#x27;s (the trustor) authorization to delegate roles to another user (the trustee), and optionally allow the trustee to impersonate the trustor. Available trusts are found under the <strong>/v3/OS-TRUST/trusts</strong> endpoint of the Keystone API.</li><li><strong>domain-id</strong> (Optional): Used to specify the id of the domain your user belongs to.</li><li><strong>domain-name</strong> (Optional): Used to specify the name of the domain your user belongs to.</li><li><strong>region</strong> (Optional): Used to specify the identifier of the region to use when running on a multi-region OpenStack cloud. A region is a general division of an OpenStack deployment. Although a region does not have a strict geographical connotation, a deployment can use a geographical name for a region identifier such as <strong>us-east</strong>. Available regions are found under the <strong>/v3/regions</strong> endpoint of the Keystone API.</li><li><strong>ca-file</strong> (Optional): Used to specify the path to your custom CA file.</li></ul><p>When using Keystone V3 - which changes tenant to project - the <strong>tenant-id</strong> value is automatically mapped to the project construct in the API.</p><p><strong>Load Balancer</strong></p><p>These configuration options for the OpenStack provider pertain to the load balancer and should appear in the <strong>[LoadBalancer]</strong> section of the <strong>cloud.conf</strong> file:</p><ul><li><strong>lb-version</strong> (Optional): Used to override automatic version detection. Valid values are <strong>v1</strong> or <strong>v2</strong>. Where no value is provided automatic detection will select the highest supported version exposed by the underlying OpenStack cloud.</li><li><strong>use-octavia</strong> (Optional): Used to determine whether to look for and use an Octavia LBaaS V2 service catalog endpoint. Valid values are <strong>true</strong> or <strong>false</strong>. Where <strong>true</strong> is specified and an Octaiva LBaaS V2 entry can not be found, the provider will fall back and attempt to find a Neutron LBaaS V2 endpoint instead. The default value is <strong>false</strong>.</li><li><strong>subnet-id</strong> (Optional): Used to specify the id of the subnet you want to create your loadbalancer on. Can be found at Network &gt; Networks. Click on the respective network to get its subnets.</li><li><strong>floating-network-id</strong> (Optional): If specified, will create a floating IP for the load balancer.</li><li><strong>lb-method</strong> (Optional): Used to specify algorithm by which load will be distributed amongst members of the load balancer pool. The value can be <strong>ROUND_ROBIN</strong>, <strong>LEAST_CONNECTIONS</strong>, or <strong>SOURCE_IP</strong>. The default behavior if none is specified is <strong>ROUND_ROBIN</strong>.</li><li><strong>lb-provider</strong> (Optional): Used to specify the provider of the load balancer. If not specified, the default provider service configured in neutron will be used.</li><li><strong>create-monitor</strong> (Optional): Indicates whether or not to create a health monitor for the Neutron load balancer. Valid values are <strong>true</strong> and <strong>false</strong>. The default is <strong>false</strong>. When <strong>true</strong> is specified then <strong>monitor-delay</strong>, <strong>monitor-timeout</strong>, and <strong>monitor-max-retries</strong> must also be set.</li><li><strong>monitor-delay</strong> (Optional): The time, in seconds, between sending probes to members of the load balancer.</li><li><strong>monitor-timeout</strong> (Optional): Maximum number of seconds for a monitor to wait for a ping reply before it times out. The value must be less than the delay value.</li><li><strong>monitor-max-retries</strong> (Optional): Number of permissible ping failures before changing the load balancer member&#x27;s status to INACTIVE. Must be a number between 1 and 10.</li><li><strong>manage-security-groups</strong> (Optional): Determines whether or not the load balancer should automatically manage the security group rules. Valid values are <strong>true</strong> and <strong>false</strong>. The default is <strong>false</strong>. When <strong>true</strong> is specified <strong>node-security-group</strong> must also be supplied.</li><li><strong>node-security-group</strong> (Optional): ID of the security group to manage.</li></ul><p><strong>Block Storage</strong></p><p>These configuration options for the OpenStack provider pertain to block storage and should appear in the <strong>[BlockStorage]</strong> section of the <strong>cloud.conf</strong> file:</p><ul><li><strong>bs-version</strong> (Optional): Used to override automatic version detection. Valid values are <strong>v1</strong>, <strong>v2</strong>, <strong>v3</strong> and <strong>auto</strong>. When <strong>auto</strong> is specified automatic detection will select the highest supported version exposed by the underlying OpenStack cloud. The default value if none is provided is <strong>auto</strong>.</li><li><strong>trust-device-path</strong> (Optional): In most scenarios the block device names provided by Cinder (e.g. <strong>/dev/vda</strong>) can not be trusted. This boolean toggles this behavior. Setting it to <strong>true</strong> results in trusting the block device names provided by Cinder. The default value of <strong>false</strong> results in the discovery of the device path based on its serial number and <strong>/dev/disk/by-id</strong> mapping and is the recommended approach.</li><li><strong>ignore-volume-az</strong> (Optional): Used to influence availability zone use when attaching Cinder volumes. When Nova and Cinder have different availability zones, this should be set to <strong>true</strong>. This is most commonly the case where there are many Nova availability zones but only one Cinder availability zone. The default value is <strong>false</strong> to preserve the behavior used in earlier releases, but may change in the future.</li></ul><p>If deploying Kubernetes versions &lt;= 1.8 on an OpenStack deployment that uses paths rather than ports to differentiate between endpoints it may be necessary to explicitly set the <strong>bs-version</strong>parameter. A path based endpoint is of the form <strong><a href="http://foo.bar/volume">http://foo.bar/volume</a></strong> while a port based endpoint is of the form <strong><a href="http://foo.bar:xxx">http://foo.bar:xxx</a></strong>.</p><p>In environments that use path based endpoints and Kubernetes is using the older auto-detection logic a <strong>BS API version autodetection failed.</strong> error will be returned on attempting volume detachment. To workaround this issue it is possible to force the use of Cinder API version 2 by adding this to the cloud provider configuration:</p><p><strong>[BlockStorage]</strong></p><p><strong>bs-version=v2</strong></p><p><strong>Metadata</strong></p><p>These configuration options for the OpenStack provider pertain to metadata and should appear in the <strong>[Metadata]</strong> section of the <strong>cloud.conf</strong> file:</p><ul><li><strong>search-order</strong> (Optional): This configuration key influences the way that the provider retrieves metadata relating to the instance(s) in which it runs. The default value of <strong>configDrive,metadataService</strong> results in the provider retrieving metadata relating to the instance from the config drive first if available and then the metadata service. Alternative values are:<ul><li><strong>configDrive</strong> - Only retrieve instance metadata from the configuration drive.</li><li><strong>metadataService</strong> - Only retrieve instance metadata from the metadata service.</li><li><strong>metadataService,configDrive</strong> - Retrieve instance metadata from the metadata service first if available, then the configuration drive.</li></ul></li></ul><p>Influencing this behavior may be desirable as the metadata on the configuration drive may grow stale over time, whereas the metadata service always provides the most up to date view. Not all OpenStack clouds provide both configuration drive and metadata service though and only one or the other may be available which is why the default is to check both.</p><p><strong>Router</strong></p><p>These configuration options for the OpenStack provider pertain to the <a href="https://kubernetes.io/docs/concepts/cluster-administration/network-plugins/#kubenet">kubenet</a> Kubernetes network plugin and should appear in the <strong>[Router]</strong> section of the <strong>cloud.conf</strong> file:</p><ul><li><strong>router-id</strong> (Optional): If the underlying cloud&#x27;s Neutron deployment supports the <strong>extraroutes</strong>extension then use <strong>router-id</strong> to specify a router to add routes to. The router chosen must span the private networks containing your cluster nodes (typically there is only one node network, and this value should be the default router for the node network). This value is required to use <a href="https://kubernetes.io/docs/concepts/cluster-administration/network-plugins/#kubenet">kubenet</a>on OpenStack.</li></ul><h3>Managing Resources</h3><p>You&#x27;ve deployed your application and exposed it via a service. Now what? Kubernetes provides a number of tools to help you manage your application deployment, including scaling and updating. Among the features that we will discuss in more depth are <a href="https://kubernetes.io/docs/concepts/configuration/overview/">configuration files</a> and <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/">labels</a>.</p><ul><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#organizing-resource-configurations"><strong>Organizing resource configurations</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#bulk-operations-in-kubectl"><strong>Bulk operations in kubectl</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#using-labels-effectively"><strong>Using labels effectively</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#canary-deployments"><strong>Canary deployments</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#updating-labels"><strong>Updating labels</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#updating-annotations"><strong>Updating annotations</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#scaling-your-application"><strong>Scaling your application</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#in-place-updates-of-resources"><strong>In-place updates of resources</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#kubectl-apply"><strong>kubectl apply</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#kubectl-edit"><strong>kubectl edit</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#kubectl-patch"><strong>kubectl patch</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#disruptive-updates"><strong>Disruptive updates</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#updating-your-application-without-a-service-outage"><strong>Updating your application without a service outage</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#whats-next"><strong>What&#x27;s next?</strong></a></li></ul><h4>Organizing resource configurations</h4><p>Many applications require multiple resources to be created, such as a Deployment and a Service. Management of multiple resources can be simplified by grouping them together in the same file (separated by <strong>---</strong> in YAML). For example:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>nginx-app.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernete">https://raw.githubusercontent.com/kubernete</a>     |
| s/website/master/docs/concepts/cluster-administration/nginx-app.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Service</strong>                                                     |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: my-nginx-svc</strong>                                                |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: nginx</strong>                                                        |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>type: LoadBalancer</strong>                                                |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- port: 80</strong>                                                        |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>app: nginx</strong>                                                        |
|                                                                       |
| <strong>---</strong>                                                             |
|                                                                       |
| <strong>apiVersion: apps/v1</strong>                                               |
|                                                                       |
| <strong>kind: Deployment</strong>                                                  |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: my-nginx</strong>                                                    |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: nginx</strong>                                                        |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>replicas: 3</strong>                                                       |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>matchLabels:</strong>                                                      |
|                                                                       |
| <strong>app: nginx</strong>                                                        |
|                                                                       |
| <strong>template:</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: nginx</strong>                                                        |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: nginx</strong>                                                     |
|                                                                       |
| <strong>image: nginx:1.7.9</strong>                                                |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- containerPort: 80</strong>                                               |
+-----------------------------------------------------------------------+</p><p>Multiple resources can be created the same way as a single resource:</p><p><strong>$ kubectl create -f <a href="https://k8s.io/docs/concepts/cluster-administration/nginx-app.yaml">https://k8s.io/docs/concepts/cluster-administration/nginx-app.yaml</a></strong></p><p><strong>service &quot;my-nginx-svc&quot; created</strong></p><p><strong>deployment &quot;my-nginx&quot; created</strong></p><p>The resources will be created in the order they appear in the file. Therefore, it&#x27;s best to specify the service first, since that will ensure the scheduler can spread the pods associated with the service as they are created by the controller(s), such as Deployment.</p><p><strong>kubectl create</strong> also accepts multiple <strong>-f</strong> arguments:</p><p><strong>$ kubectl create -f <a href="https://k8s.io/docs/concepts/cluster-administration/nginx/nginx-svc.yaml">https://k8s.io/docs/concepts/cluster-administration/nginx/nginx-svc.yaml</a> -f <a href="https://k8s.io/docs/concepts/cluster-administration/nginx/nginx-deployment.yaml">https://k8s.io/docs/concepts/cluster-administration/nginx/nginx-deployment.yaml</a></strong></p><p>And a directory can be specified rather than or in addition to individual files:</p><p><strong>$ kubectl create -f <a href="https://k8s.io/docs/concepts/cluster-administration/nginx/">https://k8s.io/docs/concepts/cluster-administration/nginx/</a></strong></p><p><strong>kubectl</strong> will read any files with suffixes <strong>.yaml</strong>, <strong>.yml</strong>, or <strong>.json</strong>.</p><p>It is a recommended practice to put resources related to the same microservice or application tier into the same file, and to group all of the files associated with your application in the same directory. If the tiers of your application bind to each other using DNS, then you can then simply deploy all of the components of your stack en masse.</p><p>A URL can also be specified as a configuration source, which is handy for deploying directly from configuration files checked into github:</p><p><strong>$ kubectl create -f <a href="https://raw.githubusercontent.com/kubernetes/website/master/docs/concepts/cluster-administration/nginx-deployment.yaml">https://raw.githubusercontent.com/kubernetes/website/master/docs/concepts/cluster-administration/nginx-deployment.yaml</a></strong></p><p><strong>deployment &quot;nginx-deployment&quot; created</strong></p><h4>Bulk operations in kubectl</h4><p>Resource creation isn&#x27;t the only operation that <strong>kubectl</strong> can perform in bulk. It can also extract resource names from configuration files in order to perform other operations, in particular to delete the same resources you created:</p><p><strong>$ kubectl delete -f <a href="https://k8s.io/docs/concepts/cluster-administration/nginx-app.yaml">https://k8s.io/docs/concepts/cluster-administration/nginx-app.yaml</a></strong></p><p><strong>deployment &quot;my-nginx&quot; deleted</strong></p><p><strong>service &quot;my-nginx-svc&quot; deleted</strong></p><p>In the case of just two resources, it&#x27;s also easy to specify both on the command line using the resource/name syntax:</p><p><strong>$ kubectl delete deployments/my-nginx services/my-nginx-svc</strong></p><p>For larger numbers of resources, you&#x27;ll find it easier to specify the selector (label query) specified using <strong>-l</strong> or <strong>--selector</strong>, to filter resources by their labels:</p><p><strong>$ kubectl delete deployment,services -l app=nginx</strong></p><p><strong>deployment &quot;my-nginx&quot; deleted</strong></p><p><strong>service &quot;my-nginx-svc&quot; deleted</strong></p><p>Because <strong>kubectl</strong> outputs resource names in the same syntax it accepts, it&#x27;s easy to chain operations using <strong>$()</strong> or <strong>xargs</strong>:</p><p><strong>$ kubectl get $(kubectl create -f docs/concepts/cluster-administration/nginx/ -o name | grep service)</strong></p><p><strong>NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE</strong></p><p><strong>my-nginx-svc 10.0.0.208 <code>&lt;pending&gt;</code> 80/TCP 0s</strong></p><p>With the above commands, we first create resources under <strong>docs/concepts/cluster-administration/nginx/</strong> and print the resources created with <strong>-o name</strong>output format (print each resource as resource/name). Then we <strong>grep</strong> only the &quot;service&quot;, and then print it with <strong>kubectl get</strong>.</p><p>If you happen to organize your resources across several subdirectories within a particular directory, you can recursively perform the operations on the subdirectories also, by specifying <strong>--recursive</strong>or <strong>-R</strong> alongside the <strong>--filename,-f</strong> flag.</p><p>For instance, assume there is a directory <strong>project/k8s/development</strong> that holds all of the manifests needed for the development environment, organized by resource type:</p><p><strong>project/k8s/development</strong></p><p><strong>├── configmap</strong></p><p><strong>│   └── my-configmap.yaml</strong></p><p><strong>├── deployment</strong></p><p><strong>│   └── my-deployment.yaml</strong></p><p><strong>└── pvc</strong></p><p><strong>└── my-pvc.yaml</strong></p><p>By default, performing a bulk operation on <strong>project/k8s/development</strong> will stop at the first level of the directory, not processing any subdirectories. If we had tried to create the resources in this directory using the following command, we would have encountered an error:</p><p><strong>$ kubectl create -f project/k8s/development</strong></p><p><strong>error: you must provide one or more resources by argument or filename (.json|.yaml|.yml|stdin)</strong></p><p>Instead, specify the <strong>--recursive</strong> or <strong>-R</strong> flag with the <strong>--filename,-f</strong> flag as such:</p><p><strong>$ kubectl create -f project/k8s/development --recursive</strong></p><p><strong>configmap &quot;my-config&quot; created</strong></p><p><strong>deployment &quot;my-deployment&quot; created</strong></p><p><strong>persistentvolumeclaim &quot;my-pvc&quot; created</strong></p><p>The <strong>--recursive</strong> flag works with any operation that accepts the <strong>--filename,-f</strong> flag such as: <strong>kubectl {create,get,delete,describe,rollout} etc.</strong></p><p>The <strong>--recursive</strong> flag also works when multiple <strong>-f</strong> arguments are provided:</p><p><strong>$ kubectl create -f project/k8s/namespaces -f project/k8s/development --recursive</strong></p><p><strong>namespace &quot;development&quot; created</strong></p><p><strong>namespace &quot;staging&quot; created</strong></p><p><strong>configmap &quot;my-config&quot; created</strong></p><p><strong>deployment &quot;my-deployment&quot; created</strong></p><p><strong>persistentvolumeclaim &quot;my-pvc&quot; created</strong></p><p>If you&#x27;re interested in learning more about <strong>kubectl</strong>, go ahead and read <a href="https://kubernetes.io/docs/reference/kubectl/overview/">kubectl Overview</a>.</p><h4>Using labels effectively</h4><p>The examples we&#x27;ve used so far apply at most a single label to any resource. There are many scenarios where multiple labels should be used to distinguish sets from one another.</p><p>For instance, different applications would use different values for the <strong>app</strong> label, but a multi-tier application, such as the <a href="https://github.com/kubernetes/examples/tree/master/guestbook/">guestbook example</a>, would additionally need to distinguish each tier. The frontend could carry the following labels:</p><p><strong>labels:</strong></p><p><strong>app: guestbook</strong></p><p><strong>tier: frontend</strong></p><p>while the Redis master and slave would have different <strong>tier</strong> labels, and perhaps even an additional <strong>role</strong> label:</p><p><strong>labels:</strong></p><p><strong>app: guestbook</strong></p><p><strong>tier: backend</strong></p><p><strong>role: master</strong></p><p>and</p><p><strong>labels:</strong></p><p><strong>app: guestbook</strong></p><p><strong>tier: backend</strong></p><p><strong>role: slave</strong></p><p>The labels allow us to slice and dice our resources along any dimension specified by a label:</p><p><strong>$ kubectl create -f examples/guestbook/all-in-one/guestbook-all-in-one.yaml</strong></p><p><strong>$ kubectl get pods -Lapp -Ltier -Lrole</strong></p><p><strong>NAME READY STATUS RESTARTS AGE APP TIER ROLE</strong></p><p><strong>guestbook-fe-4nlpb 1/1 Running 0 1m guestbook frontend <code>&lt;none&gt;</code></strong></p><p><strong>guestbook-fe-ght6d 1/1 Running 0 1m guestbook frontend <code>&lt;none&gt;</code></strong></p><p><strong>guestbook-fe-jpy62 1/1 Running 0 1m guestbook frontend <code>&lt;none&gt;</code></strong></p><p><strong>guestbook-redis-master-5pg3b 1/1 Running 0 1m guestbook backend master</strong></p><p><strong>guestbook-redis-slave-2q2yf 1/1 Running 0 1m guestbook backend slave</strong></p><p><strong>guestbook-redis-slave-qgazl 1/1 Running 0 1m guestbook backend slave</strong></p><p><strong>my-nginx-divi2 1/1 Running 0 29m nginx <code>&lt;none&gt; &lt;none&gt;</code></strong></p><p><strong>my-nginx-o0ef1 1/1 Running 0 29m nginx <code>&lt;none&gt; &lt;none&gt;</code></strong></p><p><strong>$ kubectl get pods -lapp=guestbook,role=slave</strong></p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>guestbook-redis-slave-2q2yf 1/1 Running 0 3m</strong></p><p><strong>guestbook-redis-slave-qgazl 1/1 Running 0 3m</strong></p><h4>Canary deployments</h4><p>Another scenario where multiple labels are needed is to distinguish deployments of different releases or configurations of the same component. It is common practice to deploy a canary of a new application release (specified via image tag in the pod template) side by side with the previous release so that the new release can receive live production traffic before fully rolling it out.</p><p>For instance, you can use a <strong>track</strong> label to differentiate different releases.</p><p>The primary, stable release would have a <strong>track</strong> label with value as <strong>stable</strong>:</p><p><strong>name: frontend</strong></p><p><strong>replicas: 3</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>labels:</strong></p><p><strong>app: guestbook</strong></p><p><strong>tier: frontend</strong></p><p><strong>track: stable</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>image: gb-frontend:v3</strong></p><p>and then you can create a new release of the guestbook frontend that carries the <strong>track</strong> label with different value (i.e. <strong>canary</strong>), so that two sets of pods would not overlap:</p><p><strong>name: frontend-canary</strong></p><p><strong>replicas: 1</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>labels:</strong></p><p><strong>app: guestbook</strong></p><p><strong>tier: frontend</strong></p><p><strong>track: canary</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>image: gb-frontend:v4</strong></p><p>The frontend service would span both sets of replicas by selecting the common subset of their labels (i.e. omitting the <strong>track</strong> label), so that the traffic will be redirected to both applications:</p><p><strong>selector:</strong></p><p><strong>app: guestbook</strong></p><p><strong>tier: frontend</strong></p><p>You can tweak the number of replicas of the stable and canary releases to determine the ratio of each release that will receive live production traffic (in this case, 3:1). Once you&#x27;re confident, you can update the stable track to the new application release and remove the canary one.</p><p>For a more concrete example, check the <a href="https://github.com/kelseyhightower/talks/tree/master/kubecon-eu-2016/demo#deploy-a-canary">tutorial of deploying Ghost</a>.</p><h4>Updating labels</h4><p>Sometimes existing pods and other resources need to be relabeled before creating new resources. This can be done with <strong>kubectl label</strong>. For example, if you want to label all your nginx pods as frontend tier, simply run:</p><p><strong>$ kubectl label pods -l app=nginx tier=fe</strong></p><p><strong>pod &quot;my-nginx-2035384211-j5fhi&quot; labeled</strong></p><p><strong>pod &quot;my-nginx-2035384211-u2c7e&quot; labeled</strong></p><p><strong>pod &quot;my-nginx-2035384211-u3t6x&quot; labeled</strong></p><p>This first filters all pods with the label &quot;app=nginx&quot;, and then labels them with the &quot;tier=fe&quot;. To see the pods you just labeled, run:</p><p><strong>$ kubectl get pods -l app=nginx -L tier</strong></p><p><strong>NAME READY STATUS RESTARTS AGE TIER</strong></p><p><strong>my-nginx-2035384211-j5fhi 1/1 Running 0 23m fe</strong></p><p><strong>my-nginx-2035384211-u2c7e 1/1 Running 0 23m fe</strong></p><p><strong>my-nginx-2035384211-u3t6x 1/1 Running 0 23m fe</strong></p><p>This outputs all &quot;app=nginx&quot; pods, with an additional label column of pods&#x27; tier (specified with <strong>-L</strong> or <strong>--label-columns</strong>).</p><p>For more information, please see <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/">labels</a> and <a href="https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands/#label">kubectl label</a>.</p><h4>Updating annotations</h4><p>Sometimes you would want to attach annotations to resources. Annotations are arbitrary non-identifying metadata for retrieval by API clients such as tools, libraries, etc. This can be done with <strong>kubectl annotate</strong>. For example:</p><p><strong>$ kubectl annotate pods my-nginx-v4-9gw19 description=\&#x27;my frontend running nginx\&#x27;</strong></p><p><strong>$ kubectl get pods my-nginx-v4-9gw19 -o yaml</strong></p><p><strong>apiversion: v1</strong></p><p><strong>kind: pod</strong></p><p><strong>metadata:</strong></p><p><strong>annotations:</strong></p><p><strong>description: my frontend running nginx</strong></p><p><strong>.<!-- -->..</strong></p><p>For more information, please see <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/">annotations</a> and <a href="https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands/#annotate">kubectl annotate</a> document.</p><h4>Scaling your application</h4><p>When load on your application grows or shrinks, it&#x27;s easy to scale with <strong>kubectl</strong>. For instance, to decrease the number of nginx replicas from 3 to 1, do:</p><p><strong>$ kubectl scale deployment/my-nginx --replicas=1</strong></p><p><strong>deployment &quot;my-nginx&quot; scaled</strong></p><p>Now you only have one pod managed by the deployment.</p><p><strong>$ kubectl get pods -l app=nginx</strong></p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>my-nginx-2035384211-j5fhi 1/1 Running 0 30m</strong></p><p>To have the system automatically choose the number of nginx replicas as needed, ranging from 1 to 3, do:</p><p><strong>$ kubectl autoscale deployment/my-nginx --min=1 --max=3</strong></p><p><strong>deployment &quot;my-nginx&quot; autoscaled</strong></p><p>Now your nginx replicas will be scaled up and down as needed, automatically.</p><p>For more information, please see <a href="https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands/#scale">kubectl scale</a>, <a href="https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands/#autoscale">kubectl autoscale</a> and <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">horizontal pod autoscaler</a>document.</p><h4>In-place updates of resources</h4><p>Sometimes it&#x27;s necessary to make narrow, non-disruptive updates to resources you&#x27;ve created.</p><h5><strong>kubectl apply</strong></h5><p>It is suggested to maintain a set of configuration files in source control (see <a href="http://martinfowler.com/bliki/InfrastructureAsCode.html">configuration as code</a>), so that they can be maintained and versioned along with the code for the resources they configure. Then, you can use <a href="https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands/#apply"><strong>kubectl apply</strong></a> to push your configuration changes to the cluster.</p><p>This command will compare the version of the configuration that you&#x27;re pushing with the previous version and apply the changes you&#x27;ve made, without overwriting any automated changes to properties you haven&#x27;t specified.</p><p><strong>$ kubectl apply -f docs/concepts/cluster-administration/nginx/nginx-deployment.yaml</strong></p><p><strong>deployment &quot;my-nginx&quot; configured</strong></p><p>Note that <strong>kubectl apply</strong> attaches an annotation to the resource in order to determine the changes to the configuration since the previous invocation. When it&#x27;s invoked, <strong>kubectl apply</strong> does a three-way diff between the previous configuration, the provided input and the current configuration of the resource, in order to determine how to modify the resource.</p><p>Currently, resources are created without this annotation, so the first invocation of <strong>kubectl apply</strong>will fall back to a two-way diff between the provided input and the current configuration of the resource. During this first invocation, it cannot detect the deletion of properties set when the resource was created. For this reason, it will not remove them.</p><p>All subsequent calls to <strong>kubectl apply</strong>, and other commands that modify the configuration, such as <strong>kubectl replace</strong> and <strong>kubectl edit</strong>, will update the annotation, allowing subsequent calls to <strong>kubectl apply</strong> to detect and perform deletions using a three-way diff.</p><p><strong>Note:</strong> To use apply, always create resource initially with either <strong>kubectl apply</strong> or <strong>kubectl create --save-config</strong>.</p><h5><strong>kubectl edit</strong></h5><p>Alternatively, you may also update resources with <strong>kubectl edit</strong>:</p><p><strong>$ kubectl edit deployment/my-nginx</strong></p><p>This is equivalent to first <strong>get</strong> the resource, edit it in text editor, and then <strong>apply</strong> the resource with the updated version:</p><p><strong>$ kubectl get deployment my-nginx -o yaml &gt; /tmp/nginx.yaml</strong></p><p><strong>$ vi /tmp/nginx.yaml</strong></p><p><strong><em># do some edit, and then save the file</em></strong></p><p><strong>$ kubectl apply -f /tmp/nginx.yaml</strong></p><p><strong>deployment &quot;my-nginx&quot; configured</strong></p><p><strong>$ rm /tmp/nginx.yaml</strong></p><p>This allows you to do more significant changes more easily. Note that you can specify the editor with your <strong>EDITOR</strong> or <strong>KUBE_EDITOR</strong> environment variables.</p><p>For more information, please see <a href="https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands/#edit">kubectl edit</a> document.</p><h5><strong>kubectl patch</strong></h5><p>You can use <strong>kubectl patch</strong> to update API objects in place. This command supports JSON patch, JSON merge patch, and strategic merge patch. See <a href="https://kubernetes.io/docs/tasks/run-application/update-api-object-kubectl-patch/">Update API Objects in Place Using kubectl patch</a>and <a href="https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands/#patch">kubectl patch</a>.</p><h4>Disruptive updates</h4><p>In some cases, you may need to update resource fields that cannot be updated once initialized, or you may just want to make a recursive change immediately, such as to fix broken pods created by a Deployment. To change such fields, use <strong>replace --force</strong>, which deletes and re-creates the resource. In this case, you can simply modify your original configuration file:</p><p><strong>$ kubectl replace -f docs/concepts/cluster-administration/nginx/nginx-deployment.yaml --force</strong></p><p><strong>deployment &quot;my-nginx&quot; deleted</strong></p><p><strong>deployment &quot;my-nginx&quot; replaced</strong></p><h4>Updating your application without a service outage</h4><p>At some point, you&#x27;ll eventually need to update your deployed application, typically by specifying a new image or image tag, as in the canary deployment scenario above. <strong>kubectl</strong> supports several update operations, each of which is applicable to different scenarios.</p><p>We&#x27;ll guide you through how to create and update applications with Deployments. If your deployed application is managed by Replication Controllers, you should read <a href="https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/">how to use <strong>kubectl rolling-update</strong></a> instead.</p><p>Let&#x27;s say you were running version 1.7.9 of nginx:</p><p><strong>$ kubectl run my-nginx --image=nginx:1.7.9 --replicas=3</strong></p><p><strong>deployment &quot;my-nginx&quot; created</strong></p><p>To update to version 1.9.1, simply change <strong>.spec.template.spec.containers<!-- -->[0]<!-- -->.image</strong> from <strong>nginx:1.7.9</strong> to <strong>nginx:1.9.1</strong>, with the kubectl commands we learned above.</p><p><strong>$ kubectl edit deployment/my-nginx</strong></p><p>That&#x27;s it! The Deployment will declaratively update the deployed nginx application progressively behind the scene. It ensures that only a certain number of old replicas may be down while they are being updated, and only a certain number of new replicas may be created above the desired number of pods. To learn more details about it, visit <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployment page</a>.</p><h4>What&#x27;s next?</h4><ul><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application-introspection/">Learn about how to use <strong>kubectl</strong> for application introspection and debugging.</a></li><li><a href="https://kubernetes.io/docs/concepts/configuration/overview/">Configuration Best Practices and Tips</a></li></ul><h3>Cluster Networking</h3><p>Kubernetes approaches networking somewhat differently than Docker does by default. There are 4 distinct networking problems to solve:</p><ol><li>Highly-coupled container-to-container communications: this is solved by <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/">pods</a> and <strong>localhost</strong>communications.</li><li>Pod-to-Pod communications: this is the primary focus of this document.</li><li>Pod-to-Service communications: this is covered by <a href="https://kubernetes.io/docs/concepts/services-networking/service/">services</a>.</li><li>External-to-Service communications: this is covered by <a href="https://kubernetes.io/docs/concepts/services-networking/service/">services</a>.</li></ol><ul><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/#summary"><strong>Summary</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/#docker-model"><strong>Docker model</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/#kubernetes-model"><strong>Kubernetes model</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-achieve-this"><strong>How to achieve this</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/#aci"><strong>ACI</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/#big-cloud-fabric-from-big-switch-networks"><strong>Big Cloud Fabric from Big Switch Networks</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/#cilium"><strong>Cilium</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/#contiv"><strong>Contiv</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/#contrail"><strong>Contrail</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/#flannel"><strong>Flannel</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/#google-compute-engine-gce"><strong>Google Compute Engine (GCE)</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/#kube-router"><strong>Kube-router</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/#l2-networks-and-linux-bridging"><strong>L2 networks and linux bridging</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/#multus-a-multi-network-plugin"><strong>Multus (a Multi Network plugin)</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/#nsx-t"><strong>NSX-T</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/#nuage-networks-vcs-virtualized-cloud-services"><strong>Nuage Networks VCS (Virtualized Cloud Services)</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/#openvswitch"><strong>OpenVSwitch</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/#ovn-open-virtual-networking"><strong>OVN (Open Virtual Networking)</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/#project-calico"><strong>Project Calico</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/#romana"><strong>Romana</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/#weave-net-from-weaveworks"><strong>Weave Net from Weaveworks</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/#cni-genie-from-huawei"><strong>CNI-Genie from Huawei</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/#other-reading"><strong>Other reading</strong></a></li></ul><h4>Summary</h4><p>Kubernetes assumes that pods can communicate with other pods, regardless of which host they land on. Every pod gets its own IP address so you do not need to explicitly create links between pods and you almost never need to deal with mapping container ports to host ports. This creates a clean, backwards-compatible model where pods can be treated much like VMs or physical hosts from the perspectives of port allocation, naming, service discovery, load balancing, application configuration, and migration.</p><p>There are requirements imposed on how you set up your cluster networking to achieve this.</p><h4>Docker model</h4><p>Before discussing the Kubernetes approach to networking, it is worthwhile to review the &quot;normal&quot; way that networking works with Docker. By default, Docker uses host-private networking. It creates a virtual bridge, called <strong>docker0</strong> by default, and allocates a subnet from one of the private address blocks defined in <a href="https://tools.ietf.org/html/rfc1918">RFC1918</a> for that bridge. For each container that Docker creates, it allocates a virtual Ethernet device (called <strong>veth</strong>) which is attached to the bridge. The veth is mapped to appear as <strong>eth0</strong> in the container, using Linux namespaces. The in-container <strong>eth0</strong> interface is given an IP address from the bridge&#x27;s address range.</p><p>The result is that Docker containers can talk to other containers only if they are on the same machine (and thus the same virtual bridge). Containers on different machines can not reach each other - in fact they may end up with the exact same network ranges and IP addresses.</p><p>In order for Docker containers to communicate across nodes, there must be allocated ports on the machine&#x27;s own IP address, which are then forwarded or proxied to the containers. This obviously means that containers must either coordinate which ports they use very carefully or ports must be allocated dynamically.</p><h4>Kubernetes model</h4><p>Coordinating ports across multiple developers is very difficult to do at scale and exposes users to cluster-level issues outside of their control. Dynamic port allocation brings a lot of complications to the system - every application has to take ports as flags, the API servers have to know how to insert dynamic port numbers into configuration blocks, services have to know how to find each other, etc. Rather than deal with this, Kubernetes takes a different approach.</p><p>Kubernetes imposes the following fundamental requirements on any networking implementation (barring any intentional network segmentation policies):</p><ul><li>all containers can communicate with all other containers without NAT</li><li>all nodes can communicate with all containers (and vice-versa) without NAT</li><li>the IP that a container sees itself as is the same IP that others see it as</li></ul><p>What this means in practice is that you can not just take two computers running Docker and expect Kubernetes to work. You must ensure that the fundamental requirements are met.</p><p>This model is not only less complex overall, but it is principally compatible with the desire for Kubernetes to enable low-friction porting of apps from VMs to containers. If your job previously ran in a VM, your VM had an IP and could talk to other VMs in your project. This is the same basic model.</p><p>Until now this document has talked about containers. In reality, Kubernetes applies IP addresses at the <strong>Pod</strong> scope - containers within a <strong>Pod</strong> share their network namespaces - including their IP address. This means that containers within a <strong>Pod</strong> can all reach each other&#x27;s ports on <strong>localhost</strong>. This does imply that containers within a <strong>Pod</strong> must coordinate port usage, but this is no different than processes in a VM. This is called the &quot;IP-per-pod&quot; model. This is implemented, using Docker, as a &quot;pod container&quot; which holds the network namespace open while &quot;app containers&quot; (the things the user specified) join that namespace with Docker&#x27;s <strong>--net=container:<code>&lt;id&gt;</code></strong> function.</p><p>As with Docker, it is possible to request host ports, but this is reduced to a very niche operation. In this case a port will be allocated on the host <strong>Node</strong> and traffic will be forwarded to the <strong>Pod</strong>. The <strong>Pod</strong> itself is blind to the existence or non-existence of host ports.</p><h4>How to achieve this</h4><p>There are a number of ways that this network model can be implemented. This document is not an exhaustive study of the various methods, but hopefully serves as an introduction to various technologies and serves as a jumping-off point.</p><p>The following networking options are sorted alphabetically - the order does not imply any preferential status.</p><h5><strong>ACI</strong></h5><p><a href="https://www.cisco.com/c/en/us/solutions/data-center-virtualization/application-centric-infrastructure/index.html">Cisco Application Centric Infrastructure</a> offers an integrated overlay and underlay SDN solution that supports containers, virtual machines, and bare metal servers. <a href="https://www.github.com/noironetworks/aci-containers">ACI</a> provides container networking integration for ACI. An overview of the integration is provided <a href="https://www.cisco.com/c/dam/en/us/solutions/collateral/data-center-virtualization/application-centric-infrastructure/solution-overview-c22-739493.pdf">here</a>.</p><h5><strong>Big Cloud Fabric from Big Switch Networks</strong></h5><p><a href="https://www.bigswitch.com/container-network-automation">Big Cloud Fabric</a> is a cloud native networking architecture, designed to run Kubernetes in private cloud/on-premise environments. Using unified physical &amp; virtual SDN, Big Cloud Fabric tackles inherent container networking problems such as load balancing, visibility, troubleshooting, security policies &amp; container traffic monitoring.</p><p>With the help of the Big Cloud Fabric&#x27;s virtual pod multi-tenant architecture, container orchestration systems such as Kubernetes, RedHat Openshift, Mesosphere DC/OS &amp; Docker Swarm will be natively integrated along side with VM orchestration systems such as VMware, OpenStack &amp; Nutanix. Customers will be able to securely inter-connect any number of these clusters and enable inter-tenant communication between them if needed.</p><p>BCF was recognized by Gartner as a visionary in the latest <a href="http://go.bigswitch.com/17GatedDocuments-MagicQuadrantforDataCenterNetworking_Reg.html">Magic Quadrant</a>. One of the BCF Kubernetes on premise deployments (which includes Kubernetes, DC/OS &amp; VMware running on multiple DCs across different geographic regions) is also referenced <a href="https://portworx.com/architects-corner-kubernetes-satya-komala-nio/">here</a>.</p><h5><strong>Cilium</strong></h5><p><a href="https://github.com/cilium/cilium">Cilium</a> is open source software for providing and transparently securing network connectivity between application containers. Cilium is L7/HTTP aware and can enforce network policies on L3-L7 using an identity based security model that is decoupled from network addressing.</p><h5><strong>Contiv</strong></h5><p><a href="https://github.com/contiv/netplugin">Contiv</a> provides configurable networking (native l3 using BGP, overlay using vxlan, classic l2, or Cisco-SDN/ACI) for various use cases. <a href="http://contiv.io/">Contiv</a> is all open sourced.</p><h5><strong>Contrail</strong></h5><p><a href="http://www.juniper.net/us/en/products-services/sdn/contrail/contrail-networking/">Contrail</a>, based on <a href="http://www.opencontrail.org/">OpenContrail</a>, is a truly open, multi-cloud network virtualization and policy management platform. Contrail / OpenContrail is integrated with various orchestration systems such as Kubernetes, OpenShift, OpenStack and Mesos, and provides different isolation modes for virtual machines, containers/pods and bare metal workloads.</p><h5><strong>Flannel</strong></h5><p><a href="https://github.com/coreos/flannel#flannel">Flannel</a> is a very simple overlay network that satisfies the Kubernetes requirements. Many people have reported success with Flannel and Kubernetes.</p><h5><strong>Google Compute Engine (GCE)</strong></h5><p>For the Google Compute Engine cluster configuration scripts, <a href="https://cloud.google.com/vpc/docs/routes">advanced routing</a> is used to assign each VM a subnet (default is <strong>/24</strong> - 254 IPs). Any traffic bound for that subnet will be routed directly to the VM by the GCE network fabric. This is in addition to the &quot;main&quot; IP address assigned to the VM, which is NAT&#x27;ed for outbound internet access. A linux bridge (called <strong>cbr0</strong>) is configured to exist on that subnet, and is passed to docker&#x27;s <strong>--bridge</strong> flag.</p><p>Docker is started with:</p><p><strong>DOCKER_OPTS=&quot;--bridge=cbr0 --iptables=false --ip-masq=false&quot;</strong></p><p>This bridge is created by Kubelet (controlled by the <strong>--network-plugin=kubenet</strong> flag) according to the <strong>Node</strong>&#x27;s <strong>spec.podCIDR</strong>.</p><p>Docker will now allocate IPs from the <strong>cbr-cidr</strong> block. Containers can reach each other and <strong>Nodes</strong>over the <strong>cbr0</strong> bridge. Those IPs are all routable within the GCE project network.</p><p>GCE itself does not know anything about these IPs, though, so it will not NAT them for outbound internet traffic. To achieve that an iptables rule is used to masquerade (aka SNAT - to make it seem as if packets came from the <strong>Node</strong> itself) traffic that is bound for IPs outside the GCE project network (10.0.0.0/8).</p><p><strong>iptables -t nat -A POSTROUTING ! -d 10.0.0.0/8 -o eth0 -j MASQUERADE</strong></p><p>Lastly IP forwarding is enabled in the kernel (so the kernel will process packets for bridged containers):</p><p><strong>sysctl net.ipv4.ip_forward=1</strong></p><p>The result of all this is that all <strong>Pods</strong> can reach each other and can egress traffic to the internet.</p><h5><strong>Kube-router</strong></h5><p><a href="https://github.com/cloudnativelabs/kube-router">Kube-router</a> is a purpose-built networking solution for Kubernetes that aims to provide high performance and operational simplicity. Kube-router provides a Linux <a href="http://www.linuxvirtualserver.org/software/ipvs.html">LVS/IPVS</a>-based service proxy, a Linux kernel forwarding-based pod-to-pod networking solution with no overlays, and iptables/ipset-based network policy enforcer.</p><h5><strong>L2 networks and linux bridging</strong></h5><p>If you have a &quot;dumb&quot; L2 network, such as a simple switch in a &quot;bare-metal&quot; environment, you should be able to do something similar to the above GCE setup. Note that these instructions have only been tried very casually - it seems to work, but has not been thoroughly tested. If you use this technique and perfect the process, please let us know.</p><p>Follow the &quot;With Linux Bridge devices&quot; section of <a href="http://blog.oddbit.com/2014/08/11/four-ways-to-connect-a-docker/">this very nice tutorial</a> from Lars Kellogg-Stedman.</p><h5><strong>Multus (a Multi Network plugin)</strong></h5><p><a href="https://github.com/Intel-Corp/multus-cni">Multus</a> is a Multi CNI plugin to support the Multi Networking feature in Kubernetes using CRD based network objects in Kubernetes.</p><p>Multus supports all <a href="https://github.com/containernetworking/plugins">reference plugins</a> (eg. <a href="https://github.com/containernetworking/plugins/tree/master/plugins/meta/flannel">Flannel</a>, <a href="https://github.com/containernetworking/plugins/tree/master/plugins/ipam/dhcp">DHCP</a>, <a href="https://github.com/containernetworking/plugins/tree/master/plugins/main/macvlan">Macvlan</a>) that implement the CNI specification and 3rd party plugins (eg. <a href="https://github.com/projectcalico/cni-plugin">Calico</a>, <a href="https://github.com/weaveworks/weave">Weave</a>, <a href="https://github.com/cilium/cilium">Cilium</a>, <a href="https://github.com/contiv/netplugin">Contiv</a>). In addition to it, Multus supports <a href="https://github.com/hustcat/sriov-cni">SRIOV</a>, <a href="https://github.com/Intel-Corp/sriov-cni">DPDK</a>, <a href="https://github.com/intel/vhost-user-net-plugin">OVS-DPDK &amp; VPP</a> workloads in Kubernetes with both cloud native and NFV based applications in Kubernetes.</p><h5><strong>NSX-T</strong></h5><p><a href="https://docs.vmware.com/en/VMware-NSX-T/index.html">VMware NSX-T</a> is a network virtualization and security platform. NSX-T can provide network virtualization for a multi-cloud and multi-hypervisor environment and is focused on emerging application frameworks and architectures that have heterogeneous endpoints and technology stacks. In addition to vSphere hypervisors, these environments include other hypervisors such as KVM, containers, and bare metal.</p><p><a href="https://docs.vmware.com/en/VMware-NSX-T/2.0/nsxt_20_ncp_kubernetes.pdf">NSX-T Container Plug-in (NCP)</a> provides integration between NSX-T and container orchestrators such as Kubernetes, as well as integration between NSX-T and container-based CaaS/PaaS platforms such as Pivotal Container Service (PKS) and Openshift.</p><h5><strong>Nuage Networks VCS (Virtualized Cloud Services)</strong></h5><p><a href="http://www.nuagenetworks.net/">Nuage</a> provides a highly scalable policy-based Software-Defined Networking (SDN) platform. Nuage uses the open source Open vSwitch for the data plane along with a feature rich SDN Controller built on open standards.</p><p>The Nuage platform uses overlays to provide seamless policy-based networking between Kubernetes Pods and non-Kubernetes environments (VMs and bare metal servers). Nuage&#x27;s policy abstraction model is designed with applications in mind and makes it easy to declare fine-grained policies for applications.The platform&#x27;s real-time analytics engine enables visibility and security monitoring for Kubernetes applications.</p><h5><strong>OpenVSwitch</strong></h5><p><a href="https://www.openvswitch.org/">OpenVSwitch</a> is a somewhat more mature but also complicated way to build an overlay network. This is endorsed by several of the &quot;Big Shops&quot; for networking.</p><h5><strong>OVN (Open Virtual Networking)</strong></h5><p>OVN is an opensource network virtualization solution developed by the Open vSwitch community. It lets one create logical switches, logical routers, stateful ACLs, load-balancers etc to build different virtual networking topologies. The project has a specific Kubernetes plugin and documentation at <a href="https://github.com/openvswitch/ovn-kubernetes">ovn-kubernetes</a>.</p><h5><strong>Project Calico</strong></h5><p><a href="http://docs.projectcalico.org/">Project Calico</a> is an open source container networking provider and network policy engine.</p><p>Calico provides a highly scalable networking and network policy solution for connecting Kubernetes pods based on the same IP networking principles as the internet. Calico can be deployed without encapsulation or overlays to provide high-performance, high-scale data center networking. Calico also provides fine-grained, intent based network security policy for Kubernetes pods via its distributed firewall.</p><p>Calico can also be run in policy enforcement mode in conjunction with other networking solutions such as Flannel, aka <a href="https://github.com/tigera/canal">canal</a>, or native GCE networking.</p><h5><strong>Romana</strong></h5><p><a href="http://romana.io/">Romana</a> is an open source network and security automation solution that lets you deploy Kubernetes without an overlay network. Romana supports Kubernetes <a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/">Network Policy</a> to provide isolation across network namespaces.</p><h5><strong>Weave Net from Weaveworks</strong></h5><p><a href="https://www.weave.works/products/weave-net/">Weave Net</a> is a resilient and simple to use network for Kubernetes and its hosted applications. Weave Net runs as a <a href="https://www.weave.works/docs/net/latest/cni-plugin/">CNI plug-in</a> or stand-alone. In either version, it doesn&#x27;t require any configuration or extra code to run, and in both cases, the network provides one IP address per pod - as is standard for Kubernetes.</p><h5><strong>CNI-Genie from Huawei</strong></h5><p><a href="https://github.com/Huawei-PaaS/CNI-Genie">CNI-Genie</a> is a CNI plugin that enables Kubernetes to <a href="https://github.com/Huawei-PaaS/CNI-Genie/blob/master/docs/multiple-cni-plugins/README.md#what-cni-genie-feature-1-multiple-cni-plugins-enables">simultaneously have access to different implementations</a> of the <a href="https://git.k8s.io/website/docs/concepts/cluster-administration/networking.md#kubernetes-model">Kubernetes network model</a> in runtime. This includes any implementation that runs as a <a href="https://github.com/containernetworking/cni#3rd-party-plugins">CNI plugin</a>, such as <a href="https://github.com/coreos/flannel#flannel">Flannel</a>, <a href="http://docs.projectcalico.org/">Calico</a>, <a href="http://romana.io/">Romana</a>, <a href="https://www.weave.works/products/weave-net/">Weave-net</a>.</p><p>CNI-Genie also supports <a href="https://github.com/Huawei-PaaS/CNI-Genie/blob/master/docs/multiple-ips/README.md#feature-2-extension-cni-genie-multiple-ip-addresses-per-pod">assigning multiple IP addresses to a pod</a>, each from a different CNI plugin.</p><h4>Other reading</h4><p>The early design of the networking model and its rationale, and some future plans are described in more detail in the <a href="https://git.k8s.io/community/contributors/design-proposals/network/networking.md">networking design document</a>.</p><h3>Network Plugins</h3><ul><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/network-plugins/#installation"><strong>Installation</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/network-plugins/#network-plugin-requirements"><strong>Network Plugin Requirements</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/network-plugins/#cni"><strong>CNI</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/network-plugins/#kubenet"><strong>kubenet</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/network-plugins/#customizing-the-mtu-with-kubenet"><strong>Customizing the MTU (with kubenet)</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/network-plugins/#usage-summary"><strong>Usage Summary</strong></a></li></ul><p><strong>Disclaimer</strong>: Network plugins are in alpha. Its contents will change rapidly.</p><p>Network plugins in Kubernetes come in a few flavors:</p><ul><li>CNI plugins: adhere to the appc/CNI specification, designed for interoperability.</li><li>Kubenet plugin: implements basic <strong>cbr0</strong> using the <strong>bridge</strong> and <strong>host-local</strong> CNI plugins</li></ul><h4>Installation</h4><p>The kubelet has a single default network plugin, and a default network common to the entire cluster. It probes for plugins when it starts up, remembers what it found, and executes the selected plugin at appropriate times in the pod lifecycle (this is only true for Docker, as rkt manages its own CNI plugins). There are two Kubelet command line parameters to keep in mind when using plugins:</p><ul><li><strong>cni-bin-dir</strong>: Kubelet probes this directory for plugins on startup</li><li><strong>network-plugin</strong>: The network plugin to use from <strong>cni-bin-dir</strong>. It must match the name reported by a plugin probed from the plugin directory. For CNI plugins, this is simply &quot;cni&quot;.</li></ul><h4>Network Plugin Requirements</h4><p>Besides providing the <a href="https://github.com/kubernetes/kubernetes/tree/v1.10.0/pkg/kubelet/network/plugins.go"><strong>NetworkPlugin</strong> interface</a> to configure and clean up pod networking, the plugin may also need specific support for kube-proxy. The iptables proxy obviously depends on iptables, and the plugin may need to ensure that container traffic is made available to iptables. For example, if the plugin connects containers to a Linux bridge, the plugin must set the <strong>net/bridge/bridge-nf-call-iptables</strong> sysctl to <strong>1</strong> to ensure that the iptables proxy functions correctly. If the plugin does not use a Linux bridge (but instead something like Open vSwitch or some other mechanism) it should ensure container traffic is appropriately routed for the proxy.</p><p>By default if no kubelet network plugin is specified, the <strong>noop</strong> plugin is used, which sets <strong>net/bridge/bridge-nf-call-iptables=1</strong> to ensure simple configurations (like Docker with a bridge) work correctly with the iptables proxy.</p><h5><strong>CNI</strong></h5><p>The CNI plugin is selected by passing Kubelet the <strong>--network-plugin=cni</strong> command-line option. Kubelet reads a file from <strong>--cni-conf-dir</strong> (default <strong>/etc/cni/net.d</strong>) and uses the CNI configuration from that file to set up each pod&#x27;s network. The CNI configuration file must match the <a href="https://github.com/containernetworking/cni/blob/master/SPEC.md#network-configuration">CNI specification</a>, and any required CNI plugins referenced by the configuration must be present in <strong>--cni-bin-dir</strong> (default <strong>/opt/cni/bin</strong>).</p><p>If there are multiple CNI configuration files in the directory, the first one in lexicographic order of file name is used.</p><p>In addition to the CNI plugin specified by the configuration file, Kubernetes requires the standard CNI <a href="https://github.com/containernetworking/plugins/blob/master/plugins/main/loopback/loopback.go"><strong>lo</strong></a> plugin, at minimum version 0.2.0</p><p>Limitation: Due to <a href="https://github.com/kubernetes/kubernetes/issues/31307">#31307</a>, <strong>HostPort</strong> won&#x27;t work with CNI networking plugin at the moment. That means all <strong>hostPort</strong> attribute in pod would be simply ignored.</p><h5><strong>kubenet</strong></h5><p>Kubenet is a very basic, simple network plugin, on Linux only. It does not, of itself, implement more advanced features like cross-node networking or network policy. It is typically used together with a cloud provider that sets up routing rules for communication between nodes, or in single-node environments.</p><p>Kubenet creates a Linux bridge named <strong>cbr0</strong> and creates a veth pair for each pod with the host end of each pair connected to <strong>cbr0</strong>. The pod end of the pair is assigned an IP address allocated from a range assigned to the node either through configuration or by the controller-manager. <strong>cbr0</strong> is assigned an MTU matching the smallest MTU of an enabled normal interface on the host.</p><p>The plugin requires a few things:</p><ul><li>The standard CNI <strong>bridge</strong>, <strong>lo</strong> and <strong>host-local</strong> plugins are required, at minimum version 0.2.0. Kubenet will first search for them in <strong>/opt/cni/bin</strong>. Specify <strong>cni-bin-dir</strong> to supply additional search path. The first found match will take effect.</li><li>Kubelet must be run with the <strong>--network-plugin=kubenet</strong> argument to enable the plugin</li><li>Kubelet should also be run with the <strong>--non-masquerade-cidr=<code>&lt;clusterCidr&gt;</code></strong> argument to ensure traffic to IPs outside this range will use IP masquerade.</li><li>The node must be assigned an IP subnet through either the <strong>--pod-cidr</strong> kubelet command-line option or the <strong>--allocate-node-cidrs=true --cluster-cidr=<code>&lt;cidr&gt;</code></strong> controller-manager command-line options.</li></ul><h5><strong>Customizing the MTU (with kubenet)</strong></h5><p>The MTU should always be configured correctly to get the best networking performance. Network plugins will usually try to infer a sensible MTU, but sometimes the logic will not result in an optimal MTU. For example, if the Docker bridge or another interface has a small MTU, kubenet will currently select that MTU. Or if you are using IPSEC encapsulation, the MTU must be reduced, and this calculation is out-of-scope for most network plugins.</p><p>Where needed, you can specify the MTU explicitly with the <strong>network-plugin-mtu</strong> kubelet option. For example, on AWS the <strong>eth0</strong> MTU is typically 9001, so you might specify <strong>--network-plugin-mtu=9001</strong>. If you&#x27;re using IPSEC you might reduce it to allow for encapsulation overhead e.g. <strong>--network-plugin-mtu=8873</strong>.</p><p>This option is provided to the network-plugin; currently <strong>only kubenet supports network-plugin-mtu</strong>.</p><h4>Usage Summary</h4><ul><li><strong>--network-plugin=cni</strong> specifies that we use the <strong>cni</strong> network plugin with actual CNI plugin binaries located in <strong>--cni-bin-dir</strong> (default <strong>/opt/cni/bin</strong>) and CNI plugin configuration located in <strong>--cni-conf-dir</strong> (default <strong>/etc/cni/net.d</strong>).</li><li><strong>--network-plugin=kubenet</strong> specifies that we use the <strong>kubenet</strong> network plugin with CNI <strong>bridge</strong> and <strong>host-local</strong> plugins placed in <strong>/opt/cni/bin</strong> or <strong>cni-bin-dir</strong>.</li><li><strong>--network-plugin-mtu=9001</strong> specifies the MTU to use, currently only used by the <strong>kubenet</strong>network plugin.</li></ul><h3>Logging Architecture</h3><p>Application and systems logs can help you understand what is happening inside your cluster. The logs are particularly useful for debugging problems and monitoring cluster activity. Most modern applications have some kind of logging mechanism; as such, most container engines are likewise designed to support some kind of logging. The easiest and most embraced logging method for containerized applications is to write to the standard output and standard error streams.</p><p>However, the native functionality provided by a container engine or runtime is usually not enough for a complete logging solution. For example, if a container crashes, a pod is evicted, or a node dies, you&#x27;ll usually still want to access your application&#x27;s logs. As such, logs should have a separate storage and lifecycle independent of nodes, pods, or containers. This concept is called cluster-level-logging. Cluster-level logging requires a separate backend to store, analyze, and query logs. Kubernetes provides no native storage solution for log data, but you can integrate many existing logging solutions into your Kubernetes cluster.</p><ul><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/logging/#basic-logging-in-kubernetes"><strong>Basic logging in Kubernetes</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/logging/#logging-at-the-node-level"><strong>Logging at the node level</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/logging/#system-component-logs"><strong>System component logs</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/logging/#cluster-level-logging-architectures"><strong>Cluster-level logging architectures</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/logging/#using-a-node-logging-agent"><strong>Using a node logging agent</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/logging/#using-a-sidecar-container-with-the-logging-agent"><strong>Using a sidecar container with the logging agent</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/logging/#streaming-sidecar-container"><strong>Streaming sidecar container</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/logging/#sidecar-container-with-a-logging-agent"><strong>Sidecar container with a logging agent</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/logging/#exposing-logs-directly-from-the-application"><strong>Exposing logs directly from the application</strong></a></li></ul></li></ul><p>Cluster-level logging architectures are described in assumption that a logging backend is present inside or outside of your cluster. If you&#x27;re not interested in having cluster-level logging, you might still find the description of how logs are stored and handled on the node to be useful.</p><h4>Basic logging in Kubernetes</h4><p>In this section, you can see an example of basic logging in Kubernetes that outputs data to the standard output stream. This demonstration uses a <a href="https://kubernetes.io/docs/concepts/cluster-administration/counter-pod.yaml">pod specification</a> with a container that writes some text to standard output once per second.</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>counter-pod.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/">https://raw.githubusercontent.com/kubernetes/</a> |
| website/master/docs/tasks/debug-application-cluster/counter-pod.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: counter</strong>                                                     |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: count</strong>                                                     |
|                                                                       |
| <strong>image: busybox</strong>                                                    |
|                                                                       |
| <strong>args: [/bin/sh, -c,</strong>                                              |
|                                                                       |
| <strong>\&#x27;i=0; while true; do echo &quot;$i: $(date)&quot;; i=$((i+1)); sleep 1; |
| done\&#x27;]</strong>                                                            |
+-----------------------------------------------------------------------+</p><p>To run this pod, use the following command:</p><p><strong>$ kubectl create -f <a href="https://k8s.io/docs/tasks/debug-application-cluster/counter-pod.yaml">https://k8s.io/docs/tasks/debug-application-cluster/counter-pod.yaml</a></strong></p><p><strong>pod &quot;counter&quot; created</strong></p><p>To fetch the logs, use the <strong>kubectl logs</strong> command, as follows:</p><p><strong>$ kubectl logs counter</strong></p><p><strong>0: Mon Jan 1 00:00:00 UTC 2001</strong></p><p><strong>1: Mon Jan 1 00:00:01 UTC 2001</strong></p><p><strong>2: Mon Jan 1 00:00:02 UTC 2001</strong></p><p><strong>.<!-- -->..</strong></p><p>You can use <strong>kubectl logs</strong> to retrieve logs from a previous instantiation of a container with <strong>--previous</strong> flag, in case the container has crashed. If your pod has multiple containers, you should specify which container&#x27;s logs you want to access by appending a container name to the command. See the <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#logs"><strong>kubectl logs</strong> documentation</a> for more details.</p><h4>Logging at the node level</h4><p>Everything a containerized application writes to <strong>stdout</strong> and <strong>stderr</strong> is handled and redirected somewhere by a container engine. For example, the Docker container engine redirects those two streams to <a href="https://docs.docker.com/engine/admin/logging/overview">a logging driver</a>, which is configured in Kubernetes to write to a file in json format.</p><p><strong>Note:</strong> The Docker json logging driver treats each line as a separate message. When using the Docker logging driver, there is no direct support for multi-line messages. You need to handle multi-line messages at the logging agent level or higher.</p><p>By default, if a container restarts, the kubelet keeps one terminated container with its logs. If a pod is evicted from the node, all corresponding containers are also evicted, along with their logs.</p><p>An important consideration in node-level logging is implementing log rotation, so that logs don&#x27;t consume all available storage on the node. Kubernetes currently is not responsible for rotating logs, but rather a deployment tool should set up a solution to address that. For example, in Kubernetes clusters, deployed by the <strong>kube-up.sh</strong> script, there is a <a href="https://linux.die.net/man/8/logrotate"><strong>logrotate</strong></a> tool configured to run each hour. You can also set up a container runtime to rotate application&#x27;s logs automatically, e.g. by using Docker&#x27;s <strong>log-opt</strong>. In the <strong>kube-up.sh</strong> script, the latter approach is used for COS image on GCP, and the former approach is used in any other environment. In both cases, by default rotation is configured to take place when log file exceeds 10MB.</p><p>As an example, you can find detailed information about how <strong>kube-up.sh</strong> sets up logging for COS image on GCP in the corresponding <a href="https://github.com/kubernetes/kubernetes/blob/master/cluster/gce/gci/configure-helper.sh">script</a>.</p><p>When you run <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#logs"><strong>kubectl logs</strong></a> as in the basic logging example, the kubelet on the node handles the request and reads directly from the log file, returning the contents in the response.</p><p><strong>Note:</strong> currently, if some external system has performed the rotation, only the contents of the latest log file will be available through <strong>kubectl logs</strong>. E.g. if there&#x27;s a 10MB file, <strong>logrotate</strong> performs the rotation and there are two files, one 10MB in size and one empty, <strong>kubectl logs</strong> will return an empty response.</p><h5><strong>System component logs</strong></h5><p>There are two types of system components: those that run in a container and those that do not run in a container. For example:</p><ul><li>The Kubernetes scheduler and kube-proxy run in a container.</li><li>The kubelet and container runtime, for example Docker, do not run in containers.</li></ul><p>On machines with systemd, the kubelet and container runtime write to journald. If systemd is not present, they write to <strong>.log</strong> files in the <strong>/var/log</strong> directory. System components inside containers always write to the <strong>/var/log</strong> directory, bypassing the default logging mechanism. They use the <a href="https://godoc.org/github.com/golang/glog">glog</a> logging library. You can find the conventions for logging severity for those components in the <a href="https://git.k8s.io/community/contributors/devel/logging.md">development docs on logging</a>.</p><p>Similarly to the container logs, system component logs in the <strong>/var/log</strong> directory should be rotated. In Kubernetes clusters brought up by the <strong>kube-up.sh</strong> script, those logs are configured to be rotated by the <strong>logrotate</strong> tool daily or once the size exceeds 100MB.</p><h4>Cluster-level logging architectures</h4><p>While Kubernetes does not provide a native solution for cluster-level logging, there are several common approaches you can consider. Here are some options:</p><ul><li>Use a node-level logging agent that runs on every node.</li><li>Include a dedicated sidecar container for logging in an application pod.</li><li>Push logs directly to a backend from within an application.</li></ul><h5><strong>Using a node logging agent</strong></h5><p>You can implement cluster-level logging by including a node-level logging agent on each node. The logging agent is a dedicated tool that exposes logs or pushes logs to a backend. Commonly, the logging agent is a container that has access to a directory with log files from all of the application containers on that node.</p><p>Because the logging agent must run on every node, it&#x27;s common to implement it as either a DaemonSet replica, a manifest pod, or a dedicated native process on the node. However the latter two approaches are deprecated and highly discouraged.</p><p>Using a node-level logging agent is the most common and encouraged approach for a Kubernetes cluster, because it creates only one agent per node, and it doesn&#x27;t require any changes to the applications running on the node. However, node-level logging only works for applications&#x27; standard output and standard error.</p><p>Kubernetes doesn&#x27;t specify a logging agent, but two optional logging agents are packaged with the Kubernetes release: <a href="https://kubernetes.io/docs/user-guide/logging/stackdriver">Stackdriver Logging</a> for use with Google Cloud Platform, and <a href="https://kubernetes.io/docs/user-guide/logging/elasticsearch">Elasticsearch</a>. You can find more information and instructions in the dedicated documents. Both use <a href="http://www.fluentd.org/">fluentd</a> with custom configuration as an agent on the node.</p><h5><strong>Using a sidecar container with the logging agent</strong></h5><p>You can use a sidecar container in one of the following ways:</p><ul><li>The sidecar container streams application logs to its own <strong>stdout</strong>.</li><li>The sidecar container runs a logging agent, which is configured to pick up logs from an application container.</li></ul><h6><strong>Streaming sidecar container</strong></h6><p>By having your sidecar containers stream to their own <strong>stdout</strong> and <strong>stderr</strong> streams, you can take advantage of the kubelet and the logging agent that already run on each node. The sidecar containers read logs from a file, a socket, or the journald. Each individual sidecar container prints log to its own <strong>stdout</strong> or <strong>stderr</strong> stream.</p><p>This approach allows you to separate several log streams from different parts of your application, some of which can lack support for writing to <strong>stdout</strong> or <strong>stderr</strong>. The logic behind redirecting logs is minimal, so it&#x27;s hardly a significant overhead. Additionally, because <strong>stdout</strong> and <strong>stderr</strong> are handled by the kubelet, you can use built-in tools like <strong>kubectl logs</strong>.</p><p>Consider the following example. A pod runs a single container, and the container writes to two different log files, using two different formats. Here&#x27;s a configuration file for the Pod:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>two-files-counter                                                  |
| -pod.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/website/ma">https://raw.githubusercontent.com/kubernetes/website/ma</a> |
| ster/docs/concepts/cluster-administration/two-files-counter-pod.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: counter</strong>                                                     |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: count</strong>                                                     |
|                                                                       |
| <strong>image: busybox</strong>                                                    |
|                                                                       |
| <strong>args:</strong>                                                             |
|                                                                       |
| <strong>- /bin/sh</strong>                                                         |
|                                                                       |
| <strong>- -c</strong>                                                              |
|                                                                       |
| <strong>- &gt;</strong>                                                              |
|                                                                       |
| <strong>i=0;</strong>                                                              |
|                                                                       |
| <strong>while true;</strong>                                                       |
|                                                                       |
| <strong>do</strong>                                                                |
|                                                                       |
| <strong>echo &quot;$i: $(date)&quot; &gt;&gt; /var/log/1.log;</strong>                       |
|                                                                       |
| <strong>echo &quot;$(date) INFO $i&quot; &gt;&gt; /var/log/2.log;</strong>                   |
|                                                                       |
| <strong>i=$((i+1));</strong>                                                      |
|                                                                       |
| <strong>sleep 1;</strong>                                                          |
|                                                                       |
| <strong>done</strong>                                                              |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- name: varlog</strong>                                                    |
|                                                                       |
| <strong>mountPath: /var/log</strong>                                               |
|                                                                       |
| <strong>volumes:</strong>                                                          |
|                                                                       |
| <strong>- name: varlog</strong>                                                    |
|                                                                       |
| <strong>emptyDir: {}</strong>                                                      |
+-----------------------------------------------------------------------+</p><p>It would be a mess to have log entries of different formats in the same log stream, even if you managed to redirect both components to the <strong>stdout</strong> stream of the container. Instead, you could introduce two sidecar containers. Each sidecar container could tail a particular log file from a shared volume and then redirect the logs to its own <strong>stdout</strong> stream.</p><p>Here&#x27;s a configuration file for a pod that has two sidecar containers:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>two-files-counter-pod-streaming-sidecar.yaml</strong> ]<!-- -->(http              |
| s://raw.githubusercontent.com/kubernetes/website/master/docs/concepts |
| /cluster-administration/two-files-counter-pod-streaming-sidecar.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: counter</strong>                                                     |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: count</strong>                                                     |
|                                                                       |
| <strong>image: busybox</strong>                                                    |
|                                                                       |
| <strong>args:</strong>                                                             |
|                                                                       |
| <strong>- /bin/sh</strong>                                                         |
|                                                                       |
| <strong>- -c</strong>                                                              |
|                                                                       |
| <strong>- &gt;</strong>                                                              |
|                                                                       |
| <strong>i=0;</strong>                                                              |
|                                                                       |
| <strong>while true;</strong>                                                       |
|                                                                       |
| <strong>do</strong>                                                                |
|                                                                       |
| <strong>echo &quot;$i: $(date)&quot; &gt;&gt; /var/log/1.log;</strong>                       |
|                                                                       |
| <strong>echo &quot;$(date) INFO $i&quot; &gt;&gt; /var/log/2.log;</strong>                   |
|                                                                       |
| <strong>i=$((i+1));</strong>                                                      |
|                                                                       |
| <strong>sleep 1;</strong>                                                          |
|                                                                       |
| <strong>done</strong>                                                              |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- name: varlog</strong>                                                    |
|                                                                       |
| <strong>mountPath: /var/log</strong>                                               |
|                                                                       |
| <strong>- name: count-log-1</strong>                                               |
|                                                                       |
| <strong>image: busybox</strong>                                                    |
|                                                                       |
| <strong>args: <!-- -->[/bin/sh, -c, \&#x27;tail -n+1 -f /var/log/1.log\&#x27;]</strong>            |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- name: varlog</strong>                                                    |
|                                                                       |
| <strong>mountPath: /var/log</strong>                                               |
|                                                                       |
| <strong>- name: count-log-2</strong>                                               |
|                                                                       |
| <strong>image: busybox</strong>                                                    |
|                                                                       |
| <strong>args: <!-- -->[/bin/sh, -c, \&#x27;tail -n+1 -f /var/log/2.log\&#x27;]</strong>            |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- name: varlog</strong>                                                    |
|                                                                       |
| <strong>mountPath: /var/log</strong>                                               |
|                                                                       |
| <strong>volumes:</strong>                                                          |
|                                                                       |
| <strong>- name: varlog</strong>                                                    |
|                                                                       |
| <strong>emptyDir: {}</strong>                                                      |
+-----------------------------------------------------------------------+</p><p>Now when you run this pod, you can access each log stream separately by running the following commands:</p><p><strong>$ kubectl logs counter count-log-1</strong></p><p><strong>0: Mon Jan 1 00:00:00 UTC 2001</strong></p><p><strong>1: Mon Jan 1 00:00:01 UTC 2001</strong></p><p><strong>2: Mon Jan 1 00:00:02 UTC 2001</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>$ kubectl logs counter count-log-2</strong></p><p><strong>Mon Jan 1 00:00:00 UTC 2001 INFO 0</strong></p><p><strong>Mon Jan 1 00:00:01 UTC 2001 INFO 1</strong></p><p><strong>Mon Jan 1 00:00:02 UTC 2001 INFO 2</strong></p><p><strong>.<!-- -->..</strong></p><p>The node-level agent installed in your cluster picks up those log streams automatically without any further configuration. If you like, you can configure the agent to parse log lines depending on the source container.</p><p>Note, that despite low CPU and memory usage (order of couple of millicores for cpu and order of several megabytes for memory), writing logs to a file and then streaming them to <strong>stdout</strong> can double disk usage. If you have an application that writes to a single file, it&#x27;s generally better to set <strong>/dev/stdout</strong> as destination rather than implementing the streaming sidecar container approach.</p><p>Sidecar containers can also be used to rotate log files that cannot be rotated by the application itself. <a href="https://github.com/samsung-cnct/logrotate">An example</a> of this approach is a small container running logrotate periodically. However, it&#x27;s recommended to use <strong>stdout</strong> and <strong>stderr</strong> directly and leave rotation and retention policies to the kubelet.</p><h6><strong>Sidecar container with a logging agent</strong></h6><p>If the node-level logging agent is not flexible enough for your situation, you can create a sidecar container with a separate logging agent that you have configured specifically to run with your application.</p><p><strong>Note</strong>: Using a logging agent in a sidecar container can lead to significant resource consumption. Moreover, you won&#x27;t be able to access those logs using <strong>kubectl logs</strong> command, because they are not controlled by the kubelet.</p><p>As an example, you could use <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/logging-stackdriver/">Stackdriver</a>, which uses fluentd as a logging agent. Here are two configuration files that you can use to implement this approach. The first file contains a <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/">ConfigMap</a>to configure fluentd.</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>fluentd-sidecar-con                                                |
| fig.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/website/mas">https://raw.githubusercontent.com/kubernetes/website/mas</a> |
| ter/docs/concepts/cluster-administration/fluentd-sidecar-config.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>data:</strong>                                                             |
|                                                                       |
| <strong>fluentd.conf: |</strong>                                                  |
|                                                                       |
| <strong><code>&lt;source&gt;</code></strong>                                                        |
|                                                                       |
| <strong>type tail</strong>                                                         |
|                                                                       |
| <strong>format none</strong>                                                       |
|                                                                       |
| <strong>path /var/log/1.log</strong>                                               |
|                                                                       |
| <strong>pos_file /var/log/1.log.pos</strong>                                       |
|                                                                       |
| <strong>tag count.format1</strong>                                                 |
|                                                                       |
| <strong><code>&lt;/source&gt;</code></strong>                                                       |
|                                                                       |
| <strong><code>&lt;source&gt;</code></strong>                                                        |
|                                                                       |
| <strong>type tail</strong>                                                         |
|                                                                       |
| <strong>format none</strong>                                                       |
|                                                                       |
| <strong>path /var/log/2.log</strong>                                               |
|                                                                       |
| <strong>pos_file /var/log/2.log.pos</strong>                                       |
|                                                                       |
| <strong>tag count.format2</strong>                                                 |
|                                                                       |
| <strong><code>&lt;/source&gt;</code></strong>                                                       |
|                                                                       |
| <strong><code>&lt;match \*\*&gt;</code></strong>                                                    |
|                                                                       |
| <strong>type google_cloud</strong>                                                 |
|                                                                       |
| <strong><code>&lt;/match&gt;</code></strong>                                                        |
|                                                                       |
| <strong>kind: ConfigMap</strong>                                                   |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: fluentd-config</strong>                                              |
+-----------------------------------------------------------------------+</p><p><strong>Note</strong>: The configuration of fluentd is beyond the scope of this article. For information about configuring fluentd, see the <a href="http://docs.fluentd.org/">official fluentd documentation</a>.</p><p>The second file describes a pod that has a sidecar container running fluentd. The pod mounts a volume where fluentd can pick up its configuration data.</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>two-files-counter-pod-agent-sidecar.yaml</strong> ]<!-- -->(                      |
| <a href="https://raw.githubusercontent.com/kubernetes/website/master/docs/conc">https://raw.githubusercontent.com/kubernetes/website/master/docs/conc</a> |
| epts/cluster-administration/two-files-counter-pod-agent-sidecar.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: counter</strong>                                                     |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: count</strong>                                                     |
|                                                                       |
| <strong>image: busybox</strong>                                                    |
|                                                                       |
| <strong>args:</strong>                                                             |
|                                                                       |
| <strong>- /bin/sh</strong>                                                         |
|                                                                       |
| <strong>- -c</strong>                                                              |
|                                                                       |
| <strong>- &gt;</strong>                                                              |
|                                                                       |
| <strong>i=0;</strong>                                                              |
|                                                                       |
| <strong>while true;</strong>                                                       |
|                                                                       |
| <strong>do</strong>                                                                |
|                                                                       |
| <strong>echo &quot;$i: $(date)&quot; &gt;&gt; /var/log/1.log;</strong>                       |
|                                                                       |
| <strong>echo &quot;$(date) INFO $i&quot; &gt;&gt; /var/log/2.log;</strong>                   |
|                                                                       |
| <strong>i=$((i+1));</strong>                                                      |
|                                                                       |
| <strong>sleep 1;</strong>                                                          |
|                                                                       |
| <strong>done</strong>                                                              |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- name: varlog</strong>                                                    |
|                                                                       |
| <strong>mountPath: /var/log</strong>                                               |
|                                                                       |
| <strong>- name: count-agent</strong>                                               |
|                                                                       |
| <strong>image: k8s.gcr.io/fluentd-gcp:1.30</strong>                                |
|                                                                       |
| <strong>env:</strong>                                                              |
|                                                                       |
| <strong>- name: FLUENTD_ARGS</strong>                                              |
|                                                                       |
| <strong>value: -c /etc/fluentd-config/fluentd.conf</strong>                        |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- name: varlog</strong>                                                    |
|                                                                       |
| <strong>mountPath: /var/log</strong>                                               |
|                                                                       |
| <strong>- name: config-volume</strong>                                             |
|                                                                       |
| <strong>mountPath: /etc/fluentd-config</strong>                                    |
|                                                                       |
| <strong>volumes:</strong>                                                          |
|                                                                       |
| <strong>- name: varlog</strong>                                                    |
|                                                                       |
| <strong>emptyDir: {}</strong>                                                      |
|                                                                       |
| <strong>- name: config-volume</strong>                                             |
|                                                                       |
| <strong>configMap:</strong>                                                        |
|                                                                       |
| <strong>name: fluentd-config</strong>                                              |
+-----------------------------------------------------------------------+</p><p>After some time you can find log messages in the Stackdriver interface.</p><p>Remember, that this is just an example and you can actually replace fluentd with any logging agent, reading from any source inside an application container.</p><h5><strong>Exposing logs directly from the application</strong></h5><p>You can implement cluster-level logging by exposing or pushing logs directly from every application; however, the implementation for such a logging mechanism is outside the scope of Kubernetes.</p><h3>Configuring kubelet Garbage Collection</h3><ul><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/kubelet-garbage-collection/#image-collection"><strong>Image Collection</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/kubelet-garbage-collection/#container-collection"><strong>Container Collection</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/kubelet-garbage-collection/#user-configuration"><strong>User Configuration</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/kubelet-garbage-collection/#deprecation"><strong>Deprecation</strong></a></li></ul><p>Garbage collection is a helpful function of kubelet that will clean up unused images and unused containers. Kubelet will perform garbage collection for containers every minute and garbage collection for images every five minutes.</p><p>External garbage collection tools are not recommended as these tools can potentially break the behavior of kubelet by removing containers expected to exist.</p><h5><strong>Image Collection</strong></h5><p>Kubernetes manages lifecycle of all images through imageManager, with the cooperation of cadvisor.</p><p>The policy for garbage collecting images takes two factors into consideration: <strong>HighThresholdPercent</strong> and <strong>LowThresholdPercent</strong>. Disk usage above the high threshold will trigger garbage collection. The garbage collection will delete least recently used images until the low threshold has been met.</p><h5><strong>Container Collection</strong></h5><p>The policy for garbage collecting containers considers three user-defined variables. <strong>MinAge</strong> is the minimum age at which a container can be garbage collected. <strong>MaxPerPodContainer</strong> is the maximum number of dead containers every single pod (UID, container name) pair is allowed to have. <strong>MaxContainers</strong> is the maximum number of total dead containers. These variables can be individually disabled by setting <strong>MinAge</strong> to zero and setting <strong>MaxPerPodContainer</strong> and <strong>MaxContainers</strong> respectively to less than zero.</p><p>Kubelet will act on containers that are unidentified, deleted, or outside of the boundaries set by the previously mentioned flags. The oldest containers will generally be removed first. <strong>MaxPerPodContainer</strong> and <strong>MaxContainer</strong> may potentially conflict with each other in situations where retaining the maximum number of containers per pod (<strong>MaxPerPodContainer</strong>) would go outside the allowable range of global dead containers (<strong>MaxContainers</strong>). <strong>MaxPerPodContainer</strong>would be adjusted in this situation: A worst case scenario would be to downgrade <strong>MaxPerPodContainer</strong> to 1 and evict the oldest containers. Additionally, containers owned by pods that have been deleted are removed once they are older than <strong>MinAge</strong>.</p><p>Containers that are not managed by kubelet are not subject to container garbage collection.</p><h5><strong>User Configuration</strong></h5><p>Users can adjust the following thresholds to tune image garbage collection with the following kubelet flags :</p><ol><li><strong>image-gc-high-threshold</strong>, the percent of disk usage which triggers image garbage collection. Default is 90%.</li><li><strong>image-gc-low-threshold</strong>, the percent of disk usage to which image garbage collection attempts to free. Default is 80%.</li></ol><p>We also allow users to customize garbage collection policy through the following kubelet flags:</p><ol><li><strong>minimum-container-ttl-duration</strong>, minimum age for a finished container before it is garbage collected. Default is 0 minute, which means every finished container will be garbage collected.</li><li><strong>maximum-dead-containers-per-container</strong>, maximum number of old instances to be retained per container. Default is 1.</li><li><strong>maximum-dead-containers</strong>, maximum number of old instances of containers to retain globally. Default is -1, which means there is no global limit.</li></ol><p>Containers can potentially be garbage collected before their usefulness has expired. These containers can contain logs and other data that can be useful for troubleshooting. A sufficiently large value for <strong>maximum-dead-containers-per-container</strong> is highly recommended to allow at least 1 dead container to be retained per expected container. A larger value for <strong>maximum-dead-containers</strong>is also recommended for a similar reason. See <a href="https://github.com/kubernetes/kubernetes/issues/13287">this issue</a> for more details.</p><h5><strong>Deprecation</strong></h5><p>Some kubelet Garbage Collection features in this doc will be replaced by kubelet eviction in the future.</p><p>Including:</p><p>  Existing Flag                                  New Flag                                       Rationale</p><hr/><p>  <strong>--image-gc-high-threshold</strong>                 <strong>--eviction-hard</strong> or <strong>--eviction-soft</strong>   existing eviction signals can trigger image garbage collection
<strong>--image-gc-low-threshold</strong>                  <strong>--eviction-minimum-reclaim</strong>                eviction reclaims achieve the same behavior
<strong>--maximum-dead-containers</strong>                                                                deprecated once old logs are stored outside of container&#x27;s context
<strong>--maximum-dead-containers-per-container</strong>                                                  deprecated once old logs are stored outside of container&#x27;s context
<strong>--minimum-container-ttl-duration</strong>                                                         deprecated once old logs are stored outside of container&#x27;s context
<strong>--low-diskspace-threshold-mb</strong>              <strong>--eviction-hard</strong> or <strong>eviction-soft</strong>      eviction generalizes disk thresholds to other resources
<strong>--outofdisk-transition-frequency</strong>          <strong>--eviction-pressure-transition-period</strong>     eviction generalizes disk pressure transition to other resources</p><p>See <a href="https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/">Configuring Out Of Resource Handling</a> for more details.</p><h3>Federation</h3><p>This page explains why and how to manage multiple Kubernetes clusters using federation.</p><ul><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/federation/#why-federation"><strong>Why federation</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/federation/#caveats"><strong>Caveats</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/federation/#hybrid-cloud-capabilities"><strong>Hybrid cloud capabilities</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/federation/#setting-up-federation"><strong>Setting up federation</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/federation/#api-resources"><strong>API resources</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/federation/#cascading-deletion"><strong>Cascading deletion</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/federation/#scope-of-a-single-cluster"><strong>Scope of a single cluster</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/federation/#selecting-the-right-number-of-clusters"><strong>Selecting the right number of clusters</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/federation/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Why federation</h4><p>Federation makes it easy to manage multiple clusters. It does so by providing 2 major building blocks:</p><ul><li>Sync resources across clusters: Federation provides the ability to keep resources in multiple clusters in sync. For example, you can ensure that the same deployment exists in multiple clusters.</li><li>Cross cluster discovery: Federation provides the ability to auto-configure DNS servers and load balancers with backends from all clusters. For example, you can ensure that a global VIP or DNS record can be used to access backends from multiple clusters.</li></ul><p>Some other use cases that federation enables are:</p><ul><li>High Availability: By spreading load across clusters and auto configuring DNS servers and load balancers, federation minimises the impact of cluster failure.</li><li>Avoiding provider lock-in: By making it easier to migrate applications across clusters, federation prevents cluster provider lock-in.</li></ul><p>Federation is not helpful unless you have multiple clusters. Some of the reasons why you might want multiple clusters are:</p><ul><li>Low latency: Having clusters in multiple regions minimises latency by serving users from the cluster that is closest to them.</li><li>Fault isolation: It might be better to have multiple small clusters rather than a single large cluster for fault isolation (for example: multiple clusters in different availability zones of a cloud provider).</li><li>Scalability: There are scalability limits to a single kubernetes cluster (this should not be the case for most users. For more details: <a href="https://git.k8s.io/community/sig-scalability/goals.md">Kubernetes Scaling and Performance Goals</a>).</li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/federation/#hybrid-cloud-capabilities">Hybrid cloud</a>: You can have multiple clusters on different cloud providers or on-premises data centers.</li></ul><h5><strong>Caveats</strong></h5><p>While there are a lot of attractive use cases for federation, there are also some caveats:</p><ul><li>Increased network bandwidth and cost: The federation control plane watches all clusters to ensure that the current state is as expected. This can lead to significant network cost if the clusters are running in different regions on a cloud provider or on different cloud providers.</li><li>Reduced cross cluster isolation: A bug in the federation control plane can impact all clusters. This is mitigated by keeping the logic in federation control plane to a minimum. It mostly delegates to the control plane in kubernetes clusters whenever it can. The design and implementation also errs on the side of safety and avoiding multi-cluster outage.</li><li>Maturity: The federation project is relatively new and is not very mature. Not all resources are available and many are still alpha. <a href="https://github.com/kubernetes/federation/issues/88">Issue 88</a> enumerates known issues with the system that the team is busy solving.</li></ul><h5><strong>Hybrid cloud capabilities</strong></h5><p>Federations of Kubernetes Clusters can include clusters running in different cloud providers (e.g. Google Cloud, AWS), and on-premises (e.g. on OpenStack). <a href="https://kubernetes.io/docs/tasks/federation/set-up-cluster-federation-kubefed/">Kubefed</a> is the recommended way to deploy federated clusters.</p><p>Thereafter, your <a href="https://kubernetes.io/docs/concepts/cluster-administration/federation/#api-resources">API resources</a> can span different clusters and cloud providers.</p><h4>Setting up federation</h4><p>To be able to federate multiple clusters, you first need to set up a federation control plane. Follow the <a href="https://kubernetes.io/docs/tutorials/federation/set-up-cluster-federation-kubefed/">setup guide</a> to set up the federation control plane.</p><h4>API resources</h4><p>Once you have the control plane set up, you can start creating federation API resources. The following guides explain some of the resources in detail:</p><ul><li><a href="https://kubernetes.io/docs/tasks/administer-federation/cluster/">Cluster</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-federation/configmap/">ConfigMap</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-federation/daemonset/">DaemonSets</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-federation/deployment/">Deployment</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-federation/events/">Events</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-federation/hpa/">Hpa</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-federation/ingress/">Ingress</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-federation/job/">Jobs</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-federation/namespaces/">Namespaces</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-federation/replicaset/">ReplicaSets</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-federation/secret/">Secrets</a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/federation-service-discovery/">Services</a></li></ul><p>The <a href="https://kubernetes.io/docs/reference/generated/federation/">API reference docs</a> list all the resources supported by federation apiserver.</p><h4>Cascading deletion</h4><p>Kubernetes version 1.6 includes support for cascading deletion of federated resources. With cascading deletion, when you delete a resource from the federation control plane, you also delete the corresponding resources in all underlying clusters.</p><p>Cascading deletion is not enabled by default when using the REST API. To enable it, set the option <strong>DeleteOptions.orphanDependents=false</strong> when you delete a resource from the federation control plane using the REST API. Using <strong>kubectl delete</strong> enables cascading deletion by default. You can disable it by running <strong>kubectl delete --cascade=false</strong></p><p>Note: Kubernetes version 1.5 included cascading deletion support for a subset of federation resources.</p><h4>Scope of a single cluster</h4><p>On IaaS providers such as Google Compute Engine or Amazon Web Services, a VM exists in a <a href="https://cloud.google.com/compute/docs/zones">zone</a>or <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html">availability zone</a>. We suggest that all the VMs in a Kubernetes cluster should be in the same availability zone, because:</p><ul><li>compared to having a single global Kubernetes cluster, there are fewer single-points of failure.</li><li>compared to a cluster that spans availability zones, it is easier to reason about the availability properties of a single-zone cluster.</li><li>when the Kubernetes developers are designing the system (e.g. making assumptions about latency, bandwidth, or correlated failures) they are assuming all the machines are in a single data center, or otherwise closely connected.</li></ul><p>It is recommended to run fewer clusters with more VMs per availability zone; but it is possible to run multiple clusters per availability zones.</p><p>Reasons to prefer fewer clusters per availability zone are:</p><ul><li>improved bin packing of Pods in some cases with more nodes in one cluster (less resource fragmentation).</li><li>reduced operational overhead (though the advantage is diminished as ops tooling and processes mature).</li><li>reduced costs for per-cluster fixed resource costs, e.g. apiserver VMs (but small as a percentage of overall cluster cost for medium to large clusters).</li></ul><p>Reasons to have multiple clusters include:</p><ul><li>strict security policies requiring isolation of one class of work from another (but, see Partitioning Clusters below).</li><li>test clusters to canary new Kubernetes releases or other cluster software.</li></ul><h4>Selecting the right number of clusters</h4><p>The selection of the number of Kubernetes clusters may be a relatively static choice, only revisited occasionally. By contrast, the number of nodes in a cluster and the number of pods in a service may change frequently according to load and growth.</p><p>To pick the number of clusters, first, decide which regions you need to be in to have adequate latency to all your end users, for services that will run on Kubernetes (if you use a Content Distribution Network, the latency requirements for the CDN-hosted content need not be considered). Legal issues might influence this as well. For example, a company with a global customer base might decide to have clusters in US, EU, AP, and SA regions. Call the number of regions to be in <strong>R</strong>.</p><p>Second, decide how many clusters should be able to be unavailable at the same time, while still being available. Call the number that can be unavailable <strong>U</strong>. If you are not sure, then 1 is a fine choice.</p><p>If it is allowable for load-balancing to direct traffic to any region in the event of a cluster failure, then you need at least the larger of <strong>R</strong> or <strong>U + 1</strong> clusters. If it is not (e.g. you want to ensure low latency for all users in the event of a cluster failure), then you need to have <strong>R <!-- -->*<!-- --> (U + 1)</strong> clusters (<strong>U + 1</strong> in each of <strong>R</strong> regions). In any case, try to put each cluster in a different zone.</p><p>Finally, if any of your clusters would need more than the maximum recommended number of nodes for a Kubernetes cluster, then you may need even more clusters. Kubernetes v1.3 supports clusters up to 1000 nodes in size. Kubernetes v1.8 supports clusters up to 5000 nodes. See <a href="https://kubernetes.io/docs/admin/cluster-large/">Building Large Clusters</a> for more guidance.</p><h4>What&#x27;s next</h4><ul><li>Learn more about the <a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/multicluster/federation.md">Federation proposal</a>.</li><li>See this <a href="https://kubernetes.io/docs/tutorials/federation/set-up-cluster-federation-kubefed/">setup guide</a> for cluster federation.</li><li>See this <a href="https://www.youtube.com/watch?v=pq9lbkmxpS8">Kubecon2016 talk on federation</a></li><li>See this <a href="https://www.youtube.com/watch?v=kwOvOLnFYck">Kubecon2017 Europe update on federation</a></li></ul><h3>Proxies in Kubernetes</h3><p>This page explains proxies used with Kubernetes.</p><ul><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/proxies/#proxies"><strong>Proxies</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/proxies/#requesting-redirects"><strong>Requesting redirects</strong></a></li></ul><h4>Proxies</h4><p>There are several different proxies you may encounter when using Kubernetes:</p><ol><li>The <a href="https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#directly-accessing-the-rest-api">kubectl proxy</a>:<ul><li>runs on a user&#x27;s desktop or in a pod</li><li>proxies from a localhost address to the Kubernetes apiserver</li><li>client to proxy uses HTTP</li><li>proxy to apiserver uses HTTPS</li><li>locates apiserver</li><li>adds authentication headers</li></ul></li><li>The <a href="https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#discovering-builtin-services">apiserver proxy</a>:<ul><li>is a bastion built into the apiserver</li><li>connects a user outside of the cluster to cluster IPs which otherwise might not be reachable</li><li>runs in the apiserver processes</li><li>client to proxy uses HTTPS (or http if apiserver so configured)</li><li>proxy to target may use HTTP or HTTPS as chosen by proxy using available information</li><li>can be used to reach a Node, Pod, or Service</li><li>does load balancing when used to reach a Service</li></ul></li><li>The <a href="https://kubernetes.io/docs/concepts/services-networking/service/#ips-and-vips">kube proxy</a>:<ul><li>runs on each node</li><li>proxies UDP and TCP</li><li>does not understand HTTP</li><li>provides load balancing</li><li>is just used to reach services</li></ul></li><li>A Proxy/Load-balancer in front of apiserver(s):<ul><li>existence and implementation varies from cluster to cluster (e.g. nginx)</li><li>sits between all clients and one or more apiservers</li><li>acts as load balancer if there are several apiservers.</li></ul></li><li>Cloud Load Balancers on external services:<ul><li>are provided by some cloud providers (e.g. AWS ELB, Google Cloud Load Balancer)</li><li>are created automatically when the Kubernetes service has type <strong>LoadBalancer</strong></li><li>use UDP/TCP only</li><li>implementation varies by cloud provider.</li></ul></li></ol><p>Kubernetes users will typically not need to worry about anything other than the first two types. The cluster admin will typically ensure that the latter types are setup correctly.</p><h4>Requesting redirects</h4><p>Proxies have replaced redirect capabilities. Redirects have been deprecated.</p><h3>Controller manager metrics</h3><p>Controller manager metrics provide important insight into the performance and health of the controller manager.</p><ul><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/controller-metrics/#what-are-controller-manager-metrics"><strong>What are controller manager metrics</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/controller-metrics/#configuration"><strong>Configuration</strong></a></li></ul><h4>What are controller manager metrics</h4><p>Controller manager metrics provide important insight into the performance and health of the controller manager. These metrics include common Go language runtime metrics such as go_routine count and controller specific metrics such as etcd request latencies or Cloudprovider (AWS, GCE, OpenStack) API latencies that can be used to gauge the health of a cluster.</p><p>Starting from Kubernetes 1.7, detailed Cloudprovider metrics are available for storage operations for GCE, AWS, Vsphere and OpenStack. These metrics can be used to monitor health of persistent volume operations.</p><p>For example, for GCE these metrics are called:</p><p><strong>cloudprovider_gce_api_request_duration_seconds { request = &quot;instance_list&quot;}</strong></p><p><strong>cloudprovider_gce_api_request_duration_seconds { request = &quot;disk_insert&quot;}</strong></p><p><strong>cloudprovider_gce_api_request_duration_seconds { request = &quot;disk_delete&quot;}</strong></p><p><strong>cloudprovider_gce_api_request_duration_seconds { request = &quot;attach_disk&quot;}</strong></p><p><strong>cloudprovider_gce_api_request_duration_seconds { request = &quot;detach_disk&quot;}</strong></p><p><strong>cloudprovider_gce_api_request_duration_seconds { request = &quot;list_disk&quot;}</strong></p><h4>Configuration</h4><p>In a cluster, controller-manager metrics are available from <strong>http://localhost:10252/metrics</strong>from the host where the controller-manager is running.</p><p>The metrics are emitted in <a href="https://prometheus.io/docs/instrumenting/exposition_formats/">prometheus format</a> and are human readable.</p><p>In a production environment you may want to configure prometheus or some other metrics scraper to periodically gather these metrics and make them available in some kind of time series database.</p><h3>Device Plugins</h3><p><strong>FEATURE STATE:</strong> <strong>Kubernetes v1.10</strong> <a href="https://kubernetes.io/docs/concepts/cluster-administration/device-plugins/">beta</a></p><p>Starting in version 1.8, Kubernetes provides a <a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/resource-management/device-plugin.md">device plugin framework</a> for vendors to advertise their resources to the kubelet without changing Kubernetes core code. Instead of writing custom Kubernetes code, vendors can implement a device plugin that can be deployed manually or as a DaemonSet. The targeted devices include GPUs, High-performance NICs, FPGAs, InfiniBand, and other similar computing resources that may require vendor specific initialization and setup.</p><ul><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/device-plugins/#device-plugin-registration"><strong>Device plugin registration</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/device-plugins/#device-plugin-implementation"><strong>Device plugin implementation</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/device-plugins/#device-plugin-deployment"><strong>Device plugin deployment</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/device-plugins/#examples"><strong>Examples</strong></a></li></ul><h4>Device plugin registration</h4><p>The device plugins feature is gated by the <strong>DevicePlugins</strong> feature gate which is disabled by default before 1.10. When the device plugins feature is enabled, the kubelet exports a <strong>Registration</strong> gRPC service:</p><p><strong>service Registration {</strong></p><p><strong>rpc Register(RegisterRequest) returns (Empty) {}</strong></p><p><strong>}</strong></p><p>A device plugin can register itself with the kubelet through this gRPC service. During the registration, the device plugin needs to send:</p><ul><li>The name of its Unix socket.</li><li>The Device Plugin API version against which it was built.</li><li>The <strong>ResourceName</strong> it wants to advertise. Here <strong>ResourceName</strong> needs to follow the <a href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#extended-resources">extended resource naming scheme</a> as <strong>vendor-domain/resource</strong>. For example, an Nvidia GPU is advertised as <strong>nvidia.com/gpu</strong>.</li></ul><p>Following a successful registration, the device plugin sends the kubelet the list of devices it manages, and the kubelet is then in charge of advertising those resources to the API server as part of the kubelet node status update. For example, after a device plugin registers <strong>vendor-domain/foo</strong>with the kubelet and reports two healthy devices on a node, the node status is updated to advertise 2 <strong>vendor-domain/foo</strong>.</p><p>Then, users can request devices in a <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#container-v1-core">Container</a> specification as they request other types of resources, with the following limitations:</p><ul><li>Extended resources are only supported as integer resources and cannot be overcommitted.</li><li>Devices cannot be shared among Containers.</li></ul><p>Suppose a Kubernetes cluster is running a device plugin that advertises resource <strong>vendor-domain/resource</strong> on certain nodes, here is an example user pod requesting this resource:</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: demo-pod</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: demo-container-1</strong></p><p><strong>image: gcr.io/google_containers/pause:2.0</strong></p><p><strong>resources:</strong></p><p><strong>limits:</strong></p><p><strong>vendor-domain/resource: 2 <em># requesting 2 vendor-domain/resource</em></strong></p><h4>Device plugin implementation</h4><p>The general workflow of a device plugin includes the following steps:</p><ul><li>Initialization. During this phase, the device plugin performs vendor specific initialization and setup to make sure the devices are in a ready state.</li><li>The plugin starts a gRPC service, with a Unix socket under host path <strong>/var/lib/kubelet/device-plugins/</strong>, that implements the following interfaces:</li><li><strong>service DevicePlugin {</strong></li><li><strong>// ListAndWatch returns a stream of List of Devices</strong></li><li><strong>// Whenever a Device state change or a Device disappears, ListAndWatch</strong></li><li><strong>// returns the new list</strong></li><li><strong>rpc ListAndWatch(Empty) returns (stream ListAndWatchResponse) {}</strong></li><li><strong>// Allocate is called during container creation so that the Device</strong></li><li><strong>// Plugin can run device specific operations and instruct Kubelet</strong></li><li><strong>// of the steps to make the Device available in the container</strong></li><li><strong>rpc Allocate(AllocateRequest) returns (AllocateResponse) {}</strong></li><li><strong>}</strong></li><li>The plugin registers itself with the kubelet through the Unix socket at host path <strong>/var/lib/kubelet/device-plugins/kubelet.sock</strong>.</li><li>After successfully registering itself, the device plugin runs in serving mode, during which it keeps monitoring device health and reports back to the kubelet upon any device state changes. It is also responsible for serving <strong>Allocate</strong> gRPC requests. During <strong>Allocate</strong>, the device plugin may do device-specific preparation; for example, GPU cleanup or QRNG initialization. If the operations succeed, the device plugin returns an <strong>AllocateResponse</strong> that contains container runtime configurations for accessing the allocated devices. The kubelet passes this information to the container runtime.</li></ul><p>A device plugin is expected to detect kubelet restarts and re-register itself with the new kubelet instance. In the current implementation, a new kubelet instance deletes all the existing Unix sockets under <strong>/var/lib/kubelet/device-plugins</strong> when it starts. A device plugin can monitor the deletion of its Unix socket and re-register itself upon such an event.</p><h4>Device plugin deployment</h4><p>A device plugin can be deployed manually or as a DaemonSet. Being deployed as a DaemonSet has the benefit that Kubernetes can restart the device plugin if it fails. Otherwise, an extra mechanism is needed to recover from device plugin failures. The canonical directory <strong>/var/lib/kubelet/device-plugins</strong> requires privileged access, so a device plugin must run in a privileged security context. If a device plugin is running as a DaemonSet, <strong>/var/lib/kubelet/device-plugins</strong> must be mounted as a <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#volume-v1-core">Volume</a> in the plugin&#x27;s <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#podspec-v1-core">PodSpec</a>.</p><p>Kubernetes device plugin support is still in alpha. As development continues, its API version can change in incompatible ways. We recommend that device plugin developers do the following:</p><ul><li>Watch for changes in future releases.</li><li>Support multiple versions of the device plugin API for backward/forward compatibility.</li></ul><p>If you enable the DevicePlugins feature and run device plugins on nodes that need to be upgraded to a Kubernetes release with a newer device plugin API version, upgrade your device plugins to support both versions before upgrading these nodes to ensure the continuous functioning of the device allocations during the upgrade.</p><h4>Examples</h4><p>For examples of device plugin implementations, see:</p><ul><li>The official <a href="https://github.com/NVIDIA/k8s-device-plugin">NVIDIA GPU device plugin</a><ul><li>it requires using <a href="https://github.com/NVIDIA/nvidia-docker">nvidia-docker 2.0</a> which allows you to run GPU enabled docker containers</li></ul></li><li>The <a href="https://github.com/GoogleCloudPlatform/container-engine-accelerators/tree/master/cmd/nvidia_gpu">NVIDIA GPU device plugin for COS base OS</a>.</li><li>The <a href="https://github.com/hustcat/k8s-rdma-device-plugin">RDMA device plugin</a></li><li>The <a href="https://github.com/vikaschoudhary16/sfc-device-plugin">Solarflare device plugin</a></li></ul><h3>Resource Quotas</h3><p>When several users or teams share a cluster with a fixed number of nodes, there is a concern that one team could use more than its fair share of resources.</p><p>Resource quotas are a tool for administrators to address this concern.</p><p>A resource quota, defined by a <strong>ResourceQuota</strong> object, provides constraints that limit aggregate resource consumption per namespace. It can limit the quantity of objects that can be created in a namespace by type, as well as the total amount of compute resources that may be consumed by resources in that project.</p><p>Resource quotas work like this:</p><ul><li>Different teams work in different namespaces. Currently this is voluntary, but support for making this mandatory via ACLs is planned.</li><li>The administrator creates one or more <strong>ResourceQuotas</strong> for each namespace.</li><li>Users create resources (pods, services, etc.) in the namespace, and the quota system tracks usage to ensure it does not exceed hard resource limits defined in a <strong>ResourceQuota</strong>.</li><li>If creating or updating a resource violates a quota constraint, the request will fail with HTTP status code <strong>403 FORBIDDEN</strong> with a message explaining the constraint that would have been violated.</li><li>If quota is enabled in a namespace for compute resources like <strong>cpu</strong> and <strong>memory</strong>, users must specify requests or limits for those values; otherwise, the quota system may reject pod creation. Hint: Use the <strong>LimitRanger</strong> admission controller to force defaults for pods that make no compute resource requirements. See the <a href="https://kubernetes.io/docs/tasks/administer-cluster/quota-memory-cpu-namespace/">walkthrough</a> for an example of how to avoid this problem.</li></ul><p>Examples of policies that could be created using namespaces and quotas are:</p><ul><li>In a cluster with a capacity of 32 GiB RAM, and 16 cores, let team A use 20 GiB and 10 cores, let B use 10GiB and 4 cores, and hold 2GiB and 2 cores in reserve for future allocation.</li><li>Limit the &quot;testing&quot; namespace to using 1 core and 1GiB RAM. Let the &quot;production&quot; namespace use any amount.</li></ul><p>In the case where the total capacity of the cluster is less than the sum of the quotas of the namespaces, there may be contention for resources. This is handled on a first-come-first-served basis.</p><p>Neither contention nor changes to quota will affect already created resources.</p><h4>Enabling Resource Quota</h4><p>Resource Quota support is enabled by default for many Kubernetes distributions. It is enabled when the apiserver <strong>--enable-admission-plugins=</strong> flag has <strong>ResourceQuota</strong> as one of its arguments.</p><p>A resource quota is enforced in a particular namespace when there is a <strong>ResourceQuota</strong> in that namespace.</p><h4>Compute Resource Quota</h4><p>You can limit the total sum of <a href="https://kubernetes.io/docs/user-guide/compute-resources">compute resources</a> that can be requested in a given namespace.</p><p>The following resource types are supported:</p><p>  Resource Name         Description</p><hr/><p>  <strong>cpu</strong>               Across all pods in a non-terminal state, the sum of CPU requests cannot exceed this value.
<strong>limits.cpu</strong>        Across all pods in a non-terminal state, the sum of CPU limits cannot exceed this value.
<strong>limits.memory</strong>     Across all pods in a non-terminal state, the sum of memory limits cannot exceed this value.
<strong>memory</strong>            Across all pods in a non-terminal state, the sum of memory requests cannot exceed this value.
<strong>requests.cpu</strong>      Across all pods in a non-terminal state, the sum of CPU requests cannot exceed this value.
<strong>requests.memory</strong>   Across all pods in a non-terminal state, the sum of memory requests cannot exceed this value.</p><h4>Storage Resource Quota</h4><p>You can limit the total sum of <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">storage resources</a> that can be requested in a given namespace.</p><p>In addition, you can limit consumption of storage resources based on associated storage-class.</p><p>  Resource Name                                                                   Description</p><hr/><p>  <strong>requests.storage</strong>                                                            Across all persistent volume claims, the sum of storage requests cannot exceed this value.
<strong>persistentvolumeclaims</strong>                                                      The total number of <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">persistent volume claims</a> that can exist in the namespace.
<strong><code>&lt;storage-class-name&gt;</code>.storageclass.storage.k8s.io/requests.storage</strong>         Across all persistent volume claims associated with the storage-class-name, the sum of storage requests cannot exceed this value.
<strong><code>&lt;storage-class-name&gt;</code>.storageclass.storage.k8s.io/persistentvolumeclaims</strong>   Across all persistent volume claims associated with the storage-class-name, the total number of <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">persistent volume claims</a> that can exist in the namespace.</p><p>For example, if an operator wants to quota storage with <strong>gold</strong> storage class separate from <strong>bronze</strong>storage class, the operator can define a quota as follows:</p><ul><li><strong>gold.storageclass.storage.k8s.io/requests.storage: 500Gi</strong></li><li><strong>bronze.storageclass.storage.k8s.io/requests.storage: 100Gi</strong></li></ul><p>In release 1.8, quota support for local ephemeral storage is added as an alpha feature:</p><p>  Resource Name                    Description</p><hr/><p>  <strong>requests.ephemeral-storage</strong>   Across all pods in the namespace, the sum of local ephemeral storage requests cannot exceed this value.
<strong>limits.ephemeral-storage</strong>     Across all pods in the namespace, the sum of local ephemeral storage limits cannot exceed this value.</p><h4>Object Count Quota</h4><p>The 1.9 release added support to quota all standard namespaced resource types using the following syntax:</p><ul><li><strong>count/<code>&lt;resource&gt;.&lt;group&gt;</code></strong></li></ul><p>Here is an example set of resources users may want to put under object count quota:</p><ul><li><strong>count/persistentvolumeclaims</strong></li><li><strong>count/services</strong></li><li><strong>count/secrets</strong></li><li><strong>count/configmaps</strong></li><li><strong>count/replicationcontrollers</strong></li><li><strong>count/deployments.apps</strong></li><li><strong>count/replicasets.apps</strong></li><li><strong>count/statefulsets.apps</strong></li><li><strong>count/jobs.batch</strong></li><li><strong>count/cronjobs.batch</strong></li><li><strong>count/deployments.extensions</strong></li></ul><p>When using <strong>count/<!-- -->*</strong> resource quota, an object is charged against the quota if it exists in server storage. These types of quotas are useful to protect against exhaustion of storage resources. For example, you may want to quota the number of secrets in a server given their large size. Too many secrets in a cluster can actually prevent servers and controllers from starting! You may choose to quota jobs to protect against a poorly configured cronjob creating too many jobs in a namespace causing a denial of service.</p><p>Prior to the 1.9 release, it was possible to do generic object count quota on a limited set of resources. In addition, it is possible to further constrain quota for particular resources by their type.</p><p>The following types are supported:</p><p>  Resource Name                Description</p><hr/><p>  <strong>configmaps</strong>               The total number of config maps that can exist in the namespace.
<strong>persistentvolumeclaims</strong>   The total number of <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">persistent volume claims</a> that can exist in the namespace.
<strong>pods</strong>                     The total number of pods in a non-terminal state that can exist in the namespace. A pod is in a terminal state if <strong>status.phase in (Failed, Succeeded)</strong> is true.
<strong>replicationcontrollers</strong>   The total number of replication controllers that can exist in the namespace.
<strong>resourcequotas</strong>           The total number of <a href="https://kubernetes.io/docs/admin/admission-controllers/#resourcequota">resource quotas</a> that can exist in the namespace.
<strong>services</strong>                 The total number of services that can exist in the namespace.
<strong>services.loadbalancers</strong>   The total number of services of type load balancer that can exist in the namespace.
<strong>services.nodeports</strong>       The total number of services of type node port that can exist in the namespace.
<strong>secrets</strong>                  The total number of secrets that can exist in the namespace.</p><p>For example, <strong>pods</strong> quota counts and enforces a maximum on the number of <strong>pods</strong> created in a single namespace that are not terminal. You might want to set a <strong>pods</strong> quota on a namespace to avoid the case where a user creates many small pods and exhausts the cluster&#x27;s supply of Pod IPs.</p><h4>Quota Scopes</h4><p>Each quota can have an associated set of scopes. A quota will only measure usage for a resource if it matches the intersection of enumerated scopes.</p><p>When a scope is added to the quota, it limits the number of resources it supports to those that pertain to the scope. Resources specified on the quota outside of the allowed set results in a validation error.</p><p>  Scope                Description</p><hr/><p>  <strong>Terminating</strong>      Match pods where <strong>spec.activeDeadlineSeconds &gt;= 0</strong>
<strong>NotTerminating</strong>   Match pods where <strong>spec.activeDeadlineSeconds is nil</strong>
<strong>BestEffort</strong>       Match pods that have best effort quality of service.
<strong>NotBestEffort</strong>    Match pods that do not have best effort quality of service.</p><p>The <strong>BestEffort</strong> scope restricts a quota to tracking the following resource: <strong>pods</strong></p><p>The <strong>Terminating</strong>, <strong>NotTerminating</strong>, and <strong>NotBestEffort</strong> scopes restrict a quota to tracking the following resources:</p><ul><li><strong>cpu</strong></li><li><strong>limits.cpu</strong></li><li><strong>limits.memory</strong></li><li><strong>memory</strong></li><li><strong>pods</strong></li><li><strong>requests.cpu</strong></li><li><strong>requests.memory</strong></li></ul><h4>Requests vs Limits</h4><p>When allocating compute resources, each container may specify a request and a limit value for either CPU or memory. The quota can be configured to quota either value.</p><p>If the quota has a value specified for <strong>requests.cpu</strong> or <strong>requests.memory</strong>, then it requires that every incoming container makes an explicit request for those resources. If the quota has a value specified for <strong>limits.cpu</strong> or <strong>limits.memory</strong>, then it requires that every incoming container specifies an explicit limit for those resources.</p><h4>Viewing and Setting Quotas</h4><p>Kubectl supports creating, updating, and viewing quotas:</p><p><strong>kubectl create namespace myspace</strong></p><p><strong>cat <code>&lt;&lt;EOF &gt;</code> compute-resources.yaml</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>kind: ResourceQuota</strong></p><p><strong>metadata:</strong></p><p><strong>name: compute-resources</strong></p><p><strong>spec:</strong></p><p><strong>hard:</strong></p><p><strong>pods: &quot;4&quot;</strong></p><p><strong>requests.cpu: &quot;1&quot;</strong></p><p><strong>requests.memory: 1Gi</strong></p><p><strong>limits.cpu: &quot;2&quot;</strong></p><p><strong>limits.memory: 2Gi</strong></p><p><strong>EOF</strong></p><p><strong>kubectl create -f ./compute-resources.yaml --namespace=myspace</strong></p><p><strong>cat <code>&lt;&lt;EOF &gt;</code> object-counts.yaml</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>kind: ResourceQuota</strong></p><p><strong>metadata:</strong></p><p><strong>name: object-counts</strong></p><p><strong>spec:</strong></p><p><strong>hard:</strong></p><p><strong>configmaps: &quot;10&quot;</strong></p><p><strong>persistentvolumeclaims: &quot;4&quot;</strong></p><p><strong>replicationcontrollers: &quot;20&quot;</strong></p><p><strong>secrets: &quot;10&quot;</strong></p><p><strong>services: &quot;10&quot;</strong></p><p><strong>services.loadbalancers: &quot;2&quot;</strong></p><p><strong>EOF</strong></p><p><strong>kubectl create -f ./object-counts.yaml --namespace=myspace</strong></p><p><strong>kubectl get quota --namespace=myspace</strong></p><p><strong>NAME AGE</strong></p><p><strong>compute-resources 30s</strong></p><p><strong>object-counts 32s</strong></p><p><strong>kubectl describe quota compute-resources --namespace=myspace</strong></p><p><strong>Name: compute-resources</strong></p><p><strong>Namespace: myspace</strong></p><p><strong>Resource Used Hard</strong></p><p><strong>-------- ---- ----</strong></p><p><strong>limits.cpu 0 2</strong></p><p><strong>limits.memory 0 2Gi</strong></p><p><strong>pods 0 4</strong></p><p><strong>requests.cpu 0 1</strong></p><p><strong>requests.memory 0 1Gi</strong></p><p><strong>kubectl describe quota object-counts --namespace=myspace</strong></p><p><strong>Name: object-counts</strong></p><p><strong>Namespace: myspace</strong></p><p><strong>Resource Used Hard</strong></p><p><strong>-------- ---- ----</strong></p><p><strong>configmaps 0 10</strong></p><p><strong>persistentvolumeclaims 0 4</strong></p><p><strong>replicationcontrollers 0 20</strong></p><p><strong>secrets 1 10</strong></p><p><strong>services 0 10</strong></p><p><strong>services.loadbalancers 0 2</strong></p><p>Kubectl also supports object count quota for all standard namespaced resources using the syntax <strong>count/<code>&lt;resource&gt;.&lt;group&gt;</code></strong>:</p><p><strong>kubectl create namespace myspace</strong></p><p><strong>kubectl create quota test --hard=count/deployments.extensions=2,count/replicasets.extensions=4,count/pods=3,count/secrets=4 --namespace=myspace</strong></p><p><strong>kubectl run nginx --image=nginx --replicas=2 --namespace=myspace</strong></p><p><strong>kubectl describe quota --namespace=myspace</strong></p><p><strong>Name: test</strong></p><p><strong>Namespace: myspace</strong></p><p><strong>Resource Used Hard</strong></p><p><strong>-------- ---- ----</strong></p><p><strong>count/deployments.extensions 1 2</strong></p><p><strong>count/pods 2 3</strong></p><p><strong>count/replicasets.extensions 1 4</strong></p><p><strong>count/secrets 1 4</strong></p><h4>Quota and Cluster Capacity</h4><p><strong>ResourceQuotas</strong> are independent of the cluster capacity. They are expressed in absolute units. So, if you add nodes to your cluster, this does not automatically give each namespace the ability to consume more resources.</p><p>Sometimes more complex policies may be desired, such as:</p><ul><li>Proportionally divide total cluster resources among several teams.</li><li>Allow each tenant to grow resource usage as needed, but have a generous limit to prevent accidental resource exhaustion.</li><li>Detect demand from one namespace, add nodes, and increase quota.</li></ul><p>Such policies could be implemented using <strong>ResourceQuotas</strong> as building blocks, by writing a &quot;controller&quot; that watches the quota usage and adjusts the quota hard limits of each namespace according to other signals.</p><p>Note that resource quota divides up aggregate cluster resources, but it creates no restrictions around nodes: pods from several namespaces may run on the same node.</p><h4>Example</h4><p>See a <a href="https://kubernetes.io/docs/tasks/administer-cluster/quota-api-object/">detailed example for how to use resource quota</a>.</p><h4>Read More</h4><p>See <a href="https://git.k8s.io/community/contributors/design-proposals/resource-management/admission_control_resource_quota.md">ResourceQuota design doc</a> for more information.</p><h3>Pod Security Policies</h3><p><strong>FEATURE STATE:</strong> <strong>Kubernetes v1.10</strong> <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/">beta</a></p><p>Pod Security Policies enable fine-grained authorization of pod creation and updates.</p><ul><li><a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#what-is-a-pod-security-policy"><strong>What is a Pod Security Policy?</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#enabling-pod-security-policies"><strong>Enabling Pod Security Policies</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#authorizing-policies"><strong>Authorizing Policies</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#via-rbac"><strong>Via RBAC</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#troubleshooting"><strong>Troubleshooting</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#policy-order"><strong>Policy Order</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#example"><strong>Example</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#set-up"><strong>Set up</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#create-a-policy-and-a-pod"><strong>Create a policy and a pod</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#run-another-pod"><strong>Run another pod</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#clean-up"><strong>Clean up</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#example-policies"><strong>Example Policies</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#policy-reference"><strong>Policy Reference</strong></a><ul><li><a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#privileged"><strong>Privileged</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces"><strong>Host namespaces</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#volumes-and-file-systems"><strong>Volumes and file systems</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#flexvolume-drivers"><strong>FlexVolume drivers</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#users-and-groups"><strong>Users and groups</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#privilege-escalation"><strong>Privilege Escalation</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#capabilities"><strong>Capabilities</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#selinux"><strong>SELinux</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor"><strong>AppArmor</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp"><strong>Seccomp</strong></a></li><li><a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl"><strong>Sysctl</strong></a></li></ul></li></ul><h4>What is a Pod Security Policy?</h4><p>A Pod Security Policy is a cluster-level resource that controls security sensitive aspects of the pod specification. The <strong>PodSecurityPolicy</strong> objects define a set of conditions that a pod must run with in order to be accepted into the system, as well as defaults for the related fields. They allow an administrator to control the following:</p><p>  Control Aspect                                      Field Names</p><hr/><p>  Running of privileged containers                    <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#privileged"><strong>privileged</strong></a>
Usage of the root namespaces                        <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces"><strong>hostPID</strong>, <strong>hostIPC</strong></a>
Usage of host networking and ports                  <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces"><strong>hostNetwork</strong>, <strong>hostPorts</strong></a>
Usage of volume types                               <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#volumes-and-file-systems"><strong>volumes</strong></a>
Usage of the host filesystem                        <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#volumes-and-file-systems"><strong>allowedHostPaths</strong></a>
White list of FlexVolume drivers                    <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#flexvolume-drivers"><strong>allowedFlexVolumes</strong></a>
Allocating an FSGroup that owns the pod&#x27;s volumes   <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#volumes-and-file-systems"><strong>fsGroup</strong></a>
Requiring the use of a read only root file system   <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#volumes-and-file-systems"><strong>readOnlyRootFilesystem</strong></a>
The user and group IDs of the container             <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#users-and-groups"><strong>runAsUser</strong>, <strong>supplementalGroups</strong></a>
Restricting escalation to root privileges           <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#privilege-escalation"><strong>allowPrivilegeEscalation</strong>, <strong>defaultAllowPrivilegeEscalation</strong></a>
Linux capabilities                                  <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#capabilities"><strong>defaultAddCapabilities</strong>, <strong>requiredDropCapabilities</strong>, <strong>allowedCapabilities</strong></a>
The SELinux context of the container                <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#selinux"><strong>seLinux</strong></a>
The AppArmor profile used by containers             <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor">annotations</a>
The seccomp profile used by containers              <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp">annotations</a>
The sysctl profile used by containers               <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl">annotations</a></p><h4>Enabling Pod Security Policies</h4><p>Pod security policy control is implemented as an optional (but recommended) <a href="https://kubernetes.io/docs/admin/admission-controllers/#podsecuritypolicy">admission controller</a>. PodSecurityPolicies are enforced by <a href="https://kubernetes.io/docs/admin/admission-controllers/#how-do-i-turn-on-an-admission-control-plug-in">enabling the admission controller</a>, but doing so without authorizing any policies <strong>will prevent any pods from being created</strong> in the cluster.</p><p>Since the pod security policy API (<strong>policy/v1beta1/podsecuritypolicy</strong>) is enabled independently of the admission controller, for existing clusters it is recommended that policies are added and authorized before enabling the admission controller.</p><h4>Authorizing Policies</h4><p>When a PodSecurityPolicy resource is created, it does nothing. In order to use it, the requesting user or target pod&#x27;s <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/">service account</a> must be authorized to use the policy, by allowing the <strong>use</strong> verb on the policy.</p><p>Most Kubernetes pods are not created directly by users. Instead, they are typically created indirectly as part of a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployment</a>, <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/">ReplicaSet</a>, or other templated controller via the controller manager. Granting the controller access to the policy would grant access for all pods created by that the controller, so the preferred method for authorizing policies is to grant access to the pod&#x27;s service account (see <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#run-another-pod">example</a>).</p><h5><strong>Via RBAC</strong></h5><p><a href="https://kubernetes.io/docs/admin/authorization/rbac/">RBAC</a> is a standard Kubernetes authorization mode, and can easily be used to authorize use of policies.</p><p>First, a <strong>Role</strong> or <strong>ClusterRole</strong> needs to grant access to <strong>use</strong> the desired policies. The rules to grant access look like this:</p><p><strong>kind: ClusterRole</strong></p><p><strong>apiVersion: rbac.authorization.k8s.io/v1</strong></p><p><strong>metadata:</strong></p><p><strong>name: <code>&lt;role name&gt;</code></strong></p><p><strong>rules:</strong></p><p><strong>- apiGroups: <!-- -->[\&#x27;policy\&#x27;]</strong></p><p><strong>resources: <!-- -->[\&#x27;podsecuritypolicies\&#x27;]</strong></p><p><strong>verbs: <!-- -->[\&#x27;use\&#x27;]</strong></p><p><strong>resourceNames:</strong></p><p><strong>- <code>&lt;list of policies to authorize&gt;</code></strong></p><p>Then the <strong>(Cluster)Role</strong> is bound to the authorized user(s):</p><p><strong>kind: ClusterRoleBinding</strong></p><p><strong>apiVersion: rbac.authorization.k8s.io/v1</strong></p><p><strong>metadata:</strong></p><p><strong>name: <code>&lt;binding name&gt;</code></strong></p><p><strong>roleRef:</strong></p><p><strong>kind: ClusterRole</strong></p><p><strong>name: <code>&lt;role name&gt;</code></strong></p><p><strong>apiGroup: rbac.authorization.k8s.io</strong></p><p><strong>subjects:</strong></p><p><strong><em># Authorize specific service accounts:</em></strong></p><p><strong>- kind: ServiceAccount</strong></p><p><strong>name: <code>&lt;authorized service account name&gt;</code></strong></p><p><strong>namespace: <code>&lt;authorized pod namespace&gt;</code></strong></p><p><strong><em># Authorize specific users (not recommended):</em></strong></p><p><strong>- kind: User</strong></p><p><strong>apiGroup: rbac.authorization.k8s.io</strong></p><p><strong>name: <code>&lt;authorized user name&gt;</code></strong></p><p>If a <strong>RoleBinding</strong> (not a <strong>ClusterRoleBinding</strong>) is used, it will only grant usage for pods being run in the same namespace as the binding. This can be paired with system groups to grant access to all pods run in the namespace:</p><p><strong><em># Authorize all service accounts in a namespace:</em></strong></p><p><strong>- kind: Group</strong></p><p><strong>apiGroup: rbac.authorization.k8s.io</strong></p><p><strong>name: system:serviceaccounts</strong></p><p><strong><em># Or equivalently, all authenticated users in a namespace:</em></strong></p><p><strong>- kind: Group</strong></p><p><strong>apiGroup: rbac.authorization.k8s.io</strong></p><p><strong>name: system:authenticated</strong></p><p>For more examples of RBAC bindings, see <a href="https://kubernetes.io/docs/admin/authorization/rbac#role-binding-examples">Role Binding Examples</a>. For a complete example of authorizing a PodSecurityPolicy, see <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#example">below</a>.</p><h5><strong>Troubleshooting</strong></h5><ul><li>The <a href="https://kubernetes.io/docs/admin/kube-controller-manager/">Controller Manager</a> must be run against <a href="https://kubernetes.io/docs/admin/accessing-the-api/">the secured API port</a>, and must not have superuser permissions. Otherwise requests would bypass authentication and authorization modules, all PodSecurityPolicy objects would be allowed, and users would be able to create privileged containers. For more details on configuring Controller Manager authorization, see <a href="https://kubernetes.io/docs/admin/authorization/rbac/#controller-roles">Controller Roles</a>.</li></ul><h4>Policy Order</h4><p>In addition to restricting pod creation and update, pod security policies can also be used to provide default values for many of the fields that it controls. When multiple policies are available, the pod security policy controller selects policies in the following order:</p><ol><li>If any policies successfully validate the pod without altering it, they are used.</li><li>Otherwise, the first valid policy in alphabetical order is used.</li></ol><h4>Example</h4><p>This example assumes you have a running cluster with the PodSecurityPolicy admission controller enabled and you have cluster admin privileges.</p><h5><strong>Set up</strong></h5><p>Set up a namespace and a service account to act as for this example. We&#x27;ll use this service account to mock a non-admin user.</p><p><strong>$ kubectl create namespace psp-example</strong></p><p><strong>$ kubectl create serviceaccount -n psp-example fake-user</strong></p><p><strong>$ kubectl create rolebinding -n psp-example fake-editor --clusterrole=edit --serviceaccount=psp-example:fake-user</strong></p><p>To make it clear which user we&#x27;re acting as and save some typing, create 2 aliases:</p><p><strong>$ alias kubectl-admin=\&#x27;kubectl -n psp-example\&#x27;</strong></p><p><strong>$ alias kubectl-user=\&#x27;kubectl --as=system:serviceaccount:psp-example:fake-user -n psp-example\&#x27;</strong></p><h5><strong>Create a policy and a pod</strong></h5><p>Define the example PodSecurityPolicy object in a file. This is a policy that simply prevents the creation of privileged pods.</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>example-psp.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent">https://raw.githubusercontent</a>                 |
| .com/kubernetes/website/master/docs/concepts/policy/example-psp.yaml) |
+=======================================================================+
| <strong>apiVersion: policy/v1beta1</strong>                                        |
|                                                                       |
| <strong>kind: PodSecurityPolicy</strong>                                           |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: example</strong>                                                     |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>privileged: false <em># Don\&#x27;t allow privileged pods!</em></strong>              |
|                                                                       |
| <strong><em># The rest fills in some required fields.</em></strong>                      |
|                                                                       |
| <strong>seLinux:</strong>                                                          |
|                                                                       |
| <strong>rule: RunAsAny</strong>                                                    |
|                                                                       |
| <strong>supplementalGroups:</strong>                                               |
|                                                                       |
| <strong>rule: RunAsAny</strong>                                                    |
|                                                                       |
| <strong>runAsUser:</strong>                                                        |
|                                                                       |
| <strong>rule: RunAsAny</strong>                                                    |
|                                                                       |
| <strong>fsGroup:</strong>                                                          |
|                                                                       |
| <strong>rule: RunAsAny</strong>                                                    |
|                                                                       |
| <strong>volumes:</strong>                                                          |
|                                                                       |
| <strong>- \&#x27;<!-- -->*<!-- -->\&#x27;</strong>                                                          |
+-----------------------------------------------------------------------+</p><p>And create it with kubectl:</p><p><strong>$ kubectl-admin create -f example-psp.yaml</strong></p><p>Now, as the unprivileged user, try to create a simple pod:</p><p><strong>$ kubectl-user create -f- &lt;&lt;EOF</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: pause</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: pause</strong></p><p><strong>image: gcr.io/google-containers/pause</strong></p><p><strong>EOF</strong></p><p><strong>Error from server (Forbidden): error when creating &quot;STDIN&quot;: pods &quot;pause&quot; is forbidden: unable to validate against any pod security policy: []</strong></p><p><strong>What happened?</strong> Although the PodSecurityPolicy was created, neither the pod&#x27;s service account nor <strong>fake-user</strong> have permission to use the new policy:</p><p><strong>$ kubectl-user auth can-i use podsecuritypolicy/example</strong></p><p><strong>no</strong></p><p>Create the rolebinding to grant <strong>fake-user</strong> the <strong>use</strong> verb on the example policy:</p><p>Note: This is not the recommended way! See the <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#run-another-pod"><em>next section</em></a> for the preferred approach.</p><p><strong>$ kubectl-admin create role psp:unprivileged <!-- -->\</strong></p><p><strong>--verb=use <!-- -->\</strong></p><p><strong>--resource=podsecuritypolicy <!-- -->\</strong></p><p><strong>--resource-name=example</strong></p><p><strong>role &quot;psp:unprivileged&quot; created</strong></p><p><strong>$ kubectl-admin create rolebinding fake-user:psp:unprivileged <!-- -->\</strong></p><p><strong>--role=psp:unprivileged <!-- -->\</strong></p><p><strong>--serviceaccount=psp-example:fake-user</strong></p><p><strong>rolebinding &quot;fake-user:psp:unprivileged&quot; created</strong></p><p><strong>$ kubectl-user auth can-i use podsecuritypolicy/example</strong></p><p><strong>yes</strong></p><p>Now retry creating the pod:</p><p><strong>$ kubectl-user create -f- &lt;&lt;EOF</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: pause</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: pause</strong></p><p><strong>image: gcr.io/google-containers/pause</strong></p><p><strong>EOF</strong></p><p><strong>pod &quot;pause&quot; created</strong></p><p>It works as expected! But any attempts to create a privileged pod should still be denied:</p><p><strong>$ kubectl-user create -f- &lt;&lt;EOF</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: privileged</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: pause</strong></p><p><strong>image: gcr.io/google-containers/pause</strong></p><p><strong>securityContext:</strong></p><p><strong>privileged: true</strong></p><p><strong>EOF</strong></p><p><strong>Error from server (Forbidden): error when creating &quot;STDIN&quot;: pods &quot;privileged&quot; is forbidden: unable to validate against any pod security policy: [spec.containers<!-- -->[0]<!-- -->.securityContext.privileged: Invalid value: true: Privileged containers are not allowed]</strong></p><p>Delete the pod before moving on:</p><p><strong>$ kubectl-user delete pod pause</strong></p><h5><strong>Run another pod</strong></h5><p>Let&#x27;s try that again, slightly differently:</p><p><strong>$ kubectl-user run pause --image=gcr.io/google-containers/pause</strong></p><p><strong>deployment &quot;pause&quot; created</strong></p><p><strong>$ kubectl-user get pods</strong></p><p><strong>No resources found.</strong></p><p><strong>$ kubectl-user get events | head -n 2</strong></p><p><strong>LASTSEEN FIRSTSEEN COUNT NAME KIND SUBOBJECT TYPE REASON SOURCE MESSAGE</strong></p><p><strong>1m 2m 15 pause-7774d79b5 ReplicaSet Warning FailedCreate replicaset-controller Error creating: pods &quot;pause-7774d79b5-&quot; is forbidden: no providers available to validate pod request</strong></p><p><strong>What happened?</strong> We already bound the <strong>psp:unprivileged</strong> role for our <strong>fake-user</strong>, why are we getting the error <strong>Error creating: pods &quot;pause-7774d79b5-&quot; is forbidden: no providers available to validate pod request</strong>? The answer lies in the source - <strong>replicaset-controller</strong>. Fake-user successfully created the deployment (which successfully created a replicaset), but when the replicaset went to create the pod it was not authorized to use the example podsecuritypolicy.</p><p>In order to fix this, bind the <strong>psp:unprivileged</strong> role to the pod&#x27;s service account instead. In this case (since we didn&#x27;t specify it) the service account is <strong>default</strong>:</p><p><strong>$ kubectl-admin create rolebinding default:psp:unprivileged <!-- -->\</strong></p><p><strong>--role=psp:unprivileged <!-- -->\</strong></p><p><strong>--serviceaccount=psp-example:default</strong></p><p><strong>rolebinding &quot;default:psp:unprivileged&quot; created</strong></p><p>Now if you give it a minute to retry, the replicaset-controller should eventually succeed in creating the pod:</p><p><strong>$ kubectl-user get pods --watch</strong></p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>pause-7774d79b5-qrgcb 0/1 Pending 0 1s</strong></p><p><strong>pause-7774d79b5-qrgcb 0/1 Pending 0 1s</strong></p><p><strong>pause-7774d79b5-qrgcb 0/1 ContainerCreating 0 1s</strong></p><p><strong>pause-7774d79b5-qrgcb 1/1 Running 0 2s</strong></p><p><strong>\^C</strong></p><h5><strong>Clean up</strong></h5><p>Delete the namespace to clean up most of the example resources:</p><p><strong>$ kubectl-admin delete ns psp-example</strong></p><p><strong>namespace &quot;psp-example&quot; deleted</strong></p><p>Note that <strong>PodSecurityPolicy</strong> resources are not namespaced, and must be cleaned up separately:</p><p><strong>$ kubectl-admin delete psp example</strong></p><p><strong>podsecuritypolicy &quot;example&quot; deleted</strong></p><h5><strong>Example Policies</strong></h5><p>This is the least restricted policy you can create, equivalent to not using the pod security policy admission controller:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>privileged-psp.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.co">https://raw.githubusercontent.co</a>           |
| m/kubernetes/website/master/docs/concepts/policy/privileged-psp.yaml) |
+=======================================================================+
| <strong>apiVersion: policy/v1beta1</strong>                                        |
|                                                                       |
| <strong>kind: PodSecurityPolicy</strong>                                           |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: privileged</strong>                                                  |
|                                                                       |
| <strong>annotations:</strong>                                                      |
|                                                                       |
| <strong>seccomp.security.alpha.kubernetes.io/allowedProfileNames: \&#x27;<!-- -->*<!-- -->\&#x27;</strong>  |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>privileged: true</strong>                                                  |
|                                                                       |
| <strong>allowPrivilegeEscalation: true</strong>                                    |
|                                                                       |
| <strong>allowedCapabilities:</strong>                                              |
|                                                                       |
| <strong>- \&#x27;<!-- -->*<!-- -->\&#x27;</strong>                                                          |
|                                                                       |
| <strong>volumes:</strong>                                                          |
|                                                                       |
| <strong>- \&#x27;<!-- -->*<!-- -->\&#x27;</strong>                                                          |
|                                                                       |
| <strong>hostNetwork: true</strong>                                                 |
|                                                                       |
| <strong>hostPorts:</strong>                                                        |
|                                                                       |
| <strong>- min: 0</strong>                                                          |
|                                                                       |
| <strong>max: 65535</strong>                                                        |
|                                                                       |
| <strong>hostIPC: true</strong>                                                     |
|                                                                       |
| <strong>hostPID: true</strong>                                                     |
|                                                                       |
| <strong>runAsUser:</strong>                                                        |
|                                                                       |
| <strong>rule: \&#x27;RunAsAny\&#x27;</strong>                                                |
|                                                                       |
| <strong>seLinux:</strong>                                                          |
|                                                                       |
| <strong>rule: \&#x27;RunAsAny\&#x27;</strong>                                                |
|                                                                       |
| <strong>supplementalGroups:</strong>                                               |
|                                                                       |
| <strong>rule: \&#x27;RunAsAny\&#x27;</strong>                                                |
|                                                                       |
| <strong>fsGroup:</strong>                                                          |
|                                                                       |
| <strong>rule: \&#x27;RunAsAny\&#x27;</strong>                                                |
+-----------------------------------------------------------------------+</p><p>This is an example of a restrictive policy that requires users to run as an unprivileged user, blocks possible escalations to root, and requires use of several security mechanisms.</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>restricted-psp.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.co">https://raw.githubusercontent.co</a>           |
| m/kubernetes/website/master/docs/concepts/policy/restricted-psp.yaml) |
+=======================================================================+
| <strong>apiVersion: policy/v1beta1</strong>                                        |
|                                                                       |
| <strong>kind: PodSecurityPolicy</strong>                                           |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: restricted</strong>                                                  |
|                                                                       |
| <strong>annotations:</strong>                                                      |
|                                                                       |
| <strong>seccomp.security.alpha.kubernetes.io/allowedProfileNames:           |
| \&#x27;docker/default\&#x27;</strong>                                                  |
|                                                                       |
| <strong>apparmor.security.beta.kubernetes.io/allowedProfileNames:           |
| \&#x27;runtime/default\&#x27;</strong>                                                 |
|                                                                       |
| <strong>seccomp.security.alpha.kubernetes.io/defaultProfileName:            |
| \&#x27;docker/default\&#x27;</strong>                                                  |
|                                                                       |
| <strong>apparmor.security.beta.kubernetes.io/defaultProfileName:            |
| \&#x27;runtime/default\&#x27;</strong>                                                 |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>privileged: false</strong>                                                 |
|                                                                       |
| <strong><em># Required to prevent escalations to root.</em></strong>                     |
|                                                                       |
| <strong>allowPrivilegeEscalation: false</strong>                                   |
|                                                                       |
| <strong><em># This is redundant with non-root + disallow privilege            |
| escalation,</em></strong>                                                        |
|                                                                       |
| <strong><em># but we can provide it for defense in depth.</em></strong>                  |
|                                                                       |
| <strong>requiredDropCapabilities:</strong>                                         |
|                                                                       |
| <strong>- ALL</strong>                                                             |
|                                                                       |
| <strong><em># Allow core volume types.</em></strong>                                     |
|                                                                       |
| <strong>volumes:</strong>                                                          |
|                                                                       |
| <strong>- \&#x27;configMap\&#x27;</strong>                                                   |
|                                                                       |
| <strong>- \&#x27;emptyDir\&#x27;</strong>                                                    |
|                                                                       |
| <strong>- \&#x27;projected\&#x27;</strong>                                                   |
|                                                                       |
| <strong>- \&#x27;secret\&#x27;</strong>                                                      |
|                                                                       |
| <strong>- \&#x27;downwardAPI\&#x27;</strong>                                                 |
|                                                                       |
| <strong><em># Assume that persistentVolumes set up by the cluster admin are   |
| safe to use.</em></strong>                                                       |
|                                                                       |
| <strong>- \&#x27;persistentVolumeClaim\&#x27;</strong>                                       |
|                                                                       |
| <strong>hostNetwork: false</strong>                                                |
|                                                                       |
| <strong>hostIPC: false</strong>                                                    |
|                                                                       |
| <strong>hostPID: false</strong>                                                    |
|                                                                       |
| <strong>runAsUser:</strong>                                                        |
|                                                                       |
| <strong><em># Require the container to run without root privileges.</em></strong>        |
|                                                                       |
| <strong>rule: \&#x27;MustRunAsNonRoot\&#x27;</strong>                                        |
|                                                                       |
| <strong>seLinux:</strong>                                                          |
|                                                                       |
| <strong><em># This policy assumes the nodes are using AppArmor rather than    |
| SELinux.</em></strong>                                                           |
|                                                                       |
| <strong>rule: \&#x27;RunAsAny\&#x27;</strong>                                                |
|                                                                       |
| <strong>supplementalGroups:</strong>                                               |
|                                                                       |
| <strong>rule: \&#x27;MustRunAs\&#x27;</strong>                                               |
|                                                                       |
| <strong>ranges:</strong>                                                           |
|                                                                       |
| <strong><em># Forbid adding the root group.</em></strong>                                |
|                                                                       |
| <strong>- min: 1</strong>                                                          |
|                                                                       |
| <strong>max: 65535</strong>                                                        |
|                                                                       |
| <strong>fsGroup:</strong>                                                          |
|                                                                       |
| <strong>rule: \&#x27;MustRunAs\&#x27;</strong>                                               |
|                                                                       |
| <strong>ranges:</strong>                                                           |
|                                                                       |
| <strong><em># Forbid adding the root group.</em></strong>                                |
|                                                                       |
| <strong>- min: 1</strong>                                                          |
|                                                                       |
| <strong>max: 65535</strong>                                                        |
|                                                                       |
| <strong>readOnlyRootFilesystem: false</strong>                                     |
+-----------------------------------------------------------------------+</p><h4>Policy Reference</h4><h5><strong>Privileged</strong></h5><p><strong>Privileged</strong> - determines if any container in a pod can enable privileged mode. By default a container is not allowed to access any devices on the host, but a &quot;privileged&quot; container is given access to all devices on the host. This allows the container nearly all the same access as processes running on the host. This is useful for containers that want to use linux capabilities like manipulating the network stack and accessing devices.</p><h5><strong>Host namespaces</strong></h5><p><strong>HostPID</strong> - Controls whether the pod containers can share the host process ID namespace. Note that when paired with ptrace this can be used to escalate privileges outside of the container (ptrace is forbidden by default).</p><p><strong>HostIPC</strong> - Controls whether the pod containers can share the host IPC namespace.</p><p><strong>HostNetwork</strong> - Controls whether the pod may use the node network namespace. Doing so gives the pod access to the loopback device, services listening on localhost, and could be used to snoop on network activity of other pods on the same node.</p><p><strong>HostPorts</strong> - Provides a whitelist of ranges of allowable ports in the host network namespace. Defined as a list of <strong>HostPortRange</strong>, with <strong>min</strong>(inclusive) and <strong>max</strong>(inclusive). Defaults to no allowed host ports.</p><p><strong>AllowedHostPaths</strong> - See <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#volumes-and-file-systems">Volumes and file systems</a>.</p><h5><strong>Volumes and file systems</strong></h5><p><strong>Volumes</strong> - Provides a whitelist of allowed volume types. The allowable values correspond to the volume sources that are defined when creating a volume. For the complete list of volume types, see <a href="https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes">Types of Volumes</a>. Additionally, <strong>*</strong> may be used to allow all volume types.</p><p>The <strong>recommended minimum set</strong> of allowed volumes for new PSPs are:</p><ul><li>configMap</li><li>downwardAPI</li><li>emptyDir</li><li>persistentVolumeClaim</li><li>secret</li><li>projected</li></ul><p><strong>FSGroup</strong> - Controls the supplemental group applied to some volumes.</p><ul><li>MustRunAs - Requires at least one <strong>range</strong> to be specified. Uses the minimum value of the first range as the default. Validates against all ranges.</li><li>RunAsAny - No default provided. Allows any <strong>fsGroup</strong> ID to be specified.</li></ul><p><strong>AllowedHostPaths</strong> - This specifies a whitelist of host paths that are allowed to be used by hostPath volumes. An empty list means there is no restriction on host paths used. This is defined as a list of objects with a single <strong>pathPrefix</strong> field, which allows hostPath volumes to mount a path that begins with an allowed prefix. For example:</p><p><strong>allowedHostPaths:</strong></p><p><strong><em># This allows &quot;/foo&quot;, &quot;/foo/&quot;, &quot;/foo/bar&quot; etc., but</em></strong></p><p><strong><em># disallows &quot;/fool&quot;, &quot;/etc/foo&quot; etc.</em></strong></p><p><strong><em># &quot;/foo/../&quot; is never valid.</em></strong></p><p><strong>- pathPrefix: &quot;/foo&quot;</strong></p><p>Note: There are many ways a container with unrestricted access to the host filesystem can escalate privileges, including reading data from other containers, and abusing the credentials of system services, such as Kubelet.</p><p><strong>ReadOnlyRootFilesystem</strong> - Requires that containers must run with a read-only root filesystem (i.e. no writeable layer).</p><h5><strong>FlexVolume drivers</strong></h5><p>This specifies a whiltelist of flex volume drivers that are allowed to be used by flexVolume. An empty list or nil means there is no restriction on the drivers. Please make sure <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#volumes-and-file-systems"><strong>volumes</strong></a> field contains the <strong>flexVolume</strong> volume type, no FlexVolume driver is allowed otherwise.</p><p>For example:</p><p><strong>apiVersion: extensions/v1beta1</strong></p><p><strong>kind: PodSecurityPolicy</strong></p><p><strong>metadata:</strong></p><p><strong>name: allow-flex-volumes</strong></p><p><strong>spec:</strong></p><p><strong><em># <!-- -->.<!-- -->.. other spec fields</em></strong></p><p><strong>volumes:</strong></p><p><strong>- flexVolume</strong></p><p><strong>allowedFlexVolumes:</strong></p><p><strong>- driver: example/lvm</strong></p><p><strong>- driver: example/cifs</strong></p><h5><strong>Users and groups</strong></h5><p><strong>RunAsUser</strong> - Controls the what user ID containers run as.</p><ul><li>MustRunAs - Requires at least one <strong>range</strong> to be specified. Uses the minimum value of the first range as the default. Validates against all ranges.</li><li>MustRunAsNonRoot - Requires that the pod be submitted with a non-zero <strong>runAsUser</strong> or have the <strong>USER</strong> directive defined (using a numeric UID) in the image. No default provided. Setting <strong>allowPrivilegeEscalation=false</strong> is strongly recommended with this strategy.</li><li>RunAsAny - No default provided. Allows any <strong>runAsUser</strong> to be specified.</li></ul><p><strong>SupplementalGroups</strong> - Controls which group IDs containers add.</p><ul><li>MustRunAs - Requires at least one <strong>range</strong> to be specified. Uses the minimum value of the first range as the default. Validates against all ranges.</li><li>RunAsAny - No default provided. Allows any <strong>supplementalGroups</strong> to be specified.</li></ul><h5><strong>Privilege Escalation</strong></h5><p>These options control the <strong>allowPrivilegeEscalation</strong> container option. This bool directly controls whether the <a href="https://www.kernel.org/doc/Documentation/prctl/no_new_privs.txt"><strong>no_new_privs</strong></a> flag gets set on the container process. This flag will prevent <strong>setuid</strong>binaries from changing the effective user ID, and prevent files from enabling extra capabilities (e.g. it will prevent the use of the <strong>ping</strong> tool). This behavior is required to effectively enforce <strong>MustRunAsNonRoot</strong>.</p><p>It defaults to <strong>nil</strong>. The default behavior of <strong>nil</strong> allows privilege escalation so as to not break setuid binaries. Setting it to <strong>false</strong> ensures that no child process of a container can gain more privileges than its parent.</p><p><strong>AllowPrivilegeEscalation</strong> - Gates whether or not a user is allowed to set the security context of a container to <strong>allowPrivilegeEscalation=true</strong>. This defaults to allowed. When set to false, the container&#x27;s <strong>allowPrivilegeEscalation</strong> is defaulted to false.</p><p><strong>DefaultAllowPrivilegeEscalation</strong> - Sets the default for the <strong>allowPrivilegeEscalation</strong> option. The default behavior without this is to allow privilege escalation so as to not break setuid binaries. If that behavior is not desired, this field can be used to default to disallow, while still permitting pods to request <strong>allowPrivilegeEscalation</strong> explicitly.</p><h5><strong>Capabilities</strong></h5><p>Linux capabilities provide a finer grained breakdown of the privileges traditionally associated with the superuser. Some of these capabilities can be used to escalate privileges or for container breakout, and may be restricted by the PodSecurityPolicy. For more details on Linux capabilities, see<a href="http://man7.org/linux/man-pages/man7/capabilities.7.html">capabilities(7)</a>.</p><p>The following fields take a list of capabilities, specified as the capability name in ALL_CAPS without the <strong>CAP<!-- -->_</strong> prefix.</p><p><strong>AllowedCapabilities</strong> - Provides a whitelist of capabilities that may be added to a container. The default set of capabilities are implicitly allowed. The empty set means that no additional capabilities may be added beyond the default set. <strong>*</strong> can be used to allow all capabilities.</p><p><strong>RequiredDropCapabilities</strong> - The capabilities which must be dropped from containers. These capabilities are removed from the default set, and must not be added. Capabilities listed in <strong>RequiredDropCapabilities</strong> must not be included in <strong>AllowedCapabilities</strong> or <strong>DefaultAddCapabilities</strong>.</p><p><strong>DefaultAddCapabilities</strong> - The capabilities which are added to containers by default, in addition to the runtime defaults. See the <a href="https://docs.docker.com/engine/reference/run/#runtime-privilege-and-linux-capabilities">Docker documentation</a> for the default list of capabilities when using the Docker runtime.</p><h5><strong>SELinux</strong></h5><ul><li>MustRunAs - Requires <strong>seLinuxOptions</strong> to be configured. Uses <strong>seLinuxOptions</strong> as the default. Validates against <strong>seLinuxOptions</strong>.</li><li>RunAsAny - No default provided. Allows any <strong>seLinuxOptions</strong> to be specified.</li></ul><h5><strong>AppArmor</strong></h5><p>Controlled via annotations on the PodSecurityPolicy. Refer to the <a href="https://kubernetes.io/docs/tutorials/clusters/apparmor/#podsecuritypolicy-annotations">AppArmor documentation</a>.</p><h5><strong>Seccomp</strong></h5><p>The use of seccomp profiles in pods can be controlled via annotations on the PodSecurityPolicy. Seccomp is an alpha feature in Kubernetes.</p><p><strong>seccomp.security.alpha.kubernetes.io/defaultProfileName</strong> - Annotation that specifies the default seccomp profile to apply to containers. Possible values are:</p><ul><li><strong>unconfined</strong> - Seccomp is not applied to the container processes (this is the default in Kubernetes), if no alternative is provided.</li><li><strong>docker/default</strong> - The Docker default seccomp profile is used.</li><li><code>localhost/&lt;path&gt;</code> - Specify a profile as a file on the node located at <code>&lt;seccomp_root&gt;/&lt;path&gt;</code>, where <code>&lt;seccomp_root&gt;</code> is defined via the <code>--seccomp-profile-root</code> flag on the Kubelet.</li></ul><p><strong>seccomp.security.alpha.kubernetes.io/allowedProfileNames</strong> - Annotation that specifies which values are allowed for the pod seccomp annotations. Specified as a comma-delimited list of allowed values. Possible values are those listed above, plus <strong>*</strong> to allow all profiles. Absence of this annotation means that the default cannot be changed.</p><h5><strong>Sysctl</strong></h5><p>Controlled via annotations on the PodSecurityPolicy. Refer to the <a href="https://kubernetes.io/docs/concepts/cluster-administration/sysctl-cluster/#podsecuritypolicy-annotations">Sysctl documentation</a>.</p><h1>Tutorials</h1><h2>Kubernetes Basics</h2><p>These interactive tutorials let you manage a simple cluster and its <a href="https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/#why-containers">containerized applications</a> for yourself.</p><p>Using the interactive tutorials, you can learn to:</p><ul><li>Deploy a containerized application on a cluster</li><li>Scale the deployment</li><li>Update the containerized application with a new software version</li><li>Debug the containerized application</li></ul><p>The tutorials use Katacoda to run a virtual terminal in your web browser that runs Minikube, a small-scale local deployment of Kubernetes that can run anywhere. There\&#x27;s no need to install any software or configure anything; each interactive tutorial runs directly out of your web browser itself.</p><h3>What can Kubernetes do for you?</h3><p>With modern web services, users expect applications to be available 24/7, and developers expect to deploy new versions of those applications several times a day. Containerization helps package software to serve these goals, enabling applications to be released and updated in an easy and fast way without downtime. Kubernetes helps you make sure those containerized applications run where and when you want, and helps them find the resources and tools they need to work. <a href="https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/">Kubernetes</a> is a production-ready, open source platform designed with Google\&#x27;s accumulated experience in container orchestration, combined with best-of-breed ideas from the community.</p><h2>Create a Cluster</h2><h3>Using Minikube to Create a Cluster</h3><h4>Objectives</h4><ul><li>Learn what a Kubernetes cluster is.</li><li>Learn what Minikube is.</li><li>Start a Kubernetes cluster using an online terminal.</li></ul><h4>Kubernetes Clusters</h4><p><strong>Kubernetes coordinates a highly available cluster of computers that are connected to work as a single unit.</strong> The abstractions in Kubernetes allow you to deploy containerized applications to a cluster without tying them specifically to individual machines. To make use of this new model of deployment, applications need to be packaged in a way that decouples them from individual hosts: they need to be containerized. Containerized applications are more flexible and available than in past deployment models, where applications were installed directly onto specific machines as packages deeply integrated into the host. <strong>Kubernetes automates the distribution and scheduling of application containers across a cluster in a more efficient way.</strong> Kubernetes is an <a href="https://github.com/kubernetes/kubernetes">open-source</a> platform and is production-ready.</p><p>A Kubernetes cluster consists of two types of resources:</p><ul><li>The <strong>Master</strong> coordinates the cluster</li><li><strong>Nodes</strong> are the workers that run applications</li></ul><h5>Summary</h5><ul><li>Kubernetes cluster</li><li>Minikube</li></ul><h4>Cluster Diagram</h4><p><strong>The Master is responsible for managing the cluster.</strong> The master coordinates all activities in your cluster, such as scheduling applications, maintaining applications\&#x27; desired state, scaling applications, and rolling out new updates.</p><p><strong>A node is a VM or a physical computer that serves as a worker machine in a Kubernetes cluster.</strong> Each node has a Kubelet, which is an agent for managing the node and communicating with the Kubernetes master. The node should also have tools for handling container operations, such as <a href="https://www.docker.com/">Docker</a> or <a href="https://coreos.com/rkt/">rkt</a>. A Kubernetes cluster that handles production traffic should have a minimum of three nodes.</p><p>When you deploy applications on Kubernetes, you tell the master to start the application containers. The master schedules the containers to run on the cluster\&#x27;s nodes. <strong>The nodes communicate with the master using the Kubernetes API</strong>, which the master exposes. End users can also use the Kubernetes API directly to interact with the cluster.</p><p>A Kubernetes cluster can be deployed on either physical or virtual machines. To get started with Kubernetes development, you can use <a href="https://github.com/kubernetes/minikube">Minikube</a>. Minikube is a lightweight Kubernetes implementation that creates a VM on your local machine and deploys a simple cluster containing only one node. Minikube is available for Linux, macOS, and Windows systems. The Minikube CLI provides basic bootstrapping operations for working with your cluster, including start, stop, status, and delete. For this tutorial, however, you\&#x27;ll use a provided online terminal with Minikube pre-installed.</p><p>Now that you know what Kubernetes is, let\&#x27;s go to the online tutorial and start our first cluster!</p><h2>Deploy an App</h2><h3>Using kubectl to Create a Deployment</h3><h4>Objectives</h4><ul><li>Learn about application Deployments.</li><li>Deploy your first app on Kubernetes with kubectl.</li></ul><h4>Kubernetes Deployments</h4><p>Once you have a running Kubernetes cluster, you can deploy your containerized applications on top of it. To do so, you create a Kubernetes <strong>Deployment</strong> configuration. The Deployment instructs Kubernetes how to create and update instances of your application. Once you\&#x27;ve created a Deployment, the Kubernetes master schedules mentioned application instances onto individual Nodes in the cluster.</p><p>Once the application instances are created, a Kubernetes Deployment Controller continuously monitors those instances. If the Node hosting an instance goes down or is deleted, the Deployment controller replaces it. <strong>This provides a self-healing mechanism to address machine failure or maintenance.</strong></p><p>In a pre-orchestration world, installation scripts would often be used to start applications, but they did not allow recovery from machine failure. By both creating your application instances and keeping them running across Nodes, Kubernetes Deployments provide a fundamentally different approach to application management.</p><h4>Summary:</h4><ul><li>Deployments</li><li>Kubectl</li></ul><p>A Deployment is responsible for creating and updating instances of your application</p><h4>Deploying your first app on Kubernetes</h4><p>You can create and manage a Deployment by using the Kubernetes command line interface, <strong>Kubectl</strong>. Kubectl uses the Kubernetes API to interact with the cluster. In this module, you\&#x27;ll learn the most common Kubectl commands needed to create Deployments that run your applications on a Kubernetes cluster.</p><p>When you create a Deployment, you\&#x27;ll need to specify the container image for your application and the number of replicas that you want to run. You can change that information later by updating your Deployment; Modules <a href="https://kubernetes.io/docs/tutorials/kubernetes-basics/scale-intro/">5</a> and <a href="https://kubernetes.io/docs/tutorials/kubernetes-basics/update-intro/">6</a> of the bootcamp discuss how you can scale and update your Deployments.</p><p>Applications need to be packaged into one of the supported container formats in order to be deployed on Kubernetes</p><p>For our first Deployment, we\&#x27;ll use a <a href="https://nodejs.org/">Node.js</a> application packaged in a Docker container. The source code and the Dockerfile are available in the <a href="https://github.com/kubernetes/kubernetes-bootcamp">GitHub repository</a> for the Kubernetes Bootcamp.</p><p>Now that you know what Deployments are, let\&#x27;s go to the online tutorial and deploy our first app!</p><h2>Explore your App</h2><h3>Viewing Pods and Nodes</h3><h4>Objectives</h4><ul><li>Learn about Kubernetes Pods.</li><li>Learn about Kubernetes Nodes.</li><li>Troubleshoot deployed applications.</li></ul><h4>Kubernetes Pods</h4><p>When you created a Deployment in Module <a href="https://kubernetes.io/docs/tutorials/kubernetes-basics/deploy-intro/">2</a>, Kubernetes created a <strong>Pod</strong> to host your application instance. A Pod is a Kubernetes abstraction that represents a group of one or more application containers (such as Docker or rkt), and some shared resources for those containers. Those resources include:</p><ul><li>Shared storage, as Volumes</li><li>Networking, as a unique cluster IP address</li><li>Information about how to run each container, such as the container image version or specific ports to use</li></ul><p>A Pod models an application-specific &quot;logical host&quot; and can contain different application containers which are relatively tightly coupled. For example, a Pod might include both the container with your Node.js app as well as a different container that feeds the data to be published by the Node.js webserver. The containers in a Pod share an IP Address and port space, are always co-located and co-scheduled, and run in a shared context on the same Node.</p><p>Pods are the atomic unit on the Kubernetes platform. When we create a Deployment on Kubernetes, that Deployment creates Pods with containers inside them (as opposed to creating containers directly). Each Pod is tied to the Node where it is scheduled and remains there until termination (according to restart policy) or deletion. In case of a Node failure, identical Pods are scheduled on other available Nodes in the cluster.</p><h4>Summary</h4><ul><li>Pods</li><li>Nodes</li><li>Kubectl main commands</li></ul><p>A Pod is a group of one or more application containers (such as Docker or rkt) and includes shared storage (volumes), IP address and information about how to run them.</p><h4>Pods overview</h4><h4>Nodes</h4><p>A Pod always runs on a <strong>Node</strong>. A Node is a worker machine in Kubernetes and may be either a virtual or a physical machine, depending on the cluster. Each Node is managed by the Master. A Node can have multiple pods, and the Kubernetes master automatically handles scheduling the pods across the Nodes in the cluster. The Master\&#x27;s automatic scheduling considers the available resources on each Node.</p><p>Every Kubernetes Node runs at least:</p><ul><li>Kubelet, a process responsible for communication between the Kubernetes Master and the Node; it manages the Pods and the containers running on a machine.</li><li>A container runtime (like Docker, rkt) responsible for pulling the container image from a registry, unpacking the container, and running the application.</li></ul><p>Containers should only be scheduled together in a single Pod if they are tightly coupled and need to share resources such as disk.</p><h4>Node overview</h4><h4>Troubleshooting with kubectl</h4><p>In Module <a href="https://kubernetes.io/docs/tutorials/kubernetes-basics/deploy-intro/">2</a>, you used Kubectl command-line interface. You\&#x27;ll continue to use it in Module 3 to get information about deployed applications and their environments. The most common operations can be done with the following kubectl commands:</p><ul><li><strong>kubectl get</strong> - list resources</li><li><strong>kubectl describe</strong> - show detailed information about a resource</li><li><strong>kubectl logs</strong> - print the logs from a container in a pod</li><li><strong>kubectl exec</strong> - execute a command on a container in a pod</li></ul><p>You can use these commands to see when applications were deployed, what their current statuses are, where they are running and what their configurations are.</p><p>Now that we know more about our cluster components and the command line, let\&#x27;s explore our application.</p><h2>Expose Your App</h2><h3>Using a Service to Expose Your App</h3><h4>Objectives</h4><ul><li>Learn about a Service in Kubernetes</li><li>Understand how labels and LabelSelector objects relate to a Service</li><li>Expose an application outside a Kubernetes cluster using a Service</li></ul><h4>Overview of Kubernetes Services</h4><p>Kubernetes <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/">Pods</a> are mortal. Pods in fact have a <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/">lifecycle</a>. When a worker node dies, the Pods running on the Node are also lost. A <a href="https://kubernetes.io/docs/user-guide/replication-controller/#what-is-a-replicationcontroller">ReplicationController</a> might then dynamically drive the cluster back to desired state via creation of new Pods to keep your application running. As another example, consider an image-processing backend with 3 replicas. Those replicas are fungible; the front-end system should not care about backend replicas or even if a Pod is lost and recreated. That said, each Pod in a Kubernetes cluster has a unique IP address, even Pods on the same Node, so there needs to be a way of automatically reconciling changes among Pods so that your applications continue to function.</p><p>A Service in Kubernetes is an abstraction which defines a logical set of Pods and a policy by which to access them. Services enable a loose coupling between dependent Pods. A Service is defined using YAML <a href="https://kubernetes.io/docs/concepts/configuration/overview/#general-config-tips">(preferred)</a> or JSON, like all Kubernetes objects. The set of Pods targeted by a Service is usually determined by a <em>LabelSelector</em> (see below for why you might want a Service without including <strong>selector</strong> in the spec).</p><p>Although each Pod has a unique IP address, those IPs are not exposed outside the cluster without a Service. Services allow your applications to receive traffic. Services can be exposed in different ways by specifying a <strong>type</strong> in the ServiceSpec:</p><ul><li><em>ClusterIP</em> (default) - Exposes the Service on an internal IP in the cluster. This type makes the Service only reachable from within the cluster.</li><li><em>NodePort</em> - Exposes the Service on the same port of each selected Node in the cluster using NAT. Makes a Service accessible from outside the cluster using <strong><code>&lt;NodeIP&gt;:&lt;NodePort&gt;</code></strong>. Superset of ClusterIP.</li><li><em>LoadBalancer</em> - Creates an external load balancer in the current cloud (if supported) and assigns a fixed, external IP to the Service. Superset of NodePort.</li><li><em>ExternalName</em> - Exposes the Service using an arbitrary name (specified by <strong>externalName</strong> in the spec) by returning a CNAME record with the name. No proxy is used. This type requires v1.7 or higher of <strong>kube-dns</strong>.</li><li>More information about the different types of Services can be found in the <a href="https://kubernetes.io/docs/tutorials/services/source-ip/">Using Source IP</a> tutorial. Also see <a href="https://kubernetes.io/docs/concepts/services-networking/connect-applications-service">Connecting Applications with Services</a>.</li></ul><p>Additionally, note that there are some use cases with Services that involve not defining <strong>selector</strong> in the spec. A Service created without <strong>selector</strong> will also not create the corresponding Endpoints object. This allows users to manually map a Service to specific endpoints. Another possibility why there may be no selector is you are strictly using <strong>type: ExternalName</strong>.</p><h4>Summary</h4><ul><li>Exposing Pods to external traffic</li><li>Load balancing traffic across multiple Pods</li><li>Using labels</li></ul><p>A Kubernetes Service is an abstraction layer which defines a logical set of Pods and enables external traffic exposure, load balancing and service discovery for those Pods.</p><h4>Services and Labels</h4><p>A Service routes traffic across a set of Pods. Services are the abstraction that allow pods to die and replicate in Kubernetes without impacting your application. Discovery and routing among dependent Pods (such as the frontend and backend components in an application) is handled by Kubernetes Services.</p><p>Services match a set of Pods using <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels">labels and selectors</a>, a grouping primitive that allows logical operation on objects in Kubernetes. Labels are key/value pairs attached to objects and can be used in any number of ways:</p><ul><li>Designate objects for development, test, and production</li><li>Embed version tags</li><li>Classify an object using tags</li></ul><p>You can create a Service at the same time you create a Deployment by using <em>--expose</em> in kubectl.</p><p>Labels can be attached to objects at creation time or later. They can be modified at any time. Let\&#x27;s expose our application now using a Service and apply some labels.</p><h2>Scale your App</h2><h3>Running Multiple Instances of Your App</h3><h4>Objectives</h4><ul><li>Scale an app using kubectl.</li></ul><h4>Scaling an application</h4><p>In the previous modules we created a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployment</a>, and then exposed it publicly via a <a href="https://kubernetes.io/docs/concepts/services-networking/service/">Service</a>. The Deployment created only one Pod for running our application. When traffic increases, we will need to scale the application to keep up with user demand.</p><p><strong>Scaling</strong> is accomplished by changing the number of replicas in a Deployment</p><h4>Summary</h4><ul><li>Scaling a Deployment</li></ul><p>You can create from the start a Deployment with multiple instances using the --replicas parameter for the kubectl run command</p><h4>Scaling overview</h4><p>Scaling out a Deployment will ensure new Pods are created and scheduled to Nodes with available resources. Scaling in will reduce the number of Pods to the new desired state. Kubernetes also supports <a href="http://kubernetes.io/docs/user-guide/horizontal-pod-autoscaling/">autoscaling </a>of Pods, but it is outside of the scope of this tutorial. Scaling to zero is also possible, and it will terminate all Pods of the specified Deployment.</p><p>Running multiple instances of an application will require a way to distribute the traffic to all of them. Services have an integrated load-balancer that will distribute network traffic to all Pods of an exposed Deployment. Services will monitor continuously the running Pods using endpoints, to ensure the traffic is sent only to available Pods.</p><p>Scaling is accomplished by changing the number of replicas in a Deployment.</p><p>Once you have multiple instances of an Application running, you would be able to do Rolling updates without downtime. We\&#x27;ll cover that in the next module. Now, let\&#x27;s go to the online terminal and scale our application.</p><h2>Update your App</h2><h3>Performing a Rolling Update</h3><h4>Objectives</h4><ul><li>Perform a rolling update using kubectl.</li></ul><h4>Updating an application</h4><p>Users expect applications to be available all the time and developers are expected to deploy new versions of them several times a day. In Kubernetes this is done with rolling updates. <strong>Rolling updates</strong> allow Deployments\&#x27; update to take place with zero downtime by incrementally updating Pods instances with new ones. The new Pods will be scheduled on Nodes with available resources.</p><p>In the previous module we scaled our application to run multiple instances. This is a requirement for performing updates without affecting application availability. By default, the maximum number of Pods that can be unavailable during the update and the maximum number of new Pods that can be created, is one. Both options can be configured to either numbers or percentages (of Pods). In Kubernetes, updates are versioned, and any Deployment update can be reverted to previous (stable) version.</p><h4>Summary</h4><ul><li>Updating an app</li></ul><p>Rolling updates allow Deployments\&#x27; update to take place with zero downtime by incrementally updating Pods instances with new ones.</p><h4>Rolling updates overview</h4><p>Like application Scaling, if a Deployment is exposed publicly, the Service will load-balance the traffic only to available Pods during the update. An available Pod is an instance that is available to the users of the application.</p><p>Rolling updates allow the following actions:</p><ul><li>Promote an application from one environment to another (via container image updates)</li><li>Rollback to previous versions</li><li>Continuous Integration and Continuous Delivery of applications with zero downtime</li></ul><p>If a Deployment is exposed publicly, the Service will load-balance the traffic only to available Pods during the update.</p><p>In the following interactive tutorial, we\&#x27;ll update our application to a new version, and perform a rollback.</p><h2>Overview of Kubernetes Online Training</h2><p>Here are some of the sites that offer online training for Kubernetes:</p><ul><li><a href="https://www.udacity.com/course/scalable-microservices-with-kubernetes--ud615">Scalable Microservices with Kubernetes (Udacity)</a></li><li><a href="https://www.edx.org/course/introduction-kubernetes-linuxfoundationx-lfs158x">Introduction to Kubernetes (edX)</a></li></ul><h2>Hello Minikube</h2><p>The goal of this tutorial is for you to turn a simple Hello World Node.js app into an application running on Kubernetes. The tutorial shows you how to take code that you have developed on your machine, turn it into a Docker container image and then run that image on <a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>. Minikube provides a simple way of running Kubernetes on your local machine for free.</p><h3>Objectives</h3><ul><li>Run a hello world Node.js application.</li><li>Deploy the application to Minikube.</li><li>View application logs.</li><li>Update the application image.</li></ul><h3>Before you begin</h3><ul><li>For OS X, you need <a href="https://brew.sh/">Homebrew</a> to install the <strong>xhyve</strong> driver.</li><li><a href="https://nodejs.org/en/">NodeJS</a> is required to run the sample application.</li><li>Install Docker. On OS X, we recommend <a href="https://docs.docker.com/engine/installation/mac/">Docker for Mac</a>.</li></ul><h3>Create a Minikube cluster</h3><p>This tutorial uses <a href="https://github.com/kubernetes/minikube">Minikube</a> to create a local cluster. This tutorial also assumes you are using <a href="https://docs.docker.com/engine/installation/mac/">Docker for Mac</a> on OS X. If you are on a different platform like Linux, or using VirtualBox instead of Docker for Mac, the instructions to install Minikube may be slightly different. For general Minikube installation instructions, see the <a href="https://kubernetes.io/docs/getting-started-guides/minikube/">Minikube installation guide</a>.</p><p>Use <strong>curl</strong> to download and install the latest Minikube release:</p><p>curl -Lo minikube <a href="https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64">https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64</a> &amp;&amp; <!-- -->\</p><p>chmod +x minikube &amp;&amp; <!-- -->\</p><p>sudo mv minikube /usr/local/bin/</p><p>Use Homebrew to install the xhyve driver and set its permissions:</p><p><strong>brew install docker-machine-driver-xhyve</strong></p><p><strong>sudo chown root:wheel $(brew --prefix)/opt/docker-machine-driver-xhyve/bin/docker-machine-driver-xhyve</strong></p><p><strong>sudo chmod u+s $(brew --prefix)/opt/docker-machine-driver-xhyve/bin/docker-machine-driver-xhyve</strong></p><p>Use Homebrew to download the <strong>kubectl</strong> command-line tool, which you can use to interact with Kubernetes clusters:</p><p><strong>brew install kubectl</strong></p><p>Determine whether you can access sites like <code style="background-color:lightgray">&lt;https://cloud.google.com/container-registry/&gt;</code> directly without a proxy, by opening a new terminal and using</p><p><strong>curl --proxy &quot;&quot; <a href="https://cloud.google.com/container-registry/">https://cloud.google.com/container-registry/</a></strong></p><p>Make sure that the Docker daemon is started. You can determine if docker is running by using a command such as:</p><p><strong>docker images</strong></p><p>If NO proxy is required, start the Minikube cluster:</p><p><strong>minikube start --vm-driver=xhyve</strong></p><p>If a proxy server is required, use the following method to start Minikube cluster with proxy setting:</p><p><strong>minikube start --vm-driver=xhyve --docker-env HTTP_PROXY=http://your-http-proxy-host:your-http-proxy-port --docker-env HTTPS_PROXY=http(s)://your-https-proxy-host:your-https-proxy-port</strong></p><p>The <strong>--vm-driver=xhyve</strong> flag specifies that you are using Docker for Mac. The default VM driver is VirtualBox.</p><p>Note if minikube start --vm-driver=xhyve is unsuccessful due to the error:</p><p>Error creating machine: Error in driver during machine creation: Could not convert the UUID to MAC address: exit status 1</p><p>Then the following may resolve the <strong>minikube start --vm-driver=xhyve</strong> issue:</p><p>rm -rf <!-- -->~<!-- -->/.minikube</p><p>sudo chown root:wheel $(brew --prefix)/opt/docker-machine-driver-xhyve/bin/docker-machine-driver-xhyve</p><p>sudo chmod u+s $(brew --prefix)/opt/docker-machine-driver-xhyve/bin/docker-machine-driver-xhyve</p><p>Now set the Minikube context. The context is what determines which cluster <strong>kubectl</strong> is interacting with. You can see all your available contexts in the <strong>~<!-- -->/.kube/config</strong> file.</p><p><strong>kubectl config use-context minikube</strong></p><p>Verify that <strong>kubectl</strong> is configured to communicate with your cluster:</p><p><strong>kubectl cluster-info</strong></p><p>Open the Kubernetes dashboard in a browser:</p><p><strong>minikube dashboard</strong></p><h3>Create your Node.js application</h3><p>The next step is to write the application. Save this code in a folder named <strong>hellonode</strong> with the filename <strong>server.js</strong>:</p><p>var http = require(\&#x27;http\&#x27;);</p><p>var handleRequest = function(request, response) {</p><p>console.log(\&#x27;Received request for URL: \&#x27; + request.url);</p><p>response.writeHead(200);</p><p>response.end(\&#x27;Hello World!\&#x27;);</p><p>};</p><p>var www = http.createServer(handleRequest);</p><p>www.listen(8080);</p><p>Run your application:</p><p><strong>node server.js</strong></p><p>You should be able to see your &quot;Hello World!&quot; message at http://localhost:8080/.</p><p>Stop the running Node.js server by pressing <strong>Ctrl-C</strong>.</p><p>The next step is to package your application in a Docker container.</p><h3>Create a Docker container image</h3><p>Create a file, also in the <strong>hellonode</strong> folder, named <strong>Dockerfile</strong>. A Dockerfile describes the image that you want to build. You can build a Docker container image by extending an existing image. The image in this tutorial extends an existing Node.js image.</p><p>FROM node:6.9.2</p><p>EXPOSE 8080</p><p>COPY server.js .</p><p>CMD node server.js</p><p>This recipe for the Docker image starts from the official Node.js LTS image found in the Docker registry, exposes port 8080, copies your <strong>server.js</strong> file to the image and starts the Node.js server.</p><p>Because this tutorial uses Minikube, instead of pushing your Docker image to a registry, you can simply build the image using the same Docker host as the Minikube VM, so that the images are automatically present. To do so, make sure you are using the Minikube Docker daemon:</p><p>eval $(minikube docker-env)</p><p><strong>Note:</strong> Later, when you no longer wish to use the Minikube host, you can undo this change by running eval $(minikube docker-env -u).</p><p>Build your Docker image, using the Minikube Docker daemon:</p><p>docker build -t hello-node:v1 .</p><p>Now the Minikube VM can run the image you built.</p><h3>Create a Deployment</h3><p>A Kubernetes <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/">Pod</a> is a group of one or more Containers, tied together for the purposes of administration and networking. The Pod in this tutorial has only one Container. A Kubernetes <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployment</a> checks on the health of your Pod and restarts the Pod&#x27;s Container if it terminates. Deployments are the recommended way to manage the creation and scaling of Pods.</p><p>Use the <strong>kubectl run</strong> command to create a Deployment that manages a Pod. The Pod runs a Container based on your <strong>hello-node:v1</strong> Docker image:</p><p>kubectl run hello-node --image=hello-node:v1 --port=8080</p><p>View the Deployment:</p><p>kubectl get deployments</p><p>Output:</p><p><strong>NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE</strong></p><p><strong>hello-node 1 1 1 1 3m</strong></p><p>View the Pod:</p><p>kubectl get pods</p><p>Output:</p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>hello-node-714049816-ztzrb 1/1 Running 0 6m</strong></p><p>View cluster events:</p><p>kubectl get events</p><p>View the <strong>kubectl</strong> configuration:</p><p><strong>kubectl config view</strong></p><p>For more information about <strong>kubectl</strong> commands, see the <a href="https://kubernetes.io/docs/user-guide/kubectl-overview/">kubectl overview</a>.</p><h3>Create a Service</h3><p>By default, the Pod is only accessible by its internal IP address within the Kubernetes cluster. To make the <strong>hello-node</strong> Container accessible from outside the Kubernetes virtual network, you have to expose the Pod as a Kubernetes <a href="https://kubernetes.io/docs/concepts/services-networking/service/">Service</a>.</p><p>From your development machine, you can expose the Pod to the public internet using the <strong>kubectl expose</strong> command:</p><p><strong>kubectl expose deployment hello-node --type=LoadBalancer</strong></p><p>View the Service you just created:</p><p><strong>kubectl get services</strong></p><p>Output:</p><p><strong>NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE</strong></p><p><strong>hello-node 10.0.0.71 <code>&lt;pending&gt;</code> 8080/TCP 6m</strong></p><p><strong>kubernetes 10.0.0.1 <code>&lt;none&gt;</code> 443/TCP 14d</strong></p><p>The <strong>--type=LoadBalancer</strong> flag indicates that you want to expose your Service outside of the cluster. On cloud providers that support load balancers, an external IP address would be provisioned to access the Service. On Minikube, the <strong>LoadBalancer</strong> type makes the Service accessible through the <strong>minikube service</strong> command.</p><p><strong>minikube service hello-node</strong></p><p>This automatically opens up a browser window using a local IP address that serves your app and shows the &quot;Hello World&quot; message.</p><p>Assuming you&#x27;ve sent requests to your new web service using the browser or curl, you should now be able to see some logs:</p><p><strong>kubectl logs <code>&lt;POD-NAME&gt;</code></strong></p><h3>Update your app</h3><p>Edit your <strong>server.js</strong> file to return a new message:</p><p><strong>response.end(\&#x27;Hello World Again!\&#x27;);</strong></p><p>Build a new version of your image:</p><p><strong>docker build -t hello-node:v2 .</strong></p><p>Update the image of your Deployment:</p><p><strong>kubectl set image deployment/hello-node hello-node=hello-node:v2</strong></p><p>Run your app again to view the new message:</p><p><strong>minikube service hello-node</strong></p><h3>Enable addons</h3><p>Minikube has a set of built-in addons that can be enabled, disabled and opened in the local Kubernetes environment.</p><p>First list the currently supported addons:</p><p><strong>minikube addons list</strong></p><p>Output:</p><p><strong>- storage-provisioner: enabled</strong></p><p><strong>- kube-dns: enabled</strong></p><p><strong>- registry: disabled</strong></p><p><strong>- registry-creds: disabled</strong></p><p><strong>- addon-manager: enabled</strong></p><p><strong>- dashboard: disabled</strong></p><p><strong>- default-storageclass: enabled</strong></p><p><strong>- coredns: disabled</strong></p><p><strong>- heapster: disabled</strong></p><p><strong>- efk: disabled</strong></p><p><strong>- ingress: disabled</strong></p><p>Minikube must be running for these commands to take effect. To enable <strong>heapster</strong> addon, for example:</p><p><strong>minikube addons enable heapster</strong></p><p>Output:</p><p><strong>heapster was successfully enabled</strong></p><p>View the Pod and Service you just created:</p><p><strong>kubectl get po,svc -n kube-system</strong></p><p>Output:</p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>po/heapster-zbwzv 1/1 Running 0 2m</strong></p><p><strong>po/influxdb-grafana-gtht9 2/2 Running 0 2m</strong></p><p><strong>NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE</strong></p><p><strong>svc/heapster NodePort 10.0.0.52 <code>&lt;none&gt;</code> 80:31655/TCP 2m</strong></p><p><strong>svc/monitoring-grafana NodePort 10.0.0.33 <code>&lt;none&gt;</code> 80:30002/TCP 2m</strong></p><p><strong>svc/monitoring-influxdb ClusterIP 10.0.0.43 <code>&lt;none&gt;</code> 8083/TCP,8086/TCP 2m</strong></p><p>Open the endpoint to interacting with heapster in a browser:</p><p><strong>minikube addons open heapster</strong></p><p>Output:</p><p><strong>Opening kubernetes service kube-system/monitoring-grafana in default browser<!-- -->.<!-- -->..</strong></p><h3>Clean up</h3><p>Now you can clean up the resources you created in your cluster:</p><p><strong>kubectl delete service hello-node</strong></p><p><strong>kubectl delete deployment hello-node</strong></p><p>Optionally, force removal of the Docker images created:</p><p><strong>docker rmi hello-node:v1 hello-node:v2 -f</strong></p><p>Optionally, stop the Minikube VM:</p><p><strong>minikube stop</strong></p><p><strong>eval $(minikube docker-env -u)</strong></p><p>Optionally, delete the Minikube VM:</p><p><strong>minikube delete</strong></p><h2>Kubernetes 101</h2><h3>Kubectl CLI and Pods</h3><p>For Kubernetes 101, we will cover kubectl, pods, volumes, and multiple containers</p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using <a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</p><h3>Kubectl CLI</h3><p>The easiest way to interact with Kubernetes is via the <a href="https://kubernetes.io/docs/user-guide/kubectl-overview/">kubectl</a> command-line interface.</p><p>For more info about kubectl, including its usage, commands, and parameters, see the <a href="https://kubernetes.io/docs/user-guide/kubectl-overview/">kubectl CLI reference</a>.</p><p>If you haven&#x27;t installed and configured kubectl, finish <a href="https://kubernetes.io/docs/tasks/kubectl/install/">installing kubectl</a> before continuing.</p><h3>Pods</h3><p>In Kubernetes, a group of one or more containers is called a pod. Containers in a pod are deployed together, and are started, stopped, and replicated as a group.</p><p>See <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/">pods</a> for more details.</p><h4>Pod Definition</h4><p>The simplest pod definition describes the deployment of a single container. For example, an nginx web server pod might be defined as such:</p><p>apiVersion: v1</p><p>kind: Pod</p><p>metadata:</p><p>name: nginx</p><p>spec:</p><p>containers:</p><ul><li>name: nginx</li></ul><p>image: nginx:1.7.9</p><p>ports:</p><ul><li>containerPort: 80</li></ul><p>A pod definition is a declaration of a desired state. Desired state is a very important concept in the Kubernetes model. Many things present a desired state to the system, and it is Kubernetes&#x27; responsibility to make sure that the current state matches the desired state. For example, when you create a Pod, you declare that you want the containers in it to be running. If the containers happen to not be running (e.g. program failure, ...), Kubernetes will continue to (re-)create them for you in order to drive them to the desired state. This process continues until the Pod is deleted.</p><p>See the <a href="https://git.k8s.io/community/contributors/design-proposals/README.md">design document</a> for more details.</p><h5><strong>Pod Management</strong></h5><p>Create a pod containing an nginx server (<a href="https://kubernetes.io/docs/user-guide/walkthrough/pod-nginx.yaml">pod-nginx.yaml</a>):</p><p><strong>$ kubectl create -f docs/user-guide/walkthrough/pod-nginx.yaml</strong></p><p>List all pods:</p><p><strong>$ kubectl get pods</strong></p><p>On most providers, the pod IPs are not externally accessible. The easiest way to test that the pod is working is to create a busybox pod and exec commands on it remotely. See the <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/get-shell-running-container/">command execution documentation</a> for details.</p><p>Provided the pod IP is accessible, you should be able to access its http endpoint with wget on port 80:</p><p><strong>$ kubectl run busybox --image=busybox --restart=Never --tty -i --generator=run-pod/v1 --env &quot;POD_IP=$(kubectl get pod nginx -o go-template=\&#x27;{{.status.podIP}}\&#x27;)&quot;</strong></p><p><strong>u@busybox$ wget -qO- http://$POD_IP <em># Run in the busybox container</em></strong></p><p><strong>u@busybox$ exit <em># Exit the busybox container</em></strong></p><p><strong>$ kubectl delete pod busybox <em># Clean up the pod we created with &quot;kubectl run&quot;</em></strong></p><p>Delete the pod by name:</p><p><strong>$ kubectl delete pod nginx</strong></p><h4>Volumes</h4><p>That&#x27;s great for a simple static web server, but what about persistent storage?</p><p>The container file system only lives as long as the container does. So if your app&#x27;s state needs to survive relocation, reboots, and crashes, you&#x27;ll need to configure some persistent storage.</p><p>For this example we&#x27;ll be creating a Redis pod with a named volume and volume mount that defines the path to mount the volume.</p><ol><li>Define a volume:</li></ol><p>volumes:</p><ul><li>name: redis-persistent-storage</li></ul><p>emptyDir: {}</p><ol><li>Define a volume mount within a container definition:</li></ol><p>volumeMounts:</p><p>   # name must match the volume name defined in volumes</p><p>   - name: redis-persistent-storage</p><h1>mount path within the container</h1><p>mountPath: /data/redis</p><p>Example Redis pod definition with a persistent storage volume (<a href="https://kubernetes.io/docs/user-guide/walkthrough/pod-redis.yaml">pod-redis.yaml</a>):</p><p>apiVersion: v1</p><p>kind: Pod</p><p>metadata:</p><p>name: redis</p><p>spec:</p><p>containers:</p><ul><li>name: redis</li></ul><p>image: redis</p><p>volumeMounts:</p><ul><li>name: redis-persistent-storage</li></ul><p>mountPath: /data/redis</p><p>volumes:</p><ul><li>name: redis-persistent-storage</li></ul><p>emptyDir: {}</p><p>Notes:</p><ul><li>The <strong>volumeMounts</strong> <strong>name</strong> is a reference to a specific <strong>volumes</strong> <strong>name</strong>.</li><li>The <strong>volumeMounts</strong> <strong>mountPath</strong> is the path to mount the volume within the container.</li></ul><h5>Volume Types</h5><ul><li><strong>EmptyDir</strong>: Creates a new directory that will exist if the Pod is running on the node, but it can persist across container failures and restarts.</li><li><strong>HostPath</strong>: Mounts an existing directory on the node&#x27;s file system (e.g. <strong>/var/logs</strong>).</li></ul><p>See <a href="https://kubernetes.io/docs/concepts/storage/volumes/">volumes</a> for more details.</p><h4>Multiple Containers</h4><p>Note: The examples below are syntactically correct, but some of the images (e.g. kubernetes/git-monitor) don&#x27;t exist yet. We&#x27;re working on turning these into working examples.</p><p>However, often you want to have two different containers that work together. An example of this would be a web server, and a helper job that polls a git repository for new updates:</p><p>apiVersion: v1</p><p>kind: Pod</p><p>metadata:</p><p>name: www</p><p>spec:</p><p>containers:</p><ul><li>name: nginx</li></ul><p>image: nginx</p><p>volumeMounts:</p><ul><li>mountPath: /srv/www</li></ul><p>name: www-data</p><p>readOnly: true</p><ul><li>name: git-monitor</li></ul><p>image: kubernetes/git-monitor</p><p>env:</p><ul><li>name: GIT_REPO</li></ul><p>value: <a href="http://github.com/some/repo.git">http://github.com/some/repo.git</a></p><p>volumeMounts:</p><ul><li>mountPath: /data</li></ul><p>name: www-data</p><p>volumes:</p><ul><li>name: www-data</li></ul><p>emptyDir: {}</p><p>Note that we have also added a volume here. In this case, the volume is mounted into both containers. It is marked <strong>readOnly</strong> in the web server&#x27;s case, since it doesn&#x27;t need to write to the directory.</p><p>Finally, we have also introduced an environment variable to the <strong>git-monitor</strong> container, which allows us to parameterize that container with the particular git repository that we want to track.</p><h2>Kubernetes 201</h2><h3>Labels, Deployments, Services and Health Checking</h3><p>If you went through <a href="https://kubernetes.io/docs/user-guide/walkthrough/">Kubernetes 101</a>, you learned about kubectl, Pods, Volumes, and multiple containers. For Kubernetes 201, we will pick up where 101 left off and cover some slightly more advanced topics in Kubernetes, related to application productionization, Deployment and scaling.</p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using <a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</p><h3>Labels</h3><p>Having already learned about Pods and how to create them, you may be struck by an urge to create many, many Pods. Please do! But eventually you will need a system to organize these Pods into groups. The system for achieving this in Kubernetes is Labels. Labels are key-value pairs that are attached to each object in Kubernetes. Label selectors can be passed along with a RESTful <strong>list</strong> request to the apiserver to retrieve a list of objects which match that label selector.</p><p>To add a label, add a labels section under metadata in the Pod definition:</p><p>labels:</p><p>app: nginx</p><p>For example, here is the nginx Pod definition with labels (<a href="https://kubernetes.io/docs/user-guide/walkthrough/pod-nginx-with-label.yaml">pod-nginx-with-label.yaml</a>):</p><p>apiVersion: v1</p><p>kind: Pod</p><p>metadata:</p><p>name: nginx</p><p>labels:</p><p>app: nginx</p><p>spec:</p><p>containers:</p><ul><li>name: nginx</li></ul><p>image: nginx</p><p>ports:</p><ul><li>containerPort: 80</li></ul><p>Create the labeled Pod (<a href="https://kubernetes.io/docs/user-guide/walkthrough/pod-nginx-with-label.yaml">pod-nginx-with-label.yaml</a>):</p><p>kubectl create -f <a href="https://k8s.io/docs/user-guide/walkthrough/pod-nginx-with-label.yaml">https://k8s.io/docs/user-guide/walkthrough/pod-nginx-with-label.yaml</a></p><p>List all Pods with the label <strong>app=nginx</strong>:</p><p>kubectl get pods -l app=nginx</p><p>Delete the Pod by label:</p><p>kubectl delete pod -l app=nginx</p><p>For more information, see <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/">Labels</a>. They are a core concept used by two additional Kubernetes building blocks: Deployments and Services.</p><h3>Deployments</h3><p>Now that you know how to make awesome, multi-container, labeled Pods and you want to use them to build an application, you might be tempted to just start building a whole bunch of individual Pods, but if you do that, a whole host of operational concerns pop up. For example: how will you scale the number of Pods up or down? How will you roll out a new release?</p><p>The answer to those questions and more is to use a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployment</a> to manage maintaining and updating your running Pods.</p><p>A Deployment object defines a Pod creation template (a &quot;cookie-cutter&quot; if you will) and desired replica count. The Deployment uses a label selector to identify the Pods it manages, and will create or delete Pods as needed to meet the replica count. Deployments are also used to manage safely rolling out changes to your running Pods.</p><p>Here is a Deployment that instantiates two nginx Pods:</p><p>apiVersion: apps/v1</p><p>kind: Deployment</p><p>metadata:</p><p>name: nginx-deployment</p><p>spec:</p><p>selector:</p><p>matchLabels:</p><p>app: nginx</p><p>replicas: 2 <em># tells deployment to run 2 pods matching the template</em></p><p>template: <em># create pods using pod definition in this template</em></p><p>metadata:</p><p><em># unlike pod-nginx.yaml, the name is not included in the meta data as a unique name is</em></p><p><em># generated from the deployment name</em></p><p>labels:</p><p>app: nginx</p><p>spec:</p><p>containers:</p><ul><li>name: nginx</li></ul><p>image: nginx:1.7.9</p><p>ports:</p><ul><li>containerPort: 80</li></ul><h4>Deployment Management</h4><p>Create an nginx Deployment:</p><p>kubectl create -f <a href="https://k8s.io/docs/user-guide/walkthrough/deployment.yaml">https://k8s.io/docs/user-guide/walkthrough/deployment.yaml</a></p><p>List all Deployments:</p><p>kubectl get deployment</p><p>List the Pods created by the Deployment:</p><p>kubectl get pods -l app=nginx</p><p>Upgrade the nginx container from 1.7.9 to 1.8 by changing the Deployment and calling <strong>apply</strong>. The following config contains the desired changes:</p><p>apiVersion: apps/v1</p><p>kind: Deployment</p><p>metadata:</p><p>name: nginx-deployment</p><p>spec:</p><p>selector:</p><p>matchLabels:</p><p>app: nginx</p><p>replicas: 2</p><p>template:</p><p>metadata:</p><p>labels:</p><p>app: nginx</p><p>spec:</p><p>containers:</p><ul><li>name: nginx</li></ul><p>image: nginx:1.8 <em># Update the version of nginx from 1.7.9 to 1.8</em></p><p>ports:</p><ul><li>containerPort: 80</li></ul><p>kubectl apply -f <a href="https://k8s.io/docs/user-guide/walkthrough//deployment-update.yaml">https://k8s.io/docs/user-guide/walkthrough//deployment-update.yaml</a></p><p>Watch the Deployment create Pods with new names and delete the old Pods:</p><p>kubectl get pods -l app=nginx</p><p>Delete the Deployment by name:</p><p>kubectl delete deployment nginx-deployment</p><p>For more information, such as how to rollback Deployment changes to a previous version, see <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployments</a>.</p><h3>Services</h3><p>Once you have a replicated set of Pods, you need an abstraction that enables connectivity between the layers of your application. For example, if you have a Deployment managing your backend jobs, you don&#x27;t want to have to reconfigure your front-ends whenever you re-scale your backends. Likewise, if the Pods in your backends are scheduled (or rescheduled) onto different machines, you can&#x27;t be required to re-configure your front-ends. In Kubernetes, the service abstraction achieves these goals. A service provides a way to refer to a set of Pods (selected by labels) with a single static IP address. It may also provide load balancing, if supported by the provider.</p><p>For example, here is a service that balances across the Pods created in the previous nginx Deployment example (<a href="https://kubernetes.io/docs/user-guide/walkthrough/service.yaml">service.yaml</a>):</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Service</strong></p><p><strong>metadata:</strong></p><p><strong>name: nginx-service</strong></p><p><strong>spec:</strong></p><p><strong>ports:</strong></p><p><strong>- port: 8000 <em># the port that this service should serve on</em></strong></p><p><strong><em># the container on each pod to connect to, can be a name</em></strong></p><p><strong><em># (e.g. \&#x27;www\&#x27;) or a number (e.g. 80)</em></strong></p><p><strong>targetPort: 80</strong></p><p><strong>protocol: TCP</strong></p><p><strong><em># just like the selector in the deployment,</em></strong></p><p><strong><em># but this time it identifies the set of pods to load balance</em></strong></p><p><strong><em># traffic to.</em></strong></p><p><strong>selector:</strong></p><p><strong>app: nginx</strong></p><h4>Service Management</h4><p>Create an nginx service (<a href="https://kubernetes.io/docs/user-guide/walkthrough/service.yaml">service.yaml</a>):</p><p>kubectl create -f <a href="https://k8s.io/docs/user-guide/walkthrough/service.yaml">https://k8s.io/docs/user-guide/walkthrough/service.yaml</a></p><p>List all services:</p><p>kubectl get services</p><p>On most providers, the service IPs are not externally accessible. The easiest way to test that the service is working is to create a busybox Pod and exec commands on it remotely. See the <a href="https://kubernetes.io/docs/user-guide/kubectl-overview/">command execution documentation</a> for details.</p><p>Provided the service IP is accessible, you should be able to access its http endpoint with wget on the exposed port:</p><p>$ export SERVICE_IP=$(kubectl get service nginx-service -o go-template=\&#x27;{{.spec.clusterIP}}\&#x27;)</p><p>$ export SERVICE_PORT=$(kubectl get service nginx-service -o go-template=\&#x27;{{(index .spec.ports 0).port}}\&#x27;)</p><p>$ echo &quot;$SERVICE_IP:$SERVICE_PORT&quot;</p><p>$ kubectl run busybox --generator=run-pod/v1 --image=busybox --restart=Never --tty -i --env &quot;SERVICE_IP=$SERVICE_IP&quot; --env &quot;SERVICE_PORT=$SERVICE_PORT&quot;</p><p>u@busybox$ wget -qO- http://$SERVICE_IP:$SERVICE_PORT <em># Run in the busybox container</em></p><p>u@busybox$ exit <em># Exit the busybox container</em></p><p>$ kubectl delete pod busybox <em># Clean up the pod we created with &quot;kubectl run&quot;</em></p><p>The service definition <a href="https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/">exposed the Nginx Service</a> as port 8000 (<strong>$SERVCE_PORT</strong>). We can also access the service from a host running Kubernetes using that port:</p><p>wget -qO- http://$SERVICE_IP:$SERVICE_PORT <em># Run on a Kubernetes host</em></p><p>(This works on AWS with Weave.)</p><p>To delete the service by name:</p><p>kubectl delete service nginx-service</p><p>When created, each service is assigned a unique IP address. This address is tied to the lifespan of the Service, and will not change while the Service is alive. Pods can be configured to talk to the service, and know that communication to the service will be automatically load-balanced out to some Pod that is a member of the set identified by the label selector in the Service.</p><p>For more information, see <a href="https://kubernetes.io/docs/concepts/services-networking/service/">Services</a>.</p><h3>Health Checking</h3><p>When I write code it never crashes, right? Sadly the <a href="https://github.com/kubernetes/kubernetes/issues">Kubernetes issues list</a> indicates otherwise...</p><p>Rather than trying to write bug-free code, a better approach is to use a management system to perform periodic health checking and repair of your application. That way a system outside of your application itself is responsible for monitoring the application and acting to fix it. It&#x27;s important that the system be outside of the application, since if your application fails and the health checking agent is part of your application, it may fail as well and you&#x27;ll never know. In Kubernetes, the health check monitor is the Kubelet agent.</p><h4>Process Health Checking</h4><p>The simplest form of health-checking is just process level health checking. The Kubelet constantly asks the Docker daemon if the container process is still running, and if not, the container process is restarted. In all of the Kubernetes examples you have run so far, this health checking was actually already enabled. It&#x27;s on for every single container that runs in Kubernetes.</p><h4>Application Health Checking</h4><p>However, in many cases this low-level health checking is insufficient. Consider, for example, the following code:</p><p>lockOne := sync.Mutex{}</p><p>lockTwo := sync.Mutex{}</p><p>go func() {</p><p>lockOne.Lock();</p><p>lockTwo.Lock();</p><p>.<!-- -->..</p><p>}()</p><p>lockTwo.Lock();</p><p>lockOne.Lock();</p><p>This is a classic example of a problem in computer science known as <a href="https://en.wikipedia.org/wiki/Deadlock">&quot;Deadlock&quot;</a>. From Docker&#x27;s perspective your application is still operating and the process is still running, but from your application&#x27;s perspective your code is locked up and will never respond correctly.</p><p>To address this problem, Kubernetes supports user implemented application health-checks. These checks are performed by the Kubelet to ensure that your application is operating correctly for a definition of &quot;correctly&quot; that you provide.</p><p>Currently, there are three types of application health checks that you can choose from:</p><ul><li>HTTP Health Checks - The Kubelet will call a web hook. If it returns between 200 and 399, it is considered success, failure otherwise. See health check examples <a href="https://kubernetes.io/docs/user-guide/liveness/">here</a>.</li><li>Container Exec - The Kubelet will execute a command inside your container. If it exits with status 0 it will be considered a success. See health check examples <a href="https://kubernetes.io/docs/user-guide/liveness/">here</a>.</li><li>TCP Socket - The Kubelet will attempt to open a socket to your container. If it can establish a connection, the container is considered healthy, if it can&#x27;t it is considered a failure.</li></ul><p>In all cases, if the Kubelet discovers a failure the container is restarted.</p><p>The container health checks are configured in the <strong>livenessProbe</strong> section of your container config. There you can also specify an <strong>initialDelaySeconds</strong> that is a grace period from when the container is started to when health checks are performed, to enable your container to perform any necessary initialization.</p><p>Here is an example config for a Pod with an HTTP health check (<a href="https://kubernetes.io/docs/user-guide/walkthrough/pod-with-http-healthcheck.yaml">pod-with-http-healthcheck.yaml</a>):</p><p>apiVersion: v1</p><p>kind: Pod</p><p>metadata:</p><p>name: pod-with-http-healthcheck</p><p>spec:</p><p>containers:</p><ul><li>name: nginx</li></ul><p>image: nginx</p><p><em># defines the health checking</em></p><p>livenessProbe:</p><p><em># an http probe</em></p><p>httpGet:</p><p>path: /<!-- -->_<!-- -->status/healthz</p><p>port: 80</p><p><em># length of time to wait for a pod to initialize</em></p><p><em># after pod startup, before applying health checking</em></p><p>initialDelaySeconds: 30</p><p>timeoutSeconds: 1</p><p>ports:</p><ul><li>containerPort: 80</li></ul><p>And here is an example config for a Pod with a TCP Socket health check (<a href="https://kubernetes.io/docs/user-guide/walkthrough/pod-with-tcp-socket-healthcheck.yaml">pod-with-tcp-socket-healthcheck.yaml</a>):</p><p>apiVersion: v1</p><p>kind: Pod</p><p>metadata:</p><p>name: pod-with-tcp-socket-healthcheck</p><p>spec:</p><p>containers:</p><ul><li>name: redis</li></ul><p>image: redis</p><p><em># defines the health checking</em></p><p>livenessProbe:</p><p><em># a TCP socket probe</em></p><p>tcpSocket:</p><p>port: 6379</p><p><em># length of time to wait for a pod to initialize</em></p><p><em># after pod startup, before applying health checking</em></p><p>initialDelaySeconds: 30</p><p>timeoutSeconds: 1</p><p>ports:</p><ul><li>containerPort: 6379</li></ul><h2>Configuration</h2><h3>Configuring Redis using a ConfigMap</h3><p>This page provides a real world example of how to configure Redis using a ConfigMap and builds upon the <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/">Configure Containers Using a ConfigMap</a> task.</p><h4>Objectives</h4><ul><li>Create a ConfigMap.</li><li>Create a pod specification using the ConfigMap.</li><li>Create the pod.</li><li>Verify that the configuration was correctly applied.</li></ul><h4>Before you begin</h4><ul><li>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using<a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</li><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li><li>To check the version, enter <strong>kubectl version</strong>.</li><li>Understand <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/">Configure Containers Using a ConfigMap</a>.</li></ul><h4>Real World Example: Configuring Redis using a ConfigMap</h4><p>You can follow the steps below to configure a Redis cache using data stored in a ConfigMap.</p><ol><li>Create a ConfigMap from the <strong>docs/tutorials/configuration/configmap/redis/redis-config</strong>file:</li><li><strong>kubectl create configmap example-redis-config --from-file=<a href="https://k8s.io/docs/tutorials/configuration/configmap/redis/redis-config">https://k8s.io/docs/tutorials/configuration/configmap/redis/redis-config</a></strong></li><li><strong>kubectl get configmap example-redis-config -o yaml</strong></li><li><strong>apiVersion: v1</strong></li><li><strong>data:</strong></li><li><strong>redis-config: |</strong></li><li><strong>maxmemory 2mb</strong></li><li><strong>maxmemory-policy allkeys-lru</strong></li><li><strong>kind: ConfigMap</strong></li><li><strong>metadata:</strong></li><li><strong>creationTimestamp: 2016-03-30T18:14:41Z</strong></li><li><strong>name: example-redis-config</strong></li><li><strong>namespace: default</strong></li><li><strong>resourceVersion: &quot;24686&quot;</strong></li><li><strong>selfLink: /api/v1/namespaces/default/configmaps/example-redis-config</strong></li><li><strong>uid: 460a2b6e-f6a3-11e5-8ae5-42010af00002</strong></li><li>Create a pod specification that uses the config data stored in the ConfigMap:</li><li><strong>apiVersion: v1</strong></li><li><strong>kind: Pod</strong></li><li><strong>metadata:</strong></li><li><strong>name: redis</strong></li><li><strong>spec:</strong></li><li><strong>containers:</strong></li><li><strong>- name: redis</strong></li><li><strong>image: kubernetes/redis:v1</strong></li><li><strong>env:</strong></li><li><strong>- name: MASTER</strong></li><li><strong>value: &quot;true&quot;</strong></li><li><strong>ports:</strong></li><li><strong>- containerPort: 6379</strong></li><li><strong>resources:</strong></li><li><strong>limits:</strong></li><li><strong>cpu: &quot;0.1&quot;</strong></li><li><strong>volumeMounts:</strong></li><li><strong>- mountPath: /redis-master-data</strong></li><li><strong>name: data</strong></li><li><strong>- mountPath: /redis-master</strong></li><li><strong>name: config</strong></li><li><strong>volumes:</strong></li><li><strong>- name: data</strong></li><li><strong>emptyDir: {}</strong></li><li><strong>- name: config</strong></li><li><strong>configMap:</strong></li><li><strong>name: example-redis-config</strong></li><li><strong>items:</strong></li><li><strong>- key: redis-config</strong></li><li><strong>path: redis.conf</strong></li><li>Create the pod:</li><li><strong>kubectl create -f <a href="https://k8s.io/tutorials/configuration/configmap/redis/redis-pod.yaml">https://k8s.io/tutorials/configuration/configmap/redis/redis-pod.yaml</a></strong></li></ol><p>In the example, the config volume is mounted at <strong>/redis-master</strong>. It uses <strong>path</strong> to add the <strong>redis-config</strong> key to a file named <strong>redis.conf</strong>. The file path for the redis config, therefore, is <strong>/redis-master/redis.conf</strong>. This is where the image will look for the config file for the redis master.</p><ol><li>Use <strong>kubectl exec</strong> to enter the pod and run the <strong>redis-cli</strong> tool to verify that the configuration was correctly applied:</li><li><strong>kubectl exec -it redis redis-cli</strong></li><li><strong>127.0.0.1:6379&gt; CONFIG GET maxmemory</strong></li><li><strong>1) &quot;maxmemory&quot;</strong></li><li><strong>2) &quot;2097152&quot;</strong></li><li><strong>127.0.0.1:6379&gt; CONFIG GET maxmemory-policy</strong></li><li><strong>1) &quot;maxmemory-policy&quot;</strong></li><li><strong>2) &quot;allkeys-lru&quot;</strong></li></ol><h3>StateLess Applications</h3><h4>Run a Stateless Application Using a Deployment</h4><p>This page shows how to run an application using a Kubernetes Deployment object.</p><h5>Objectives</h5><ul><li>Create an nginx deployment.</li><li>Use kubectl to list information about the deployment.</li><li>Update the deployment.</li></ul><h5>Before you begin</h5><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using <a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>Your Kubernetes server must be version v1.9 or later. To check the version, enter <strong>kubectl version</strong>.</p><h5>Creating and exploring an nginx deployment</h5><p>You can run an application by creating a Kubernetes Deployment object, and you can describe a Deployment in a YAML file. For example, this YAML file describes a Deployment that runs the nginx:1.7.9 Docker image:</p><p>apiVersion: apps/v1 <em># for versions before 1.9.0 use apps/v1beta2</em></p><p>kind: Deployment</p><p>metadata:</p><p>name: nginx-deployment</p><p>spec:</p><p>selector:</p><p>matchLabels:</p><p>app: nginx</p><p>replicas: 2 <em># tells deployment to run 2 pods matching the template</em></p><p>template: <em># create pods using pod definition in this template</em></p><p>metadata:</p><p><em># unlike pod-nginx.yaml, the name is not included in the meta data as a unique name is</em></p><p><em># generated from the deployment name</em></p><p>labels:</p><p>app: nginx</p><p>spec:</p><p>containers:</p><ul><li>name: nginx</li></ul><p>image: nginx:1.7.9</p><p>ports:</p><ul><li>containerPort: 80</li></ul><p>Create a Deployment based on the YAML file:</p><p>kubectl apply -f <a href="https://k8s.io/docs/tasks/run-application/deployment.yaml">https://k8s.io/docs/tasks/run-application/deployment.yaml</a></p><p>Display information about the Deployment:</p><p>kubectl describe deployment nginx-deployment</p><p>The output is similar to this:</p><p>user@computer:<!-- -->~<!-- -->/website$ kubectl describe deployment nginx-deployment</p><p>Name: nginx-deployment</p><p>Namespace: default</p><p>CreationTimestamp: Tue, 30 Aug 2016 18:11:37 -0700</p><p>Labels: app=nginx</p><p>Annotations: deployment.kubernetes.io/revision=1</p><p>Selector: app=nginx</p><p>Replicas: 2 desired | 2 updated | 2 total | 2 available | 0 unavailable</p><p>StrategyType: RollingUpdate</p><p>MinReadySeconds: 0</p><p>RollingUpdateStrategy: 1 max unavailable, 1 max surge</p><p>Pod Template:</p><p>Labels: app=nginx</p><p>Containers:</p><p>nginx:</p><p>Image: nginx:1.7.9</p><p>Port: 80/TCP</p><p>Environment: <code style="background-color:lightgray">&lt;none&gt;</code></p><p>Mounts: <code style="background-color:lightgray">&lt;none&gt;</code></p><p>Volumes: <code style="background-color:lightgray">&lt;none&gt;</code></p><p>Conditions:</p><p>Type Status Reason</p><hr/><p>Available True MinimumReplicasAvailable</p><p>Progressing True NewReplicaSetAvailable</p><p>OldReplicaSets: <code style="background-color:lightgray">&lt;none&gt;</code></p><p>NewReplicaSet: nginx-deployment-1771418926 (2/2 replicas created)</p><p>No events.</p><p>List the pods created by the deployment:</p><p>kubectl get pods -l app=nginx</p><p>The output is like this:</p><p>NAME READY STATUS RESTARTS AGE</p><p>nginx-deployment-1771418926-7o5ns 1/1 Running 0 16h</p><p>nginx-deployment-1771418926-r18az 1/1 Running 0 16h</p><p>Display information about a pod:</p><p>kubectl describe pod <code style="background-color:lightgray">&lt;pod-name&gt;</code></p><p>where <code style="background-color:lightgray">&lt;pod-name&gt;</code> is the name of one of your pods.</p><h5>Updating the deployment</h5><p>You can update the deployment by applying a new YAML file. This YAML file specifies that the deployment should be updated to use nginx 1.8.</p><p>apiVersion: apps/v1 <em># for versions before 1.9.0 use apps/v1beta2</em></p><p>kind: Deployment</p><p>metadata:</p><p>name: nginx-deployment</p><p>spec:</p><p>selector:</p><p>matchLabels:</p><p>app: nginx</p><p>replicas: 2</p><p>template:</p><p>metadata:</p><p>labels:</p><p>app: nginx</p><p>spec:</p><p>containers:</p><ul><li>name: nginx</li></ul><p>image: nginx:1.8 <em># Update the version of nginx from 1.7.9 to 1.8</em></p><p>ports:</p><ul><li>containerPort: 80</li></ul><p>Apply the new YAML file:</p><p>kubectl apply -f <a href="https://k8s.io/docs/tasks/run-application/deployment-update.yaml">https://k8s.io/docs/tasks/run-application/deployment-update.yaml</a></p><p>Watch the deployment create pods with new names and delete the old pods:</p><p>kubectl get pods -l app=nginx</p><h5>Scaling the application by increasing the replica count</h5><p>You can increase the number of pods in your Deployment by applying a new YAML file. This YAML file sets <strong>replicas</strong> to 4, which specifies that the Deployment should have four pods:</p><p>apiVersion: apps/v1 <em># for versions before 1.9.0 use apps/v1beta2</em></p><p>kind: Deployment</p><p>metadata:</p><p>name: nginx-deployment</p><p>spec:</p><p>selector:</p><p>matchLabels:</p><p>app: nginx</p><p>replicas: 4 <em># Update the replicas from 2 to 4</em></p><p>template:</p><p>metadata:</p><p>labels:</p><p>app: nginx</p><p>spec:</p><p>containers:</p><ul><li>name: nginx</li></ul><p>image: nginx:1.8</p><p>ports:</p><ul><li>containerPort: 80</li></ul><p>Apply the new YAML file:</p><p>kubectl apply -f <a href="https://k8s.io/docs/tasks/run-application/deployment-scale.yaml">https://k8s.io/docs/tasks/run-application/deployment-scale.yaml</a></p><p>Verify that the Deployment has four pods:</p><p>kubectl get pods -l app=nginx</p><p>The output is like this:</p><p>NAME READY STATUS RESTARTS AGE</p><p>nginx-deployment-148880595-4zdqq 1/1 Running 0 25s</p><p>nginx-deployment-148880595-6zgi1 1/1 Running 0 25s</p><p>nginx-deployment-148880595-fxcez 1/1 Running 0 2m</p><p>nginx-deployment-148880595-rwovn 1/1 Running 0 2m</p><h5>Deleting a deployment</h5><p>Delete the deployment by name:</p><p><strong>kubectl delete deployment nginx-deployment</strong></p><h5>Replication Controllers -- the Old Way</h5><p>The preferred way to create a replicated application is to use a Deployment, which in turn uses a ReplicaSet. Before the Deployment and ReplicaSet were added to Kubernetes, replicated applications were configured by using a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/">ReplicationController</a>.</p><h4>Example: Deploying PHP Guestbook application with Redis</h4><p>This tutorial shows you how to build and deploy a simple, multi-tier web application using Kubernetes and <a href="https://www.docker.com/">Docker</a>. This example consists of the following components:</p><ul><li><a href="https://kubernetes.io/docs/tutorials/stateless-application/guestbook/#objectives"><strong>Objectives</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/stateless-application/guestbook/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/stateless-application/guestbook/#start-up-the-redis-master"><strong>Start up the Redis Master</strong></a><ul><li><a href="https://kubernetes.io/docs/tutorials/stateless-application/guestbook/#creating-the-redis-master-deployment"><strong>Creating the Redis Master Deployment</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/stateless-application/guestbook/#creating-the-redis-master-service"><strong>Creating the Redis Master Service</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tutorials/stateless-application/guestbook/#start-up-the-redis-slaves"><strong>Start up the Redis Slaves</strong></a><ul><li><a href="https://kubernetes.io/docs/tutorials/stateless-application/guestbook/#creating-the-redis-slave-deployment"><strong>Creating the Redis Slave Deployment</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/stateless-application/guestbook/#creating-the-redis-slave-service"><strong>Creating the Redis Slave Service</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tutorials/stateless-application/guestbook/#set-up-and-expose-the-guestbook-frontend"><strong>Set up and Expose the Guestbook Frontend</strong></a><ul><li><a href="https://kubernetes.io/docs/tutorials/stateless-application/guestbook/#creating-the-guestbook-frontend-deployment"><strong>Creating the Guestbook Frontend Deployment</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/stateless-application/guestbook/#creating-the-frontend-service"><strong>Creating the Frontend Service</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/stateless-application/guestbook/#viewing-the-frontend-service-via-nodeport"><strong>Viewing the Frontend Service via NodePort</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/stateless-application/guestbook/#viewing-the-frontend-service-via-loadbalancer"><strong>Viewing the Frontend Service via LoadBalancer</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tutorials/stateless-application/guestbook/#scale-the-web-frontend"><strong>Scale the Web Frontend</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/stateless-application/guestbook/#cleaning-up"><strong>Cleaning up</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/stateless-application/guestbook/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h5><strong>Objectives</strong></h5><ul><li>Start up a Redis master.</li><li>Start up Redis slaves.</li><li>Start up the guestbook frontend.</li><li>Expose and view the Frontend Service.</li><li>Clean up.</li></ul><h5><strong>Before you begin</strong></h5><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using<a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <strong>kubectl version</strong>.</p><p>Download the following configuration files:</p><ol><li><a href="https://kubernetes.io/docs/tutorials/stateless-application/guestbook/redis-master-deployment.yaml">redis-master-deployment.yaml</a></li><li><a href="https://kubernetes.io/docs/tutorials/stateless-application/guestbook/redis-master-service.yaml">redis-master-service.yaml</a></li><li><a href="https://kubernetes.io/docs/tutorials/stateless-application/guestbook/redis-slave-deployment.yaml">redis-slave-deployment.yaml</a></li><li><a href="https://kubernetes.io/docs/tutorials/stateless-application/guestbook/redis-slave-service.yaml">redis-slave-service.yaml</a></li><li><a href="https://kubernetes.io/docs/tutorials/stateless-application/guestbook/frontend-deployment.yaml">frontend-deployment.yaml</a></li><li><a href="https://kubernetes.io/docs/tutorials/stateless-application/guestbook/frontend-service.yaml">frontend-service.yaml</a></li></ol><h5><strong>Start up the Redis Master</strong></h5><p>The guestbook application uses Redis to store its data. It writes its data to a Redis master instance and reads data from multiple Redis slave instances.</p><h6>Creating the Redis Master Deployment</h6><p>The manifest file, included below, specifies a Deployment controller that runs a single replica Redis master Pod.</p><ol><li>Launch a terminal window in the directory you downloaded the manifest files.</li><li>Apply the Redis Master Deployment from the <strong>redis-master-deployment.yaml</strong> file:</li><li><strong>kubectl apply -f redis-master-deployment.yaml</strong></li></ol><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>guestbook/redis-master-deployment.yaml</strong>                           |
| ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/website/master/docs/tu">https://raw.githubusercontent.com/kubernetes/website/master/docs/tu</a> |
| torials/stateless-application/guestbook/redis-master-deployment.yaml) |
+=======================================================================+
| <strong>apiVersion: apps/v1 <em># for versions before 1.9.0 use               |
| apps/v1beta2</em></strong>                                                       |
|                                                                       |
| <strong>kind: Deployment</strong>                                                  |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: redis-master</strong>                                                |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>matchLabels:</strong>                                                      |
|                                                                       |
| <strong>app: redis</strong>                                                        |
|                                                                       |
| <strong>role: master</strong>                                                      |
|                                                                       |
| <strong>tier: backend</strong>                                                     |
|                                                                       |
| <strong>replicas: 1</strong>                                                       |
|                                                                       |
| <strong>template:</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: redis</strong>                                                        |
|                                                                       |
| <strong>role: master</strong>                                                      |
|                                                                       |
| <strong>tier: backend</strong>                                                     |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: master</strong>                                                    |
|                                                                       |
| <strong>image: k8s.gcr.io/redis:e2e <em># or just image: redis</em></strong>             |
|                                                                       |
| <strong>resources:</strong>                                                        |
|                                                                       |
| <strong>requests:</strong>                                                         |
|                                                                       |
| <strong>cpu: 100m</strong>                                                         |
|                                                                       |
| <strong>memory: 100Mi</strong>                                                     |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- containerPort: 6379</strong>                                             |
+-----------------------------------------------------------------------+</p><ol><li>Query the list of Pods to verify that the Redis Master Pod is running:</li><li><strong>kubectl get pods</strong></li></ol><p>The response should be similar to this:</p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>redis-master-1068406935-3lswp 1/1 Running 0 28s</strong></p><ol><li>Run the following command to view the logs from the Redis Master Pod:</li><li><strong>kubectl logs -f POD-NAME</strong></li></ol><p><strong>Note:</strong> Replace POD-NAME with the name of your Pod.</p><h6>Creating the Redis Master Service</h6><p>The guestbook applications needs to communicate to the Redis master to write its data. You need to apply a <a href="https://kubernetes.io/docs/concepts/services-networking/service/">Service</a> to proxy the traffic to the Redis master Pod. A Service defines a policy to access the Pods.</p><ol><li>Apply the Redis Master Service from the following <strong>redis-master-service.yaml</strong> file:</li><li><strong>kubectl apply -f redis-master-service.yaml</strong></li></ol><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>guestbook/redis-master-service.yaml                                |
| </strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/website/master/docs">https://raw.githubusercontent.com/kubernetes/website/master/docs</a> |
| /tutorials/stateless-application/guestbook/redis-master-service.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Service</strong>                                                     |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: redis-master</strong>                                                |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: redis</strong>                                                        |
|                                                                       |
| <strong>role: master</strong>                                                      |
|                                                                       |
| <strong>tier: backend</strong>                                                     |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- port: 6379</strong>                                                      |
|                                                                       |
| <strong>targetPort: 6379</strong>                                                  |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>app: redis</strong>                                                        |
|                                                                       |
| <strong>role: master</strong>                                                      |
|                                                                       |
| <strong>tier: backend</strong>                                                     |
+-----------------------------------------------------------------------+</p><p><strong>Note:</strong> This manifest file creates a Service named <strong>redis-master</strong> with a set of labels that match the labels previously defined, so the Service routes network traffic to the Redis master Pod.</p><ol><li>Query the list of Services to verify that the Redis Master Service is running:</li><li><strong>kubectl get service</strong></li></ol><p>The response should be similar to this:</p><p><strong>NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE</strong></p><p><strong>kubernetes 10.0.0.1 <code>&lt;none&gt;</code> 443/TCP 1m</strong></p><p><strong>redis-master 10.0.0.151 <code>&lt;none&gt;</code> 6379/TCP 8s</strong></p><h5><strong>Start up the Redis Slaves</strong></h5><p>Although the Redis master is a single pod, you can make it highly available to meet traffic demands by adding replica Redis slaves.</p><h6>Creating the Redis Slave Deployment</h6><p>Deployments scale based off of the configurations set in the manifest file. In this case, the Deployment object specifies two replicas.</p><p>If there are not any replicas running, this Deployment would start the two replicas on your container cluster. Conversely, if there are more than two replicas are running, it would scale down until two replicas are running.</p><ol><li>Apply the Redis Slave Deployment from the <strong>redis-slave-deployment.yaml</strong> file:</li><li><strong>kubectl apply -f redis-slave-deployment.yaml</strong></li></ol><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>guestbook/redis-slave-deployment.yaml</strong>                            |
|  ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/website/master/docs/t">https://raw.githubusercontent.com/kubernetes/website/master/docs/t</a> |
| utorials/stateless-application/guestbook/redis-slave-deployment.yaml) |
+=======================================================================+
| <strong>apiVersion: apps/v1 <em># for versions before 1.9.0 use               |
| apps/v1beta2</em></strong>                                                       |
|                                                                       |
| <strong>kind: Deployment</strong>                                                  |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: redis-slave</strong>                                                 |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>matchLabels:</strong>                                                      |
|                                                                       |
| <strong>app: redis</strong>                                                        |
|                                                                       |
| <strong>role: slave</strong>                                                       |
|                                                                       |
| <strong>tier: backend</strong>                                                     |
|                                                                       |
| <strong>replicas: 2</strong>                                                       |
|                                                                       |
| <strong>template:</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: redis</strong>                                                        |
|                                                                       |
| <strong>role: slave</strong>                                                       |
|                                                                       |
| <strong>tier: backend</strong>                                                     |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: slave</strong>                                                     |
|                                                                       |
| <strong>image: gcr.io/google_samples/gb-redisslave:v1</strong>                     |
|                                                                       |
| <strong>resources:</strong>                                                        |
|                                                                       |
| <strong>requests:</strong>                                                         |
|                                                                       |
| <strong>cpu: 100m</strong>                                                         |
|                                                                       |
| <strong>memory: 100Mi</strong>                                                     |
|                                                                       |
| <strong>env:</strong>                                                              |
|                                                                       |
| <strong>- name: GET_HOSTS_FROM</strong>                                            |
|                                                                       |
| <strong>value: dns</strong>                                                        |
|                                                                       |
| <strong><em># Using <code>GET_HOSTS_FROM=dns</code> requires your cluster to</em></strong>        |
|                                                                       |
| <strong><em># provide a dns service. As of Kubernetes 1.3, DNS is a           |
| built-in</em></strong>                                                           |
|                                                                       |
| <strong><em># service launched automatically. However, if the cluster you are |
| using</em></strong>                                                              |
|                                                                       |
| <strong><em># does not have a built-in DNS service, you can instead</em></strong>        |
|                                                                       |
| <strong><em># access an environment variable to find the master</em></strong>            |
|                                                                       |
| <strong><em># service\&#x27;s host. To do so, comment out the \&#x27;value: dns\&#x27; line  |
| above, and</em></strong>                                                         |
|                                                                       |
| <strong><em># uncomment the line below:</em></strong>                                    |
|                                                                       |
| <strong><em># value: env</em></strong>                                                   |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- containerPort: 6379</strong>                                             |
+-----------------------------------------------------------------------+</p><ol><li>Query the list of Pods to verify that the Redis Slave Pods are running:</li><li><strong>kubectl get pods</strong></li></ol><p>The response should be similar to this:</p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>redis-master-1068406935-3lswp 1/1 Running 0 1m</strong></p><p><strong>redis-slave-2005841000-fpvqc 0/1 ContainerCreating 0 6s</strong></p><p><strong>redis-slave-2005841000-phfv9 0/1 ContainerCreating 0 6s</strong></p><h6>Creating the Redis Slave Service</h6><p>The guestbook application needs to communicate to Redis slaves to read data. To make the Redis slaves discoverable, you need to set up a Service. A Service provides transparent load balancing to a set of Pods.</p><ol><li>Apply the Redis Slave Service from the following <strong>redis-slave-service.yaml</strong> file:</li><li><strong>kubectl apply -f redis-slave-service.yaml</strong></li></ol><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>guestbook/redis-slave-service.yam                                  |
| l</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/website/master/doc">https://raw.githubusercontent.com/kubernetes/website/master/doc</a> |
| s/tutorials/stateless-application/guestbook/redis-slave-service.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Service</strong>                                                     |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: redis-slave</strong>                                                 |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: redis</strong>                                                        |
|                                                                       |
| <strong>role: slave</strong>                                                       |
|                                                                       |
| <strong>tier: backend</strong>                                                     |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- port: 6379</strong>                                                      |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>app: redis</strong>                                                        |
|                                                                       |
| <strong>role: slave</strong>                                                       |
|                                                                       |
| <strong>tier: backend</strong>                                                     |
+-----------------------------------------------------------------------+</p><ol><li>Query the list of Services to verify that the Redis Slave Service is running:</li><li><strong>kubectl get services</strong></li></ol><p>The response should be similar to this:</p><p><strong>NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE</strong></p><p><strong>kubernetes 10.0.0.1 <code>&lt;none&gt;</code> 443/TCP 2m</strong></p><p><strong>redis-master 10.0.0.151 <code>&lt;none&gt;</code> 6379/TCP 1m</strong></p><p><strong>redis-slave 10.0.0.223 <code>&lt;none&gt;</code> 6379/TCP 6s</strong></p><h5><strong>Set up and Expose the Guestbook Frontend</strong></h5><p>The guestbook application has a web frontend serving the HTTP requests written in PHP. It is configured to connect to the <strong>redis-master</strong> Service for write requests and the <strong>redis-slave</strong>service for Read requests.</p><h6>Creating the Guestbook Frontend Deployment</h6><ol><li>Apply the frontend Deployment from the following <strong>frontend-deployment.yaml</strong> file:</li><li><strong>kubectl apply -f frontend-deployment.yaml</strong></li></ol><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>guestbook/frontend-deployment.yam                                  |
| l</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/website/master/doc">https://raw.githubusercontent.com/kubernetes/website/master/doc</a> |
| s/tutorials/stateless-application/guestbook/frontend-deployment.yaml) |
+=======================================================================+
| <strong>apiVersion: apps/v1 <em># for versions before 1.9.0 use               |
| apps/v1beta2</em></strong>                                                       |
|                                                                       |
| <strong>kind: Deployment</strong>                                                  |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: frontend</strong>                                                    |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>matchLabels:</strong>                                                      |
|                                                                       |
| <strong>app: guestbook</strong>                                                    |
|                                                                       |
| <strong>tier: frontend</strong>                                                    |
|                                                                       |
| <strong>replicas: 3</strong>                                                       |
|                                                                       |
| <strong>template:</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: guestbook</strong>                                                    |
|                                                                       |
| <strong>tier: frontend</strong>                                                    |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: php-redis</strong>                                                 |
|                                                                       |
| <strong>image: gcr.io/google-samples/gb-frontend:v4</strong>                       |
|                                                                       |
| <strong>resources:</strong>                                                        |
|                                                                       |
| <strong>requests:</strong>                                                         |
|                                                                       |
| <strong>cpu: 100m</strong>                                                         |
|                                                                       |
| <strong>memory: 100Mi</strong>                                                     |
|                                                                       |
| <strong>env:</strong>                                                              |
|                                                                       |
| <strong>- name: GET_HOSTS_FROM</strong>                                            |
|                                                                       |
| <strong>value: dns</strong>                                                        |
|                                                                       |
| <strong><em># Using <code>GET_HOSTS_FROM=dns</code> requires your cluster to</em></strong>        |
|                                                                       |
| <strong><em># provide a dns service. As of Kubernetes 1.3, DNS is a           |
| built-in</em></strong>                                                           |
|                                                                       |
| <strong><em># service launched automatically. However, if the cluster you are |
| using</em></strong>                                                              |
|                                                                       |
| <strong><em># does not have a built-in DNS service, you can instead</em></strong>        |
|                                                                       |
| <strong><em># access an environment variable to find the master</em></strong>            |
|                                                                       |
| <strong><em># service\&#x27;s host. To do so, comment out the \&#x27;value: dns\&#x27; line  |
| above, and</em></strong>                                                         |
|                                                                       |
| <strong><em># uncomment the line below:</em></strong>                                    |
|                                                                       |
| <strong><em># value: env</em></strong>                                                   |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- containerPort: 80</strong>                                               |
+-----------------------------------------------------------------------+</p><ol><li>Query the list of Pods to verify that the three frontend replicas are running:</li><li><strong>kubectl get pods -l app=guestbook -l tier=frontend</strong></li></ol><p>The response should be similar to this:</p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>frontend-3823415956-dsvc5 1/1 Running 0 54s</strong></p><p><strong>frontend-3823415956-k22zn 1/1 Running 0 54s</strong></p><p><strong>frontend-3823415956-w9gbt 1/1 Running 0 54s</strong></p><h6>Creating the Frontend Service</h6><p>The <strong>redis-slave</strong> and <strong>redis-master</strong> Services you applied are only accessible within the container cluster because the default type for a Service is <a href="https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services---service-types">ClusterIP</a>. <strong>ClusterIP</strong> provides a single IP address for the set of Pods the Service is pointing to. This IP address is accessible only within the cluster.</p><p>If you want guests to be able to access your guestbook, you must configure the frontend Service to be externally visible, so a client can request the Service from outside the container cluster. Minikube can only expose Services through <strong>NodePort</strong>.</p><p><strong>Note:</strong> Some cloud providers, like Google Compute Engine or Google Kubernetes Engine, support external load balancers. If your cloud provider supports load balancers and you want to use it, simply delete or comment out <strong>type: NodePort</strong>, and uncomment <strong>type: LoadBalancer</strong>.</p><ol><li>Apply the frontend Service from the following <strong>frontend-service.yaml</strong> file:</li><li><strong>kubectl apply -f frontend-service.yaml</strong></li></ol><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>guestbook/frontend-service.                                        |
| yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/website/master/">https://raw.githubusercontent.com/kubernetes/website/master/</a> |
| docs/tutorials/stateless-application/guestbook/frontend-service.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Service</strong>                                                     |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: frontend</strong>                                                    |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: guestbook</strong>                                                    |
|                                                                       |
| <strong>tier: frontend</strong>                                                    |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong><em># comment or delete the following line if you want to use a       |
| LoadBalancer</em></strong>                                                       |
|                                                                       |
| <strong>type: NodePort</strong>                                                    |
|                                                                       |
| <strong><em># if your cluster supports it, uncomment the following to         |
| automatically create</em></strong>                                               |
|                                                                       |
| <strong><em># an external load-balanced IP for the frontend service.</em></strong>       |
|                                                                       |
| <strong><em># type: LoadBalancer</em></strong>                                           |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- port: 80</strong>                                                        |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>app: guestbook</strong>                                                    |
|                                                                       |
| <strong>tier: frontend</strong>                                                    |
+-----------------------------------------------------------------------+</p><ol><li>Query the list of Services to verify that the frontend Service is running:</li><li><strong>kubectl get services</strong></li></ol><p>The response should be similar to this:</p><p><strong>NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE</strong></p><p><strong>frontend 10.0.0.112 <code>&lt;nodes&gt;</code> 80:31323/TCP 6s</strong></p><p><strong>kubernetes 10.0.0.1 <code>&lt;none&gt;</code> 443/TCP 4m</strong></p><p><strong>redis-master 10.0.0.151 <code>&lt;none&gt;</code> 6379/TCP 2m</strong></p><p><strong>redis-slave 10.0.0.223 <code>&lt;none&gt;</code> 6379/TCP 1m</strong></p><h6>Viewing the Frontend Service via NodePort</h6><p>If you deployed this application to Minikube or a local cluster, you need to find the IP address to view your Guestbook.</p><ol><li>Run the following command to get the IP address for the frontend Service.</li><li><strong>minikube service frontend --url</strong></li></ol><p>The response should be similar to this:</p><p><strong><a href="http://192.168.99.100:31323">http://192.168.99.100:31323</a></strong></p><ol><li>Copy the IP address, and load the page in your browser to view your guestbook.</li></ol><h6>Viewing the Frontend Service via LoadBalancer</h6><p>If you deployed the <strong>frontend-service.yaml</strong> manifest with type: <strong>LoadBalancer</strong> you need to find the IP address to view your Guestbook.</p><ol><li>Run the following command to get the IP address for the frontend Service.</li><li><strong>kubectl get service frontend</strong></li></ol><p>The response should be similar to this:</p><p><strong>NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE</strong></p><p><strong>frontend 10.51.242.136 109.197.92.229 80:32372/TCP 1m</strong></p><ol><li>Copy the External IP address, and load the page in your browser to view your guestbook.</li></ol><h5><strong>Scale the Web Frontend</strong></h5><p>Scaling up or down is easy because your servers are defined as a Service that uses a Deployment controller.</p><ol><li>Run the following command to scale up the number of frontend Pods:</li><li><strong>kubectl scale deployment frontend --replicas=5</strong></li><li>Query the list of Pods to verify the number of frontend Pods running:</li><li><strong>kubectl get pods</strong></li></ol><p>The response should look similar to this:</p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>frontend-3823415956-70qj5 1/1 Running 0 5s</strong></p><p><strong>frontend-3823415956-dsvc5 1/1 Running 0 54m</strong></p><p><strong>frontend-3823415956-k22zn 1/1 Running 0 54m</strong></p><p><strong>frontend-3823415956-w9gbt 1/1 Running 0 54m</strong></p><p><strong>frontend-3823415956-x2pld 1/1 Running 0 5s</strong></p><p><strong>redis-master-1068406935-3lswp 1/1 Running 0 56m</strong></p><p><strong>redis-slave-2005841000-fpvqc 1/1 Running 0 55m</strong></p><p><strong>redis-slave-2005841000-phfv9 1/1 Running 0 55m</strong></p><ol><li>Run the following command to scale down the number of frontend Pods:</li><li><strong>kubectl scale deployment frontend --replicas=2</strong></li><li>Query the list of Pods to verify the number of frontend Pods running:</li><li><strong>kubectl get pods</strong></li></ol><p>The response should look similar to this:</p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>frontend-3823415956-k22zn 1/1 Running 0 1h</strong></p><p><strong>frontend-3823415956-w9gbt 1/1 Running 0 1h</strong></p><p><strong>redis-master-1068406935-3lswp 1/1 Running 0 1h</strong></p><p><strong>redis-slave-2005841000-fpvqc 1/1 Running 0 1h</strong></p><p><strong>redis-slave-2005841000-phfv9 1/1 Running 0 1h</strong></p><h5><strong>Cleaning up</strong></h5><p>Deleting the Deployments and Services also deletes any running Pods. Use labels to delete multiple resources with one command.</p><ol><li>Run the following commands to delete all Pods, Deployments, and Services.</li><li><strong>kubectl delete deployment -l app=redis</strong></li><li><strong>kubectl delete service -l app=redis</strong></li><li><strong>kubectl delete deployment -l app=guestbook</strong></li><li><strong>kubectl delete service -l app=guestbook</strong></li></ol><p>The responses should be:</p><p><strong>deployment &quot;redis-master&quot; deleted</strong></p><p><strong>deployment &quot;redis-slave&quot; deleted</strong></p><p><strong>service &quot;redis-master&quot; deleted</strong></p><p><strong>service &quot;redis-slave&quot; deleted</strong></p><p><strong>deployment &quot;frontend&quot; deleted</strong></p><p><strong>service &quot;frontend&quot; deleted</strong></p><ol><li>Query the list of Pods to verify that no Pods are running:</li><li><strong>kubectl get pods</strong></li></ol><p>The response should be this:</p><p><strong>No resources found.</strong></p><h5><strong>What&#x27;s next</strong></h5><ul><li>Complete the <a href="https://kubernetes.io/docs/tutorials/kubernetes-basics/">Kubernetes Basics</a> Interactive Tutorials</li><li>Use Kubernetes to create a blog using <a href="https://kubernetes.io/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/#visit-your-new-wordpress-blog">Persistent Volumes for MySQL and Wordpress</a></li><li>Read more about <a href="https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/">connecting applications</a></li><li>Read more about <a href="https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#using-labels-effectively">Managing Resources</a></li></ul><h4>Use a Service to Access an Application in a Cluster</h4><p>This page shows how to create a Kubernetes Service object that external clients can use to access an application running in a cluster. The Service provides load balancing for an application that has two running instances.</p><ul><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/service-access-application-cluster/#objectives"><strong>Objectives</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/service-access-application-cluster/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/service-access-application-cluster/#creating-a-service-for-an-application-running-in-two-pods"><strong>Creating a service for an application running in two pods</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/service-access-application-cluster/#using-a-service-configuration-file"><strong>Using a service configuration file</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/service-access-application-cluster/#cleaning-up"><strong>Cleaning up</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/service-access-application-cluster/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h5><strong>Objectives</strong></h5><ul><li>Run two instances of a Hello World application.</li><li>Create a Service object that exposes a node port.</li><li>Use the Service object to access the running application.</li></ul><h5><strong>Before you begin</strong></h5><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using<a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <strong>kubectl version</strong>.</p><h5><strong>Creating a service for an application running in two pods</strong></h5><ol><li>Run a Hello World application in your cluster:</li><li><strong>kubectl run hello-world --replicas=2 --labels=&quot;run=load-balancer-example&quot; --image=gcr.io/google-samples/node-hello:1.0 --port=8080</strong></li></ol><p>The preceding command creates a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployment</a> object and an associated <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/">ReplicaSet</a> object. The ReplicaSet has two <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/">Pods</a>, each of which runs the Hello World application.</p><ol><li>Display information about the Deployment:</li><li><strong>kubectl get deployments hello-world</strong></li><li><strong>kubectl describe deployments hello-world</strong></li><li>Display information about your ReplicaSet objects:</li><li><strong>kubectl get replicasets</strong></li><li><strong>kubectl describe replicasets</strong></li><li>Create a Service object that exposes the deployment:</li><li><strong>kubectl expose deployment hello-world --type=NodePort --name=example-service</strong></li><li>Display information about the Service:</li><li><strong>kubectl describe services example-service</strong></li></ol><p>The output is similar to this:</p><p><strong>Name: example-service</strong></p><p><strong>Namespace: default</strong></p><p><strong>Labels: run=load-balancer-example</strong></p><p><strong>Annotations: <code>&lt;none&gt;</code></strong></p><p><strong>Selector: run=load-balancer-example</strong></p><p><strong>Type: NodePort</strong></p><p><strong>IP: 10.32.0.16</strong></p><p><strong>Port: <code>&lt;unset&gt;</code> 8080/TCP</strong></p><p><strong>TargetPort: 8080/TCP</strong></p><p><strong>NodePort: <code>&lt;unset&gt;</code> 31496/TCP</strong></p><p><strong>Endpoints: 10.200.1.4:8080,10.200.2.5:8080</strong></p><p><strong>Session Affinity: None</strong></p><p><strong>Events: <code>&lt;none&gt;</code></strong></p><p>Make a note of the NodePort value for the service. For example, in the preceding output, the NodePort value is 31496.</p><ol><li>List the pods that are running the Hello World application:</li><li><strong>kubectl get pods --selector=&quot;run=load-balancer-example&quot; --output=wide</strong></li></ol><p>The output is similar to this:</p><p><strong>NAME READY STATUS <!-- -->.<!-- -->.. IP NODE</strong></p><p><strong>hello-world-2895499144-bsbk5 1/1 Running <!-- -->.<!-- -->.. 10.200.1.4 worker1</strong></p><p><strong>hello-world-2895499144-m1pwt 1/1 Running <!-- -->.<!-- -->.. 10.200.2.5 worker2</strong></p><ol><li>Get the public IP address of one of your nodes that is running a Hello World pod. How you get this address depends on how you set up your cluster. For example, if you are using Minikube, you can see the node address by running <strong>kubectl cluster-info</strong>. If you are using Google Compute Engine instances, you can use the <strong>gcloud compute instances list</strong> command to see the public addresses of your nodes. For more information about this command, see the <a href="https://cloud.google.com/sdk/gcloud/reference/compute/instances/list">GCE documentation</a>.</li><li>On your chosen node, create a firewall rule that allows TCP traffic on your node port. For example, if your Service has a NodePort value of 31568, create a firewall rule that allows TCP traffic on port 31568. Different cloud providers offer different ways of configuring firewall rules. See <a href="https://cloud.google.com/compute/docs/vpc/firewalls">the GCE documentation on firewall rules</a>, for example.</li><li>Use the node address and node port to access the Hello World application:</li><li><code>curl http://&lt;public-node-ip&gt;:&lt;node-port&gt;</code></li></ol><p>where <code style="background-color:lightgray">&lt;public-node-ip&gt; is the public IP address of your node, and &lt;node-port&gt;</code>** is the NodePort value for your service.</p><p>The response to a successful request is a hello message:</p><p><strong>Hello Kubernetes!</strong></p><h5><strong>Using a service configuration file</strong></h5><p>As an alternative to using <strong>kubectl expose</strong>, you can use a <a href="https://kubernetes.io/docs/concepts/services-networking/service/">service configuration file</a> to create a Service.</p><h5><strong>Cleaning up</strong></h5><p>To delete the Service, enter this command:</p><p><strong>kubectl delete services example-service</strong></p><p>To delete the Deployment, the ReplicaSet, and the Pods that are running the Hello World application, enter this command:</p><p><strong>kubectl delete deployment hello-world</strong></p><h5><strong>What&#x27;s next</strong></h5><p>Learn more about <a href="https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/">connecting applications with services</a>.</p><h4>Exposing an External IP Address to Access an Application in a Cluster</h4><p>This page shows how to create a Kubernetes Service object that exposes an external IP address.</p><ul><li><a href="https://kubernetes.io/docs/tutorials/stateless-application/expose-external-ip-address/#objectives"><strong>Objectives</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/stateless-application/expose-external-ip-address/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/stateless-application/expose-external-ip-address/#creating-a-service-for-an-application-running-in-five-pods"><strong>Creating a service for an application running in five pods</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/stateless-application/expose-external-ip-address/#cleaning-up"><strong>Cleaning up</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/stateless-application/expose-external-ip-address/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h5><strong>Objectives</strong></h5><ul><li>Run five instances of a Hello World application.</li><li>Create a Service object that exposes an external IP address.</li><li>Use the Service object to access the running application.</li></ul><h5><strong>Before you begin</strong></h5><ul><li>Install <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">kubectl</a>.</li><li>Use a cloud provider like Google Kubernetes Engine or Amazon Web Services to create a Kubernetes cluster. This tutorial creates an <a href="https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/">external load balancer</a>, which requires a cloud provider.</li><li>Configure <strong>kubectl</strong> to communicate with your Kubernetes API server. For instructions, see the documentation for your cloud provider.</li></ul><h5><strong>Creating a service for an application running in five pods</strong></h5><ol><li>Run a Hello World application in your cluster:</li><li><strong>kubectl run hello-world --replicas=5 --labels=&quot;run=load-balancer-example&quot; --image=gcr.io/google-samples/node-hello:1.0 --port=8080</strong></li></ol><p>The preceding command creates a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployment</a> object and an associated <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/">ReplicaSet</a> object. The ReplicaSet has five <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/">Pods</a>, each of which runs the Hello World application.</p><ol><li>Display information about the Deployment:</li><li><strong>kubectl get deployments hello-world</strong></li><li><strong>kubectl describe deployments hello-world</strong></li><li>Display information about your ReplicaSet objects:</li><li><strong>kubectl get replicasets</strong></li><li><strong>kubectl describe replicasets</strong></li><li>Create a Service object that exposes the deployment:</li><li><strong>kubectl expose deployment hello-world --type=LoadBalancer --name=my-service</strong></li><li>Display information about the Service:</li><li><strong>kubectl get services my-service</strong></li></ol><p>The output is similar to this:</p><p><strong>NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE</strong></p><p><strong>my-service 10.3.245.137 104.198.205.71 8080/TCP 54s</strong></p><p>Note: If the external IP address is shown as <code style="background-color:lightgray">&lt;pending&gt;</code>, wait for a minute and enter the same command again.</p><ol><li>Display detailed information about the Service:</li><li><strong>kubectl describe services my-service</strong></li></ol><p>The output is similar to this:</p><p><strong>Name: my-service</strong></p><p><strong>Namespace: default</strong></p><p><strong>Labels: run=load-balancer-example</strong></p><p><strong>Annotations: <code>&lt;none&gt;</code></strong></p><p><strong>Selector: run=load-balancer-example</strong></p><p><strong>Type: LoadBalancer</strong></p><p><strong>IP: 10.3.245.137</strong></p><p><strong>LoadBalancer Ingress: 104.198.205.71</strong></p><p><strong>Port: <code>&lt;unset&gt;</code> 8080/TCP</strong></p><p><strong>NodePort: <code>&lt;unset&gt;</code> 32377/TCP</strong></p><p><strong>Endpoints: 10.0.0.6:8080,10.0.1.6:8080,10.0.1.7:8080 + 2 more<!-- -->.<!-- -->..</strong></p><p><strong>Session Affinity: None</strong></p><p><strong>Events: <code>&lt;none&gt;</code></strong></p><p>Make a note of the external IP address (<strong>LoadBalancer Ingress</strong>) exposed by your service. In this example, the external IP address is 104.198.205.71. Also note the value of <strong>Port</strong> and <strong>NodePort</strong>. In this example, the <strong>Port</strong> is 8080 and the <strong>NodePort</strong> is 32377.</p><ol><li>In the preceding output, you can see that the service has several endpoints: 10.0.0.6:8080,10.0.1.6:8080,10.0.1.7:8080 + 2 more. These are internal addresses of the pods that are running the Hello World application. To verify these are pod addresses, enter this command:</li><li><strong>kubectl get pods --output=wide</strong></li></ol><p>The output is similar to this:</p><p><strong>NAME <!-- -->.<!-- -->.. IP NODE</strong></p><p><strong>hello-world-2895499144-1jaz9 <!-- -->.<!-- -->.. 10.0.1.6 gke-cluster-1-default-pool-e0b8d269-1afc</strong></p><p><strong>hello-world-2895499144-2e5uh <!-- -->.<!-- -->.. 10.0.1.8 gke-cluster-1-default-pool-e0b8d269-1afc</strong></p><p><strong>hello-world-2895499144-9m4h1 <!-- -->.<!-- -->.. 10.0.0.6 gke-cluster-1-default-pool-e0b8d269-5v7a</strong></p><p><strong>hello-world-2895499144-o4z13 <!-- -->.<!-- -->.. 10.0.1.7 gke-cluster-1-default-pool-e0b8d269-1afc</strong></p><p><strong>hello-world-2895499144-segjf <!-- -->.<!-- -->.. 10.0.2.5 gke-cluster-1-default-pool-e0b8d269-cpuc</strong></p><ol><li>Use the external IP address (<strong>LoadBalancer Ingress</strong>) to access the Hello World application:</li><li><code>curl http://&lt;external-ip&gt;:&lt;port&gt;</code></li></ol><p>where <code style="background-color:lightgray">&lt;external-ip&gt; is the external IP address (LoadBalancer Ingress) of your Service, and &lt;port&gt;</code> is the value of <strong>NodePort</strong> in your Service description. If you are using minikube, typing <strong>minikube service my-service</strong> will automatically open the Hello World application in a browser.</p><p>The response to a successful request is a hello message:</p><p><strong>Hello Kubernetes!</strong></p><h5><strong>Cleaning up</strong></h5><p>To delete the Service, enter this command:</p><p><strong>kubectl delete services my-service</strong></p><p>To delete the Deployment, the ReplicaSet, and the Pods that are running the Hello World application, enter this command:</p><p><strong>kubectl delete deployment hello-world</strong></p><h5><strong>What&#x27;s next</strong></h5><p>Learn more about <a href="https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/">connecting applications with services</a>.</p><h2>Stateful Applications</h2><h3>Run a Single-Instance Stateful Application</h3><p>This page shows you how to run a single-instance stateful application in Kubernetes using a Persistent Volume and a Deployment. The application is MySQL.</p><h4>Objectives</h4><ul><li>Create a PersistentVolume referencing a disk in your environment.</li><li>Create a MySQL Deployment.</li><li>Expose MySQL to other pods in the cluster at a known DNS name.</li></ul><h4>Before you begin</h4><ul><li>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using<a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</li><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li><li>To check the version, enter <strong>kubectl version</strong>.</li><li>You need to either have a dynamic PersistentVolume provisioner with a default <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/">StorageClass</a>, or <a href="https://kubernetes.io/docs/user-guide/persistent-volumes/#provisioning">statically provision PersistentVolumes</a> yourself to satisfy the <a href="https://kubernetes.io/docs/user-guide/persistent-volumes/#persistentvolumeclaims">PersistentVolumeClaims</a> used here.</li></ul><h4>Deploy MySQL</h4><p>You can run a stateful application by creating a Kubernetes Deployment and connecting it to an existing PersistentVolume using a PersistentVolumeClaim. For example, this YAML file describes a Deployment that runs MySQL and references the PersistentVolumeClaim. The file defines a volume mount for /var/lib/mysql, and then creates a PersistentVolumeClaim that looks for a 20G volume. This claim is satisfied by any existing volume that meets the requirements, or by a dynamic provisioner.</p><p>Note: The password is defined in the config yaml, and this is insecure. See <a href="https://kubernetes.io/docs/concepts/configuration/secret/">Kubernetes Secrets</a> for a secure solution.</p><p>apiVersion: v1</p><p>kind: Service</p><p>metadata:</p><p>name: mysql</p><p>spec:</p><p>ports:</p><ul><li>port: 3306</li></ul><p>selector:</p><p>app: mysql</p><p>clusterIP: None</p><hr/><p>apiVersion: v1</p><p>kind: PersistentVolumeClaim</p><p>metadata:</p><p>name: mysql-pv-claim</p><p>spec:</p><p>accessModes:</p><ul><li>ReadWriteOnce</li></ul><p>resources:</p><p>requests:</p><p>storage: 20Gi</p><hr/><p>apiVersion: apps/v1 <em># for versions before 1.9.0 use apps/v1beta2</em></p><p>kind: Deployment</p><p>metadata:</p><p>name: mysql</p><p>spec:</p><p>selector:</p><p>matchLabels:</p><p>app: mysql</p><p>strategy:</p><p>type: Recreate</p><p>template:</p><p>metadata:</p><p>labels:</p><p>app: mysql</p><p>spec:</p><p>containers:</p><ul><li>image: mysql:5.6</li></ul><p>name: mysql</p><p>env:</p><p><em># Use secret in real usage</em></p><ul><li>name: MYSQL_ROOT_PASSWORD</li></ul><p>value: password</p><p>ports:</p><ul><li>containerPort: 3306</li></ul><p>name: mysql</p><p>volumeMounts:</p><ul><li>name: mysql-persistent-storage</li></ul><p>mountPath: /var/lib/mysql</p><p>volumes:</p><ul><li>name: mysql-persistent-storage</li></ul><p>persistentVolumeClaim:</p><p>claimName: mysql-pv-claim</p><p>Deploy the contents of the YAML file:</p><p>kubectl create -f <a href="https://k8s.io/docs/tasks/run-application/mysql-deployment.yaml">https://k8s.io/docs/tasks/run-application/mysql-deployment.yaml</a></p><p>Display information about the Deployment:</p><p>kubectl describe deployment mysql</p><p>Name: mysql</p><p>Namespace: default</p><p>CreationTimestamp: Tue, 01 Nov 2016 11:18:45 -0700</p><p>Labels: app=mysql</p><p>Annotations: deployment.kubernetes.io/revision=1</p><p>Selector: app=mysql</p><p>Replicas: 1 desired | 1 updated | 1 total | 0 available | 1 unavailable</p><p>StrategyType: Recreate</p><p>MinReadySeconds: 0</p><p>Pod Template:</p><p>Labels: app=mysql</p><p>Containers:</p><p>mysql:</p><p>Image: mysql:5.6</p><p>Port: 3306/TCP</p><p>Environment:</p><p>MYSQL_ROOT_PASSWORD: password</p><p>Mounts:</p><p>/var/lib/mysql from mysql-persistent-storage (rw)</p><p>Volumes:</p><p>mysql-persistent-storage:</p><p>Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)</p><p>ClaimName: mysql-pv-claim</p><p>ReadOnly: false</p><p>Conditions:</p><p>Type Status Reason</p><hr/><p>Available False MinimumReplicasUnavailable</p><p>Progressing True ReplicaSetUpdated</p><p>OldReplicaSets: <code style="background-color:lightgray">&lt;none&gt;</code></p><p>NewReplicaSet: mysql-63082529 (1/1 replicas created)</p><p>Events:</p><p>FirstSeen LastSeen Count From SubobjectPath Type Reason Message</p><hr/><p>33s 33s 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set mysql-63082529 to 1</p><p>List the pods created by the Deployment:</p><p>kubectl get pods -l app=mysql</p><p>NAME READY STATUS RESTARTS AGE</p><p>mysql-63082529-2z3ki 1/1 Running 0 3m</p><p>Inspect the PersistentVolumeClaim:</p><p>kubectl describe pvc mysql-pv-claim</p><p>Name: mysql-pv-claim</p><p>Namespace: default</p><p>StorageClass:</p><p>Status: Bound</p><p>Volume: mysql-pv</p><p>Labels: <code style="background-color:lightgray">&lt;none&gt;</code></p><p>Annotations: pv.kubernetes.io/bind-completed=yes</p><p>pv.kubernetes.io/bound-by-controller=yes</p><p>Capacity: 20Gi</p><p>Access Modes: RWO</p><p>Events: <code style="background-color:lightgray">&lt;none&gt;</code></p><h4>Accessing the MySQL instance</h4><p>The preceding YAML file creates a service that allows other Pods in the cluster to access the database. The Service option <strong>clusterIP: None</strong> lets the Service DNS name resolve directly to the Pod&#x27;s IP address. This is optimal when you have only one Pod behind a Service and you don&#x27;t intend to increase the number of Pods.</p><p>Run a MySQL client to connect to the server:</p><p>kubectl run -it --rm --image=mysql:5.6 --restart=Never mysql-client -- mysql -h mysql -ppassword</p><p>This command creates a new Pod in the cluster running a MySQL client and connects it to the server through the Service. If it connects, you know your stateful MySQL database is up and running.</p><p>Waiting for pod default/mysql-client-274442439-zyp6i to be running, status is Pending, pod ready: false</p><p>If you don\&#x27;t see a command prompt, try pressing enter.</p><p>mysql&gt;</p><h4>Updating</h4><p>The image or any other part of the Deployment can be updated as usual with the <strong>kubectl apply</strong>command. Here are some precautions that are specific to stateful apps:</p><ul><li>Don&#x27;t scale the app. This setup is for single-instance apps only. The underlying PersistentVolume can only be mounted to one Pod. For clustered stateful apps, see the <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">StatefulSet documentation</a>.</li><li>Use <strong>strategy:</strong> <strong>type: Recreate</strong> in the Deployment configuration YAML file. This instructs Kubernetes to not use rolling updates. Rolling updates will not work, as you cannot have more than one Pod running at a time. The <strong>Recreate</strong> strategy will stop the first pod before creating a new one with the updated configuration.</li></ul><h4>Deleting a deployment</h4><p>Delete the deployed objects by name:</p><p>kubectl delete deployment,svc mysql</p><p>kubectl delete pvc mysql-pv-claim</p><p>If you manually provisioned a Persistent Volume, you also need to manually delete it, as well as release the underlying resource. If you used a dynamic provisioner, it automatically deletes the Persistent Volume when it sees that you deleted the Persistent Volume Claim. Some dynamic provisioners (such as those for EBS and PD) also release the underlying resource upon deleting the Persistent Volume.</p><h3>Run a Replicated Stateful Application</h3><p>This page shows how to run a replicated stateful application using a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">StatefulSet</a> controller. The example is a MySQL single-master topology with multiple slaves running asynchronous replication.</p><p>Note that <strong>this is not a production configuration</strong>. MySQL settings remain on insecure defaults to keep the focus on general patterns for running stateful applications in Kubernetes.</p><h4>Objectives</h4><ul><li>Deploy a replicated MySQL topology with a StatefulSet controller.</li><li>Send MySQL client traffic.</li><li>Observe resistance to downtime.</li><li>Scale the StatefulSet up and down.</li></ul><h4>Before you begin</h4><ul><li>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using<a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</li><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li><li>To check the version, enter <strong>kubectl version</strong>.</li><li>You need to either have a dynamic PersistentVolume provisioner with a default <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/">StorageClass</a>, or <a href="https://kubernetes.io/docs/user-guide/persistent-volumes/#provisioning">statically provision PersistentVolumes</a> yourself to satisfy the <a href="https://kubernetes.io/docs/user-guide/persistent-volumes/#persistentvolumeclaims">PersistentVolumeClaims</a> used here.</li><li>This tutorial assumes you are familiar with <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">PersistentVolumes</a> and <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">StatefulSets</a>, as well as other core concepts like <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/">Pods</a>, <a href="https://kubernetes.io/docs/concepts/services-networking/service/">Services</a>, and <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/">ConfigMaps</a>.</li><li>Some familiarity with MySQL helps, but this tutorial aims to present general patterns that should be useful for other systems.</li></ul><h4>Deploy MySQL</h4><p>The example MySQL deployment consists of a ConfigMap, two Services, and a StatefulSet.</p><h5>ConfigMap</h5><p>Create the ConfigMap from the following YAML configuration file:</p><p>kubectl create -f <code style="background-color:lightgray">&lt;https://k8s.io/docs/tasks/run-application/mysql-configmap.yaml&gt;</code></p><p>apiVersion: v1</p><p>kind: ConfigMap</p><p>metadata:</p><p>name: mysql</p><p>labels:</p><p>app: mysql</p><p>data:</p><p>master.cnf: |</p><h1>Apply this config only on the master.</h1><p>[mysqld]</p><p>log-bin</p><p>slave.cnf: |</p><h1>Apply this config only on slaves.</h1><p>[mysqld]</p><p>super-read-only</p><p>This ConfigMap provides <strong>my.cnf</strong> overrides that let you independently control configuration on the MySQL master and slaves. In this case, you want the master to be able to serve replication logs to slaves and you want slaves to reject any writes that don&#x27;t come via replication.</p><p>There&#x27;s nothing special about the ConfigMap itself that causes different portions to apply to different Pods. Each Pod decides which portion to look at as it&#x27;s initializing, based on information provided by the StatefulSet controller.</p><h5>Services</h5><p>Create the Services from the following YAML configuration file:</p><p>kubectl create -f <code style="background-color:lightgray">&lt;https://k8s.io/docs/tasks/run-application/mysql-services.yaml&gt;</code></p><p><em># Headless service for stable DNS entries of StatefulSet members.</em></p><p>apiVersion: v1</p><p>kind: Service</p><p>metadata:</p><p>name: mysql</p><p>labels:</p><p>app: mysql</p><p>spec:</p><p>ports:</p><ul><li>name: mysql</li></ul><p>port: 3306</p><p>clusterIP: None</p><p>selector:</p><p>app: mysql</p><hr/><p><em># Client service for connecting to any MySQL instance for reads.</em></p><p><em># For writes, you must instead connect to the master: mysql-0.mysql.</em></p><p>apiVersion: v1</p><p>kind: Service</p><p>metadata:</p><p>name: mysql-read</p><p>labels:</p><p>app: mysql</p><p>spec:</p><p>ports:</p><ul><li>name: mysql</li></ul><p>port: 3306</p><p>selector:</p><p>app: mysql</p><p>The Headless Service provides a home for the DNS entries that the StatefulSet controller creates for each Pod that&#x27;s part of the set. Because the Headless Service is named <strong>mysql</strong>, the Pods are accessible by resolving <code style="background-color:lightgray">&lt;pod-name&gt;.mysql</code> from within any other Pod in the same Kubernetes cluster and namespace.</p><p>The Client Service, called <strong>mysql-read</strong>, is a normal Service with its own cluster IP that distributes connections across all MySQL Pods that report being Ready. The set of potential endpoints includes the MySQL master and all slaves.</p><p>Note that only read queries can use the load-balanced Client Service. Because there is only one MySQL master, clients should connect directly to the MySQL master Pod (through its DNS entry within the Headless Service) to execute writes.</p><h5>StatefulSet</h5><p>Finally, create the StatefulSet from the following YAML configuration file:</p><p>kubectl create -f <code style="background-color:lightgray">&lt;https://k8s.io/docs/tasks/run-application/mysql-statefulset.yaml&gt;</code></p><p>apiVersion: apps/v1</p><p>kind: StatefulSet</p><p>metadata:</p><p>name: mysql</p><p>spec:</p><p>selector:</p><p>matchLabels:</p><p>app: mysql</p><p>serviceName: mysql</p><p>replicas: 3</p><p>template:</p><p>metadata:</p><p>labels:</p><p>app: mysql</p><p>spec:</p><p>initContainers:</p><ul><li>name: init-mysql</li></ul><p>image: mysql:5.7</p><p>command:</p><ul><li><p>bash</p></li><li><p>&quot;-c&quot;</p></li><li><p>|</p></li></ul><p>set -ex</p><h1>Generate mysql server-id from pod ordinal index.</h1><p>[[ <code style="background-color:lightgray">hostname</code> =<!-- -->~<!-- --> -(<!-- -->[0-9]<!-- -->+)$ ]] || exit 1</p><p>ordinal=${BASH_REMATCH<!-- -->[1]<!-- -->}</p><p>echo <!-- -->[mysqld]<!-- --> &gt; /mnt/conf.d/server-id.cnf</p><h1>Add an offset to avoid reserved server-id=0 value.</h1><p>echo server-id=$((100 + $ordinal)) &gt;&gt; /mnt/conf.d/server-id.cnf</p><h1>Copy appropriate conf.d files from config-map to emptyDir.</h1><p>if [<!-- -->[ $ordinal -eq 0 ]<!-- -->]; then</p><p>cp /mnt/config-map/master.cnf /mnt/conf.d/</p><p>else</p><p>cp /mnt/config-map/slave.cnf /mnt/conf.d/</p><p>fi</p><p>volumeMounts:</p><ul><li>name: conf</li></ul><p>mountPath: /mnt/conf.d</p><ul><li>name: config-map</li></ul><p>mountPath: /mnt/config-map</p><ul><li>name: clone-mysql</li></ul><p>image: gcr.io/google-samples/xtrabackup:1.0</p><p>command:</p><ul><li><p>bash</p></li><li><p>&quot;-c&quot;</p></li><li><p>|</p></li></ul><p>set -ex</p><h1>Skip the clone if data already exists.</h1><p>[<!-- -->[ -d /var/lib/mysql/mysql ]<!-- -->] &amp;&amp; exit 0</p><h1>Skip the clone on master (ordinal index 0).</h1><p>[[ <code style="background-color:lightgray">hostname</code> =<!-- -->~<!-- --> -(<!-- -->[0-9]<!-- -->+)$ ]] || exit 1</p><p>ordinal=${BASH_REMATCH<!-- -->[1]<!-- -->}</p><p>[<!-- -->[ $ordinal -eq 0 ]<!-- -->] &amp;&amp; exit 0</p><h1>Clone data from previous peer.</h1><p>ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql</p><h1>Prepare the backup.</h1><p>xtrabackup --prepare --target-dir=/var/lib/mysql</p><p>volumeMounts:</p><ul><li>name: data</li></ul><p>mountPath: /var/lib/mysql</p><p>subPath: mysql</p><ul><li>name: conf</li></ul><p>mountPath: /etc/mysql/conf.d</p><p>containers:</p><ul><li>name: mysql</li></ul><p>image: mysql:5.7</p><p>env:</p><ul><li>name: MYSQL_ALLOW_EMPTY_PASSWORD</li></ul><p>value: &quot;1&quot;</p><p>ports:</p><ul><li>name: mysql</li></ul><p>containerPort: 3306</p><p>volumeMounts:</p><ul><li>name: data</li></ul><p>mountPath: /var/lib/mysql</p><p>subPath: mysql</p><ul><li>name: conf</li></ul><p>mountPath: /etc/mysql/conf.d</p><p>resources:</p><p>requests:</p><p>cpu: 500m</p><p>memory: 1Gi</p><p>livenessProbe:</p><p>exec:</p><p>command: <!-- -->[&quot;mysqladmin&quot;, &quot;ping&quot;]</p><p>initialDelaySeconds: 30</p><p>periodSeconds: 10</p><p>timeoutSeconds: 5</p><p>readinessProbe:</p><p>exec:</p><p><em># Check we can execute queries over TCP (skip-networking is off).</em></p><p>command: <!-- -->[&quot;mysql&quot;, &quot;-h&quot;, &quot;127.0.0.1&quot;, &quot;-e&quot;, &quot;SELECT 1&quot;]</p><p>initialDelaySeconds: 5</p><p>periodSeconds: 2</p><p>timeoutSeconds: 1</p><ul><li>name: xtrabackup</li></ul><p>image: gcr.io/google-samples/xtrabackup:1.0</p><p>ports:</p><ul><li>name: xtrabackup</li></ul><p>containerPort: 3307</p><p>command:</p><ul><li><p>bash</p></li><li><p>&quot;-c&quot;</p></li><li><p>|</p></li></ul><p>set -ex</p><p>cd /var/lib/mysql</p><h1>Determine binlog position of cloned data, if any.</h1><p>if [<!-- -->[ -f xtrabackup_slave_info ]<!-- -->]; then</p><h1>XtraBackup already generated a partial &quot;CHANGE MASTER TO&quot; query</h1><h1>because we\&#x27;re cloning from an existing slave.</h1><p>mv xtrabackup_slave_info change_master_to.sql.in</p><h1>Ignore xtrabackup_binlog_info in this case (it\&#x27;s useless).</h1><p>rm -f xtrabackup_binlog_info</p><p>elif [<!-- -->[ -f xtrabackup_binlog_info ]<!-- -->]; then</p><h1>We\&#x27;re cloning directly from master. Parse binlog position.</h1><p>[[ <code style="background-color:lightgray">cat xtrabackup_binlog_info</code> =<!-- -->~<!-- --> \^(.<!-- -->*<!-- -->?)[<!-- -->[:space:]<!-- -->]+(.<!-- -->*<!-- -->?)$ ]] || exit 1</p><p>rm xtrabackup_binlog_info</p><p>echo &quot;CHANGE MASTER TO MASTER_LOG_FILE=\&#x27;${BASH_REMATCH<!-- -->[1]<!-- -->}\&#x27;,<!-- -->\</p><p>MASTER_LOG_POS=${BASH_REMATCH<!-- -->[2]<!-- -->}&quot; &gt; change_master_to.sql.in</p><p>fi</p><h1>Check if we need to complete a clone by starting replication.</h1><p>if [<!-- -->[ -f change_master_to.sql.in ]<!-- -->]; then</p><p>echo &quot;Waiting for mysqld to be ready (accepting connections)&quot;</p><p>until mysql -h 127.0.0.1 -e &quot;SELECT 1&quot;; do sleep 1; done</p><p>echo &quot;Initializing replication from clone position&quot;</p><h1>In case of container restart, attempt this at-most-once.</h1><p>mv change_master_to.sql.in change_master_to.sql.orig</p><p>mysql -h 127.0.0.1 &lt;&lt;EOF</p><p>$(&lt;change_master_to.sql.orig),</p><p>MASTER_HOST=\&#x27;mysql-0.mysql\&#x27;,</p><p>MASTER_USER=\&#x27;root\&#x27;,</p><p>MASTER_PASSWORD=\&#x27;\&#x27;,</p><p>MASTER_CONNECT_RETRY=10;</p><p>START SLAVE;</p><p>EOF</p><p>fi</p><h1>Start a server to send backups when requested by peers.</h1><p>exec ncat --listen --keep-open --send-only --max-conns=1 3307 -c <!-- -->\</p><p>&quot;xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1 --user=root&quot;</p><p>volumeMounts:</p><ul><li>name: data</li></ul><p>mountPath: /var/lib/mysql</p><p>subPath: mysql</p><ul><li>name: conf</li></ul><p>mountPath: /etc/mysql/conf.d</p><p>resources:</p><p>requests:</p><p>cpu: 100m</p><p>memory: 100Mi</p><p>volumes:</p><ul><li>name: conf</li></ul><p>emptyDir: {}</p><ul><li>name: config-map</li></ul><p>configMap:</p><p>name: mysql</p><p>volumeClaimTemplates:</p><ul><li>metadata:</li></ul><p>name: data</p><p>spec:</p><p>accessModes: <!-- -->[&quot;ReadWriteOnce&quot;]</p><p>resources:</p><p>requests:</p><p>storage: 10Gi</p><p>You can watch the startup progress by running:</p><p>kubectl get pods -l app=mysql --watch</p><p>After a while, you should see all 3 Pods become Running:</p><p>NAME READY STATUS RESTARTS AGE</p><p>mysql-0 2/2 Running 0 2m</p><p>mysql-1 2/2 Running 0 1m</p><p>mysql-2 2/2 Running 0 1m</p><p>Press <strong>Ctrl+C</strong> to cancel the watch. If you don&#x27;t see any progress, make sure you have a dynamic PersistentVolume provisioner enabled as mentioned in the <a href="https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/#before-you-begin">prerequisites</a>.</p><p>This manifest uses a variety of techniques for managing stateful Pods as part of a StatefulSet. The next section highlights some of these techniques to explain what happens as the StatefulSet creates Pods.</p><h4>Understanding stateful Pod initialization</h4><p>The StatefulSet controller starts Pods one at a time, in order by their ordinal index. It waits until each Pod reports being Ready before starting the next one.</p><p>In addition, the controller assigns each Pod a unique, stable name of the form <strong><code>&lt;statefulset-name&gt;-&lt;ordinal-index&gt;</code></strong>. In this case, that results in Pods named <strong>mysql-0</strong>, <strong>mysql-1</strong>, and <strong>mysql-2</strong>.</p><p>The Pod template in the above StatefulSet manifest takes advantage of these properties to perform orderly startup of MySQL replication.</p><h5>Generating configuration</h5><p>Before starting any of the containers in the Pod spec, the Pod first runs any <a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/">Init Containers</a> in the order defined.</p><p>The first Init Container, named <strong>init-mysql</strong>, generates special MySQL config files based on the ordinal index.</p><p>The script determines its own ordinal index by extracting it from the end of the Pod name, which is returned by the <strong>hostname</strong> command. Then it saves the ordinal (with a numeric offset to avoid reserved values) into a file called <strong>server-id.cnf</strong> in the MySQL <strong>conf.d</strong> directory. This translates the unique, stable identity provided by the StatefulSet controller into the domain of MySQL server IDs, which require the same properties.</p><p>The script in the <strong>init-mysql</strong> container also applies either <strong>master.cnf</strong> or <strong>slave.cnf</strong> from the ConfigMap by copying the contents into <strong>conf.d</strong>. Because the example topology consists of a single MySQL master and any number of slaves, the script simply assigns ordinal <strong>0</strong> to be the master, and everyone else to be slaves. Combined with the StatefulSet controller&#x27;s <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#deployment-and-scaling-guarantees/">deployment order guarantee</a>, this ensures the MySQL master is Ready before creating slaves, so they can begin replicating.</p><h5>Cloning existing data</h5><p>In general, when a new Pod joins the set as a slave, it must assume the MySQL master might already have data on it. It also must assume that the replication logs might not go all the way back to the beginning of time. These conservative assumptions are the key to allow a running StatefulSet to scale up and down over time, rather than being fixed at its initial size.</p><p>The second Init Container, named <strong>clone-mysql</strong>, performs a clone operation on a slave Pod the first time it starts up on an empty PersistentVolume. That means it copies all existing data from another running Pod, so its local state is consistent enough to begin replicating from the master.</p><p>MySQL itself does not provide a mechanism to do this, so the example uses a popular open-source tool called Percona XtraBackup. During the clone, the source MySQL server might suffer reduced performance. To minimize impact on the MySQL master, the script instructs each Pod to clone from the Pod whose ordinal index is one lower. This works because the StatefulSet controller always ensures Pod <strong>N</strong> is Ready before starting Pod <strong>N+1</strong>.</p><h5>Starting replication</h5><p>After the Init Containers complete successfully, the regular containers run. The MySQL Pods consist of a <strong>mysql</strong> container that runs the actual <strong>mysqld</strong> server, and an <strong>xtrabackup</strong> container that acts as a<a href="http://blog.kubernetes.io/2015/06/the-distributed-system-toolkit-patterns.html">sidecar</a>.</p><p>The <strong>xtrabackup</strong> sidecar looks at the cloned data files and determines if it&#x27;s necessary to initialize MySQL replication on the slave. If so, it waits for <strong>mysqld</strong> to be ready and then executes the <strong>CHANGE MASTER TO</strong>and <strong>START SLAVE</strong> commands with replication parameters extracted from the XtraBackup clone files.</p><p>Once a slave begins replication, it remembers its MySQL master and reconnects automatically if the server restarts or the connection dies. Also, because slaves look for the master at its stable DNS name (<strong>mysql-0.mysql</strong>), they automatically find the master even if it gets a new Pod IP due to being rescheduled.</p><p>Lastly, after starting replication, the <strong>xtrabackup</strong> container listens for connections from other Pods requesting a data clone. This server remains up indefinitely in case the StatefulSet scales up, or in case the next Pod loses its PersistentVolumeClaim and needs to redo the clone.</p><h4>Sending client traffic</h4><p>You can send test queries to the MySQL master (hostname <strong>mysql-0.mysql</strong>) by running a temporary container with the <strong>mysql:5.7</strong> image and running the <strong>mysql</strong> client binary.</p><p>kubectl run mysql-client --image=mysql:5.7 -i --rm --restart=Never --<!-- -->\</p><p>mysql -h mysql-0.mysql &lt;&lt;EOF</p><p>CREATE DATABASE test;</p><p>CREATE TABLE test.messages (message VARCHAR(250));</p><p>INSERT INTO test.messages VALUES (\&#x27;hello\&#x27;);</p><p>EOF</p><p>Use the hostname <strong>mysql-read</strong> to send test queries to any server that reports being Ready:</p><p>kubectl run mysql-client --image=mysql:5.7 -i -t --rm --restart=Never --<!-- -->\</p><p>mysql -h mysql-read -e &quot;SELECT <!-- -->*<!-- --> FROM test.messages&quot;</p><p>You should get output like this:</p><p>Waiting for pod default/mysql-client to be running, status is Pending, pod ready: false</p><p>+---------+</p><p>| message |</p><p>+---------+</p><p>| hello |</p><p>+---------+</p><p>pod &quot;mysql-client&quot; deleted</p><p>To demonstrate that the <strong>mysql-read</strong> Service distributes connections across servers, you can run <strong>SELECT @@server_id</strong> in a loop:</p><p>kubectl run mysql-client-loop --image=mysql:5.7 -i -t --rm --restart=Never --<!-- -->\</p><p>bash -ic &quot;while sleep 1; do mysql -h mysql-read -e \&#x27;SELECT @@server_id,NOW()\&#x27;; done&quot;</p><p>You should see the reported <strong>@@server_id</strong> change randomly, because a different endpoint might be selected upon each connection attempt:</p><p>+-------------+---------------------+</p><p>| @@server_id | NOW() |</p><p>+-------------+---------------------+</p><p>| 100 | 2006-01-02 15:04:05 |</p><p>+-------------+---------------------+</p><p>+-------------+---------------------+</p><p>| @@server_id | NOW() |</p><p>+-------------+---------------------+</p><p>| 102 | 2006-01-02 15:04:06 |</p><p>+-------------+---------------------+</p><p>+-------------+---------------------+</p><p>| @@server_id | NOW() |</p><p>+-------------+---------------------+</p><p>| 101 | 2006-01-02 15:04:07 |</p><p>+-------------+---------------------+</p><p>You can press <strong>Ctrl+C</strong> when you want to stop the loop, but it&#x27;s useful to keep it running in another window so you can see the effects of the following steps.</p><h4>Simulating Pod and Node downtime</h4><p>To demonstrate the increased availability of reading from the pool of slaves instead of a single server, keep the <strong>SELECT @@server_id</strong> loop from above running while you force a Pod out of the Ready state.</p><h5><strong>Break the Readiness Probe</strong></h5><p>The <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#define-readiness-probes">readiness probe</a> for the <strong>mysql</strong> container runs the command <strong>mysql -h 127.0.0.1 -e \&#x27;SELECT 1\&#x27;</strong>to make sure the server is up and able to execute queries.</p><p>One way to force this readiness probe to fail is to break that command:</p><p>kubectl exec mysql-2 -c mysql -- mv /usr/bin/mysql /usr/bin/mysql.off</p><p>This reaches into the actual container&#x27;s filesystem for Pod <strong>mysql-2</strong> and renames the <strong>mysql</strong> command so the readiness probe can&#x27;t find it. After a few seconds, the Pod should report one of its containers as not Ready, which you can check by running:</p><p>kubectl get pod mysql-2</p><p>Look for <strong>1/2</strong> in the <strong>READY</strong> column:</p><p>NAME READY STATUS RESTARTS AGE</p><p>mysql-2 1/2 Running 0 3m</p><p>At this point, you should see your <strong>SELECT @@server_id</strong> loop continues to run, although it never reports <strong>102</strong> anymore. Recall that the <strong>init-mysql</strong> script defined <strong>server-id</strong> as <strong>100 + $ordinal</strong>, so server ID <strong>102</strong> corresponds to Pod <strong>mysql-2</strong>.</p><p>Now repair the Pod and it should reappear in the loop output after a few seconds:</p><p>kubectl exec mysql-2 -c mysql -- mv /usr/bin/mysql.off /usr/bin/mysql</p><h5>Delete Pods</h5><p>The StatefulSet also recreates Pods if they&#x27;re deleted, similar to what a ReplicaSet does for stateless Pods.</p><p>kubectl delete pod mysql-2</p><p>The StatefulSet controller notices that no <strong>mysql-2</strong> Pod exists anymore, and creates a new one with the same name and linked to the same PersistentVolumeClaim. You should see server ID <strong>102</strong> disappear from the loop output for a while and then return on its own.</p><h5>Drain a Node</h5><p>If your Kubernetes cluster has multiple Nodes, you can simulate Node downtime (such as when Nodes are upgraded) by issuing a <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#drain">drain</a>.</p><p>First determine which Node one of the MySQL Pods is on:</p><p>kubectl get pod mysql-2 -o wide</p><p>The Node name should show up in the last column:</p><p>NAME READY STATUS RESTARTS AGE IP NODE</p><p>mysql-2 2/2 Running 0 15m 10.244.5.27 kubernetes-minion-group-9l2t</p><p>Then drain the Node by running the following command, which cordons it so no new Pods may schedule there, and then evicts any existing Pods. Replace <strong><code>&lt;node-name&gt;</code></strong> with the name of the Node you found in the last step.</p><p>This might impact other applications on the Node, so it&#x27;s best to <strong>only do this in a test cluster</strong>.</p><p>kubectl drain <code style="background-color:lightgray">&lt;node-name&gt;</code> --force --delete-local-data --ignore-daemonsets</p><p>Now you can watch as the Pod reschedules on a different Node:</p><p>kubectl get pod mysql-2 -o wide --watch</p><p>It should look something like this:</p><p>NAME READY STATUS RESTARTS AGE IP NODE</p><p>mysql-2 2/2 Terminating 0 15m 10.244.1.56 kubernetes-minion-group-9l2t</p><p>[.<!-- -->..]</p><p>mysql-2 0/2 Pending 0 0s <code style="background-color:lightgray">&lt;none&gt;</code> kubernetes-minion-group-fjlm</p><p>mysql-2 0/2 Init:0/2 0 0s <code style="background-color:lightgray">&lt;none&gt;</code> kubernetes-minion-group-fjlm</p><p>mysql-2 0/2 Init:1/2 0 20s 10.244.5.32 kubernetes-minion-group-fjlm</p><p>mysql-2 0/2 PodInitializing 0 21s 10.244.5.32 kubernetes-minion-group-fjlm</p><p>mysql-2 1/2 Running 0 22s 10.244.5.32 kubernetes-minion-group-fjlm</p><p>mysql-2 2/2 Running 0 30s 10.244.5.32 kubernetes-minion-group-fjlm</p><p>And again, you should see server ID <strong>102</strong> disappear from the <strong>SELECT @@server_id</strong> loop output for a while and then return.</p><p>Now uncordon the Node to return it to a normal state:</p><p>kubectl uncordon <code style="background-color:lightgray">&lt;node-name&gt;</code></p><h4>Scaling the number of slaves</h4><p>With MySQL replication, you can scale your read query capacity by adding slaves. With StatefulSet, you can do this with a single command:</p><p><strong>kubectl scale statefulset mysql --replicas=5</strong></p><p>Watch the new Pods come up by running:</p><p><strong>kubectl get pods -l app=mysql --watch</strong></p><p>Once they&#x27;re up, you should see server IDs <strong>103</strong> and <strong>104</strong> start appearing in the <strong>SELECT @@server_id</strong>loop output.</p><p>You can also verify that these new servers have the data you added before they existed:</p><p><strong>kubectl run mysql-client --image=mysql:5.7 -i -t --rm --restart=Never --<!-- -->\</strong></p><p><strong>mysql -h mysql-3.mysql -e &quot;SELECT <!-- -->*<!-- --> FROM test.messages&quot;</strong></p><p><strong>Waiting for pod default/mysql-client to be running, status is Pending, pod ready: false</strong></p><p><strong>+---------+</strong></p><p><strong>| message |</strong></p><p><strong>+---------+</strong></p><p><strong>| hello |</strong></p><p><strong>+---------+</strong></p><p><strong>pod &quot;mysql-client&quot; deleted</strong></p><p>Scaling back down is also seamless:</p><p><strong>kubectl scale statefulset mysql --replicas=3</strong></p><p>Note, however, that while scaling up creates new PersistentVolumeClaims automatically, scaling down does not automatically delete these PVCs. This gives you the choice to keep those initialized PVCs around to make scaling back up quicker, or to extract data before deleting them.</p><p>You can see this by running:</p><p><strong>kubectl get pvc -l app=mysql</strong></p><p>Which shows that all 5 PVCs still exist, despite having scaled the StatefulSet down to 3:</p><p>NAME STATUS VOLUME CAPACITY ACCESSMODES AGE</p><p>data-mysql-0 Bound pvc-8acbf5dc-b103-11e6-93fa-42010a800002 10Gi RWO 20m</p><p>data-mysql-1 Bound pvc-8ad39820-b103-11e6-93fa-42010a800002 10Gi RWO 20m</p><p>data-mysql-2 Bound pvc-8ad69a6d-b103-11e6-93fa-42010a800002 10Gi RWO 20m</p><p>data-mysql-3 Bound pvc-50043c45-b1c5-11e6-93fa-42010a800002 10Gi RWO 2m</p><p>data-mysql-4 Bound pvc-500a9957-b1c5-11e6-93fa-42010a800002 10Gi RWO 2m</p><p>If you don&#x27;t intend to reuse the extra PVCs, you can delete them:</p><p>kubectl delete pvc data-mysql-3</p><p>kubectl delete pvc data-mysql-4</p><h4>Cleaning up</h4><p>Cancel the <strong>SELECT @@server_id</strong> loop by pressing <strong>Ctrl+C</strong> in its terminal, or running the following from another terminal:</p><p>kubectl delete pod mysql-client-loop --now</p><p>Delete the StatefulSet. This also begins terminating the Pods.</p><p>kubectl delete statefulset mysql</p><p>Verify that the Pods disappear. They might take some time to finish terminating.</p><p>kubectl get pods -l app=mysql</p><p>You&#x27;ll know the Pods have terminated when the above returns:</p><p><strong>No resources found.</strong></p><p>Delete the ConfigMap, Services, and PersistentVolumeClaims.</p><p>kubectl delete configmap,service,pvc -l app=mysql</p><p>If you manually provisioned PersistentVolumes, you also need to manually delete them, as well as release the underlying resources. If you used a dynamic provisioner, it automatically deletes the PersistentVolumes when it sees that you deleted the PersistentVolumeClaims. Some dynamic provisioners (such as those for EBS and PD) also release the underlying resources upon deleting the PersistentVolumes.</p><h3>Example: Deploying WordPress and MySQL with Persistent Volumes</h3><p>This tutorial shows you how to deploy a WordPress site and a MySQL database using Minikube. Both applications use PersistentVolumes and PersistentVolumeClaims to store data.</p><p>A <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">PersistentVolume</a> (PV) is a piece of storage in the cluster that has been manually provisioned by an administrator, or dynamically provisioned by Kubernetes using a <a href="https://kubernetes.io/docs/concepts/storage/storage-classes">StorageClass</a>. A <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">PersistentVolumeClaim</a> (PVC) is a request for storage by a user that can be fulfilled by a PV. PersistentVolumes and PersistentVolumeClaims are independent from Pod lifecycles and preserve data through restarting, rescheduling, and even deleting Pods.</p><p><strong>Warning:</strong> This deployment is not suitable for production use cases, as it uses single instance WordPress and MySQL Pods. Consider using <a href="https://github.com/kubernetes/charts/tree/master/stable/wordpress">WordPress Helm Chart</a> to deploy WordPress in production.</p><p><strong>Note:</strong> The files provided in this tutorial are using GA Deployment APIs and are specific to kubernetes version 1.9 and later. If you wish to use this tutorial with an earlier version of Kubernetes, please update the API version appropriately, or reference earlier versions of this tutorial.</p><ul><li><a href="https://kubernetes.io/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/#objectives"><strong>Objectives</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/#create-persistentvolumeclaims-and-persistentvolumes"><strong>Create PersistentVolumeClaims and PersistentVolumes</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/#create-a-secret-for-mysql-password"><strong>Create a Secret for MySQL Password</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/#deploy-mysql"><strong>Deploy MySQL</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/#deploy-wordpress"><strong>Deploy WordPress</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/#cleaning-up"><strong>Cleaning up</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Objectives</h4><ul><li>Create PersistentVolumeClaims and PersistentVolumes</li><li>Create a Secret</li><li>Deploy MySQL</li><li>Deploy WordPress</li><li>Clean up</li></ul><h4>Before you begin</h4><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using<a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <strong>kubectl version</strong>.</p><p>Download the following configuration files:</p><ol><li><a href="https://kubernetes.io/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/mysql-deployment.yaml">mysql-deployment.yaml</a></li><li><a href="https://kubernetes.io/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/wordpress-deployment.yaml">wordpress-deployment.yaml</a></li></ol><h4>Create PersistentVolumeClaims and PersistentVolumes</h4><p>MySQL and Wordpress each require a PersistentVolume to store data. Their PersistentVolumeClaims will be created at the deployment step.</p><p>Many cluster environments have a default StorageClass installed. When a StorageClass is not specified in the PersistentVolumeClaim, the cluster&#x27;s default StorageClass is used instead.</p><p>When a PersistentVolumeClaim is created, a PersistentVolume is dynamically provisioned based on the StorageClass configuration.</p><p><strong>Warning:</strong> In local clusters, the default StorageClass uses the <strong>hostPath</strong> provisioner.<strong>hostPath</strong> volumes are only suitable for development and testing. With <strong>hostPath</strong> volumes, your data lives in <strong>/tmp</strong> on the node the Pod is scheduled onto and does not move between nodes. If a Pod dies and gets scheduled to another node in the cluster, or the node is rebooted, the data is lost.</p><p><strong>Note:</strong> If you are bringing up a cluster that needs to use the <strong>hostPath</strong> provisioner, the <strong>--enable-hostpath-provisioner</strong> flag must be set in the <strong>controller-manager</strong>component.</p><p><strong>Note:</strong> If you have a Kubernetes cluster running on Google Kubernetes Engine, please follow <a href="https://cloud.google.com/kubernetes-engine/docs/tutorials/persistent-disk">this guide</a>.</p><h4>Create a Secret for MySQL Password</h4><p>A <a href="https://kubernetes.io/docs/concepts/configuration/secret/">Secret</a> is an object that stores a piece of sensitive data like a password or key. The manifest files are already configured to use a Secret, but you have to create your own Secret.</p><ol><li>Create the Secret object from the following command:</li><li><strong>kubectl create secret generic mysql-pass --from-literal=password=YOUR_PASSWORD</strong></li></ol><p><strong>Note:</strong> Replace <strong>YOUR_PASSWORD</strong> with the password you want to apply.</p><ol><li>Verify that the Secret exists by running the following command:</li><li><strong>kubectl get secrets</strong></li></ol><p>The response should be like this:</p><p><strong>NAME TYPE DATA AGE</strong></p><p><strong>mysql-pass Opaque 1 42s</strong></p><p><strong>Note:</strong> To protect the Secret from exposure, neither <strong>get</strong> nor <strong>describe</strong> show its contents.</p><h4>Deploy MySQL</h4><p>The following manifest describes a single-instance MySQL Deployment. The MySQL container mounts the PersistentVolume at /var/lib/mysql. The <strong>MYSQL_ROOT_PASSWORD</strong> environment variable sets the database password from the Secret.</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>mysql                                                              |
| -wordpress-persistent-volume/mysql-deployment.yaml</strong> ]<!-- -->(<a href="https://raw.gi">https://raw.gi</a> |
| thubusercontent.com/kubernetes/website/master/docs/tutorials/stateful |
| -application/mysql-wordpress-persistent-volume/mysql-deployment.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Service</strong>                                                     |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: wordpress-mysql</strong>                                             |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: wordpress</strong>                                                    |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- port: 3306</strong>                                                      |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>app: wordpress</strong>                                                    |
|                                                                       |
| <strong>tier: mysql</strong>                                                       |
|                                                                       |
| <strong>clusterIP: None</strong>                                                   |
|                                                                       |
| <strong>---</strong>                                                             |
|                                                                       |
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: PersistentVolumeClaim</strong>                                       |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: mysql-pv-claim</strong>                                              |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: wordpress</strong>                                                    |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>accessModes:</strong>                                                      |
|                                                                       |
| <strong>- ReadWriteOnce</strong>                                                   |
|                                                                       |
| <strong>resources:</strong>                                                        |
|                                                                       |
| <strong>requests:</strong>                                                         |
|                                                                       |
| <strong>storage: 20Gi</strong>                                                     |
|                                                                       |
| <strong>---</strong>                                                             |
|                                                                       |
| <strong>apiVersion: apps/v1 <em># for versions before 1.9.0 use               |
| apps/v1beta2</em></strong>                                                       |
|                                                                       |
| <strong>kind: Deployment</strong>                                                  |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: wordpress-mysql</strong>                                             |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: wordpress</strong>                                                    |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>matchLabels:</strong>                                                      |
|                                                                       |
| <strong>app: wordpress</strong>                                                    |
|                                                                       |
| <strong>tier: mysql</strong>                                                       |
|                                                                       |
| <strong>strategy:</strong>                                                         |
|                                                                       |
| <strong>type: Recreate</strong>                                                    |
|                                                                       |
| <strong>template:</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: wordpress</strong>                                                    |
|                                                                       |
| <strong>tier: mysql</strong>                                                       |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- image: mysql:5.6</strong>                                                |
|                                                                       |
| <strong>name: mysql</strong>                                                       |
|                                                                       |
| <strong>env:</strong>                                                              |
|                                                                       |
| <strong>- name: MYSQL_ROOT_PASSWORD</strong>                                       |
|                                                                       |
| <strong>valueFrom:</strong>                                                        |
|                                                                       |
| <strong>secretKeyRef:</strong>                                                     |
|                                                                       |
| <strong>name: mysql-pass</strong>                                                  |
|                                                                       |
| <strong>key: password</strong>                                                     |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- containerPort: 3306</strong>                                             |
|                                                                       |
| <strong>name: mysql</strong>                                                       |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- name: mysql-persistent-storage</strong>                                  |
|                                                                       |
| <strong>mountPath: /var/lib/mysql</strong>                                         |
|                                                                       |
| <strong>volumes:</strong>                                                          |
|                                                                       |
| <strong>- name: mysql-persistent-storage</strong>                                  |
|                                                                       |
| <strong>persistentVolumeClaim:</strong>                                            |
|                                                                       |
| <strong>claimName: mysql-pv-claim</strong>                                         |
+-----------------------------------------------------------------------+</p><ol><li>Deploy MySQL from the <strong>mysql-deployment.yaml</strong> file:</li><li><strong>kubectl create -f mysql-deployment.yaml</strong></li><li>Verify that a PersistentVolume got dynamically provisioned:</li><li><strong>kubectl get pvc</strong></li></ol><p><strong>Note:</strong> It can take up to a few minutes for the PVs to be provisioned and bound.</p><p>The response should be like this:</p><p><strong>NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE</strong></p><p><strong>mysql-pv-claim Bound pvc-91e44fbf-d477-11e7-ac6a-42010a800002 20Gi RWO standard 29s</strong></p><ol><li>Verify that the Pod is running by running the following command:</li><li><strong>kubectl get pods</strong></li></ol><p><strong>Note:</strong> It can take up to a few minutes for the Pod&#x27;s Status to be <strong>RUNNING</strong>.</p><p>The response should be like this:</p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>wordpress-mysql-1894417608-x5dzt 1/1 Running 0 40s</strong></p><h4>Deploy WordPress</h4><p>The following manifest describes a single-instance WordPress Deployment and Service. It uses many of the same features like a PVC for persistent storage and a Secret for the password. But it also uses a different setting: <strong>type: NodePort</strong>. This setting exposes WordPress to traffic from outside of the cluster.</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>mysql-wordpre                                                      |
| ss-persistent-volume/wordpress-deployment.yaml</strong> ]<!-- -->(<a href="https://raw.github">https://raw.github</a> |
| usercontent.com/kubernetes/website/master/docs/tutorials/stateful-app |
| lication/mysql-wordpress-persistent-volume/wordpress-deployment.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Service</strong>                                                     |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: wordpress</strong>                                                   |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: wordpress</strong>                                                    |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- port: 80</strong>                                                        |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>app: wordpress</strong>                                                    |
|                                                                       |
| <strong>tier: frontend</strong>                                                    |
|                                                                       |
| <strong>type: LoadBalancer</strong>                                                |
|                                                                       |
| <strong>---</strong>                                                             |
|                                                                       |
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: PersistentVolumeClaim</strong>                                       |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: wp-pv-claim</strong>                                                 |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: wordpress</strong>                                                    |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>accessModes:</strong>                                                      |
|                                                                       |
| <strong>- ReadWriteOnce</strong>                                                   |
|                                                                       |
| <strong>resources:</strong>                                                        |
|                                                                       |
| <strong>requests:</strong>                                                         |
|                                                                       |
| <strong>storage: 20Gi</strong>                                                     |
|                                                                       |
| <strong>---</strong>                                                             |
|                                                                       |
| <strong>apiVersion: apps/v1 <em># for versions before 1.9.0 use               |
| apps/v1beta2</em></strong>                                                       |
|                                                                       |
| <strong>kind: Deployment</strong>                                                  |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: wordpress</strong>                                                   |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: wordpress</strong>                                                    |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>matchLabels:</strong>                                                      |
|                                                                       |
| <strong>app: wordpress</strong>                                                    |
|                                                                       |
| <strong>tier: frontend</strong>                                                    |
|                                                                       |
| <strong>strategy:</strong>                                                         |
|                                                                       |
| <strong>type: Recreate</strong>                                                    |
|                                                                       |
| <strong>template:</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: wordpress</strong>                                                    |
|                                                                       |
| <strong>tier: frontend</strong>                                                    |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- image: wordpress:4.8-apache</strong>                                     |
|                                                                       |
| <strong>name: wordpress</strong>                                                   |
|                                                                       |
| <strong>env:</strong>                                                              |
|                                                                       |
| <strong>- name: WORDPRESS_DB_HOST</strong>                                         |
|                                                                       |
| <strong>value: wordpress-mysql</strong>                                            |
|                                                                       |
| <strong>- name: WORDPRESS_DB_PASSWORD</strong>                                     |
|                                                                       |
| <strong>valueFrom:</strong>                                                        |
|                                                                       |
| <strong>secretKeyRef:</strong>                                                     |
|                                                                       |
| <strong>name: mysql-pass</strong>                                                  |
|                                                                       |
| <strong>key: password</strong>                                                     |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- containerPort: 80</strong>                                               |
|                                                                       |
| <strong>name: wordpress</strong>                                                   |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- name: wordpress-persistent-storage</strong>                              |
|                                                                       |
| <strong>mountPath: /var/www/html</strong>                                          |
|                                                                       |
| <strong>volumes:</strong>                                                          |
|                                                                       |
| <strong>- name: wordpress-persistent-storage</strong>                              |
|                                                                       |
| <strong>persistentVolumeClaim:</strong>                                            |
|                                                                       |
| <strong>claimName: wp-pv-claim</strong>                                            |
+-----------------------------------------------------------------------+</p><ol><li>Create a WordPress Service and Deployment from the <strong>wordpress-deployment.yaml</strong> file:</li><li><strong>kubectl create -f wordpress-deployment.yaml</strong></li><li>Verify that a PersistentVolume got dynamically provisioned:</li><li><strong>kubectl get pvc</strong></li></ol><p><strong>Note:</strong> It can take up to a few minutes for the PVs to be provisioned and bound.</p><p>The response should be like this:</p><p><strong>NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE</strong></p><p><strong>wp-pv-claim Bound pvc-e69d834d-d477-11e7-ac6a-42010a800002 20Gi RWO standard 7s</strong></p><ol><li>Verify that the Service is running by running the following command:</li><li><strong>kubectl get services wordpress</strong></li></ol><p>The response should be like this:</p><p><strong>NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE</strong></p><p><strong>wordpress 10.0.0.89 <code>&lt;pending&gt;</code> 80:32406/TCP 4m</strong></p><p><strong>Note:</strong> Minikube can only expose Services through <strong>NodePort</strong>. \
\
The <strong>EXTERNAL-IP</strong> is always <strong><code>&lt;pending&gt;</code></strong>.</p><ol><li>Run the following command to get the IP Address for the WordPress Service:</li><li><strong>minikube service wordpress --url</strong></li></ol><p>The response should be like this:</p><p><strong><a href="http://1.2.3.4:32406">http://1.2.3.4:32406</a></strong></p><ol><li>Copy the IP address, and load the page in your browser to view your site.</li></ol><p>You should see the WordPress set up page similar to the following screenshot.</p><p><strong>Warning:</strong> Do not leave your WordPress installation on this page. If another user finds it, they can set up a website on your instance and use it to serve malicious content. \
\
Either install WordPress by creating a username and password or delete your instance.</p><h4>Cleaning up</h4><ol><li>Run the following command to delete your Secret:</li><li><strong>kubectl delete secret mysql-pass</strong></li><li>Run the following commands to delete all Deployments and Services:</li><li><strong>kubectl delete deployment -l app=wordpress</strong></li><li><strong>kubectl delete service -l app=wordpress</strong></li><li>Run the following commands to delete the PersistentVolumeClaims. The dynamically provisioned PersistentVolumes will be automatically deleted.</li><li><strong>kubectl delete pvc -l app=wordpress</strong></li></ol><h4>What&#x27;s next</h4><ul><li>Learn more about <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application-introspection/">Introspection and Debugging</a></li><li>Learn more about <a href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/">Jobs</a></li><li>Learn more about <a href="https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/">Port Forwarding</a></li><li>Learn how to <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/get-shell-running-container/">Get a Shell to a Container</a></li></ul><h3>Example: Deploying Cassandra with Stateful Sets</h3><p>This tutorial shows you how to develop a native cloud <a href="http://cassandra.apache.org/">Cassandra</a> deployment on Kubernetes. In this instance, a custom Cassandra <strong>SeedProvider</strong> enables Cassandra to discover new Cassandra nodes as they join the cluster.</p><p>Deploying stateful distributed applications, like Cassandra, within a clustered environment can be challenging. StatefulSets greatly simplify this process. Please read about <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">StatefulSets</a> for more information about the features used in this tutorial.</p><p><strong>Cassandra Docker</strong></p><p>The Pods use the <a href="https://github.com/kubernetes/examples/blob/master/cassandra/image/Dockerfile"><strong>gcr.io/google-samples/cassandra:v13</strong></a> image from Google&#x27;s <a href="https://cloud.google.com/container-registry/docs/">container registry</a>. The docker image above is based on <a href="https://github.com/kubernetes/kubernetes/tree/master/build/debian-base">debian-base</a> and includes OpenJDK 8. This image includes a standard Cassandra installation from the Apache Debian repo. By using environment variables you can change values that are inserted into <strong>cassandra.yaml</strong>.</p><p>  ENV VAR                  DEFAULT VALUE</p><hr/><p>  CASSANDRA_CLUSTER_NAME   &#x27;Test Cluster&#x27;
CASSANDRA_NUM_TOKENS     32
CASSANDRA_RPC_ADDRESS    0.0.0.0</p><ul><li><a href="https://kubernetes.io/docs/tutorials/stateful-application/cassandra/#objectives"><strong>Objectives</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/stateful-application/cassandra/#before-you-begin"><strong>Before you begin</strong></a><ul><li><a href="https://kubernetes.io/docs/tutorials/stateful-application/cassandra/#additional-minikube-setup-instructions"><strong>Additional Minikube Setup Instructions</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tutorials/stateful-application/cassandra/#creating-a-cassandra-headless-service"><strong>Creating a Cassandra Headless Service</strong></a><ul><li><a href="https://kubernetes.io/docs/tutorials/stateful-application/cassandra/#validating-optional"><strong>Validating (optional)</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tutorials/stateful-application/cassandra/#using-a-statefulset-to-create-a-cassandra-ring"><strong>Using a StatefulSet to Create a Cassandra Ring</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/stateful-application/cassandra/#validating-the-cassandra-statefulset"><strong>Validating The Cassandra StatefulSet</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/stateful-application/cassandra/#modifying-the-cassandra-statefulset"><strong>Modifying the Cassandra StatefulSet</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/stateful-application/cassandra/#cleaning-up"><strong>Cleaning up</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/stateful-application/cassandra/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Objectives</h4><ul><li>Create and Validate a Cassandra headless <a href="https://kubernetes.io/docs/concepts/services-networking/service/">Services</a>.</li><li>Use a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">StatefulSet</a> to create a Cassandra ring.</li><li>Validate the <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">StatefulSet</a>.</li><li>Modify the <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">StatefulSet</a>.</li><li>Delete the <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">StatefulSet</a> and its <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/">Pods</a>.</li></ul><h4>Before you begin</h4><p>To complete this tutorial, you should already have a basic familiarity with <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/">Pods</a>, <a href="https://kubernetes.io/docs/concepts/services-networking/service/">Services</a>, and <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">StatefulSets</a>. In addition, you should:</p><ul><li><a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">Install and Configure</a> the <strong>kubectl</strong> command line</li><li>Download <a href="https://kubernetes.io/docs/tutorials/stateful-application/cassandra/cassandra-service.yaml">cassandra-service.yaml</a> and <a href="https://kubernetes.io/docs/tutorials/stateful-application/cassandra/cassandra-statefulset.yaml">cassandra-statefulset.yaml</a></li><li>Have a supported Kubernetes Cluster running</li></ul><p><strong>Note:</strong> Please read the <a href="https://kubernetes.io/docs/setup/pick-right-solution/">getting started guides</a> if you do not already have a cluster.</p><h5><strong>Additional Minikube Setup Instructions</strong></h5><p><strong>Caution:</strong> <a href="https://kubernetes.io/docs/getting-started-guides/minikube/">Minikube</a> defaults to 1024MB of memory and 1 CPU which results in an insufficient resource errors during this tutorial.</p><p>To avoid these errors, run minikube with:</p><p><strong>minikube start --memory 5120 --cpus=4</strong></p><h4>Creating a Cassandra Headless Service</h4><p>A Kubernetes <a href="https://kubernetes.io/docs/concepts/services-networking/service/">Service</a> describes a set of <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/">Pods</a> that perform the same task.</p><p>The following <strong>Service</strong> is used for DNS lookups between Cassandra Pods and clients within the Kubernetes Cluster.</p><ol><li>Launch a terminal window in the directory you downloaded the manifest files.</li><li>Create a <strong>Service</strong> to track all Cassandra StatefulSet Nodes from the <strong>cassandra-service.yaml</strong> file:</li><li><strong>kubectl create -f cassandra-service.yaml</strong></li></ol><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>cassandra/cassandra-service.                                       |
| yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/website/master/">https://raw.githubusercontent.com/kubernetes/website/master/</a> |
| docs/tutorials/stateful-application/cassandra/cassandra-service.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Service</strong>                                                     |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: cassandra</strong>                                                    |
|                                                                       |
| <strong>name: cassandra</strong>                                                   |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>clusterIP: None</strong>                                                   |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- port: 9042</strong>                                                      |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>app: cassandra</strong>                                                    |
+-----------------------------------------------------------------------+</p><h5><strong>Validating (optional)</strong></h5><p>Get the Cassandra <strong>Service</strong>.</p><p><strong>kubectl get svc cassandra</strong></p><p>The response should be</p><p><strong>NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE</strong></p><p><strong>cassandra None <code>&lt;none&gt;</code> 9042/TCP 45s</strong></p><p>If anything else returns, the service was not successfully created. Read <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/">Debug Services</a> for common issues.</p><h4>Using a StatefulSet to Create a Cassandra Ring</h4><p>The StatefulSet manifest, included below, creates a Cassandra ring that consists of three Pods.</p><p><strong>Note:</strong> This example uses the default provisioner for Minikube. Please update the following StatefulSet for the cloud you are working with.</p><ol><li>Update the StatefulSet if necessary.</li><li>Create the Cassandra StatefulSet from the <strong>cassandra-statefulset.yaml</strong> file:</li><li><strong>kubectl create -f cassandra-statefulset.yaml</strong></li></ol><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>cassandra/cassandra-statefulset.yaml                               |
| </strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/website/master/docs">https://raw.githubusercontent.com/kubernetes/website/master/docs</a> |
| /tutorials/stateful-application/cassandra/cassandra-statefulset.yaml) |
+=======================================================================+
| <strong>apiVersion: apps/v1</strong>                                               |
|                                                                       |
| <strong>kind: StatefulSet</strong>                                                 |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: cassandra</strong>                                                   |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: cassandra</strong>                                                    |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>serviceName: cassandra</strong>                                            |
|                                                                       |
| <strong>replicas: 3</strong>                                                       |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>matchLabels:</strong>                                                      |
|                                                                       |
| <strong>app: cassandra</strong>                                                    |
|                                                                       |
| <strong>template:</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: cassandra</strong>                                                    |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>terminationGracePeriodSeconds: 1800</strong>                               |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: cassandra</strong>                                                 |
|                                                                       |
| <strong>image: gcr.io/google-samples/cassandra:v13</strong>                        |
|                                                                       |
| <strong>imagePullPolicy: Always</strong>                                           |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- containerPort: 7000</strong>                                             |
|                                                                       |
| <strong>name: intra-node</strong>                                                  |
|                                                                       |
| <strong>- containerPort: 7001</strong>                                             |
|                                                                       |
| <strong>name: tls-intra-node</strong>                                              |
|                                                                       |
| <strong>- containerPort: 7199</strong>                                             |
|                                                                       |
| <strong>name: jmx</strong>                                                         |
|                                                                       |
| <strong>- containerPort: 9042</strong>                                             |
|                                                                       |
| <strong>name: cql</strong>                                                         |
|                                                                       |
| <strong>resources:</strong>                                                        |
|                                                                       |
| <strong>limits:</strong>                                                           |
|                                                                       |
| <strong>cpu: &quot;500m&quot;</strong>                                                     |
|                                                                       |
| <strong>memory: 1Gi</strong>                                                       |
|                                                                       |
| <strong>requests:</strong>                                                         |
|                                                                       |
| <strong>cpu: &quot;500m&quot;</strong>                                                     |
|                                                                       |
| <strong>memory: 1Gi</strong>                                                       |
|                                                                       |
| <strong>securityContext:</strong>                                                  |
|                                                                       |
| <strong>capabilities:</strong>                                                     |
|                                                                       |
| <strong>add:</strong>                                                              |
|                                                                       |
| <strong>- IPC_LOCK</strong>                                                        |
|                                                                       |
| <strong>lifecycle:</strong>                                                        |
|                                                                       |
| <strong>preStop:</strong>                                                          |
|                                                                       |
| <strong>exec:</strong>                                                             |
|                                                                       |
| <strong>command:</strong>                                                          |
|                                                                       |
| <strong>- /bin/sh</strong>                                                         |
|                                                                       |
| <strong>- -c</strong>                                                              |
|                                                                       |
| <strong>- nodetool drain</strong>                                                  |
|                                                                       |
| <strong>env:</strong>                                                              |
|                                                                       |
| <strong>- name: MAX_HEAP_SIZE</strong>                                             |
|                                                                       |
| <strong>value: 512M</strong>                                                       |
|                                                                       |
| <strong>- name: HEAP_NEWSIZE</strong>                                              |
|                                                                       |
| <strong>value: 100M</strong>                                                       |
|                                                                       |
| <strong>- name: CASSANDRA_SEEDS</strong>                                           |
|                                                                       |
| <strong>value: &quot;cassandra-0.cassandra.default.svc.cluster.local&quot;</strong>        |
|                                                                       |
| <strong>- name: CASSANDRA_CLUSTER_NAME</strong>                                    |
|                                                                       |
| <strong>value: &quot;K8Demo&quot;</strong>                                                 |
|                                                                       |
| <strong>- name: CASSANDRA_DC</strong>                                              |
|                                                                       |
| <strong>value: &quot;DC1-K8Demo&quot;</strong>                                             |
|                                                                       |
| <strong>- name: CASSANDRA_RACK</strong>                                            |
|                                                                       |
| <strong>value: &quot;Rack1-K8Demo&quot;</strong>                                           |
|                                                                       |
| <strong>- name: POD_IP</strong>                                                    |
|                                                                       |
| <strong>valueFrom:</strong>                                                        |
|                                                                       |
| <strong>fieldRef:</strong>                                                         |
|                                                                       |
| <strong>fieldPath: status.podIP</strong>                                           |
|                                                                       |
| <strong>readinessProbe:</strong>                                                   |
|                                                                       |
| <strong>exec:</strong>                                                             |
|                                                                       |
| <strong>command:</strong>                                                          |
|                                                                       |
| <strong>- /bin/bash</strong>                                                       |
|                                                                       |
| <strong>- -c</strong>                                                              |
|                                                                       |
| <strong>- /ready-probe.sh</strong>                                                 |
|                                                                       |
| <strong>initialDelaySeconds: 15</strong>                                           |
|                                                                       |
| <strong>timeoutSeconds: 5</strong>                                                 |
|                                                                       |
| <strong><em># These volume mounts are persistent. They are like inline        |
| claims,</em></strong>                                                            |
|                                                                       |
| <strong><em># but not exactly because the names need to match exactly one     |
| of</em></strong>                                                                 |
|                                                                       |
| <strong><em># the stateful pod volumes.</em></strong>                                    |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- name: cassandra-data</strong>                                            |
|                                                                       |
| <strong>mountPath: /cassandra_data</strong>                                        |
|                                                                       |
| <strong><em># These are converted to volume claims by the controller</em></strong>       |
|                                                                       |
| <strong><em># and mounted at the paths mentioned above.</em></strong>                    |
|                                                                       |
| <strong><em># do not use these in production until ssd GCEPersistentDisk or   |
| other ssd pd</em></strong>                                                       |
|                                                                       |
| <strong>volumeClaimTemplates:</strong>                                             |
|                                                                       |
| <strong>- metadata:</strong>                                                       |
|                                                                       |
| <strong>name: cassandra-data</strong>                                              |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>accessModes: <!-- -->[ &quot;ReadWriteOnce&quot; ]</strong>                              |
|                                                                       |
| <strong>storageClassName: fast</strong>                                            |
|                                                                       |
| <strong>resources:</strong>                                                        |
|                                                                       |
| <strong>requests:</strong>                                                         |
|                                                                       |
| <strong>storage: 1Gi</strong>                                                      |
|                                                                       |
| <strong>---</strong>                                                             |
|                                                                       |
| <strong>kind: StorageClass</strong>                                                |
|                                                                       |
| <strong>apiVersion: storage.k8s.io/v1</strong>                                     |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: fast</strong>                                                        |
|                                                                       |
| <strong>provisioner: k8s.io/minikube-hostpath</strong>                             |
|                                                                       |
| <strong>parameters:</strong>                                                       |
|                                                                       |
| <strong>type: pd-ssd</strong>                                                      |
+-----------------------------------------------------------------------+</p><h4>Validating The Cassandra StatefulSet</h4><ol><li>Get the Cassandra StatefulSet:</li><li><strong>kubectl get statefulset cassandra</strong></li></ol><p>The response should be</p><p><strong>NAME DESIRED CURRENT AGE</strong></p><p><strong>cassandra 3 0 13s</strong></p><p>The StatefulSet resource deploys Pods sequentially.</p><ol><li>Get the Pods to see the ordered creation status:</li><li><strong>kubectl get pods -l=&quot;app=cassandra&quot;</strong></li></ol><p>The response should be</p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>cassandra-0 1/1 Running 0 1m</strong></p><p><strong>cassandra-1 0/1 ContainerCreating 0 8s</strong></p><p><strong>Note:</strong> It can take up to ten minutes for all three Pods to deploy.</p><p>Once all Pods are deployed, the same command returns:</p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>cassandra-0 1/1 Running 0 10m</strong></p><p><strong>cassandra-1 1/1 Running 0 9m</strong></p><p><strong>cassandra-2 1/1 Running 0 8m</strong></p><ol><li>Run the Cassandra utility nodetool to display the status of the ring.</li><li><strong>kubectl exec cassandra-0 -- nodetool status</strong></li></ol><p>The response is:</p><p><strong>Datacenter: DC1-K8Demo</strong></p><p><strong>======================</strong></p><p><strong>Status=Up/Down</strong></p><p><strong>|/ State=Normal/Leaving/Joining/Moving</strong></p><p><strong>-- Address Load Tokens Owns (effective) Host ID Rack</strong></p><p><strong>UN 172.17.0.5 83.57 KiB 32 74.0% e2dd09e6-d9d3-477e-96c5-45094c08db0f Rack1-K8Demo</strong></p><p><strong>UN 172.17.0.4 101.04 KiB 32 58.8% f89d6835-3a42-4419-92b3-0e62cae1479c Rack1-K8Demo</strong></p><p><strong>UN 172.17.0.6 84.74 KiB 32 67.1% a6a1e8c2-3dc5-4417-b1a0-26507af2aaad Rack1-K8Demo</strong></p><h4>Modifying the Cassandra StatefulSet</h4><p>Use <strong>kubectl edit</strong> to modify the size of a Cassandra StatefulSet.</p><ol><li>Run the following command:</li><li><strong>kubectl edit statefulset cassandra</strong></li></ol><p>This command opens an editor in your terminal. The line you need to change is the <strong>replicas</strong>field.</p><p><strong>Note:</strong> The following sample is an excerpt of the StatefulSet file.</p><p><strong># Please edit the object below. Lines beginning with a \&#x27;#\&#x27; will be ignored,</strong></p><p><strong># and an empty file will abort the edit. If an error occurs while saving this file will be</strong></p><p><strong># reopened with the relevant failures.</strong></p><p><strong>#</strong></p><p><strong>apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2</strong></p><p><strong>kind: StatefulSet</strong></p><p><strong>metadata:</strong></p><p><strong>creationTimestamp: 2016-08-13T18:40:58Z</strong></p><p><strong>generation: 1</strong></p><p><strong>labels:</strong></p><p><strong>app: cassandra</strong></p><p><strong>name: cassandra</strong></p><p><strong>namespace: default</strong></p><p><strong>resourceVersion: &quot;323&quot;</strong></p><p><strong>selfLink: /apis/apps/v1/namespaces/default/statefulsets/cassandra</strong></p><p><strong>uid: 7a219483-6185-11e6-a910-42010a8a0fc0</strong></p><p><strong>spec:</strong></p><p><strong>replicas: 3</strong></p><ol><li>Change the number of replicas to 4, and then save the manifest.</li></ol><p>The StatefulSet now contains 4 Pods.</p><ol><li>Get the Cassandra StatefulSet to verify:</li><li><strong>kubectl get statefulset cassandra</strong></li></ol><p>The response should be</p><p><strong>NAME DESIRED CURRENT AGE</strong></p><p><strong>cassandra 4 4 36m</strong></p><h4>Cleaning up</h4><p>Deleting or scaling a StatefulSet down does not delete the volumes associated with the StatefulSet. This ensures safety first: your data is more valuable than an auto purge of all related StatefulSet resources.</p><p><strong>Warning:</strong> Depending on the storage class and reclaim policy, deleting the Persistent Volume Claims may cause the associated volumes to also be deleted. Never assume you&#x27;ll be able to access data if its volume claims are deleted.</p><ol><li>Run the following commands to delete everything in a <strong>StatefulSet</strong>:</li><li><strong>grace=$(kubectl get po cassandra-0 -o=jsonpath=\&#x27;{.spec.terminationGracePeriodSeconds}\&#x27;) <!-- -->\</strong></li><li><strong>&amp;&amp; kubectl delete statefulset -l app=cassandra <!-- -->\</strong></li><li><strong>&amp;&amp; echo &quot;Sleeping $grace&quot; <!-- -->\</strong></li><li><strong>&amp;&amp; sleep $grace <!-- -->\</strong></li><li><strong>&amp;&amp; kubectl delete pvc -l app=cassandra</strong></li><li>Run the following command to delete the Cassandra <strong>Service</strong>.</li><li><strong>kubectl delete service -l app=cassandra</strong></li></ol><h4>What&#x27;s next</h4><ul><li>Learn how to <a href="https://kubernetes.io/docs/tasks/run-application/scale-stateful-set/">Scale a StatefulSet</a>.</li><li>Learn more about the <a href="https://github.com/kubernetes/examples/blob/master/cassandra/java/src/main/java/io/k8s/cassandra/KubernetesSeedProvider.java">KubernetesSeedProvider</a></li><li>See more custom <a href="https://git.k8s.io/examples/cassandra/java/README.md">Seed Provider Configurations</a></li></ul><h3>Running ZooKeeper, A CP Distributed System</h3><p>This tutorial demonstrates <a href="https://zookeeper.apache.org/">Apache Zookeeper</a> on Kubernetes using <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">StatefulSets</a>,<a href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#specifying-a-poddisruptionbudget">PodDisruptionBudgets</a>, and <a href="https://kubernetes.io/docs/user-guide/node-selection/#inter-pod-affinity-and-anti-affinity-beta-feature">PodAntiAffinity</a>.</p><ul><li><a href="https://kubernetes.io/docs/tutorials/stateful-application/zookeeper/#objectives"><strong>Objectives</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/stateful-application/zookeeper/#before-you-begin"><strong>Before you begin</strong></a><ul><li><a href="https://kubernetes.io/docs/tutorials/stateful-application/zookeeper/#zookeeper-basics"><strong>ZooKeeper Basics</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tutorials/stateful-application/zookeeper/#creating-a-zookeeper-ensemble"><strong>Creating a ZooKeeper Ensemble</strong></a><ul><li><a href="https://kubernetes.io/docs/tutorials/stateful-application/zookeeper/#facilitating-leader-election"><strong>Facilitating Leader Election</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/stateful-application/zookeeper/#achieving-consensus"><strong>Achieving Consensus</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/stateful-application/zookeeper/#sanity-testing-the-ensemble"><strong>Sanity Testing the Ensemble</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/stateful-application/zookeeper/#providing-durable-storage"><strong>Providing Durable Storage</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tutorials/stateful-application/zookeeper/#ensuring-consistent-configuration"><strong>Ensuring Consistent Configuration</strong></a><ul><li><a href="https://kubernetes.io/docs/tutorials/stateful-application/zookeeper/#configuring-logging"><strong>Configuring Logging</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/stateful-application/zookeeper/#configuring-a-non-privileged-user"><strong>Configuring a Non-Privileged User</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tutorials/stateful-application/zookeeper/#managing-the-zookeeper-process"><strong>Managing the ZooKeeper Process</strong></a><ul><li><a href="https://kubernetes.io/docs/tutorials/stateful-application/zookeeper/#updating-the-ensemble"><strong>Updating the Ensemble</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/stateful-application/zookeeper/#handling-process-failure"><strong>Handling Process Failure</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/stateful-application/zookeeper/#testing-for-liveness"><strong>Testing for Liveness</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/stateful-application/zookeeper/#testing-for-readiness"><strong>Testing for Readiness</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tutorials/stateful-application/zookeeper/#tolerating-node-failure"><strong>Tolerating Node Failure</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/stateful-application/zookeeper/#surviving-maintenance"><strong>Surviving Maintenance</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/stateful-application/zookeeper/#cleaning-up"><strong>Cleaning up</strong></a></li></ul><h4>Objectives</h4><p>After this tutorial, you will know the following.</p><ul><li>How to deploy a ZooKeeper ensemble using StatefulSet.</li><li>How to consistently configure the ensemble using ConfigMaps.</li><li>How to spread the deployment of ZooKeeper servers in the ensemble.</li><li>How to use PodDisruptionBudgets to ensure service availability during planned maintenance.</li></ul><h4>Before you begin</h4><p>Before starting this tutorial, you should be familiar with the following Kubernetes concepts.</p><ul><li><a href="https://kubernetes.io/docs/user-guide/pods/single-container/">Pods</a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/">Cluster DNS</a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#headless-services">Headless Services</a></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/">PersistentVolumes</a></li><li><a href="https://github.com/kubernetes/examples/tree/master/staging/persistent-volume-provisioning/">PersistentVolume Provisioning</a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">StatefulSets</a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#specifying-a-poddisruptionbudget">PodDisruptionBudgets</a></li><li><a href="https://kubernetes.io/docs/user-guide/node-selection/#inter-pod-affinity-and-anti-affinity-beta-feature">PodAntiAffinity</a></li><li><a href="https://kubernetes.io/docs/user-guide/kubectl/">kubectl CLI</a></li></ul><p>You will require a cluster with at least four nodes, and each node will require at least 2 CPUs and 4 GiB of memory. In this tutorial you will cordon and drain the cluster&#x27;s nodes. <strong>This means that all Pods on the cluster&#x27;s nodes will be terminated and evicted, and the nodes will, temporarily, become unschedulable.</strong> You should use a dedicated cluster for this tutorial, or you should ensure that the disruption you cause will not interfere with other tenants.</p><p>This tutorial assumes that your cluster is configured to dynamically provision PersistentVolumes. If your cluster is not configured to do so, you will have to manually provision three 20 GiB volumes prior to starting this tutorial.</p><h5><strong>ZooKeeper Basics</strong></h5><p><a href="https://zookeeper.apache.org/doc/current/">Apache ZooKeeper</a> is a distributed, open-source coordination service for distributed applications. ZooKeeper allows you to read, write, and observe updates to data. Data are organized in a file system like hierarchy and replicated to all ZooKeeper servers in the ensemble (a set of ZooKeeper servers). All operations on data are atomic and sequentially consistent. ZooKeeper ensures this by using the <a href="https://pdfs.semanticscholar.org/b02c/6b00bd5dbdbd951fddb00b906c82fa80f0b3.pdf">Zab</a> consensus protocol to replicate a state machine across all servers in the ensemble.</p><p>The ensemble uses the Zab protocol to elect a leader, and data can not be written until a leader is elected. Once a leader is elected, the ensemble uses Zab to ensure that all writes are replicated to a quorum before they are acknowledged and made visible to clients. Without respect to weighted quorums, a quorum is a majority component of the ensemble containing the current leader. For instance, if the ensemble has three servers, a component that contains the leader and one other server constitutes a quorum. If the ensemble can not achieve a quorum, data can not be written.</p><p>ZooKeeper servers keep their entire state machine in memory, but every mutation is written to a durable WAL (Write Ahead Log) on storage media. When a server crashes, it can recover its previous state by replaying the WAL. In order to prevent the WAL from growing without bound, ZooKeeper servers will periodically snapshot their in memory state to storage media. These snapshots can be loaded directly into memory, and all WAL entries that preceded the snapshot may be safely discarded.</p><h4>Creating a ZooKeeper Ensemble</h4><p>The manifest below contains a <a href="https://kubernetes.io/docs/concepts/services-networking/service/#headless-services">Headless Service</a>, a <a href="https://kubernetes.io/docs/concepts/services-networking/service/">Service</a>, a <a href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#specifying-a-poddisruptionbudget">PodDisruptionBudget</a>, and a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">StatefulSet</a>.</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>zookeeper.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernet">https://raw.githubusercontent.com/kubernet</a>      |
| es/website/master/docs/tutorials/stateful-application/zookeeper.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Service</strong>                                                     |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: zk-hs</strong>                                                       |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: zk</strong>                                                           |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- port: 2888</strong>                                                      |
|                                                                       |
| <strong>name: server</strong>                                                      |
|                                                                       |
| <strong>- port: 3888</strong>                                                      |
|                                                                       |
| <strong>name: leader-election</strong>                                             |
|                                                                       |
| <strong>clusterIP: None</strong>                                                   |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>app: zk</strong>                                                           |
|                                                                       |
| <strong>---</strong>                                                             |
|                                                                       |
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Service</strong>                                                     |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: zk-cs</strong>                                                       |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: zk</strong>                                                           |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- port: 2181</strong>                                                      |
|                                                                       |
| <strong>name: client</strong>                                                      |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>app: zk</strong>                                                           |
|                                                                       |
| <strong>---</strong>                                                             |
|                                                                       |
| <strong>apiVersion: policy/v1beta1</strong>                                        |
|                                                                       |
| <strong>kind: PodDisruptionBudget</strong>                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: zk-pdb</strong>                                                      |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>matchLabels:</strong>                                                      |
|                                                                       |
| <strong>app: zk</strong>                                                           |
|                                                                       |
| <strong>maxUnavailable: 1</strong>                                                 |
|                                                                       |
| <strong>---</strong>                                                             |
|                                                                       |
| <strong>apiVersion: apps/v1</strong>                                               |
|                                                                       |
| <strong>kind: StatefulSet</strong>                                                 |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: zk</strong>                                                          |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>matchLabels:</strong>                                                      |
|                                                                       |
| <strong>app: zk</strong>                                                           |
|                                                                       |
| <strong>serviceName: zk-hs</strong>                                                |
|                                                                       |
| <strong>replicas: 3</strong>                                                       |
|                                                                       |
| <strong>updateStrategy:</strong>                                                   |
|                                                                       |
| <strong>type: RollingUpdate</strong>                                               |
|                                                                       |
| <strong>podManagementPolicy: Parallel</strong>                                     |
|                                                                       |
| <strong>template:</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: zk</strong>                                                           |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>affinity:</strong>                                                         |
|                                                                       |
| <strong>podAntiAffinity:</strong>                                                  |
|                                                                       |
| <strong>requiredDuringSchedulingIgnoredDuringExecution:</strong>                   |
|                                                                       |
| <strong>- labelSelector:</strong>                                                  |
|                                                                       |
| <strong>matchExpressions:</strong>                                                 |
|                                                                       |
| <strong>- key: &quot;app&quot;</strong>                                                    |
|                                                                       |
| <strong>operator: In</strong>                                                      |
|                                                                       |
| <strong>values:</strong>                                                           |
|                                                                       |
| <strong>- zk-hs</strong>                                                           |
|                                                                       |
| <strong>topologyKey: &quot;kubernetes.io/hostname&quot;</strong>                           |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: kubernetes-zookeeper</strong>                                      |
|                                                                       |
| <strong>imagePullPolicy: Always</strong>                                           |
|                                                                       |
| <strong>image: &quot;k8s.gcr.io/kubernetes-zookeeper:1.0-3.4.10&quot;</strong>             |
|                                                                       |
| <strong>resources:</strong>                                                        |
|                                                                       |
| <strong>requests:</strong>                                                         |
|                                                                       |
| <strong>memory: &quot;1Gi&quot;</strong>                                                   |
|                                                                       |
| <strong>cpu: &quot;0.5&quot;</strong>                                                      |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- containerPort: 2181</strong>                                             |
|                                                                       |
| <strong>name: client</strong>                                                      |
|                                                                       |
| <strong>- containerPort: 2888</strong>                                             |
|                                                                       |
| <strong>name: server</strong>                                                      |
|                                                                       |
| <strong>- containerPort: 3888</strong>                                             |
|                                                                       |
| <strong>name: leader-election</strong>                                             |
|                                                                       |
| <strong>command:</strong>                                                          |
|                                                                       |
| <strong>- sh</strong>                                                              |
|                                                                       |
| <strong>- -c</strong>                                                              |
|                                                                       |
| <strong>- &quot;start-zookeeper <!-- -->\</strong>                                            |
|                                                                       |
| <strong>--servers=3 <!-- -->\</strong>                                                   |
|                                                                       |
| <strong>--data_dir=/var/lib/zookeeper/data <!-- -->\</strong>                            |
|                                                                       |
| <strong>--data_log_dir=/var/lib/zookeeper/data/log <!-- -->\</strong>                    |
|                                                                       |
| <strong>--conf_dir=/opt/zookeeper/conf <!-- -->\</strong>                                |
|                                                                       |
| <strong>--client_port=2181 <!-- -->\</strong>                                            |
|                                                                       |
| <strong>--election_port=3888 <!-- -->\</strong>                                          |
|                                                                       |
| <strong>--server_port=2888 <!-- -->\</strong>                                            |
|                                                                       |
| <strong>--tick_time=2000 <!-- -->\</strong>                                              |
|                                                                       |
| <strong>--init_limit=10 <!-- -->\</strong>                                               |
|                                                                       |
| <strong>--sync_limit=5 <!-- -->\</strong>                                                |
|                                                                       |
| <strong>--heap=512M <!-- -->\</strong>                                                   |
|                                                                       |
| <strong>--max_client_cnxns=60 <!-- -->\</strong>                                         |
|                                                                       |
| <strong>--snap_retain_count=3 <!-- -->\</strong>                                         |
|                                                                       |
| <strong>--purge_interval=12 <!-- -->\</strong>                                           |
|                                                                       |
| <strong>--max_session_timeout=40000 <!-- -->\</strong>                                   |
|                                                                       |
| <strong>--min_session_timeout=4000 <!-- -->\</strong>                                    |
|                                                                       |
| <strong>--log_level=INFO&quot;</strong>                                               |
|                                                                       |
| <strong>readinessProbe:</strong>                                                   |
|                                                                       |
| <strong>exec:</strong>                                                             |
|                                                                       |
| <strong>command:</strong>                                                          |
|                                                                       |
| <strong>- sh</strong>                                                              |
|                                                                       |
| <strong>- -c</strong>                                                              |
|                                                                       |
| <strong>- &quot;zookeeper-ready 2181&quot;</strong>                                        |
|                                                                       |
| <strong>initialDelaySeconds: 10</strong>                                           |
|                                                                       |
| <strong>timeoutSeconds: 5</strong>                                                 |
|                                                                       |
| <strong>livenessProbe:</strong>                                                    |
|                                                                       |
| <strong>exec:</strong>                                                             |
|                                                                       |
| <strong>command:</strong>                                                          |
|                                                                       |
| <strong>- sh</strong>                                                              |
|                                                                       |
| <strong>- -c</strong>                                                              |
|                                                                       |
| <strong>- &quot;zookeeper-ready 2181&quot;</strong>                                        |
|                                                                       |
| <strong>initialDelaySeconds: 10</strong>                                           |
|                                                                       |
| <strong>timeoutSeconds: 5</strong>                                                 |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- name: datadir</strong>                                                   |
|                                                                       |
| <strong>mountPath: /var/lib/zookeeper</strong>                                     |
|                                                                       |
| <strong>securityContext:</strong>                                                  |
|                                                                       |
| <strong>runAsUser: 1000</strong>                                                   |
|                                                                       |
| <strong>fsGroup: 1000</strong>                                                     |
|                                                                       |
| <strong>volumeClaimTemplates:</strong>                                             |
|                                                                       |
| <strong>- metadata:</strong>                                                       |
|                                                                       |
| <strong>name: datadir</strong>                                                     |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>accessModes: <!-- -->[ &quot;ReadWriteOnce&quot; ]</strong>                              |
|                                                                       |
| <strong>resources:</strong>                                                        |
|                                                                       |
| <strong>requests:</strong>                                                         |
|                                                                       |
| <strong>storage: 10Gi</strong>                                                     |
+-----------------------------------------------------------------------+</p><p>Open a command terminal, and use <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#apply"><strong>kubectl apply</strong></a> to create the manifest.</p><p><strong>kubectl apply -f <a href="https://raw.githubusercontent.com/kubernetes/website/master/docs/tutorials/stateful-application/zookeeper.yaml">https://raw.githubusercontent.com/kubernetes/website/master/docs/tutorials/stateful-application/zookeeper.yaml</a></strong></p><p>This creates the <strong>zk-hs</strong> Headless Service, the <strong>zk-cs</strong> Service, the <strong>zk-pdb</strong> PodDisruptionBudget, and the <strong>zk</strong> StatefulSet.</p><p><strong>service &quot;zk-hs&quot; created</strong></p><p><strong>service &quot;zk-cs&quot; created</strong></p><p><strong>poddisruptionbudget &quot;zk-pdb&quot; created</strong></p><p><strong>statefulset &quot;zk&quot; created</strong></p><p>Use <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#get"><strong>kubectl get</strong></a> to watch the StatefulSet controller create the StatefulSet&#x27;s Pods.</p><p><strong>kubectl get pods -w -l app=zk</strong></p><p>Once the <strong>zk-2</strong> Pod is Running and Ready, use <strong>CTRL-C</strong> to terminate kubectl.</p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>zk-0 0/1 Pending 0 0s</strong></p><p><strong>zk-0 0/1 Pending 0 0s</strong></p><p><strong>zk-0 0/1 ContainerCreating 0 0s</strong></p><p><strong>zk-0 0/1 Running 0 19s</strong></p><p><strong>zk-0 1/1 Running 0 40s</strong></p><p><strong>zk-1 0/1 Pending 0 0s</strong></p><p><strong>zk-1 0/1 Pending 0 0s</strong></p><p><strong>zk-1 0/1 ContainerCreating 0 0s</strong></p><p><strong>zk-1 0/1 Running 0 18s</strong></p><p><strong>zk-1 1/1 Running 0 40s</strong></p><p><strong>zk-2 0/1 Pending 0 0s</strong></p><p><strong>zk-2 0/1 Pending 0 0s</strong></p><p><strong>zk-2 0/1 ContainerCreating 0 0s</strong></p><p><strong>zk-2 0/1 Running 0 19s</strong></p><p><strong>zk-2 1/1 Running 0 40s</strong></p><p>The StatefulSet controller creates three Pods, and each Pod has a container with a <a href="http://www-us.apache.org/dist/zookeeper/stable/">ZooKeeper</a>server.</p><h5><strong>Facilitating Leader Election</strong></h5><p>As there is no terminating algorithm for electing a leader in an anonymous network, Zab requires explicit membership configuration in order to perform leader election. Each server in the ensemble needs to have a unique identifier, all servers need to know the global set of identifiers, and each identifier needs to be associated with a network address.</p><p>Use <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#exec"><strong>kubectl exec</strong></a> to get the hostnames of the Pods in the <strong>zk</strong> StatefulSet.</p><p><strong>for i in 0 1 2; do kubectl exec zk-$i -- hostname; done</strong></p><p>The StatefulSet controller provides each Pod with a unique hostname based on its ordinal index. The hostnames take the form <strong><code>&lt;statefulset name&gt;-&lt;ordinal index&gt;</code></strong>. As the <strong>replicas</strong> field of the <strong>zk</strong> StatefulSet is set to <strong>3</strong>, the Set&#x27;s controller creates three Pods with their hostnames set to <strong>zk-0</strong>, <strong>zk-1</strong>, and <strong>zk-2</strong>.</p><p><strong>zk-0</strong></p><p><strong>zk-1</strong></p><p><strong>zk-2</strong></p><p>The servers in a ZooKeeper ensemble use natural numbers as unique identifiers, and each server&#x27;s identifier is stored in a file called <strong>myid</strong> in the server&#x27;s data directory.</p><p>Examine the contents of the <strong>myid</strong> file for each server.</p><p><strong>for i in 0 1 2; do echo &quot;myid zk-$i&quot;;kubectl exec zk-$i -- cat /var/lib/zookeeper/data/myid; done</strong></p><p>As the identifiers are natural numbers and the ordinal indices are non-negative integers, you can generate an identifier by adding one to the ordinal.</p><p><strong>myid zk-0</strong></p><p><strong>1</strong></p><p><strong>myid zk-1</strong></p><p><strong>2</strong></p><p><strong>myid zk-2</strong></p><p><strong>3</strong></p><p>Get the FQDN (Fully Qualified Domain Name) of each Pod in the <strong>zk</strong> StatefulSet.</p><p><strong>for i in 0 1 2; do kubectl exec zk-$i -- hostname -f; done</strong></p><p>The <strong>zk-hs</strong> Service creates a domain for all of the Pods, <strong>zk-hs.default.svc.cluster.local</strong>.</p><p><strong>zk-0.zk-hs.default.svc.cluster.local</strong></p><p><strong>zk-1.zk-hs.default.svc.cluster.local</strong></p><p><strong>zk-2.zk-hs.default.svc.cluster.local</strong></p><p>The A records in <a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/">Kubernetes DNS</a> resolve the FQDNs to the Pods&#x27; IP addresses. If the Pods are rescheduled, the A records will be updated with the Pods&#x27; new IP addresses, but the A record&#x27;s names will not change.</p><p>ZooKeeper stores its application configuration in a file named <strong>zoo.cfg</strong>. Use <strong>kubectl exec</strong> to view the contents of the <strong>zoo.cfg</strong> file in the <strong>zk-0</strong> Pod.</p><p><strong>kubectl exec zk-0 -- cat /opt/zookeeper/conf/zoo.cfg</strong></p><p>For the <strong>server.1</strong>, <strong>server.2</strong>, and <strong>server.3</strong> properties at the bottom of the file, the <strong>1</strong>, <strong>2</strong>, and <strong>3</strong>correspond to the identifiers in the ZooKeeper servers&#x27; <strong>myid</strong> files. They are set to the FQDNs for the Pods in the <strong>zk</strong> StatefulSet.</p><p><strong>clientPort=2181</strong></p><p><strong>dataDir=/var/lib/zookeeper/data</strong></p><p><strong>dataLogDir=/var/lib/zookeeper/log</strong></p><p><strong>tickTime=2000</strong></p><p><strong>initLimit=10</strong></p><p><strong>syncLimit=2000</strong></p><p><strong>maxClientCnxns=60</strong></p><p><strong>minSessionTimeout= 4000</strong></p><p><strong>maxSessionTimeout= 40000</strong></p><p><strong>autopurge.snapRetainCount=3</strong></p><p><strong>autopurge.purgeInterval=0</strong></p><p><strong>server.1=zk-0.zk-hs.default.svc.cluster.local:2888:3888</strong></p><p><strong>server.2=zk-1.zk-hs.default.svc.cluster.local:2888:3888</strong></p><p><strong>server.3=zk-2.zk-hs.default.svc.cluster.local:2888:3888</strong></p><h5><strong>Achieving Consensus</strong></h5><p>Consensus protocols require that the identifiers of each participant be unique. No two participants in the Zab protocol should claim the same unique identifier. This is necessary to allow the processes in the system to agree on which processes have committed which data. If two Pods were launched with the same ordinal, two ZooKeeper servers would both identify themselves as the same server.</p><p><strong>kubectl get pods -w -l app=zk</strong></p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>zk-0 0/1 Pending 0 0s</strong></p><p><strong>zk-0 0/1 Pending 0 0s</strong></p><p><strong>zk-0 0/1 ContainerCreating 0 0s</strong></p><p><strong>zk-0 0/1 Running 0 19s</strong></p><p><strong>zk-0 1/1 Running 0 40s</strong></p><p><strong>zk-1 0/1 Pending 0 0s</strong></p><p><strong>zk-1 0/1 Pending 0 0s</strong></p><p><strong>zk-1 0/1 ContainerCreating 0 0s</strong></p><p><strong>zk-1 0/1 Running 0 18s</strong></p><p><strong>zk-1 1/1 Running 0 40s</strong></p><p><strong>zk-2 0/1 Pending 0 0s</strong></p><p><strong>zk-2 0/1 Pending 0 0s</strong></p><p><strong>zk-2 0/1 ContainerCreating 0 0s</strong></p><p><strong>zk-2 0/1 Running 0 19s</strong></p><p><strong>zk-2 1/1 Running 0 40s</strong></p><p>The A records for each Pod are only entered when the Pod becomes Ready. Therefore, the FQDNs of the ZooKeeper servers will only resolve to a single endpoint, and that endpoint will be the unique ZooKeeper server claiming the identity configured in its <strong>myid</strong> file.</p><p><strong>zk-0.zk-hs.default.svc.cluster.local</strong></p><p><strong>zk-1.zk-hs.default.svc.cluster.local</strong></p><p><strong>zk-2.zk-hs.default.svc.cluster.local</strong></p><p>This ensures that the <strong>servers</strong> properties in the ZooKeepers&#x27; <strong>zoo.cfg</strong> files represents a correctly configured ensemble.</p><p><strong>server.1=zk-0.zk-hs.default.svc.cluster.local:2888:3888</strong></p><p><strong>server.2=zk-1.zk-hs.default.svc.cluster.local:2888:3888</strong></p><p><strong>server.3=zk-2.zk-hs.default.svc.cluster.local:2888:3888</strong></p><p>When the servers use the Zab protocol to attempt to commit a value, they will either achieve consensus and commit the value (if leader election has succeeded and at least two of the Pods are Running and Ready), or they will fail to do so (if either of the aforementioned conditions are not met). No state will arise where one server acknowledges a write on behalf of another.</p><h5><strong>Sanity Testing the Ensemble</strong></h5><p>The most basic sanity test is to write some data to one ZooKeeper server and to read the data from another.</p><p>Use the <strong>zkCli.sh</strong> script to write <strong>world</strong> to the path <strong>/hello</strong> on the <strong>zk-0</strong> Pod.</p><p><strong>kubectl exec zk-0 zkCli.sh create /hello world</strong></p><p>This will write <strong>world</strong> to the <strong>/hello</strong> path in the ensemble.</p><p><strong>WATCHER::</strong></p><p><strong>WatchedEvent state:SyncConnected type:None path:null</strong></p><p><strong>Created /hello</strong></p><p>Get the data from the <strong>zk-1</strong> Pod.</p><p><strong>kubectl exec zk-1 zkCli.sh get /hello</strong></p><p>The data that you created on <strong>zk-0</strong> is available on all of the servers in the ensemble.</p><p><strong>WATCHER::</strong></p><p><strong>WatchedEvent state:SyncConnected type:None path:null</strong></p><p><strong>world</strong></p><p><strong>cZxid = 0x100000002</strong></p><p><strong>ctime = Thu Dec 08 15:13:30 UTC 2016</strong></p><p><strong>mZxid = 0x100000002</strong></p><p><strong>mtime = Thu Dec 08 15:13:30 UTC 2016</strong></p><p><strong>pZxid = 0x100000002</strong></p><p><strong>cversion = 0</strong></p><p><strong>dataVersion = 0</strong></p><p><strong>aclVersion = 0</strong></p><p><strong>ephemeralOwner = 0x0</strong></p><p><strong>dataLength = 5</strong></p><p><strong>numChildren = 0</strong></p><h5><strong>Providing Durable Storage</strong></h5><p>As mentioned in the <a href="https://kubernetes.io/docs/tutorials/stateful-application/zookeeper/#zookeeper-basics">ZooKeeper Basics</a> section, ZooKeeper commits all entries to a durable WAL, and periodically writes snapshots in memory state, to storage media. Using WALs to provide durability is a common technique for applications that use consensus protocols to achieve a replicated state machine and for storage applications in general.</p><p>Use <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#delete"><strong>kubectl delete</strong></a> to delete the <strong>zk</strong> StatefulSet.</p><p><strong>kubectl delete statefulset zk</strong></p><p><strong>statefulset &quot;zk&quot; deleted</strong></p><p>Watch the termination of the Pods in the StatefulSet.</p><p><strong>kubectl get pods -w -l app=zk</strong></p><p>When <strong>zk-0</strong> if fully terminated, use <strong>CTRL-C</strong> to terminate kubectl.</p><p><strong>zk-2 1/1 Terminating 0 9m</strong></p><p><strong>zk-0 1/1 Terminating 0 11m</strong></p><p><strong>zk-1 1/1 Terminating 0 10m</strong></p><p><strong>zk-2 0/1 Terminating 0 9m</strong></p><p><strong>zk-2 0/1 Terminating 0 9m</strong></p><p><strong>zk-2 0/1 Terminating 0 9m</strong></p><p><strong>zk-1 0/1 Terminating 0 10m</strong></p><p><strong>zk-1 0/1 Terminating 0 10m</strong></p><p><strong>zk-1 0/1 Terminating 0 10m</strong></p><p><strong>zk-0 0/1 Terminating 0 11m</strong></p><p><strong>zk-0 0/1 Terminating 0 11m</strong></p><p><strong>zk-0 0/1 Terminating 0 11m</strong></p><p>Reapply the manifest in <strong>zookeeper.yaml</strong>.</p><p><strong>kubectl apply -f <a href="https://raw.githubusercontent.com/kubernetes/website/master/docs/tutorials/stateful-application/zookeeper.yaml">https://raw.githubusercontent.com/kubernetes/website/master/docs/tutorials/stateful-application/zookeeper.yaml</a></strong></p><p>The <strong>zk</strong> StatefulSet will be created, but, as they already exist, the other API Objects in the manifest will not be modified.</p><p>Watch the StatefulSet controller recreate the StatefulSet&#x27;s Pods.</p><p><strong>kubectl get pods -w -l app=zk</strong></p><p>Once the <strong>zk-2</strong> Pod is Running and Ready, use <strong>CTRL-C</strong> to terminate kubectl.</p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>zk-0 0/1 Pending 0 0s</strong></p><p><strong>zk-0 0/1 Pending 0 0s</strong></p><p><strong>zk-0 0/1 ContainerCreating 0 0s</strong></p><p><strong>zk-0 0/1 Running 0 19s</strong></p><p><strong>zk-0 1/1 Running 0 40s</strong></p><p><strong>zk-1 0/1 Pending 0 0s</strong></p><p><strong>zk-1 0/1 Pending 0 0s</strong></p><p><strong>zk-1 0/1 ContainerCreating 0 0s</strong></p><p><strong>zk-1 0/1 Running 0 18s</strong></p><p><strong>zk-1 1/1 Running 0 40s</strong></p><p><strong>zk-2 0/1 Pending 0 0s</strong></p><p><strong>zk-2 0/1 Pending 0 0s</strong></p><p><strong>zk-2 0/1 ContainerCreating 0 0s</strong></p><p><strong>zk-2 0/1 Running 0 19s</strong></p><p><strong>zk-2 1/1 Running 0 40s</strong></p><p>Get the value you entered during the <a href="https://kubernetes.io/docs/tutorials/stateful-application/zookeeper/#sanity-testing-the-ensemble">sanity test</a>, from the <strong>zk-2</strong> Pod.</p><p><strong>kubectl exec zk-2 zkCli.sh get /hello</strong></p><p>Even though all of the Pods in the <strong>zk</strong> StatefulSet have been terminated and recreated, the ensemble still serves the original value.</p><p><strong>WATCHER::</strong></p><p><strong>WatchedEvent state:SyncConnected type:None path:null</strong></p><p><strong>world</strong></p><p><strong>cZxid = 0x100000002</strong></p><p><strong>ctime = Thu Dec 08 15:13:30 UTC 2016</strong></p><p><strong>mZxid = 0x100000002</strong></p><p><strong>mtime = Thu Dec 08 15:13:30 UTC 2016</strong></p><p><strong>pZxid = 0x100000002</strong></p><p><strong>cversion = 0</strong></p><p><strong>dataVersion = 0</strong></p><p><strong>aclVersion = 0</strong></p><p><strong>ephemeralOwner = 0x0</strong></p><p><strong>dataLength = 5</strong></p><p><strong>numChildren = 0</strong></p><p>The <strong>volumeClaimTemplates</strong> field, of the <strong>zk</strong> StatefulSet&#x27;s <strong>spec</strong>, specifies a PersistentVolume that will be provisioned for each Pod.</p><p><strong>volumeClaimTemplates:</strong></p><p><strong>- metadata:</strong></p><p><strong>name: datadir</strong></p><p><strong>annotations:</strong></p><p><strong>volume.alpha.kubernetes.io/storage-class: anything</strong></p><p><strong>spec:</strong></p><p><strong>accessModes: <!-- -->[ &quot;ReadWriteOnce&quot; ]</strong></p><p><strong>resources:</strong></p><p><strong>requests:</strong></p><p><strong>storage: 20Gi</strong></p><p>The StatefulSet controller generates a PersistentVolumeClaim for each Pod in the StatefulSet.</p><p>Get the StatefulSet&#x27;s PersistentVolumeClaims.</p><p><strong>kubectl get pvc -l app=zk</strong></p><p>When the StatefulSet recreated its Pods, the Pods&#x27; PersistentVolumes were remounted.</p><p><strong>NAME STATUS VOLUME CAPACITY ACCESSMODES AGE</strong></p><p><strong>datadir-zk-0 Bound pvc-bed742cd-bcb1-11e6-994f-42010a800002 20Gi RWO 1h</strong></p><p><strong>datadir-zk-1 Bound pvc-bedd27d2-bcb1-11e6-994f-42010a800002 20Gi RWO 1h</strong></p><p><strong>datadir-zk-2 Bound pvc-bee0817e-bcb1-11e6-994f-42010a800002 20Gi RWO 1h</strong></p><p>The <strong>volumeMounts</strong> section of the StatefulSet&#x27;s container <strong>template</strong> causes the PersistentVolumes to be mounted to the ZooKeeper servers&#x27; data directories.</p><p><strong>volumeMounts:</strong></p><p><strong>- name: datadir</strong></p><p><strong>mountPath: /var/lib/zookeeper</strong></p><p>When a Pod in the <strong>zk</strong> StatefulSet is (re)scheduled, it will always have the same PersistentVolume mounted to the ZooKeeper server&#x27;s data directory. Even when the Pods are rescheduled, all of the writes made to the ZooKeeper servers&#x27; WALs, and all of their snapshots, remain durable.</p><h4>Ensuring Consistent Configuration</h4><p>As noted in the <a href="https://kubernetes.io/docs/tutorials/stateful-application/zookeeper/#facilitating-leader-election">Facilitating Leader Election</a> and <a href="https://kubernetes.io/docs/tutorials/stateful-application/zookeeper/#achieving-consensus">Achieving Consensus</a> sections, the servers in a ZooKeeper ensemble require consistent configuration in order to elect a leader and form a quorum. They also require consistent configuration of the Zab protocol in order for the protocol to work correctly over a network. In our example we achieve consistent configuration by embedding the configuration directly into the manifest.</p><p>Get the <strong>zk</strong> StatefulSet.</p><p><strong>kubectl get sts zk -o yaml</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>command:</strong></p><p><strong>- sh</strong></p><p><strong>- -c</strong></p><p><strong>- &quot;start-zookeeper <!-- -->\</strong></p><p><strong>--servers=3 <!-- -->\</strong></p><p><strong>--data_dir=/var/lib/zookeeper/data <!-- -->\</strong></p><p><strong>--data_log_dir=/var/lib/zookeeper/data/log <!-- -->\</strong></p><p><strong>--conf_dir=/opt/zookeeper/conf <!-- -->\</strong></p><p><strong>--client_port=2181 <!-- -->\</strong></p><p><strong>--election_port=3888 <!-- -->\</strong></p><p><strong>--server_port=2888 <!-- -->\</strong></p><p><strong>--tick_time=2000 <!-- -->\</strong></p><p><strong>--init_limit=10 <!-- -->\</strong></p><p><strong>--sync_limit=5 <!-- -->\</strong></p><p><strong>--heap=512M <!-- -->\</strong></p><p><strong>--max_client_cnxns=60 <!-- -->\</strong></p><p><strong>--snap_retain_count=3 <!-- -->\</strong></p><p><strong>--purge_interval=12 <!-- -->\</strong></p><p><strong>--max_session_timeout=40000 <!-- -->\</strong></p><p><strong>--min_session_timeout=4000 <!-- -->\</strong></p><p><strong>--log_level=INFO&quot;</strong></p><p><strong>.<!-- -->..</strong></p><p>Notice that the command used to start the ZooKeeper servers passed the configuration as command line parameter. Environment variables are another, equally good, way to pass configuration to ensemble.</p><h5><strong>Configuring Logging</strong></h5><p>One of the files generated by the <strong>zkGenConfig.sh</strong> script controls ZooKeeper&#x27;s logging. ZooKeeper uses <a href="http://logging.apache.org/log4j/2.x/">Log4j</a>, and, by default, it uses a time and size based rolling file appender for its logging configuration. Get the logging configuration from one of Pods in the <strong>zk</strong> StatefulSet.</p><p><strong>kubectl exec zk-0 cat /usr/etc/zookeeper/log4j.properties</strong></p><p>The logging configuration below will cause the ZooKeeper process to write all of its logs to the standard output file stream.</p><p><strong>zookeeper.root.logger=CONSOLE</strong></p><p><strong>zookeeper.console.threshold=INFO</strong></p><p><strong>log4j.rootLogger=${zookeeper.root.logger}</strong></p><p><strong>log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender</strong></p><p><strong>log4j.appender.CONSOLE.Threshold=${zookeeper.console.threshold}</strong></p><p><strong>log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout</strong></p><p><strong>log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} <!-- -->[myid:%X{myid}]<!-- --> - %-5p <!-- -->[%t:%C{1}@%L]<!-- --> - %m%n</strong></p><p>This is the simplest possible way to safely log inside the container. As the application&#x27;s logs are being written to standard out, Kubernetes will handle log rotation for you. Kubernetes also implements a sane retention policy that ensures application logs written to standard out and standard error do not exhaust local storage media.</p><p>Use <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#logs"><strong>kubectl logs</strong></a> to retrieve the last few log lines from one of the Pods.</p><p><strong>kubectl logs zk-0 --tail 20</strong></p><p>Application logs that are written to standard out or standard error are viewable using <strong>kubectl logs</strong>and from the Kubernetes Dashboard.</p><p><strong>2016-12-06 19:34:16,236 <!-- -->[myid:1]<!-- --> - INFO <!-- -->[NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@827]<!-- --> - Processing ruok command from /127.0.0.1:52740</strong></p><p><strong>2016-12-06 19:34:16,237 <!-- -->[myid:1]<!-- --> - INFO <!-- -->[Thread-1136:NIOServerCnxn@1008]<!-- --> - Closed socket connection for client /127.0.0.1:52740 (no session established for client)</strong></p><p><strong>2016-12-06 19:34:26,155 <!-- -->[myid:1]<!-- --> - INFO <!-- -->[NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192]<!-- --> - Accepted socket connection from /127.0.0.1:52749</strong></p><p><strong>2016-12-06 19:34:26,155 <!-- -->[myid:1]<!-- --> - INFO <!-- -->[NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@827]<!-- --> - Processing ruok command from /127.0.0.1:52749</strong></p><p><strong>2016-12-06 19:34:26,156 <!-- -->[myid:1]<!-- --> - INFO <!-- -->[Thread-1137:NIOServerCnxn@1008]<!-- --> - Closed socket connection for client /127.0.0.1:52749 (no session established for client)</strong></p><p><strong>2016-12-06 19:34:26,222 <!-- -->[myid:1]<!-- --> - INFO <!-- -->[NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192]<!-- --> - Accepted socket connection from /127.0.0.1:52750</strong></p><p><strong>2016-12-06 19:34:26,222 <!-- -->[myid:1]<!-- --> - INFO <!-- -->[NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@827]<!-- --> - Processing ruok command from /127.0.0.1:52750</strong></p><p><strong>2016-12-06 19:34:26,226 <!-- -->[myid:1]<!-- --> - INFO <!-- -->[Thread-1138:NIOServerCnxn@1008]<!-- --> - Closed socket connection for client /127.0.0.1:52750 (no session established for client)</strong></p><p><strong>2016-12-06 19:34:36,151 <!-- -->[myid:1]<!-- --> - INFO <!-- -->[NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192]<!-- --> - Accepted socket connection from /127.0.0.1:52760</strong></p><p><strong>2016-12-06 19:34:36,152 <!-- -->[myid:1]<!-- --> - INFO <!-- -->[NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@827]<!-- --> - Processing ruok command from /127.0.0.1:52760</strong></p><p><strong>2016-12-06 19:34:36,152 <!-- -->[myid:1]<!-- --> - INFO <!-- -->[Thread-1139:NIOServerCnxn@1008]<!-- --> - Closed socket connection for client /127.0.0.1:52760 (no session established for client)</strong></p><p><strong>2016-12-06 19:34:36,230 <!-- -->[myid:1]<!-- --> - INFO <!-- -->[NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192]<!-- --> - Accepted socket connection from /127.0.0.1:52761</strong></p><p><strong>2016-12-06 19:34:36,231 <!-- -->[myid:1]<!-- --> - INFO <!-- -->[NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@827]<!-- --> - Processing ruok command from /127.0.0.1:52761</strong></p><p><strong>2016-12-06 19:34:36,231 <!-- -->[myid:1]<!-- --> - INFO <!-- -->[Thread-1140:NIOServerCnxn@1008]<!-- --> - Closed socket connection for client /127.0.0.1:52761 (no session established for client)</strong></p><p><strong>2016-12-06 19:34:46,149 <!-- -->[myid:1]<!-- --> - INFO <!-- -->[NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192]<!-- --> - Accepted socket connection from /127.0.0.1:52767</strong></p><p><strong>2016-12-06 19:34:46,149 <!-- -->[myid:1]<!-- --> - INFO <!-- -->[NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@827]<!-- --> - Processing ruok command from /127.0.0.1:52767</strong></p><p><strong>2016-12-06 19:34:46,149 <!-- -->[myid:1]<!-- --> - INFO <!-- -->[Thread-1141:NIOServerCnxn@1008]<!-- --> - Closed socket connection for client /127.0.0.1:52767 (no session established for client)</strong></p><p><strong>2016-12-06 19:34:46,230 <!-- -->[myid:1]<!-- --> - INFO <!-- -->[NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192]<!-- --> - Accepted socket connection from /127.0.0.1:52768</strong></p><p><strong>2016-12-06 19:34:46,230 <!-- -->[myid:1]<!-- --> - INFO <!-- -->[NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@827]<!-- --> - Processing ruok command from /127.0.0.1:52768</strong></p><p><strong>2016-12-06 19:34:46,230 <!-- -->[myid:1]<!-- --> - INFO <!-- -->[Thread-1142:NIOServerCnxn@1008]<!-- --> - Closed socket connection for client /127.0.0.1:52768 (no session established for client)</strong></p><p>Kubernetes also supports more powerful, but more complex, logging integrations with <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/logging-stackdriver/">Logging Using Stackdriver</a> and <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/logging-elasticsearch-kibana/">Logging Using Elasticsearch and Kibana</a>. For cluster level log shipping and aggregation, you should consider deploying a <a href="http://blog.kubernetes.io/2015/06/the-distributed-system-toolkit-patterns.html">sidecar</a> container to rotate and ship your logs.</p><h5><strong>Configuring a Non-Privileged User</strong></h5><p>The best practices with respect to allowing an application to run as a privileged user inside of a container are a matter of debate. If your organization requires that applications be run as a non-privileged user you can use a <a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/">SecurityContext</a> to control the user that the entry point runs as.</p><p>The <strong>zk</strong> StatefulSet&#x27;s Pod <strong>template</strong> contains a SecurityContext.</p><p><strong>securityContext:</strong></p><p><strong>runAsUser: 1000</strong></p><p><strong>fsGroup: 1000</strong></p><p>In the Pods&#x27; containers, UID 1000 corresponds to the zookeeper user and GID 1000 corresponds to the zookeeper group.</p><p>Get the ZooKeeper process information from the <strong>zk-0</strong> Pod.</p><p><strong>kubectl exec zk-0 -- ps -elf</strong></p><p>As the <strong>runAsUser</strong> field of the <strong>securityContext</strong> object is set to 1000, instead of running as root, the ZooKeeper process runs as the zookeeper user.</p><p><strong>F S UID PID PPID C PRI NI ADDR SZ WCHAN STIME TTY TIME CMD</strong></p><p><strong>4 S zookeep+ 1 0 0 80 0 - 1127 - 20:46 ? 00:00:00 sh -c zkGenConfig.sh &amp;&amp; zkServer.sh start-foreground</strong></p><p><strong>0 S zookeep+ 27 1 0 80 0 - 1155556 - 20:46 ? 00:00:19 /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Dzookeeper.log.dir=/var/log/zookeeper -Dzookeeper.root.logger=INFO,CONSOLE -cp /usr/bin/../build/classes:/usr/bin/../build/lib/<!-- -->*<!-- -->.jar:/usr/bin/../share/zookeeper/zookeeper-3.4.9.jar:/usr/bin/../share/zookeeper/slf4j-log4j12-1.6.1.jar:/usr/bin/../share/zookeeper/slf4j-api-1.6.1.jar:/usr/bin/../share/zookeeper/netty-3.10.5.Final.jar:/usr/bin/../share/zookeeper/log4j-1.2.16.jar:/usr/bin/../share/zookeeper/jline-0.9.94.jar:/usr/bin/../src/java/lib/<!-- -->*<!-- -->.jar:/usr/bin/../etc/zookeeper: -Xmx2G -Xms2G -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.local.only=false org.apache.zookeeper.server.quorum.QuorumPeerMain /usr/bin/../etc/zookeeper/zoo.cfg</strong></p><p>By default, when the Pod&#x27;s PersistentVolume is mounted to the ZooKeeper server&#x27;s data directory, it is only accessible by the root user. This configuration prevents the ZooKeeper process from writing to its WAL and storing its snapshots.</p><p>Get the file permissions of the ZooKeeper data directory on the <strong>zk-0</strong> Pod.</p><p><strong>kubectl exec -ti zk-0 -- ls -ld /var/lib/zookeeper/data</strong></p><p>As the <strong>fsGroup</strong> field of the <strong>securityContext</strong> object is set to 1000, the ownership of the Pods&#x27; PersistentVolumes is set to the zookeeper group, and the ZooKeeper process is able to successfully read and write its data.</p><p><strong>drwxr-sr-x 3 zookeeper zookeeper 4096 Dec 5 20:45 /var/lib/zookeeper/data</strong></p><h4>Managing the ZooKeeper Process</h4><p>The <a href="https://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_supervision">ZooKeeper documentation</a> indicates that &quot;You will want to have a supervisory process that manages each of your ZooKeeper server processes (JVM).&quot; Utilizing a watchdog (supervisory process) to restart failed processes in a distributed system is a common pattern. When deploying an application in Kubernetes, rather than using an external utility as a supervisory process, you should use Kubernetes as the watchdog for your application.</p><h5><strong>Updating the Ensemble</strong></h5><p>The <strong>zk</strong> StatefulSet is configured to use the RollingUpdate update strategy.</p><p>You can use <strong>kubectl patch</strong> to update the number of <strong>cpus</strong> allocated to the servers.</p><p><strong>kubectl patch sts zk --type=\&#x27;json\&#x27; -p=\&#x27;<!-- -->[{&quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/spec/template/spec/containers/0/resources/requests/cpu&quot;, &quot;value&quot;:&quot;0.3&quot;}]<!-- -->\&#x27;</strong></p><p><strong>statefulset &quot;zk&quot; patched</strong></p><p>Use <strong>kubectl rollout status</strong> to watch the status of the update.</p><p><strong>kubectl rollout status sts/zk</strong></p><p><strong>waiting for statefulset rolling update to complete 0 pods at revision zk-5db4499664<!-- -->.<!-- -->..</strong></p><p><strong>Waiting for 1 pods to be ready<!-- -->.<!-- -->..</strong></p><p><strong>Waiting for 1 pods to be ready<!-- -->.<!-- -->..</strong></p><p><strong>waiting for statefulset rolling update to complete 1 pods at revision zk-5db4499664<!-- -->.<!-- -->..</strong></p><p><strong>Waiting for 1 pods to be ready<!-- -->.<!-- -->..</strong></p><p><strong>Waiting for 1 pods to be ready<!-- -->.<!-- -->..</strong></p><p><strong>waiting for statefulset rolling update to complete 2 pods at revision zk-5db4499664<!-- -->.<!-- -->..</strong></p><p><strong>Waiting for 1 pods to be ready<!-- -->.<!-- -->..</strong></p><p><strong>Waiting for 1 pods to be ready<!-- -->.<!-- -->..</strong></p><p><strong>statefulset rolling update complete 3 pods at revision zk-5db4499664<!-- -->.<!-- -->..</strong></p><p>The Pods are terminated, one at a time, in reverse ordinal order, and they are recreated with the new configuration. This ensures that quorum is maintained during a rolling update.</p><p>Use <strong>kubectl rollout history</strong> to view a history or previous configurations.</p><p><strong>kubectl rollout history sts/zk</strong></p><p><strong>statefulsets &quot;zk&quot;</strong></p><p><strong>REVISION</strong></p><p><strong>1</strong></p><p><strong>2</strong></p><p>Use <strong>kubectl rollout undo</strong> to roll back the modification.</p><p><strong>kubectl rollout undo sts/zk</strong></p><p><strong>statefulset &quot;zk&quot; rolled back</strong></p><h5><strong>Handling Process Failure</strong></h5><p><a href="https://kubernetes.io/docs/user-guide/pod-states/#restartpolicy">Restart Policies</a> control how Kubernetes handles process failures for the entry point of the container in a Pod. For Pods in a StatefulSet, the only appropriate RestartPolicy is Always, and this is the default value. For stateful applications you should <strong>never</strong> override the default policy.</p><p>Examine the process tree for the ZooKeeper server running in the <strong>zk-0</strong> Pod.</p><p><strong>kubectl exec zk-0 -- ps -ef</strong></p><p>The command used as the container&#x27;s entry point has PID 1, and the ZooKeeper process, a child of the entry point, has PID 23.</p><p><strong>UID PID PPID C STIME TTY TIME CMD</strong></p><p><strong>zookeep+ 1 0 0 15:03 ? 00:00:00 sh -c zkGenConfig.sh &amp;&amp; zkServer.sh start-foreground</strong></p><p><strong>zookeep+ 27 1 0 15:03 ? 00:00:03 /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Dzookeeper.log.dir=/var/log/zookeeper -Dzookeeper.root.logger=INFO,CONSOLE -cp /usr/bin/../build/classes:/usr/bin/../build/lib/<!-- -->*<!-- -->.jar:/usr/bin/../share/zookeeper/zookeeper-3.4.9.jar:/usr/bin/../share/zookeeper/slf4j-log4j12-1.6.1.jar:/usr/bin/../share/zookeeper/slf4j-api-1.6.1.jar:/usr/bin/../share/zookeeper/netty-3.10.5.Final.jar:/usr/bin/../share/zookeeper/log4j-1.2.16.jar:/usr/bin/../share/zookeeper/jline-0.9.94.jar:/usr/bin/../src/java/lib/<!-- -->*<!-- -->.jar:/usr/bin/../etc/zookeeper: -Xmx2G -Xms2G -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.local.only=false org.apache.zookeeper.server.quorum.QuorumPeerMain /usr/bin/../etc/zookeeper/zoo.cfg</strong></p><p>In one terminal watch the Pods in the <strong>zk</strong> StatefulSet.</p><p><strong>kubectl get pod -w -l app=zk</strong></p><p>In another terminal, kill the ZooKeeper process in Pod <strong>zk-0</strong>.</p><p><strong>kubectl exec zk-0 -- pkill java</strong></p><p>The death of the ZooKeeper process caused its parent process to terminate. As the RestartPolicy of the container is Always, the parent process was relaunched.</p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>zk-0 1/1 Running 0 21m</strong></p><p><strong>zk-1 1/1 Running 0 20m</strong></p><p><strong>zk-2 1/1 Running 0 19m</strong></p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>zk-0 0/1 Error 0 29m</strong></p><p><strong>zk-0 0/1 Running 1 29m</strong></p><p><strong>zk-0 1/1 Running 1 29m</strong></p><p>If your application uses a script (such as zkServer.sh) to launch the process that implements the application&#x27;s business logic, the script must terminate with the child process. This ensures that Kubernetes will restart the application&#x27;s container when the process implementing the application&#x27;s business logic fails.</p><h5><strong>Testing for Liveness</strong></h5><p>Configuring your application to restart failed processes is not sufficient to keep a distributed system healthy. There are many scenarios where a system&#x27;s processes can be both alive and unresponsive, or otherwise unhealthy. You should use liveness probes in order to notify Kubernetes that your application&#x27;s processes are unhealthy and should be restarted.</p><p>The Pod <strong>template</strong> for the <strong>zk</strong> StatefulSet specifies a liveness probe.</p><p><strong>livenessProbe:</strong></p><p><strong>exec:</strong></p><p><strong>command:</strong></p><p><strong>- sh</strong></p><p><strong>- -c</strong></p><p><strong>- &quot;zookeeper-ready 2181&quot;</strong></p><p><strong>initialDelaySeconds: 15</strong></p><p><strong>timeoutSeconds: 5</strong></p><p>The probe calls a simple bash script that uses the ZooKeeper <strong>ruok</strong> four letter word to test the server&#x27;s health.</p><p><strong>OK=$(echo ruok | nc 127.0.0.1 $1)</strong></p><p><strong>if <!-- -->[ &quot;$OK&quot; == &quot;imok&quot; ]<!-- -->; then</strong></p><p><strong>exit 0</strong></p><p><strong>else</strong></p><p><strong>exit 1</strong></p><p><strong>fi</strong></p><p>In one terminal window, watch the Pods in the <strong>zk</strong> StatefulSet.</p><p><strong>kubectl get pod -w -l app=zk</strong></p><p>In another window, delete the <strong>zkOk.sh</strong> script from the file system of Pod <strong>zk-0</strong>.</p><p><strong>kubectl exec zk-0 -- rm /usr/bin/zookeeper-ready</strong></p><p>When the liveness probe for the ZooKeeper process fails, Kubernetes will automatically restart the process for you, ensuring that unhealthy processes in the ensemble are restarted.</p><p><strong>kubectl get pod -w -l app=zk</strong></p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>zk-0 1/1 Running 0 1h</strong></p><p><strong>zk-1 1/1 Running 0 1h</strong></p><p><strong>zk-2 1/1 Running 0 1h</strong></p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>zk-0 0/1 Running 0 1h</strong></p><p><strong>zk-0 0/1 Running 1 1h</strong></p><p><strong>zk-0 1/1 Running 1 1h</strong></p><h5><strong>Testing for Readiness</strong></h5><p>Readiness is not the same as liveness. If a process is alive, it is scheduled and healthy. If a process is ready, it is able to process input. Liveness is a necessary, but not sufficient, condition for readiness. There are many cases, particularly during initialization and termination, when a process can be alive but not ready.</p><p>If you specify a readiness probe, Kubernetes will ensure that your application&#x27;s processes will not receive network traffic until their readiness checks pass.</p><p>For a ZooKeeper server, liveness implies readiness. Therefore, the readiness probe from the <strong>zookeeper.yaml</strong> manifest is identical to the liveness probe.</p><p><strong>readinessProbe:</strong></p><p><strong>exec:</strong></p><p><strong>command:</strong></p><p><strong>- sh</strong></p><p><strong>- -c</strong></p><p><strong>- &quot;zookeeper-ready 2181&quot;</strong></p><p><strong>initialDelaySeconds: 15</strong></p><p><strong>timeoutSeconds: 5</strong></p><p>Even though the liveness and readiness probes are identical, it is important to specify both. This ensures that only healthy servers in the ZooKeeper ensemble receive network traffic.</p><h4>Tolerating Node Failure</h4><p>ZooKeeper needs a quorum of servers in order to successfully commit mutations to data. For a three server ensemble, two servers must be healthy in order for writes to succeed. In quorum based systems, members are deployed across failure domains to ensure availability. In order to avoid an outage, due to the loss of an individual machine, best practices preclude co-locating multiple instances of the application on the same machine.</p><p>By default, Kubernetes may co-locate Pods in a StatefulSet on the same node. For the three server ensemble you created, if two servers reside on the same node, and that node fails, the clients of your ZooKeeper service will experience an outage until at least one of the Pods can be rescheduled.</p><p>You should always provision additional capacity to allow the processes of critical systems to be rescheduled in the event of node failures. If you do so, then the outage will only last until the Kubernetes scheduler reschedules one of the ZooKeeper servers. However, if you want your service to tolerate node failures with no downtime, you should set <strong>podAntiAffinity</strong>.</p><p>Get the nodes for Pods in the <strong>zk</strong> Stateful Set.</p><p><strong>for i in 0 1 2; do kubectl get pod zk-$i --template {{.spec.nodeName}}; echo &quot;&quot;; done</strong></p><p>All of the Pods in the <strong>zk</strong> StatefulSet are deployed on different nodes.</p><p><strong>kubernetes-minion-group-cxpk</strong></p><p><strong>kubernetes-minion-group-a5aq</strong></p><p><strong>kubernetes-minion-group-2g2d</strong></p><p>This is because the Pods in the <strong>zk</strong> StatefulSet have a PodAntiAffinity specified.</p><p><strong>affinity:</strong></p><p><strong>podAntiAffinity:</strong></p><p><strong>requiredDuringSchedulingIgnoredDuringExecution:</strong></p><p><strong>- labelSelector:</strong></p><p><strong>matchExpressions:</strong></p><p><strong>- key: &quot;app&quot;</strong></p><p><strong>operator: In</strong></p><p><strong>values:</strong></p><p><strong>- zk-hs</strong></p><p><strong>topologyKey: &quot;kubernetes.io/hostname&quot;</strong></p><p>The <strong>requiredDuringSchedulingIgnoredDuringExecution</strong> field tells the Kubernetes Scheduler that it should never co-locate two Pods from the <strong>zk-hs</strong> Service in the domain defined by the <strong>topologyKey</strong>. The <strong>topologyKey</strong> <strong>kubernetes.io/hostname</strong> indicates that the domain is an individual node. Using different rules, labels, and selectors, you can extend this technique to spread your ensemble across physical, network, and power failure domains.</p><h4>Surviving Maintenance</h4><p><strong>In this section you will cordon and drain nodes. If you are using this tutorial on a shared cluster, be sure that this will not adversely affect other tenants.</strong></p><p>The previous section showed you how to spread your Pods across nodes to survive unplanned node failures, but you also need to plan for temporary node failures that occur due to planned maintenance.</p><p>Get the nodes in your cluster.</p><p><strong>kubectl get nodes</strong></p><p>Use <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#cordon"><strong>kubectl cordon</strong></a> to cordon all but four of the nodes in your cluster.</p><p><strong>kubectl cordon <code>&lt; node name &gt;</code></strong></p><p>Get the <strong>zk-pdb</strong> PodDisruptionBudget.</p><p><strong>kubectl get pdb zk-pdb</strong></p><p>The <strong>max-unavailable</strong> field indicates to Kubernetes that at most one Pod from <strong>zk</strong> StatefulSet can be unavailable at any time.</p><p><strong>NAME MIN-AVAILABLE MAX-UNAVAILABLE ALLOWED-DISRUPTIONS AGE</strong></p><p><strong>zk-pdb N/A 1 1</strong></p><p>In one terminal, watch the Pods in the <strong>zk</strong> StatefulSet.</p><p><strong>kubectl get pods -w -l app=zk</strong></p><p>In another terminal, get the nodes that the Pods are currently scheduled on.</p><p><strong>for i in 0 1 2; do kubectl get pod zk-$i --template {{.spec.nodeName}}; echo &quot;&quot;; done</strong></p><p><strong>kubernetes-minion-group-pb41</strong></p><p><strong>kubernetes-minion-group-ixsl</strong></p><p><strong>kubernetes-minion-group-i4c4</strong></p><p>Use <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#drain"><strong>kubectl drain</strong></a> to cordon and drain the node on which the <strong>zk-0</strong> Pod is scheduled.</p><p><strong>kubectl drain $(kubectl get pod zk-0 --template {{.spec.nodeName}}) --ignore-daemonsets --force --delete-local-data</strong></p><p><strong>node &quot;kubernetes-minion-group-pb41&quot; cordoned</strong></p><p><strong>WARNING: Deleting pods not managed by ReplicationController, ReplicaSet, Job, or DaemonSet: fluentd-cloud-logging-kubernetes-minion-group-pb41, kube-proxy-kubernetes-minion-group-pb41; Ignoring DaemonSet-managed pods: node-problem-detector-v0.1-o5elz</strong></p><p><strong>pod &quot;zk-0&quot; deleted</strong></p><p><strong>node &quot;kubernetes-minion-group-pb41&quot; drained</strong></p><p>As there are four nodes in your cluster, <strong>kubectl drain</strong>, succeeds and the <strong>zk-0</strong> is rescheduled to another node.</p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>zk-0 1/1 Running 2 1h</strong></p><p><strong>zk-1 1/1 Running 0 1h</strong></p><p><strong>zk-2 1/1 Running 0 1h</strong></p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>zk-0 1/1 Terminating 2 2h</strong></p><p><strong>zk-0 0/1 Terminating 2 2h</strong></p><p><strong>zk-0 0/1 Terminating 2 2h</strong></p><p><strong>zk-0 0/1 Terminating 2 2h</strong></p><p><strong>zk-0 0/1 Pending 0 0s</strong></p><p><strong>zk-0 0/1 Pending 0 0s</strong></p><p><strong>zk-0 0/1 ContainerCreating 0 0s</strong></p><p><strong>zk-0 0/1 Running 0 51s</strong></p><p><strong>zk-0 1/1 Running 0 1m</strong></p><p>Keep watching the StatefulSet&#x27;s Pods in the first terminal and drain the node on which <strong>zk-1</strong> is scheduled.</p><p><strong>kubectl drain $(kubectl get pod zk-1 --template {{.spec.nodeName}}) --ignore-daemonsets --force --delete-local-data &quot;kubernetes-minion-group-ixsl&quot; cordoned</strong></p><p><strong>WARNING: Deleting pods not managed by ReplicationController, ReplicaSet, Job, or DaemonSet: fluentd-cloud-logging-kubernetes-minion-group-ixsl, kube-proxy-kubernetes-minion-group-ixsl; Ignoring DaemonSet-managed pods: node-problem-detector-v0.1-voc74</strong></p><p><strong>pod &quot;zk-1&quot; deleted</strong></p><p><strong>node &quot;kubernetes-minion-group-ixsl&quot; drained</strong></p><p>The <strong>zk-1</strong> Pod can not be scheduled. As the <strong>zk</strong> StatefulSet contains a PodAntiAffinity rule preventing co-location of the Pods, and as only two nodes are schedulable, the Pod will remain in a Pending state.</p><p><strong>kubectl get pods -w -l app=zk</strong></p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>zk-0 1/1 Running 2 1h</strong></p><p><strong>zk-1 1/1 Running 0 1h</strong></p><p><strong>zk-2 1/1 Running 0 1h</strong></p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>zk-0 1/1 Terminating 2 2h</strong></p><p><strong>zk-0 0/1 Terminating 2 2h</strong></p><p><strong>zk-0 0/1 Terminating 2 2h</strong></p><p><strong>zk-0 0/1 Terminating 2 2h</strong></p><p><strong>zk-0 0/1 Pending 0 0s</strong></p><p><strong>zk-0 0/1 Pending 0 0s</strong></p><p><strong>zk-0 0/1 ContainerCreating 0 0s</strong></p><p><strong>zk-0 0/1 Running 0 51s</strong></p><p><strong>zk-0 1/1 Running 0 1m</strong></p><p><strong>zk-1 1/1 Terminating 0 2h</strong></p><p><strong>zk-1 0/1 Terminating 0 2h</strong></p><p><strong>zk-1 0/1 Terminating 0 2h</strong></p><p><strong>zk-1 0/1 Terminating 0 2h</strong></p><p><strong>zk-1 0/1 Pending 0 0s</strong></p><p><strong>zk-1 0/1 Pending 0 0s</strong></p><p>Continue to watch the Pods of the stateful set, and drain the node on which <strong>zk-2</strong> is scheduled.</p><p><strong>kubectl drain $(kubectl get pod zk-2 --template {{.spec.nodeName}}) --ignore-daemonsets --force --delete-local-data</strong></p><p><strong>node &quot;kubernetes-minion-group-i4c4&quot; cordoned</strong></p><p><strong>WARNING: Deleting pods not managed by ReplicationController, ReplicaSet, Job, or DaemonSet: fluentd-cloud-logging-kubernetes-minion-group-i4c4, kube-proxy-kubernetes-minion-group-i4c4; Ignoring DaemonSet-managed pods: node-problem-detector-v0.1-dyrog</strong></p><p><strong>WARNING: Ignoring DaemonSet-managed pods: node-problem-detector-v0.1-dyrog; Deleting pods not managed by ReplicationController, ReplicaSet, Job, or DaemonSet: fluentd-cloud-logging-kubernetes-minion-group-i4c4, kube-proxy-kubernetes-minion-group-i4c4</strong></p><p><strong>There are pending pods when an error occurred: Cannot evict pod as it would violate the pod\&#x27;s disruption budget.</strong></p><p><strong>pod/zk-2</strong></p><p>Use <strong>CTRL-C</strong> to terminate to kubectl.</p><p>You can not drain the third node because evicting <strong>zk-2</strong> would violate <strong>zk-budget</strong>. However, the node will remain cordoned.</p><p>Use <strong>zkCli.sh</strong> to retrieve the value you entered during the sanity test from <strong>zk-0</strong>.</p><p><strong>kubectl exec zk-0 zkCli.sh get /hello</strong></p><p>The service is still available because its PodDisruptionBudget is respected.</p><p><strong>WatchedEvent state:SyncConnected type:None path:null</strong></p><p><strong>world</strong></p><p><strong>cZxid = 0x200000002</strong></p><p><strong>ctime = Wed Dec 07 00:08:59 UTC 2016</strong></p><p><strong>mZxid = 0x200000002</strong></p><p><strong>mtime = Wed Dec 07 00:08:59 UTC 2016</strong></p><p><strong>pZxid = 0x200000002</strong></p><p><strong>cversion = 0</strong></p><p><strong>dataVersion = 0</strong></p><p><strong>aclVersion = 0</strong></p><p><strong>ephemeralOwner = 0x0</strong></p><p><strong>dataLength = 5</strong></p><p><strong>numChildren = 0</strong></p><p>Use <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#uncordon"><strong>kubectl uncordon</strong></a> to uncordon the first node.</p><p><strong>kubectl uncordon kubernetes-minion-group-pb41</strong></p><p><strong>node &quot;kubernetes-minion-group-pb41&quot; uncordoned</strong></p><p><strong>zk-1</strong> is rescheduled on this node. Wait until <strong>zk-1</strong> is Running and Ready.</p><p><strong>kubectl get pods -w -l app=zk</strong></p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>zk-0 1/1 Running 2 1h</strong></p><p><strong>zk-1 1/1 Running 0 1h</strong></p><p><strong>zk-2 1/1 Running 0 1h</strong></p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>zk-0 1/1 Terminating 2 2h</strong></p><p><strong>zk-0 0/1 Terminating 2 2h</strong></p><p><strong>zk-0 0/1 Terminating 2 2h</strong></p><p><strong>zk-0 0/1 Terminating 2 2h</strong></p><p><strong>zk-0 0/1 Pending 0 0s</strong></p><p><strong>zk-0 0/1 Pending 0 0s</strong></p><p><strong>zk-0 0/1 ContainerCreating 0 0s</strong></p><p><strong>zk-0 0/1 Running 0 51s</strong></p><p><strong>zk-0 1/1 Running 0 1m</strong></p><p><strong>zk-1 1/1 Terminating 0 2h</strong></p><p><strong>zk-1 0/1 Terminating 0 2h</strong></p><p><strong>zk-1 0/1 Terminating 0 2h</strong></p><p><strong>zk-1 0/1 Terminating 0 2h</strong></p><p><strong>zk-1 0/1 Pending 0 0s</strong></p><p><strong>zk-1 0/1 Pending 0 0s</strong></p><p><strong>zk-1 0/1 Pending 0 12m</strong></p><p><strong>zk-1 0/1 ContainerCreating 0 12m</strong></p><p><strong>zk-1 0/1 Running 0 13m</strong></p><p><strong>zk-1 1/1 Running 0 13m</strong></p><p>Attempt to drain the node on which <strong>zk-2</strong> is scheduled.</p><p><strong>kubectl drain $(kubectl get pod zk-2 --template {{.spec.nodeName}}) --ignore-daemonsets --force --delete-local-data</strong></p><p><strong>node &quot;kubernetes-minion-group-i4c4&quot; already cordoned</strong></p><p><strong>WARNING: Deleting pods not managed by ReplicationController, ReplicaSet, Job, or DaemonSet: fluentd-cloud-logging-kubernetes-minion-group-i4c4, kube-proxy-kubernetes-minion-group-i4c4; Ignoring DaemonSet-managed pods: node-problem-detector-v0.1-dyrog</strong></p><p><strong>pod &quot;heapster-v1.2.0-2604621511-wht1r&quot; deleted</strong></p><p><strong>pod &quot;zk-2&quot; deleted</strong></p><p><strong>node &quot;kubernetes-minion-group-i4c4&quot; drained</strong></p><p>This time <strong>kubectl drain</strong> succeeds.</p><p>Uncordon the second node to allow <strong>zk-2</strong> to be rescheduled.</p><p><strong>kubectl uncordon kubernetes-minion-group-ixsl</strong></p><p><strong>node &quot;kubernetes-minion-group-ixsl&quot; uncordoned</strong></p><p>You can use <strong>kubectl drain</strong> in conjunction with PodDisruptionBudgets to ensure that your service remains available during maintenance. If drain is used to cordon nodes and evict pods prior to taking the node offline for maintenance, services that express a disruption budget will have that budget respected. You should always allocate additional capacity for critical services so that their Pods can be immediately rescheduled.</p><h4>Cleaning up</h4><ul><li>Use <strong>kubectl uncordon</strong> to uncordon all the nodes in your cluster.</li><li>You will need to delete the persistent storage media for the PersistentVolumes used in this tutorial. Follow the necessary steps, based on your environment, storage configuration, and provisioning method, to ensure that all storage is reclaimed.</li></ul><h2>Cluster</h2><h3>AppArmor</h3><p><strong>FEATURE STATE:</strong> <strong>Kubernetes v1.4</strong> <a href="https://kubernetes.io/docs/tutorials/clusters/apparmor/">beta</a></p><p>AppArmor is a Linux kernel security module that supplements the standard Linux user and group based permissions to confine programs to a limited set of resources. AppArmor can be configured for any application to reduce its potential attack surface and provide greater in-depth defense. It is configured through profiles tuned to whitelist the access needed by a specific program or container, such as Linux capabilities, network access, file permissions, etc. Each profile can be run in eitherenforcing mode, which blocks access to disallowed resources, or complain mode, which only reports violations.</p><p>AppArmor can help you to run a more secure deployment by restricting what containers are allowed to do, and/or provide better auditing through system logs. However, it is important to keep in mind that AppArmor is not a silver bullet and can only do so much to protect against exploits in your application code. It is important to provide good, restrictive profiles, and harden your applications and cluster from other angles as well.</p><ul><li><a href="https://kubernetes.io/docs/tutorials/clusters/apparmor/#objectives"><strong>Objectives</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/clusters/apparmor/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/clusters/apparmor/#securing-a-pod"><strong>Securing a Pod</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/clusters/apparmor/#example"><strong>Example</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/clusters/apparmor/#administration"><strong>Administration</strong></a><ul><li><a href="https://kubernetes.io/docs/tutorials/clusters/apparmor/#setting-up-nodes-with-profiles"><strong>Setting up nodes with profiles</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/clusters/apparmor/#restricting-profiles-with-the-podsecuritypolicy"><strong>Restricting profiles with the PodSecurityPolicy</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/clusters/apparmor/#disabling-apparmor"><strong>Disabling AppArmor</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/clusters/apparmor/#upgrading-to-kubernetes-v14-with-apparmor"><strong>Upgrading to Kubernetes v1.4 with AppArmor</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/clusters/apparmor/#upgrade-path-to-general-availability"><strong>Upgrade path to General Availability</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tutorials/clusters/apparmor/#authoring-profiles"><strong>Authoring Profiles</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/clusters/apparmor/#api-reference"><strong>API Reference</strong></a><ul><li><a href="https://kubernetes.io/docs/tutorials/clusters/apparmor/#pod-annotation"><strong>Pod Annotation</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/clusters/apparmor/#profile-reference"><strong>Profile Reference</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/clusters/apparmor/#podsecuritypolicy-annotations"><strong>PodSecurityPolicy Annotations</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tutorials/clusters/apparmor/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Objectives</h4><ul><li>See an example of how to load a profile on a node</li><li>Learn how to enforce the profile on a Pod</li><li>Learn how to check that the profile is loaded</li><li>See what happens when a profile is violated</li><li>See what happens when a profile cannot be loaded</li></ul><h4>Before you begin</h4><p>Make sure:</p><ol><li>Kubernetes version is at least v1.4 -- Kubernetes support for AppArmor was added in v1.4. Kubernetes components older than v1.4 are not aware of the new AppArmor annotations, and will <strong>silently ignore</strong> any AppArmor settings that are provided. To ensure that your Pods are receiving the expected protections, it is important to verify the Kubelet version of your nodes:</li><li><strong>$ kubectl get nodes -o=jsonpath=$\&#x27;{range .items<!-- -->[*]<!-- -->}{@.metadata.name}: {@.status.nodeInfo.kubeletVersion}<!-- -->\<!-- -->n{end}\&#x27;</strong></li><li><strong>gke-test-default-pool-239f5d02-gyn2: v1.4.0</strong></li><li><strong>gke-test-default-pool-239f5d02-x1kf: v1.4.0</strong></li><li><strong>gke-test-default-pool-239f5d02-xwux: v1.4.0</strong></li><li>AppArmor kernel module is enabled -- For the Linux kernel to enforce an AppArmor profile, the AppArmor kernel module must be installed and enabled. Several distributions enable the module by default, such as Ubuntu and SUSE, and many others provide optional support. To check whether the module is enabled, check the <strong>/sys/module/apparmor/parameters/enabled</strong> file:</li><li><strong>$ cat /sys/module/apparmor/parameters/enabled</strong></li><li><strong>Y</strong></li></ol><p>If the Kubelet contains AppArmor support (&gt;= v1.4), it will refuse to run a Pod with AppArmor options if the kernel module is not enabled.</p><p><strong>Note:</strong> Ubuntu carries many AppArmor patches that have not been merged into the upstream Linux kernel, including patches that add additional hooks and features. Kubernetes has only been tested with the upstream version, and does not promise support for other features.</p><ol><li>Container runtime is Docker -- Currently the only Kubernetes-supported container runtime that also supports AppArmor is Docker. As more runtimes add AppArmor support, the options will be expanded. You can verify that your nodes are running docker with:</li><li><strong>$ kubectl get nodes -o=jsonpath=$\&#x27;{range .items<!-- -->[*]<!-- -->}{@.metadata.name}: {@.status.nodeInfo.containerRuntimeVersion}<!-- -->\<!-- -->n{end}\&#x27;</strong></li><li><strong>gke-test-default-pool-239f5d02-gyn2: docker://1.11.2</strong></li><li><strong>gke-test-default-pool-239f5d02-x1kf: docker://1.11.2</strong></li><li><strong>gke-test-default-pool-239f5d02-xwux: docker://1.11.2</strong></li></ol><p>If the Kubelet contains AppArmor support (&gt;= v1.4), it will refuse to run a Pod with AppArmor options if the runtime is not Docker.</p><ol><li>Profile is loaded -- AppArmor is applied to a Pod by specifying an AppArmor profile that each container should be run with. If any of the specified profiles is not already loaded in the kernel, the Kubelet (&gt;= v1.4) will reject the Pod. You can view which profiles are loaded on a node by checking the <strong>/sys/kernel/security/apparmor/profiles</strong> file. For example:</li><li><strong>$ ssh gke-test-default-pool-239f5d02-gyn2 &quot;sudo cat /sys/kernel/security/apparmor/profiles | sort&quot;</strong></li><li><strong>apparmor-test-deny-write (enforce)</strong></li><li><strong>apparmor-test-audit-write (enforce)</strong></li><li><strong>docker-default (enforce)</strong></li><li><strong>k8s-nginx (enforce)</strong></li></ol><p>For more details on loading profiles on nodes, see <a href="https://kubernetes.io/docs/tutorials/clusters/apparmor/#setting-up-nodes-with-profiles">Setting up nodes with profiles</a>.</p><p>As long as the Kubelet version includes AppArmor support (&gt;= v1.4), the Kubelet will reject a Pod with AppArmor options if any of the prerequisites are not met. You can also verify AppArmor support on nodes by checking the node ready condition message (though this is likely to be removed in a later release):</p><p><strong>$ kubectl get nodes -o=jsonpath=$\&#x27;{range .items<!-- -->[*]<!-- -->}{@.metadata.name}: {.status.conditions<!-- -->[?(@.reason==&quot;KubeletReady&quot;)]<!-- -->.message}<!-- -->\<!-- -->n{end}\&#x27;</strong></p><p><strong>gke-test-default-pool-239f5d02-gyn2: kubelet is posting ready status. AppArmor enabled</strong></p><p><strong>gke-test-default-pool-239f5d02-x1kf: kubelet is posting ready status. AppArmor enabled</strong></p><p><strong>gke-test-default-pool-239f5d02-xwux: kubelet is posting ready status. AppArmor enabled</strong></p><h4>Securing a Pod</h4><p><strong>Note:</strong> AppArmor is currently in beta, so options are specified as annotations. Once support graduates to general availability, the annotations will be replaced with first-class fields (more details in <a href="https://kubernetes.io/docs/tutorials/clusters/apparmor/#upgrade-path-to-general-availability">Upgrade path to GA</a>).</p><p>AppArmor profiles are specified per-container. To specify the AppArmor profile to run a Pod container with, add an annotation to the Pod&#x27;s metadata:</p><p><strong>container.apparmor.security.beta.kubernetes.io/<code>&lt;container_name&gt;: &lt;profile_ref&gt;</code></strong></p><p>Where <strong>`&lt;container_name&gt;</strong> is the name of the container to apply the profile to, and <strong>&lt;profile_ref&gt;`</strong>specifies the profile to apply. The <strong>profile_ref</strong> can be one of:</p><ul><li><strong>runtime/default</strong> to apply the runtime&#x27;s default profile</li><li><strong>localhost/`&lt;profile_name&gt;</strong> to apply the profile loaded on the host with the name <strong>&lt;profile_name&gt;`</strong></li><li><strong>unconfined</strong> to indicate that no profiles will be loaded</li></ul><p>See the <a href="https://kubernetes.io/docs/tutorials/clusters/apparmor/#api-reference">API Reference</a> for the full details on the annotation and profile name formats.</p><p>Kubernetes AppArmor enforcement works by first checking that all the prerequisites have been met, and then forwarding the profile selection to the container runtime for enforcement. If the prerequisites have not been met, the Pod will be rejected, and will not run.</p><p>To verify that the profile was applied, you can look for the AppArmor security option listed in the container created event:</p><p><strong>$ kubectl get events | grep Created</strong></p><p><strong>22s 22s 1 hello-apparmor Pod spec.containers{hello} Normal Created {kubelet e2e-test-stclair-minion-group-31nt} Created container with docker id 269a53b202d3; Security:<!-- -->[seccomp=unconfined apparmor=k8s-apparmor-example-deny-write]</strong></p><p>You can also verify directly that the container&#x27;s root process is running with the correct profile by checking its proc attr:</p><p><strong>$ kubectl exec <code>&lt;pod_name&gt;</code> cat /proc/1/attr/current</strong></p><p><strong>k8s-apparmor-example-deny-write (enforce)</strong></p><h4>Example</h4><p>This example assumes you have already set up a cluster with AppArmor support.</p><p>First, we need to load the profile we want to use onto our nodes. The profile we&#x27;ll use simply denies all file writes:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>deny-write.profile</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/">https://raw.githubusercontent.com/</a>          |
| kubernetes/website/master/docs/tutorials/clusters/deny-write.profile) |
+=======================================================================+
| <strong>#include <code>&lt;tunables/global&gt;</code></strong>                                     |
|                                                                       |
| <strong>profile k8s-apparmor-example-deny-write flags=(attach_disconnected) |
| {</strong>                                                                   |
|                                                                       |
| <strong>#include <code>&lt;abstractions/base&gt;</code></strong>                                   |
|                                                                       |
| <strong>file,</strong>                                                             |
|                                                                       |
| <strong># Deny all file writes.</strong>                                          |
|                                                                       |
| <strong>deny /<!-- -->*<!-- -->*<!-- --> w,</strong>                                                     |
|                                                                       |
| <strong>}</strong>                                                                 |
+-----------------------------------------------------------------------+</p><p>Since we don&#x27;t know where the Pod will be scheduled, we&#x27;ll need to load the profile on all our nodes. For this example we&#x27;ll just use SSH to install the profiles, but other approaches are discussed in <a href="https://kubernetes.io/docs/tutorials/clusters/apparmor/#setting-up-nodes-with-profiles">Setting up nodes with profiles</a>.</p><p><strong>$ NODES=(</strong></p><p><strong><em># The SSH-accessible domain names of your nodes</em></strong></p><p><strong>gke-test-default-pool-239f5d02-gyn2.us-central1-a.my-k8s</strong></p><p><strong>gke-test-default-pool-239f5d02-x1kf.us-central1-a.my-k8s</strong></p><p><strong>gke-test-default-pool-239f5d02-xwux.us-central1-a.my-k8s)</strong></p><p><strong>$ for NODE in ${NODES<!-- -->[*]<!-- -->}; do ssh $NODE \&#x27;sudo apparmor_parser -q &lt;&lt;EOF</strong></p><p><strong>#include <code>&lt;tunables/global&gt;</code></strong></p><p><strong>profile k8s-apparmor-example-deny-write flags=(attach_disconnected) {</strong></p><p><strong>#include <code>&lt;abstractions/base&gt;</code></strong></p><p><strong>file,</strong></p><p><strong># Deny all file writes.</strong></p><p><strong>deny /<!-- -->*<!-- -->*<!-- --> w,</strong></p><p><strong>}</strong></p><p><strong>EOF\&#x27;</strong></p><p><strong>done</strong></p><p>Next, we&#x27;ll run a simple &quot;Hello AppArmor&quot; pod with the deny-write profile:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[                                                                     |
| <strong>hello-apparmor-pod.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kuber">https://raw.githubusercontent.com/kuber</a> |
| netes/website/master/docs/tutorials/clusters/hello-apparmor-pod.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: hello-apparmor</strong>                                              |
|                                                                       |
| <strong>annotations:</strong>                                                      |
|                                                                       |
| <strong><em># Tell Kubernetes to apply the AppArmor profile                   |
| &quot;k8s-apparmor-example-deny-write&quot;.</em></strong>                               |
|                                                                       |
| <strong><em># Note that this is ignored if the Kubernetes node is not running |
| version 1.4 or greater.</em></strong>                                            |
|                                                                       |
| <strong>container.apparmor.security.beta.kubernetes.io/hello:               |
| localhost/k8s-apparmor-example-deny-write</strong>                           |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: hello</strong>                                                     |
|                                                                       |
| <strong>image: busybox</strong>                                                    |
|                                                                       |
| <strong>command: <!-- -->[ &quot;sh&quot;, &quot;-c&quot;, &quot;echo \&#x27;Hello AppArmor!\&#x27; &amp;&amp; sleep     |
| 1h&quot; ]</strong>                                                             |
+-----------------------------------------------------------------------+</p><p><strong>$ kubectl create -f ./hello-apparmor-pod.yaml</strong></p><p>If we look at the pod events, we can see that the Pod container was created with the AppArmor profile &quot;k8s-apparmor-example-deny-write&quot;:</p><p><strong>$ kubectl get events | grep hello-apparmor</strong></p><p><strong>14s 14s 1 hello-apparmor Pod Normal Scheduled {default-scheduler } Successfully assigned hello-apparmor to gke-test-default-pool-239f5d02-gyn2</strong></p><p><strong>14s 14s 1 hello-apparmor Pod spec.containers{hello} Normal Pulling {kubelet gke-test-default-pool-239f5d02-gyn2} pulling image &quot;busybox&quot;</strong></p><p><strong>13s 13s 1 hello-apparmor Pod spec.containers{hello} Normal Pulled {kubelet gke-test-default-pool-239f5d02-gyn2} Successfully pulled image &quot;busybox&quot;</strong></p><p><strong>13s 13s 1 hello-apparmor Pod spec.containers{hello} Normal Created {kubelet gke-test-default-pool-239f5d02-gyn2} Created container with docker id 06b6cd1c0989; Security:<!-- -->[seccomp=unconfined apparmor=k8s-apparmor-example-deny-write]</strong></p><p><strong>13s 13s 1 hello-apparmor Pod spec.containers{hello} Normal Started {kubelet gke-test-default-pool-239f5d02-gyn2} Started container with docker id 06b6cd1c0989</strong></p><p>We can verify that the container is actually running with that profile by checking its proc attr:</p><p><strong>$ kubectl exec hello-apparmor cat /proc/1/attr/current</strong></p><p><strong>k8s-apparmor-example-deny-write (enforce)</strong></p><p>Finally, we can see what happens if we try to violate the profile by writing to a file:</p><p><strong>$ kubectl exec hello-apparmor touch /tmp/test</strong></p><p><strong>touch: /tmp/test: Permission denied</strong></p><p><strong>error: error executing remote command: command terminated with non-zero exit code: Error executing in Docker Container: 1</strong></p><p>To wrap up, let&#x27;s look at what happens if we try to specify a profile that hasn&#x27;t been loaded:</p><p><strong>$ kubectl create -f /dev/stdin &lt;&lt;EOF</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: hello-apparmor-2</strong></p><p><strong>annotations:</strong></p><p><strong>container.apparmor.security.beta.kubernetes.io/hello: localhost/k8s-apparmor-example-allow-write</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: hello</strong></p><p><strong>image: busybox</strong></p><p><strong>command: <!-- -->[ &quot;sh&quot;, &quot;-c&quot;, &quot;echo \&#x27;Hello AppArmor!\&#x27; &amp;&amp; sleep 1h&quot; ]</strong></p><p><strong>EOF</strong></p><p><strong>pod &quot;hello-apparmor-2&quot; created</strong></p><p><strong>$ kubectl describe pod hello-apparmor-2</strong></p><p><strong>Name: hello-apparmor-2</strong></p><p><strong>Namespace: default</strong></p><p><strong>Node: gke-test-default-pool-239f5d02-x1kf/</strong></p><p><strong>Start Time: Tue, 30 Aug 2016 17:58:56 -0700</strong></p><p><strong>Labels: <code>&lt;none&gt;</code></strong></p><p><strong>Annotations: container.apparmor.security.beta.kubernetes.io/hello=localhost/k8s-apparmor-example-allow-write</strong></p><p><strong>Status: Pending</strong></p><p><strong>Reason: AppArmor</strong></p><p><strong>Message: Pod Cannot enforce AppArmor: profile &quot;k8s-apparmor-example-allow-write&quot; is not loaded</strong></p><p><strong>IP:</strong></p><p><strong>Controllers: <code>&lt;none&gt;</code></strong></p><p><strong>Containers:</strong></p><p><strong>hello:</strong></p><p><strong>Container ID:</strong></p><p><strong>Image: busybox</strong></p><p><strong>Image ID:</strong></p><p><strong>Port:</strong></p><p><strong>Command:</strong></p><p><strong>sh</strong></p><p><strong>-c</strong></p><p><strong>echo \&#x27;Hello AppArmor!\&#x27; &amp;&amp; sleep 1h</strong></p><p><strong>State: Waiting</strong></p><p><strong>Reason: Blocked</strong></p><p><strong>Ready: False</strong></p><p><strong>Restart Count: 0</strong></p><p><strong>Environment: <code>&lt;none&gt;</code></strong></p><p><strong>Mounts:</strong></p><p><strong>/var/run/secrets/kubernetes.io/serviceaccount from default-token-dnz7v (ro)</strong></p><p><strong>Conditions:</strong></p><p><strong>Type Status</strong></p><p><strong>Initialized True</strong></p><p><strong>Ready False</strong></p><p><strong>PodScheduled True</strong></p><p><strong>Volumes:</strong></p><p><strong>default-token-dnz7v:</strong></p><p><strong>Type: Secret (a volume populated by a Secret)</strong></p><p><strong>SecretName: default-token-dnz7v</strong></p><p><strong>Optional: false</strong></p><p><strong>QoS Class: BestEffort</strong></p><p><strong>Node-Selectors: <code>&lt;none&gt;</code></strong></p><p><strong>Tolerations: <code>&lt;none&gt;</code></strong></p><p><strong>Events:</strong></p><p><strong>FirstSeen LastSeen Count From SubobjectPath Type Reason Message</strong></p><p><strong>--------- -------- ----- ---- ------------- -------- ------ -------</strong></p><p><strong>23s 23s 1 {default-scheduler } Normal Scheduled Successfully assigned hello-apparmor-2 to e2e-test-stclair-minion-group-t1f5</strong></p><p><strong>23s 23s 1 {kubelet e2e-test-stclair-minion-group-t1f5} Warning AppArmor Cannot enforce AppArmor: profile &quot;k8s-apparmor-example-allow-write&quot; is not loaded</strong></p><p>Note the pod status is Failed, with a helpful error message: <strong>Pod Cannot enforce AppArmor: profile &quot;k8s-apparmor-example-allow-write&quot; is not loaded</strong>. An event was also recorded with the same message.</p><h4>Administration</h4><h5><strong>Setting up nodes with profiles</strong></h5><p>Kubernetes does not currently provide any native mechanisms for loading AppArmor profiles onto nodes. There are lots of ways to setup the profiles though, such as:</p><ul><li>Through a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/">DaemonSet</a> that runs a Pod on each node to ensure the correct profiles are loaded. An example implementation can be found <a href="https://git.k8s.io/contrib/apparmor/loader">here</a>.</li><li>At node initialization time, using your node initialization scripts (e.g. Salt, Ansible, etc.) or image.</li><li>By copying the profiles to each node and loading them through SSH, as demonstrated in the<a href="https://kubernetes.io/docs/tutorials/clusters/apparmor/#example">Example</a>.</li></ul><p>The scheduler is not aware of which profiles are loaded onto which node, so the full set of profiles must be loaded onto every node. An alternative approach is to add a node label for each profile (or class of profiles) on the node, and use a <a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/">node selector</a> to ensure the Pod is run on a node with the required profile.</p><h5><strong>Restricting profiles with the PodSecurityPolicy</strong></h5><p>If the PodSecurityPolicy extension is enabled, cluster-wide AppArmor restrictions can be applied. To enable the PodSecurityPolicy, the following flag must be set on the <strong>apiserver</strong>:</p><p><strong>--enable-admission-plugins=PodSecurityPolicy<!-- -->[,others<!-- -->.<!-- -->..]</strong></p><p>The AppArmor options can be specified as annotations on the PodSecurityPolicy:</p><p><strong>apparmor.security.beta.kubernetes.io/defaultProfileName: <code>&lt;profile_ref&gt;</code></strong></p><p><strong>apparmor.security.beta.kubernetes.io/allowedProfileNames: <code>&lt;profile_ref&gt;</code>[,others<!-- -->.<!-- -->..]</strong></p><p>The default profile name option specifies the profile to apply to containers by default when none is specified. The allowed profile names option specifies a list of profiles that Pod containers are allowed to be run with. If both options are provided, the default must be allowed. The profiles are specified in the same format as on containers. See the <a href="https://kubernetes.io/docs/tutorials/clusters/apparmor/#api-reference">API Reference</a> for the full specification.</p><h5><strong>Disabling AppArmor</strong></h5><p>If you do not want AppArmor to be available on your cluster, it can be disabled by a command-line flag:</p><p><strong>--feature-gates=AppArmor=false</strong></p><p>When disabled, any Pod that includes an AppArmor profile will fail validation with a &quot;Forbidden&quot; error. Note that by default docker always enables the &quot;docker-default&quot; profile on non-privileged pods (if the AppArmor kernel module is enabled), and will continue to do so even if the feature-gate is disabled. The option to disable AppArmor will be removed when AppArmor graduates to general availability (GA).</p><h5><strong>Upgrading to Kubernetes v1.4 with AppArmor</strong></h5><p>No action is required with respect to AppArmor to upgrade your cluster to v1.4. However, if any existing pods had an AppArmor annotation, they will not go through validation (or PodSecurityPolicy admission). If permissive profiles are loaded on the nodes, a malicious user could pre-apply a permissive profile to escalate the pod privileges above the docker-default. If this is a concern, it is recommended to scrub the cluster of any pods containing an annotation with <strong>apparmor.security.beta.kubernetes.io</strong>.</p><h5><strong>Upgrade path to General Availability</strong></h5><p>When AppArmor is ready to be graduated to general availability (GA), the options currently specified through annotations will be converted to fields. Supporting all the upgrade and downgrade paths through the transition is very nuanced, and will be explained in detail when the transition occurs. We will commit to supporting both fields and annotations for at least 2 releases, and will explicitly reject the annotations for at least 2 releases after that.</p><h4>Authoring Profiles</h4><p>Getting AppArmor profiles specified correctly can be a tricky business. Fortunately there are some tools to help with that:</p><ul><li><strong>aa-genprof</strong> and <strong>aa-logprof</strong> generate profile rules by monitoring an application&#x27;s activity and logs, and admitting the actions it takes. Further instructions are provided by the <a href="http://wiki.apparmor.net/index.php/Profiling_with_tools">AppArmor documentation</a>.</li><li><a href="https://github.com/jfrazelle/bane">bane</a> is an AppArmor profile generator for Docker that uses a simplified profile language.</li></ul><p>It is recommended to run your application through Docker on a development workstation to generate the profiles, but there is nothing preventing running the tools on the Kubernetes node where your Pod is running.</p><p>To debug problems with AppArmor, you can check the system logs to see what, specifically, was denied. AppArmor logs verbose messages to <strong>dmesg</strong>, and errors can usually be found in the system logs or through <strong>journalctl</strong>. More information is provided in <a href="http://wiki.apparmor.net/index.php/AppArmor_Failures">AppArmor failures</a>.</p><h4>API Reference</h4><h5><strong>Pod Annotation</strong></h5><p>Specifying the profile a container will run with:</p><ul><li><strong>key</strong>: <strong>container.apparmor.security.beta.kubernetes.io/`&lt;container_name&gt;</strong> Where <strong>&lt;container_name&gt;`</strong> matches the name of a container in the Pod. A separate profile can be specified for each container in the Pod.</li><li><strong>value</strong>: a profile reference, described below</li></ul><h5><strong>Profile Reference</strong></h5><ul><li><strong>runtime/default</strong>: Refers to the default runtime profile.<ul><li>Equivalent to not specifying a profile (without a PodSecurityPolicy default), except it still requires AppArmor to be enabled.</li><li>For Docker, this resolves to the <a href="https://docs.docker.com/engine/security/apparmor/"><strong>docker-default</strong></a> profile for non-privileged containers, and unconfined (no profile) for privileged containers.</li></ul></li><li><strong>localhost/<code>&lt;profile_name&gt;</code></strong>: Refers to a profile loaded on the node (localhost) by name.<ul><li>The possible profile names are detailed in the <a href="http://wiki.apparmor.net/index.php/AppArmor_Core_Policy_Reference#Profile_names_and_attachment_specifications">core policy reference</a>.</li></ul></li><li><strong>unconfined</strong>: This effectively disables AppArmor on the container.</li></ul><p>Any other profile reference format is invalid.</p><h5><strong>PodSecurityPolicy Annotations</strong></h5><p>Specifying the default profile to apply to containers when none is provided:</p><ul><li><strong>key</strong>: <strong>apparmor.security.beta.kubernetes.io/defaultProfileName</strong></li><li><strong>value</strong>: a profile reference, described above</li></ul><p>Specifying the list of profiles Pod containers is allowed to specify:</p><ul><li><strong>key</strong>: <strong>apparmor.security.beta.kubernetes.io/allowedProfileNames</strong></li><li><strong>value</strong>: a comma-separated list of profile references (described above)<ul><li>Although an escaped comma is a legal character in a profile name, it cannot be explicitly allowed here.</li></ul></li></ul><h4>What&#x27;s next</h4><p>Additional resources:</p><ul><li><a href="http://wiki.apparmor.net/index.php/QuickProfileLanguage">Quick guide to the AppArmor profile language</a></li><li><a href="http://wiki.apparmor.net/index.php/ProfileLanguage">AppArmor core policy reference</a></li></ul><h2>Services</h2><h3>Using Source IP</h3><p>Applications running in a Kubernetes cluster find and communicate with each other, and the outside world, through the Service abstraction. This document explains what happens to the source IP of packets sent to different types of Services, and how you can toggle this behavior according to your needs.</p><ul><li><a href="https://kubernetes.io/docs/tutorials/services/source-ip/#objectives"><strong>Objectives</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/services/source-ip/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/services/source-ip/#terminology"><strong>Terminology</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/services/source-ip/#prerequisites"><strong>Prerequisites</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-typeclusterip"><strong>Source IP for Services with Type=ClusterIP</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-typenodeport"><strong>Source IP for Services with Type=NodePort</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-typeloadbalancer"><strong>Source IP for Services with Type=LoadBalancer</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/services/source-ip/#cleaning-up"><strong>Cleaning up</strong></a></li><li><a href="https://kubernetes.io/docs/tutorials/services/source-ip/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Objectives</h4><ul><li>Expose a simple application through various types of Services</li><li>Understand how each Service type handles source IP NAT</li><li>Understand the tradeoffs involved in preserving source IP</li></ul><h4>Before you begin</h4><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using<a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <strong>kubectl version</strong>.</p><h4>Terminology</h4><p>This document makes use of the following terms:</p><ul><li><a href="https://en.wikipedia.org/wiki/Network_address_translation">NAT</a>: network address translation</li><li><a href="https://en.wikipedia.org/wiki/Network_address_translation#SNAT">Source NAT</a>: replacing the source IP on a packet, usually with a node&#x27;s IP</li><li><a href="https://en.wikipedia.org/wiki/Network_address_translation#DNAT">Destination NAT</a>: replacing the destination IP on a packet, usually with a pod IP</li><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies">VIP</a>: a virtual IP, such as the one assigned to every Kubernetes Service</li><li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies">Kube-proxy</a>: a network daemon that orchestrates Service VIP management on every node</li></ul><h4>Prerequisites</h4><p>You must have a working Kubernetes 1.5 cluster to run the examples in this document. The examples use a small nginx webserver that echoes back the source IP of requests it receives through an HTTP header. You can create it as follows:</p><p><strong>$ kubectl run source-ip-app --image=k8s.gcr.io/echoserver:1.4</strong></p><p><strong>deployment &quot;source-ip-app&quot; created</strong></p><h4>Source IP for Services with Type=ClusterIP</h4><p>Packets sent to ClusterIP from within the cluster are never source NAT&#x27;d if you&#x27;re running kube-proxy in <a href="https://kubernetes.io/docs/concepts/services-networking/service/#proxy-mode-iptables">iptables mode</a>, which is the default since Kubernetes 1.2. Kube-proxy exposes its mode through a <strong>proxyMode</strong> endpoint:</p><p><strong>$ kubectl get nodes</strong></p><p><strong>NAME STATUS AGE VERSION</strong></p><p><strong>kubernetes-minion-group-6jst Ready 2h v1.6.0+fff5156</strong></p><p><strong>kubernetes-minion-group-cx31 Ready 2h v1.6.0+fff5156</strong></p><p><strong>kubernetes-minion-group-jj1t Ready 2h v1.6.0+fff5156</strong></p><p><strong>kubernetes-minion-group-6jst $ curl localhost:10249/proxyMode</strong></p><p><strong>iptables</strong></p><p>You can test source IP preservation by creating a Service over the source IP app:</p><p><strong>$ kubectl expose deployment source-ip-app --name=clusterip --port=80 --target-port=8080</strong></p><p><strong>service &quot;clusterip&quot; exposed</strong></p><p><strong>$ kubectl get svc clusterip</strong></p><p><strong>NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE</strong></p><p><strong>clusterip 10.0.170.92 <code>&lt;none&gt;</code> 80/TCP 51s</strong></p><p>And hitting the <strong>ClusterIP</strong> from a pod in the same cluster:</p><p><strong>$ kubectl run busybox -it --image=busybox --restart=Never --rm</strong></p><p><strong>Waiting for pod default/busybox to be running, status is Pending, pod ready: false</strong></p><p><strong>If you don\&#x27;t see a command prompt, try pressing enter.</strong></p><p><strong># ip addr</strong></p><p><strong>1: lo: <code>&lt;LOOPBACK,UP,LOWER_UP&gt;</code> mtu 65536 qdisc noqueue</strong></p><p><strong>link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</strong></p><p><strong>inet 127.0.0.1/8 scope host lo</strong></p><p><strong>valid_lft forever preferred_lft forever</strong></p><p><strong>inet6 ::1/128 scope host</strong></p><p><strong>valid_lft forever preferred_lft forever</strong></p><p><strong>3: eth0: <code>&lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt;</code> mtu 1460 qdisc noqueue</strong></p><p><strong>link/ether 0a:58:0a:f4:03:08 brd ff:ff:ff:ff:ff:ff</strong></p><p><strong>inet 10.244.3.8/24 scope global eth0</strong></p><p><strong>valid_lft forever preferred_lft forever</strong></p><p><strong>inet6 fe80::188a:84ff:feb0:26a5/64 scope link</strong></p><p><strong>valid_lft forever preferred_lft forever</strong></p><p><strong># wget -qO - 10.0.170.92</strong></p><p><strong>CLIENT VALUES:</strong></p><p><strong>client_address=10.244.3.8</strong></p><p><strong>command=GET</strong></p><p><strong><em>.<!-- -->..</em></strong></p><p>If the client pod and server pod are in the same node, the client_address is the client pod&#x27;s IP address. However, if the client pod and server pod are in different nodes, the client_address is the client pod&#x27;s node flannel IP address.</p><h4>Source IP for Services with Type=NodePort</h4><p>As of Kubernetes 1.5, packets sent to Services with <a href="https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport">Type=NodePort</a> are source NAT&#x27;d by default. You can test this by creating a <strong>NodePort</strong> Service:</p><p><strong>$ kubectl expose deployment source-ip-app --name=nodeport --port=80 --target-port=8080 --type=NodePort</strong></p><p><strong>service &quot;nodeport&quot; exposed</strong></p><p><strong>$ NODEPORT=$(kubectl get -o jsonpath=&quot;{.spec.ports<!-- -->[0]<!-- -->.nodePort}&quot; services nodeport)</strong></p><p><strong>$ NODES=$(kubectl get nodes -o jsonpath=\&#x27;{ $.items<!-- -->[*]<!-- -->.status.addresses<!-- -->[?(@.type==&quot;ExternalIP&quot;)]<!-- -->.address }\&#x27;)</strong></p><p>If you&#x27;re running on a cloudprovider, you may need to open up a firewall-rule for the <strong>nodes:nodeport</strong> reported above. Now you can try reaching the Service from outside the cluster through the node port allocated above.</p><p><strong>$ for node in $NODES; do curl -s $node:$NODEPORT | grep -i client_address; done</strong></p><p><strong>client_address=10.180.1.1</strong></p><p><strong>client_address=10.240.0.5</strong></p><p><strong>client_address=10.240.0.3</strong></p><p>Note that these are not the correct client IPs, they&#x27;re cluster internal IPs. This is what happens:</p><ul><li>Client sends packet to <strong>node2:nodePort</strong></li><li><strong>node2</strong> replaces the source IP address (SNAT) in the packet with its own IP address</li><li><strong>node2</strong> replaces the destination IP on the packet with the pod IP</li><li>packet is routed to node 1, and then to the endpoint</li><li>the pod&#x27;s reply is routed back to node2</li><li>the pod&#x27;s reply is sent back to the client</li></ul><p>Visually:</p><p><strong>client</strong></p><p><strong>\<!-- --> \^</strong></p><p><strong>\<!-- --> <!-- -->\</strong></p><p><strong>v <!-- -->\</strong></p><p><strong>node 1 &lt;--- node 2</strong></p><p><strong>| \^ SNAT</strong></p><p><strong>| | ---&gt;</strong></p><p><strong>v |</strong></p><p><strong>endpoint</strong></p><p>To avoid this, Kubernetes has a feature to preserve the client source IP <a href="https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip">(check here for feature availability)</a>. Setting <strong>service.spec.externalTrafficPolicy</strong> to the value <strong>Local</strong> will only proxy requests to local endpoints, never forwarding traffic to other nodes and thereby preserving the original source IP address. If there are no local endpoints, packets sent to the node are dropped, so you can rely on the correct source-ip in any packet processing rules you might apply a packet that make it through to the endpoint.</p><p>Set the <strong>service.spec.externalTrafficPolicy</strong> field as follows:</p><p><strong>$ kubectl patch svc nodeport -p \&#x27;{&quot;spec&quot;:{&quot;externalTrafficPolicy&quot;:&quot;Local&quot;}}\&#x27;</strong></p><p><strong>service &quot;nodeport&quot; patched</strong></p><p>Now, re-run the test:</p><p><strong>$ for node in $NODES; do curl --connect-timeout 1 -s $node:$NODEPORT | grep -i client_address; done</strong></p><p><strong>client_address=104.132.1.79</strong></p><p>Note that you only got one reply, with the right client IP, from the one node on which the endpoint pod is running.</p><p>This is what happens:</p><ul><li>client sends packet to <strong>node2:nodePort</strong>, which doesn&#x27;t have any endpoints</li><li>packet is dropped</li><li>client sends packet to <strong>node1:nodePort</strong>, which does have endpoints</li><li>node1 routes packet to endpoint with the correct source IP</li></ul><p>Visually:</p><p><strong>client</strong></p><p><strong>\^ / <!-- -->\</strong></p><p><strong>/ / <!-- -->\</strong></p><p><strong>/ v X</strong></p><p><strong>node 1 node 2</strong></p><p><strong>\^ |</strong></p><p><strong>| |</strong></p><p><strong>| v</strong></p><p><strong>endpoint</strong></p><h4>Source IP for Services with Type=LoadBalancer</h4><p>As of Kubernetes 1.5, packets sent to Services with <a href="https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer">Type=LoadBalancer</a> are source NAT&#x27;d by default, because all schedulable Kubernetes nodes in the <strong>Ready</strong> state are eligible for loadbalanced traffic. So if packets arrive at a node without an endpoint, the system proxies it to a node with an endpoint, replacing the source IP on the packet with the IP of the node (as described in the previous section).</p><p>You can test this by exposing the source-ip-app through a loadbalancer</p><p><strong>$ kubectl expose deployment source-ip-app --name=loadbalancer --port=80 --target-port=8080 --type=LoadBalancer</strong></p><p><strong>service &quot;loadbalancer&quot; exposed</strong></p><p><strong>$ kubectl get svc loadbalancer</strong></p><p><strong>NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE</strong></p><p><strong>loadbalancer 10.0.65.118 104.198.149.140 80/TCP 5m</strong></p><p><strong>$ curl 104.198.149.140</strong></p><p><strong>CLIENT VALUES:</strong></p><p><strong>client_address=10.240.0.5</strong></p><p><strong><em>.<!-- -->..</em></strong></p><p>However, if you&#x27;re running on Google Kubernetes Engine/GCE, setting the same <strong>service.spec.externalTrafficPolicy</strong> field to <strong>Local</strong> forces nodes without Service endpoints to remove themselves from the list of nodes eligible for loadbalanced traffic by deliberately failing health checks.</p><p>Visually:</p><p><strong>client</strong></p><p><strong>|</strong></p><p><strong>lb VIP</strong></p><p><strong>/ \^</strong></p><p><strong>v /</strong></p><p><strong>health check ---&gt; node 1 node 2 &lt;--- health check</strong></p><p><strong>200 <code>&lt;--- \^ | ---&gt;</code> 500</strong></p><p><strong>| V</strong></p><p><strong>endpoint</strong></p><p>You can test this by setting the annotation:</p><p><strong>$ kubectl patch svc loadbalancer -p \&#x27;{&quot;spec&quot;:{&quot;externalTrafficPolicy&quot;:&quot;Local&quot;}}\&#x27;</strong></p><p>You should immediately see the <strong>service.spec.healthCheckNodePort</strong> field allocated by Kubernetes:</p><p><strong>$ kubectl get svc loadbalancer -o yaml | grep -i healthCheckNodePort</strong></p><p><strong>healthCheckNodePort: 32122</strong></p><p>The <strong>service.spec.healthCheckNodePort</strong> field points to a port on every node serving the health check at <strong>/healthz</strong>. You can test this:</p><p><strong>$ kubectl get pod -o wide -l run=source-ip-app</strong></p><p><strong>NAME READY STATUS RESTARTS AGE IP NODE</strong></p><p><strong>source-ip-app-826191075-qehz4 1/1 Running 0 20h 10.180.1.136 kubernetes-minion-group-6jst</strong></p><p><strong>kubernetes-minion-group-6jst $ curl localhost:32122/healthz</strong></p><p><strong>1 Service Endpoints found</strong></p><p><strong>kubernetes-minion-group-jj1t $ curl localhost:32122/healthz</strong></p><p><strong>No Service Endpoints Found</strong></p><p>A service controller running on the master is responsible for allocating the cloud loadbalancer, and when it does so, it also allocates HTTP health checks pointing to this port/path on each node. Wait about 10 seconds for the 2 nodes without endpoints to fail health checks, then curl the lb ip:</p><p><strong>$ curl 104.198.149.140</strong></p><p><strong>CLIENT VALUES:</strong></p><p><strong>client_address=104.132.1.79</strong></p><p><strong><em>.<!-- -->..</em></strong></p><p><strong>Cross platform support</strong></p><p>As of Kubernetes 1.5, support for source IP preservation through Services with Type=LoadBalancer is only implemented in a subset of cloudproviders (GCP and Azure). The cloudprovider you&#x27;re running on might fulfill the request for a loadbalancer in a few different ways:</p><ol><li>With a proxy that terminates the client connection and opens a new connection to your nodes/endpoints. In such cases the source IP will always be that of the cloud LB, not that of the client.</li><li>With a packet forwarder, such that requests from the client sent to the loadbalancer VIP end up at the node with the source IP of the client, not an intermediate proxy.</li></ol><p>Loadbalancers in the first category must use an agreed upon protocol between the loadbalancer and backend to communicate the true client IP such as the HTTP <a href="https://en.wikipedia.org/wiki/X-Forwarded-For">X-FORWARDED-FOR</a> header, or the <a href="http://www.haproxy.org/download/1.5/doc/proxy-protocol.txt">proxy protocol</a>. Loadbalancers in the second category can leverage the feature described above by simply creating an HTTP health check pointing at the port stored in the <strong>service.spec.healthCheckNodePort</strong> field on the Service.</p><h4>Cleaning up</h4><p>Delete the Services:</p><p><strong>$ kubectl delete svc -l run=source-ip-app</strong></p><p>Delete the Deployment, ReplicaSet and Pod:</p><p><strong>$ kubectl delete deployment source-ip-app</strong></p><h4>What&#x27;s next</h4><ul><li>Learn more about <a href="https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/">connecting applications via services</a></li><li>Learn more about <a href="https://kubernetes.io/docs/user-guide/load-balancer">loadbalancing</a></li></ul><h1>Tasks</h1><h2>Tasks</h2><p>This section of the Kubernetes documentation contains pages that show how to do individual tasks. A task page shows how to do a single thing, typically by giving a short sequence of steps.</p><h5><strong>Web UI (Dashboard)</strong></h5><p>Deploy and access the Dashboard web user interface to help you manage and monitor containerized applications in a Kubernetes cluster.</p><h5><strong>Using the kubectl Command-line</strong></h5><p>Install and setup the <strong>kubectl</strong> command-line tool used to directly manage Kubernetes clusters.</p><h5><strong>Configuring Pods and Containers</strong></h5><p>Perform common configuration tasks for Pods and Containers.</p><h5><strong>Running Applications</strong></h5><p>Perform common application management tasks, such as rolling updates, injecting information into pods, and horizontal Pod autoscaling.</p><h5><strong>Running Jobs</strong></h5><p>Run Jobs using parallel processing.</p><h5><strong>Accessing Applications in a Cluster</strong></h5><p>Configure load balancing, port forwarding, or setup firewall or DNS configurations to access applications in a cluster.</p><h5><strong>Monitoring, Logging, and Debugging</strong></h5><p>Setup monitoring and logging to troubleshoot a cluster or debug a containerized application.</p><h5><strong>Accessing the Kubernetes API</strong></h5><p>Learn various methods to directly access the Kubernetes API.</p><h5><strong>Using TLS</strong></h5><p>Configure your application to trust and use the cluster root Certificate Authority (CA).</p><h5><strong>Administering a Cluster</strong></h5><p>Learn common tasks for administering a cluster.</p><h5><strong>Administering Federation</strong></h5><p>Configure components in a cluster federation.</p><h5><strong>Managing Stateful Applications</strong></h5><p>Perform common tasks for managing Stateful applications, including scaling, deleting, and debugging StatefulSets.</p><h5>Cluster Daemons</h5><p>Perform common tasks for managing a DaemonSet, such as performing a rolling update.</p><h5>Managing GPUs</h5><p>Configure and schedule NVIDIA GPUs for use as a resource by nodes in a cluster.</p><h5>Managing HugePages</h5><p>Configure and schedule huge pages as a schedulable resource in a cluster.</p><h4>What&#x27;s next</h4><p>If you would like to write a task page, see <a href="https://kubernetes.io/docs/home/contribute/create-pull-request/">Creating a Documentation Pull Request</a>.</p><h2>Install Tools</h2><h3>Install and Set Up kubectl</h3><h4>Install kubectl</h4><p>Here are a few methods to install kubectl. Pick the one that suits your environment best.</p><p>Use the Kubernetes command-line tool, <a href="https://kubernetes.io/docs/user-guide/kubectl/">kubectl</a>, to deploy and manage applications on Kubernetes. Using kubectl, you can inspect cluster resources; create, delete, and update components; and look at your new cluster and bring up example apps.</p><h4>Before you begin</h4><p>Use a version of kubectl that is the same version as your server or later. Using an older kubectl with a newer server might produce validation errors.</p><h5><strong>Install kubectl binary via curl</strong></h5><ul><li><a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/#tabset-1">Linux</a></li></ul><ol><li>Download the latest release with the command:</li><li><strong>curl -LO <a href="https://storage.googleapis.com/kubernetes-release/release/$(curl">https://storage.googleapis.com/kubernetes-release/release/$(curl</a> -s <a href="https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl">https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl</a></strong></li></ol><p>To download a specific version, replace the <strong>$(curl -s <a href="https://storage.googleapis.com/kubernetes-release/release/stable.txt">https://storage.googleapis.com/kubernetes-release/release/stable.txt</a>)</strong>portion of the command with the specific version.</p><p>For example, to download version v1.10.0 on Linux, type:</p><p><strong>curl -LO <a href="https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/linux/amd64/kubectl">https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/linux/amd64/kubectl</a></strong></p><ol><li>Make the kubectl binary executable.</li><li><strong>chmod +x ./kubectl</strong></li><li>Move the binary in to your PATH.</li><li><strong>sudo mv ./kubectl /usr/local/bin/kubectl</strong></li></ol><h5><strong>Download as part of the Google Cloud SDK</strong></h5><p>kubectl can be installed as part of the Google Cloud SDK.</p><ol><li>Install the <a href="https://cloud.google.com/sdk/">Google Cloud SDK</a>.</li><li>Run the following command to install <strong>kubectl</strong>:</li><li><strong>gcloud components install kubectl</strong></li><li>Run <strong>kubectl version</strong> to verify that the version you&#x27;ve installed is sufficiently up-to-date.</li></ol><h5><strong>Install with snap on Ubuntu</strong></h5><p>kubectl is available as a <a href="https://snapcraft.io/">snap</a> application.</p><ol><li>If you are on Ubuntu or one of other Linux distributions that support <a href="https://snapcraft.io/docs/core/install">snap</a> package manager, you can install with:</li><li><strong>sudo snap install kubectl --classic</strong></li><li>Run <strong>kubectl version</strong> to verify that the version you&#x27;ve installed is sufficiently up-to-date.</li></ol><h5><strong>Install with Homebrew on macOS</strong></h5><ol><li>If you are on macOS and using <a href="https://brew.sh/">Homebrew</a> package manager, you can install with:</li><li><strong>brew install kubectl</strong></li><li>Run <strong>kubectl version</strong> to verify that the version you&#x27;ve installed is sufficiently up-to-date.</li></ol><h5><strong>Install with Powershell from PSGallery</strong></h5><ol><li>If you are on Windows and using <a href="https://www.powershellgallery.com/">Powershell Gallery</a> package manager, you can install and update with:</li><li><code>Install-Script -Name install-kubectl -Scope CurrentUser -Force</code></li><li><code>install-kubectl.ps1 [-DownloadLocation &lt;path&gt;]</code></li></ol><p>If no Downloadlocation is specified, kubectl will be installed in users temp Directory</p><ol><li>The installer creates $HOME/.kube and instructs it to create a config file</li><li>Updating re-run Install-Script to update the installer re-run install-kubectl.ps1 to install latest binaries</li></ol><h5><strong>Install with Chocolatey on Windows</strong></h5><ol><li>If you are on Windows and using <a href="https://chocolatey.org/">Chocolatey</a> package manager, you can install with:</li><li><strong>choco install kubernetes-cli</strong></li><li>Run <strong>kubectl version</strong> to verify that the version you&#x27;ve installed is sufficiently up-to-date.</li><li>Configure kubectl to use a remote Kubernetes cluster:</li><li><strong>cd C:<!-- -->\<!-- -->users<!-- -->\<!-- -->yourusername (Or wherever your %HOME% directory is)</strong></li><li><strong>mkdir .kube</strong></li><li><strong>cd .kube</strong></li><li><strong>New-Item config -type file</strong></li></ol><p>Edit the config file with a text editor of your choice, such as Notepad for example.</p><h4>Configure kubectl</h4><p>In order for kubectl to find and access a Kubernetes cluster, it needs a <a href="https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/">kubeconfig file</a>, which is created automatically when you create a cluster using kube-up.sh or successfully deploy a Minikube cluster. See the <a href="https://kubernetes.io/docs/setup/">getting started guides</a> for more about creating clusters. If you need access to a cluster you didn&#x27;t create, see the <a href="https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/">Sharing Cluster Access document</a>. By default, kubectl configuration is located at <strong>~<!-- -->/.kube/config</strong>.</p><h4>Check the kubectl configuration</h4><p>Check that kubectl is properly configured by getting the cluster state:</p><p><strong>kubectl cluster-info</strong></p><p>If you see a URL response, kubectl is correctly configured to access your cluster.</p><p>If you see a message similar to the following, kubectl is not correctly configured or not able to connect to a Kubernetes cluster.</p><p><strong>The connection to the server <code>&lt;server-name:port&gt;</code> was refused - did you specify the right host or port?</strong></p><p>For example, if you are intending to run a Kubernetes cluster on your laptop (locally), you will need a tool like minikube to be installed first and then re-run the commands stated above.</p><p>If kubectl cluster-info returns the url response but you can&#x27;t access your cluster, to check whether it is configured properly, use:</p><p><strong>kubectl cluster-info dump</strong></p><h4>Enabling shell autocompletion</h4><p>kubectl includes autocompletion support, which can save a lot of typing!</p><p>The completion script itself is generated by kubectl, so you typically just need to invoke it from your profile.</p><p>Common examples are provided here. For more details, consult <strong>kubectl completion -h</strong>.</p><h5><strong>On Linux, using bash</strong></h5><p>To add kubectl autocompletion to your current shell, run <strong>source &lt;(kubectl completion bash)</strong>.</p><p>To add kubectl autocompletion to your profile, so it is automatically loaded in future shells run:</p><p><strong>echo &quot;source <code>&lt;(kubectl completion bash)&quot; &gt;&gt;</code> <!-- -->~<!-- -->/.bashrc</strong></p><h5><strong>On macOS, using bash</strong></h5><p>On macOS, you will need to install bash-completion support via <a href="https://brew.sh/">Homebrew</a> first:</p><p><strong><em>## If running Bash 3.2 included with macOS</em></strong></p><p><strong>brew install bash-completion</strong></p><p><strong><em>## or, if running Bash 4.1+</em></strong></p><p><strong>brew install bash-completion@2</strong></p><p>Follow the &quot;caveats&quot; section of brew&#x27;s output to add the appropriate bash completion path to your local .bashrc.</p><p>If you&#x27;ve installed kubectl using the <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-with-homebrew-on-macos">Homebrew instructions</a> then kubectl completion should start working immediately.</p><p>If you have installed kubectl manually, you need to add kubectl autocompletion to the bash-completion:</p><p><strong>kubectl completion bash &gt; $(brew --prefix)/etc/bash_completion.d/kubectl</strong></p><p>The Homebrew project is independent from Kubernetes, so the bash-completion packages are not guaranteed to work.</p><h5><strong>Using Zsh</strong></h5><p>If you are using zsh edit the <!-- -->~<!-- -->/.zshrc file and add the following code to enable kubectl autocompletion:</p><p><strong>if [ $commands<!-- -->[kubectl]<!-- --> ]; then</strong></p><p><strong>source &lt;(kubectl completion zsh)</strong></p><p><strong>fi</strong></p><p>Or when using <a href="http://ohmyz.sh/">Oh-My-Zsh</a>, edit the <!-- -->~<!-- -->/.zshrc file and update the <strong>plugins=</strong> line to include the kubectl plugin.</p><p><strong>source &lt;(kubectl completion zsh)</strong></p><h4>What&#x27;s next</h4><p><a href="https://kubernetes.io/docs/tasks/access-application-cluster/service-access-application-cluster/">Learn how to launch and expose your application.</a></p><h3>Install Minikube</h3><p>This page shows how to install Minikube.</p><ul><li><a href="https://kubernetes.io/docs/tasks/tools/install-minikube/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/tools/install-minikube/#install-a-hypervisor"><strong>Install a Hypervisor</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/tools/install-minikube/#install-kubectl"><strong>Install kubectl</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/tools/install-minikube/#install-minikube"><strong>Install Minikube</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/tools/install-minikube/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Before you begin</h4><p>VT-x or AMD-v virtualization must be enabled in your computer&#x27;s BIOS.</p><h4>Install a Hypervisor</h4><p>If you do not already have a hypervisor installed, install one now.</p><ul><li>For OS X, install <a href="https://www.virtualbox.org/wiki/Downloads">VirtualBox</a> or <a href="https://www.vmware.com/products/fusion">VMware Fusion</a>, or <a href="https://github.com/moby/hyperkit">HyperKit</a>.</li><li>For Linux, install <a href="https://www.virtualbox.org/wiki/Downloads">VirtualBox</a> or <a href="http://www.linux-kvm.org/">KVM</a>.</li></ul><p><strong>Note:</strong> Minikube also supports a <strong>--vm-driver=none</strong> option that runs the Kubernetes components on the host and not in a VM. Docker is required to use this driver but a hypervisor is not required.</p><ul><li>For Windows, install <a href="https://www.virtualbox.org/wiki/Downloads">VirtualBox</a> or <a href="https://msdn.microsoft.com/en-us/virtualization/hyperv_on_windows/quick_start/walkthrough_install">Hyper-V</a>.</li></ul><h4>Install kubectl</h4><ul><li><a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">Install kubectl</a>.</li></ul><h4>Install Minikube</h4><ul><li>Install Minikube according to the instructions for the <a href="https://github.com/kubernetes/minikube/releases">latest release</a>.</li></ul><h4>What&#x27;s next</h4><ul><li><a href="https://kubernetes.io/docs/getting-started-guides/minikube/">Running Kubernetes Locally via Minikube</a></li></ul><h3>Installing kubeadm</h3><p>**</p><ul><li><a href="https://kubernetes.io/docs/setup/independent/install-kubeadm/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/setup/independent/install-kubeadm/#verify-the-mac-address-and-product_uuid-are-unique-for-every-node"><strong>Verify the MAC address and product_uuid are unique for every node</strong></a></li><li><a href="https://kubernetes.io/docs/setup/independent/install-kubeadm/#check-network-adapters"><strong>Check network adapters</strong></a></li><li><a href="https://kubernetes.io/docs/setup/independent/install-kubeadm/#check-required-ports"><strong>Check required ports</strong></a><ul><li><a href="https://kubernetes.io/docs/setup/independent/install-kubeadm/#master-nodes"><strong>Master node(s)</strong></a></li><li><a href="https://kubernetes.io/docs/setup/independent/install-kubeadm/#worker-nodes"><strong>Worker node(s)</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/setup/independent/install-kubeadm/#installing-docker"><strong>Installing Docker</strong></a></li><li><a href="https://kubernetes.io/docs/setup/independent/install-kubeadm/#installing-kubeadm-kubelet-and-kubectl"><strong>Installing kubeadm, kubelet and kubectl</strong></a></li><li><a href="https://kubernetes.io/docs/setup/independent/install-kubeadm/#configure-cgroup-driver-used-by-kubelet-on-master-node"><strong>Configure cgroup driver used by kubelet on Master Node</strong></a></li><li><a href="https://kubernetes.io/docs/setup/independent/install-kubeadm/#troubleshooting"><strong>Troubleshooting</strong></a></li><li><a href="https://kubernetes.io/docs/setup/independent/install-kubeadm/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Before you begin</h4><ul><li>One or more machines running one of:<ul><li>Ubuntu 16.04+</li><li>Debian 9</li><li>CentOS 7</li><li>RHEL 7</li><li>Fedora 25/26 (best-effort)</li><li>HypriotOS v1.0.1+</li><li>Container Linux (tested with 1576.4.0)</li></ul></li><li>2 GB or more of RAM per machine (any less will leave little room for your apps)</li><li>2 CPUs or more</li><li>Full network connectivity between all machines in the cluster (public or private network is fine)</li><li>Unique hostname, MAC address, and product_uuid for every node. See <a href="https://kubernetes.io/docs/setup/independent/install-kubeadm/#verify-the-mac-address-and-product_uuid-are-unique-for-every-node">here</a> for more details.</li><li>Certain ports are open on your machines. See <a href="https://kubernetes.io/docs/setup/independent/install-kubeadm/#check-required-ports">here</a> for more details.</li><li>Swap disabled. You <strong>MUST</strong> disable swap in order for the kubelet to work properly.</li></ul><h4>Verify the MAC address and product_uuid are unique for every node</h4><ul><li>You can get the MAC address of the network interfaces using the command <strong>ip link</strong> or <strong>ifconfig -a</strong></li><li>The product_uuid can be checked by using the command <strong>sudo cat /sys/class/dmi/id/product_uuid</strong></li></ul><p>It is very likely that hardware devices will have unique addresses, although some virtual machines may have identical values. Kubernetes uses these values to uniquely identify the nodes in the cluster. If these values are not unique to each node, the installation process may <a href="https://github.com/kubernetes/kubeadm/issues/31">fail</a>.</p><h4>Check network adapters</h4><p>If you have more than one network adapter, and your Kubernetes components are not reachable on the default route, we recommend you add IP route(s) so Kubernetes cluster addresses go via the appropriate adapter.</p><h4>Check required ports</h4><h5><strong>Master node(s)</strong></h5><p>  Protocol   Direction   Port Range   Purpose</p><hr/><p>  TCP        Inbound     6443<!-- -->*<!-- -->       Kubernetes API server
TCP        Inbound     2379-2380    etcd server client API
TCP        Inbound     10250        Kubelet API
TCP        Inbound     10251        kube-scheduler
TCP        Inbound     10252        kube-controller-manager
TCP        Inbound     10255        Read-only Kubelet API</p><h5><strong>Worker node(s)</strong></h5><p>  Protocol   Direction   Port Range    Purpose</p><hr/><p>  TCP        Inbound     10250         Kubelet API
TCP        Inbound     10255         Read-only Kubelet API
TCP        Inbound     30000-32767   NodePort Services<!-- -->*<!-- -->*</p><p>*<!-- -->*<!-- --> Default port range for <a href="https://kubernetes.io/docs/concepts/services-networking/service/">NodePort Services</a>.</p><p>Any port numbers marked with <!-- -->*<!-- --> are overridable, so you will need to ensure any custom ports you provide are also open.</p><p>Although etcd ports are included in master nodes, you can also host your own etcd cluster externally or on custom ports.</p><p>The pod network plugin you use (see below) may also require certain ports to be open. Since this differs with each pod network plugin, please see the documentation for the plugins about what port(s) those need.</p><h4>Installing Docker</h4><p>On each of your machines, install Docker. Version v1.12 is recommended, but v1.11, v1.13 and 17.03 are known to work as well. Versions 17.06+ might work, but have not yet been tested and verified by the Kubernetes node team.</p><p>Please proceed with executing the following commands based on your OS as root. You may become the root user by executing <strong>sudo -i</strong> after SSH-ing to each host.</p><p>If you already have the required versions of the Docker installed, you can move on to next section. If not, you can use the following commands to install Docker on your system:</p><ul><li><a href="https://kubernetes.io/docs/setup/independent/install-kubeadm/#docker-install-0">Ubuntu, Debian or HypriotOS</a></li><li><a href="https://kubernetes.io/docs/setup/independent/install-kubeadm/#docker-install-1">CentOS, RHEL or Fedora</a></li><li><a href="https://kubernetes.io/docs/setup/independent/install-kubeadm/#docker-install-2">Container Linux</a></li></ul><p>Install Docker from Ubuntu&#x27;s repositories:</p><p><strong>apt-get update</strong></p><p><strong>apt-get install -y docker.io</strong></p><p>or install Docker CE 17.03 from Docker&#x27;s repositories for Ubuntu or Debian:</p><p><strong>apt-get update</strong></p><p><strong>apt-get install -y <!-- -->\</strong></p><p><strong>apt-transport-https <!-- -->\</strong></p><p><strong>ca-certificates <!-- -->\</strong></p><p><strong>curl <!-- -->\</strong></p><p><strong>software-properties-common</strong></p><p><strong>curl -fsSL <a href="https://download.docker.com/linux/ubuntu/gpg">https://download.docker.com/linux/ubuntu/gpg</a> | apt-key add -</strong></p><p><strong>add-apt-repository <!-- -->\</strong></p><p><strong>&quot;deb <a href="https://download.docker.com/linux/$(">https://download.docker.com/linux/$(</a>. /etc/os-release; echo &quot;$ID&quot;) <!-- -->\</strong></p><p><strong>$(lsb_release -cs) <!-- -->\</strong></p><p><strong>stable&quot;</strong></p><p><strong>apt-get update &amp;&amp; apt-get install -y docker-ce=$(apt-cache madison docker-ce | grep 17.03 | head -1 | awk \&#x27;{print $3}\&#x27;)</strong></p><p>Refer to the <a href="https://docs.docker.com/engine/installation/">official Docker installation guides</a> for more information.</p><h4>Installing kubeadm, kubelet and kubectl</h4><p>You will install these packages on all of your machines:</p><ul><li><strong>kubeadm</strong>: the command to bootstrap the cluster.</li><li><strong>kubelet</strong>: the component that runs on all of the machines in your cluster and does things like starting pods and containers.</li><li><strong>kubectl</strong>: the command line util to talk to your cluster.</li></ul><p>kubeadm <strong>will not</strong> install or manage <strong>kubelet</strong> or <strong>kubectl</strong> for you, so you will need to ensure they match the version of the Kubernetes control panel you want kubeadm to install for you. If you do not, there is a risk of a version skew occurring that can lead to unexpected, buggy behaviour. However, one minor version skew between the kubelet and the control plane is supported, but the kubelet version may never exceed the API server version. For example, kubelets running 1.7.0 should be fully compatible with a 1.8.0 API server, but not vice versa.</p><p>For more information on version skews, please read our <a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#version-skew-policy">version skew policy</a>.</p><ul><li><a href="https://kubernetes.io/docs/setup/independent/install-kubeadm/#k8s-install-0">Ubuntu, Debian or HypriotOS</a></li><li><a href="https://kubernetes.io/docs/setup/independent/install-kubeadm/#k8s-install-1">CentOS, RHEL or Fedora</a></li><li><a href="https://kubernetes.io/docs/setup/independent/install-kubeadm/#k8s-install-2">Container Linux</a></li></ul><p><strong>apt-get update &amp;&amp; apt-get install -y apt-transport-https</strong></p><p><strong>curl -s <a href="https://packages.cloud.google.com/apt/doc/apt-key.gpg">https://packages.cloud.google.com/apt/doc/apt-key.gpg</a> | apt-key add -</strong></p><p><strong>cat <code>&lt;&lt;EOF &gt;</code>/etc/apt/sources.list.d/kubernetes.list</strong></p><p><strong>deb <a href="http://apt.kubernetes.io/">http://apt.kubernetes.io/</a> kubernetes-xenial main</strong></p><p><strong>EOF</strong></p><p><strong>apt-get update</strong></p><p><strong>apt-get install -y kubelet kubeadm kubectl</strong></p><p>The kubelet is now restarting every few seconds, as it waits in a crashloop for kubeadm to tell it what to do.</p><h4>Configure cgroup driver used by kubelet on Master Node</h4><p>Make sure that the cgroup driver used by kubelet is the same as the one used by Docker. Verify that your Docker cgroup driver matches the kubelet config:</p><p><strong>docker info | grep -i cgroup</strong></p><p><strong>cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</strong></p><p>If the Docker cgroup driver and the kubelet config don&#x27;t match, change the kubelet config to match the Docker cgroup driver. The flag you need to change is <strong>--cgroup-driver</strong>. If it&#x27;s already set, you can update like so:</p><p><strong>sed -i &quot;s/cgroup-driver=systemd/cgroup-driver=cgroupfs/g&quot; /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</strong></p><p>Otherwise, you will need to open the systemd file and add the flag to an existing environment line.</p><p>Then restart kubelet:</p><p><strong>systemctl daemon-reload</strong></p><p><strong>systemctl restart kubelet</strong></p><h4>Troubleshooting</h4><p>If you are running into difficulties with kubeadm, please consult our <a href="https://kubernetes.io/docs/setup/independent/troubleshooting-kubeadm/">troubleshooting docs</a>.</p><h4>What&#x27;s next</h4><ul><li><a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/">Using kubeadm to Create a Cluster</a></li></ul><h2>Configure Pods and Containers</h2><h3>Assign Memory Resources to Containers and Pods</h3><p>This page shows how to assign a memory request and a memory limit to a Container. A Container is guaranteed to have as much memory as it requests, but is not allowed to use more memory than its limit.</p><ul><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/#create-a-namespace"><strong>Create a namespace</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/#specify-a-memory-request-and-a-memory-limit"><strong>Specify a memory request and a memory limit</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/#exceed-a-containers-memory-limit"><strong>Exceed a Container&#x27;s memory limit</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/#specify-a-memory-request-that-is-too-big-for-your-nodes"><strong>Specify a memory request that is too big for your Nodes</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/#memory-units"><strong>Memory units</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/#if-you-dont-specify-a-memory-limit"><strong>If you don&#x27;t specify a memory limit</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/#motivation-for-memory-requests-and-limits"><strong>Motivation for memory requests and limits</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/#clean-up"><strong>Clean up</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/#whats-next"><strong>What&#x27;s next</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/#for-app-developers"><strong>For app developers</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/#for-cluster-administrators"><strong>For cluster administrators</strong></a></li></ul></li></ul><h4>Before you begin</h4><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using<a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <strong>kubectl version</strong>.</p><p>Each node in your cluster must have at least 300 MiB of memory.</p><p>A few of the steps on this page require that the <a href="https://github.com/kubernetes/heapster">Heapster</a> service is running in your cluster. But if you don&#x27;t have Heapster running, you can do most of the steps, and it won&#x27;t be a problem if you skip the Heapster steps.</p><p>If you are running minikube, run the following command to enable heapster:</p><p><strong>minikube addons enable heapster</strong></p><p>To see whether the Heapster service is running, enter this command:</p><p><strong>kubectl get services --namespace=kube-system</strong></p><p>If the Heapster service is running, it shows in the output:</p><p><strong>NAMESPACE NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE</strong></p><p><strong>kube-system heapster 10.11.240.9 <code>&lt;none&gt;</code> 80/TCP 6d</strong></p><h4>Create a namespace</h4><p>Create a namespace so that the resources you create in this exercise are isolated from the rest of your cluster.</p><p><strong>kubectl create namespace mem-example</strong></p><h4>Specify a memory request and a memory limit</h4><p>To specify a memory request for a Container, include the <strong>resources:requests</strong> field in the Container&#x27;s resource manifest. To specify a memory limit, include <strong>resources:limits</strong>.</p><p>In this exercise, you create a Pod that has one Container. The Container has a memory request of 100 MiB and a memory limit of 200 MiB. Here&#x27;s the configuration file for the Pod:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>memory-reques                                                      |
| t-limit.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/website">https://raw.githubusercontent.com/kubernetes/website</a> |
| /master/docs/tasks/configure-pod-container/memory-request-limit.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: memory-demo</strong>                                                 |
|                                                                       |
| <strong>namespace: mem-example</strong>                                            |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: memory-demo-ctr</strong>                                           |
|                                                                       |
| <strong>image: polinux/stress</strong>                                             |
|                                                                       |
| <strong>resources:</strong>                                                        |
|                                                                       |
| <strong>limits:</strong>                                                           |
|                                                                       |
| <strong>memory: &quot;200Mi&quot;</strong>                                                 |
|                                                                       |
| <strong>requests:</strong>                                                         |
|                                                                       |
| <strong>memory: &quot;100Mi&quot;</strong>                                                 |
|                                                                       |
| <strong>command: <!-- -->[&quot;stress&quot;]</strong>                                           |
|                                                                       |
| <strong>args: <!-- -->[&quot;--vm&quot;, &quot;1&quot;, &quot;--vm-bytes&quot;, &quot;150M&quot;,                |
| &quot;--vm-hang&quot;, &quot;1&quot;]</strong>                                             |
+-----------------------------------------------------------------------+</p><p>In the configuration file, the <strong>args</strong> section provides arguments for the Container when it starts. The <strong>&quot;--vm-bytes&quot;, &quot;150M&quot;</strong> arguments tell the Container to attempt to allocate 150 MiB of memory.</p><p>Create the Pod:</p><p><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/configure-pod-container/memory-request-limit.yaml">https://k8s.io/docs/tasks/configure-pod-container/memory-request-limit.yaml</a> --namespace=mem-example</strong></p><p>Verify that the Pod&#x27;s Container is running:</p><p><strong>kubectl get pod memory-demo --namespace=mem-example</strong></p><p>View detailed information about the Pod:</p><p><strong>kubectl get pod memory-demo --output=yaml --namespace=mem-example</strong></p><p>The output shows that the one Container in the Pod has a memory request of 100 MiB and a memory limit of 200 MiB.</p><p><strong>.<!-- -->..</strong></p><p><strong>resources:</strong></p><p><strong>limits:</strong></p><p><strong>memory: 200Mi</strong></p><p><strong>requests:</strong></p><p><strong>memory: 100Mi</strong></p><p><strong>.<!-- -->..</strong></p><p>Start a proxy so that you can call the Heapster service:</p><p><strong>kubectl proxy</strong></p><p>In another command window, get the memory usage from the Heapster service:</p><p><strong>curl http://localhost:8001/api/v1/proxy/namespaces/kube-system/services/heapster/api/v1/model/namespaces/mem-example/pods/memory-demo/metrics/memory/usage</strong></p><p>The output shows that the Pod is using about 162,900,000 bytes of memory, which is about 150 MiB. This is greater than the Pod&#x27;s 100 MiB request, but within the Pod&#x27;s 200 MiB limit.</p><p><strong>{</strong></p><p><strong>&quot;timestamp&quot;: &quot;2017-06-20T18:54:00Z&quot;,</strong></p><p><strong>&quot;value&quot;: 162856960</strong></p><p><strong>}</strong></p><p>Delete your Pod:</p><p><strong>kubectl delete pod memory-demo --namespace=mem-example</strong></p><h4>Exceed a Container&#x27;s memory limit</h4><p>A Container can exceed its memory request if the Node has memory available. But a Container is not allowed to use more than its memory limit. If a Container allocates more memory than its limit, the Container becomes a candidate for termination. If the Container continues to consume memory beyond its limit, the Container is terminated. If a terminated Container is restartable, the kubelet will restart it, as with any other type of runtime failure.</p><p>In this exercise, you create a Pod that attempts to allocate more memory than its limit. Here is the configuration file for a Pod that has one Container. The Container has a memory request of 50 MiB and a memory limit of 100 MiB.</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>memory-request-li                                                  |
| mit-2.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/website/m">https://raw.githubusercontent.com/kubernetes/website/m</a> |
| aster/docs/tasks/configure-pod-container/memory-request-limit-2.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: memory-demo-2</strong>                                               |
|                                                                       |
| <strong>namespace: mem-example</strong>                                            |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: memory-demo-2-ctr</strong>                                         |
|                                                                       |
| <strong>image: polinux/stress</strong>                                             |
|                                                                       |
| <strong>resources:</strong>                                                        |
|                                                                       |
| <strong>requests:</strong>                                                         |
|                                                                       |
| <strong>memory: &quot;50Mi&quot;</strong>                                                  |
|                                                                       |
| <strong>limits:</strong>                                                           |
|                                                                       |
| <strong>memory: &quot;100Mi&quot;</strong>                                                 |
|                                                                       |
| <strong>command: <!-- -->[&quot;stress&quot;]</strong>                                           |
|                                                                       |
| <strong>args: <!-- -->[&quot;--vm&quot;, &quot;1&quot;, &quot;--vm-bytes&quot;, &quot;250M&quot;,                |
| &quot;--vm-hang&quot;, &quot;1&quot;]</strong>                                             |
+-----------------------------------------------------------------------+</p><p>In the configuration file, in the <strong>args</strong> section, you can see that the Container will attempt to allocate 250 MiB of memory, which is well above the 100 MiB limit.</p><p>Create the Pod:</p><p><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/configure-pod-container/memory-request-limit-2.yaml">https://k8s.io/docs/tasks/configure-pod-container/memory-request-limit-2.yaml</a> --namespace=mem-example</strong></p><p>View detailed information about the Pod:</p><p><strong>kubectl get pod memory-demo-2 --namespace=mem-example</strong></p><p>At this point, the Container might be running, or it might have been killed. If the Container has not yet been killed, repeat the preceding command until you see that the Container has been killed:</p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>memory-demo-2 0/1 OOMKilled 1 24s</strong></p><p>Get a more detailed view of the Container&#x27;s status:</p><p><strong>kubectl get pod memory-demo-2 --output=yaml --namespace=mem-example</strong></p><p>The output shows that the Container has been killed because it is out of memory (OOM).</p><p><strong>lastState:</strong></p><p><strong>terminated:</strong></p><p><strong>containerID: docker://65183c1877aaec2e8427bc95609cc52677a454b56fcb24340dbd22917c23b10f</strong></p><p><strong>exitCode: 137</strong></p><p><strong>finishedAt: 2017-06-20T20:52:19Z</strong></p><p><strong>reason: OOMKilled</strong></p><p><strong>startedAt: null</strong></p><p>The Container in this exercise is restartable, so the kubelet will restart it. Enter this command several times to see that the Container gets repeatedly killed and restarted:</p><p><strong>kubectl get pod memory-demo-2 --namespace=mem-example</strong></p><p>The output shows that the Container gets killed, restarted, killed again, restarted again, and so on:</p><p><strong>stevepe@sperry-1:<!-- -->~<!-- -->/steveperry-53.github.io$ kubectl get pod memory-demo-2 --namespace=mem-example</strong></p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>memory-demo-2 0/1 OOMKilled 1 37s</strong></p><p><strong>stevepe@sperry-1:<!-- -->~<!-- -->/steveperry-53.github.io$ kubectl get pod memory-demo-2 --namespace=mem-example</strong></p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>memory-demo-2 1/1 Running 2 40s</strong></p><p>View detailed information about the Pod&#x27;s history:</p><p><strong>kubectl describe pod memory-demo-2 --namespace=mem-example</strong></p><p>The output shows that the Container starts and fails repeatedly:</p><p><strong>.<!-- -->.. Normal Created Created container with id 66a3a20aa7980e61be4922780bf9d24d1a1d8b7395c09861225b0eba1b1f8511</strong></p><p><strong>.<!-- -->.. Warning BackOff Back-off restarting failed container</strong></p><p>View detailed information about your cluster&#x27;s Nodes:</p><p><strong>kubectl describe nodes</strong></p><p>The output includes a record of the Container being killed because of an out-of-memory condition:</p><p><strong>Warning OOMKilling Memory cgroup out of memory: Kill process 4481 (stress) score 1994 or sacrifice child</strong></p><p>Delete your Pod:</p><p><strong>kubectl delete pod memory-demo-2 --namespace=mem-example</strong></p><h4>Specify a memory request that is too big for your Nodes</h4><p>Memory requests and limits are associated with Containers, but it is useful to think of a Pod as having a memory request and limit. The memory request for the Pod is the sum of the memory requests for all the Containers in the Pod. Likewise, the memory limit for the Pod is the sum of the limits of all the Containers in the Pod.</p><p>Pod scheduling is based on requests. A Pod is scheduled to run on a Node only if the Node has enough available memory to satisfy the Pod&#x27;s memory request.</p><p>In this exercise, you create a Pod that has a memory request so big that it exceeds the capacity of any Node in your cluster. Here is the configuration file for a Pod that has one Container. The Container requests 1000 GiB of memory, which is likely to exceed the capacity of any Node in your cluster.</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>memory-request-li                                                  |
| mit-3.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/website/m">https://raw.githubusercontent.com/kubernetes/website/m</a> |
| aster/docs/tasks/configure-pod-container/memory-request-limit-3.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: memory-demo-3</strong>                                               |
|                                                                       |
| <strong>namespace: mem-example</strong>                                            |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: memory-demo-3-ctr</strong>                                         |
|                                                                       |
| <strong>image: polinux/stress</strong>                                             |
|                                                                       |
| <strong>resources:</strong>                                                        |
|                                                                       |
| <strong>limits:</strong>                                                           |
|                                                                       |
| <strong>memory: &quot;1000Gi&quot;</strong>                                                |
|                                                                       |
| <strong>requests:</strong>                                                         |
|                                                                       |
| <strong>memory: &quot;1000Gi&quot;</strong>                                                |
|                                                                       |
| <strong>command: <!-- -->[&quot;stress&quot;]</strong>                                           |
|                                                                       |
| <strong>args: <!-- -->[&quot;--vm&quot;, &quot;1&quot;, &quot;--vm-bytes&quot;, &quot;150M&quot;,                |
| &quot;--vm-hang&quot;, &quot;1&quot;]</strong>                                             |
+-----------------------------------------------------------------------+</p><p>Create the Pod:</p><p><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/configure-pod-container/memory-request-limit-3.yaml">https://k8s.io/docs/tasks/configure-pod-container/memory-request-limit-3.yaml</a> --namespace=mem-example</strong></p><p>View the Pod&#x27;s status:</p><p><strong>kubectl get pod memory-demo-3 --namespace=mem-example</strong></p><p>The output shows that the Pod&#x27;s status is PENDING. That is, the Pod has not been scheduled to run on any Node, and it will remain in the PENDING state indefinitely:</p><p><strong>kubectl get pod memory-demo-3 --namespace=mem-example</strong></p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>memory-demo-3 0/1 Pending 0 25s</strong></p><p>View detailed information about the Pod, including events:</p><p><strong>kubectl describe pod memory-demo-3 --namespace=mem-example</strong></p><p>The output shows that the Container cannot be scheduled because of insufficient memory on the Nodes:</p><p><strong>Events:</strong></p><p><strong>.<!-- -->.. Reason Message</strong></p><p><strong>------ -------</strong></p><p><strong>.<!-- -->.. FailedScheduling No nodes are available that match all of the following predicates:: Insufficient memory (3).</strong></p><h4>Memory units</h4><p>The memory resource is measured in bytes. You can express memory as a plain integer or a fixed-point integer with one of these suffixes: E, P, T, G, M, K, Ei, Pi, Ti, Gi, Mi, Ki. For example, the following represent approximately the same value:</p><p><strong>128974848, 129e6, 129M , 123Mi</strong></p><p>Delete your Pod:</p><p><strong>kubectl delete pod memory-demo-3 --namespace=mem-example</strong></p><h4>If you don&#x27;t specify a memory limit</h4><p>If you don&#x27;t specify a memory limit for a Container, then one of these situations applies:</p><ul><li>The Container has no upper bound on the amount of memory it uses. The Container could use all of the memory available on the Node where it is running.</li><li>The Container is running in a namespace that has a default memory limit, and the Container is automatically assigned the default limit. Cluster administrators can use a <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#limitrange-v1-core">LimitRange</a> to specify a default value for the memory limit.</li></ul><h4>Motivation for memory requests and limits</h4><p>By configuring memory requests and limits for the Containers that run in your cluster, you can make efficient use of the memory resources available on your cluster&#x27;s Nodes. By keeping a Pod&#x27;s memory request low, you give the Pod a good chance of being scheduled. By having a memory limit that is greater than the memory request, you accomplish two things:</p><ul><li>The Pod can have bursts of activity where it makes use of memory that happens to be available.</li><li>The amount of memory a Pod can use during a burst is limited to some reasonable amount.</li></ul><h4>Clean up</h4><p>Delete your namespace. This deletes all the Pods that you created for this task:</p><p><strong>kubectl delete namespace mem-example</strong></p><h4>What&#x27;s next</h4><h5><strong>For app developers</strong></h5><ul><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/">Assign CPU Resources to Containers and Pods</a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/">Configure Quality of Service for Pods</a></li></ul><h5><strong>For cluster administrators</strong></h5><ul><li><a href="https://kubernetes.io/docs/tasks/administer-cluster/memory-default-namespace/">Configure Default Memory Requests and Limits for a Namespace</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-cluster/cpu-default-namespace/">Configure Default CPU Requests and Limits for a Namespace</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-cluster/memory-constraint-namespace/">Configure Minimum and Maximum Memory Constraints for a Namespace</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-cluster/cpu-constraint-namespace/">Configure Minimum and Maximum CPU Constraints for a Namespace</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-cluster/quota-memory-cpu-namespace/">Configure Memory and CPU Quotas for a Namespace</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-cluster/quota-pod-namespace/">Configure a Pod Quota for a Namespace</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-cluster/quota-api-object/">Configure Quotas for API Objects</a></li></ul><h3>Assign CPU Resources to Containers and Pods</h3><p>This page shows how to assign a CPU request and a CPU limit to a Container. A Container is guaranteed to have as much CPU as it requests, but is not allowed to use more CPU than its limit.</p><ul><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/#create-a-namespace"><strong>Create a namespace</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/#specify-a-cpu-request-and-a-cpu-limit"><strong>Specify a CPU request and a CPU limit</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/#cpu-units"><strong>CPU units</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/#specify-a-cpu-request-that-is-too-big-for-your-nodes"><strong>Specify a CPU request that is too big for your Nodes</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/#if-you-dont-specify-a-cpu-limit"><strong>If you don&#x27;t specify a CPU limit</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/#motivation-for-cpu-requests-and-limits"><strong>Motivation for CPU requests and limits</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/#clean-up"><strong>Clean up</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/#whats-next"><strong>What&#x27;s next</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/#for-app-developers"><strong>For app developers</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/#for-cluster-administrators"><strong>For cluster administrators</strong></a></li></ul></li></ul><h4>Before you begin</h4><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using<a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <strong>kubectl version</strong>.</p><p>Each node in your cluster must have at least 1 cpu.</p><p>A few of the steps on this page require that the <a href="https://github.com/kubernetes/heapster">Heapster</a> service is running in your cluster. But if you don&#x27;t have Heapster running, you can do most of the steps, and it won&#x27;t be a problem if you skip the Heapster steps.</p><p>If you are running minikube, run the following command to enable heapster:</p><p><strong>minikube addons enable heapster</strong></p><p>To see whether the Heapster service is running, enter this command:</p><p><strong>kubectl get services --namespace=kube-system</strong></p><p>If the heapster service is running, it shows in the output:</p><p><strong>NAMESPACE NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE</strong></p><p><strong>kube-system heapster 10.11.240.9 <code>&lt;none&gt;</code> 80/TCP 6d</strong></p><h4>Create a namespace</h4><p>Create a namespace so that the resources you create in this exercise are isolated from the rest of your cluster.</p><p><strong>kubectl create namespace cpu-example</strong></p><h4>Specify a CPU request and a CPU limit</h4><p>To specify a CPU request for a Container, include the <strong>resources:requests</strong> field in the Container&#x27;s resource manifest. To specify a CPU limit, include <strong>resources:limits</strong>.</p><p>In this exercise, you create a Pod that has one Container. The Container has a CPU request of 0.5 cpu and a CPU limit of 1 cpu. Here&#x27;s the configuration file for the Pod:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>cpu-req                                                            |
| uest-limit.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/webs">https://raw.githubusercontent.com/kubernetes/webs</a> |
| ite/master/docs/tasks/configure-pod-container/cpu-request-limit.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: cpu-demo</strong>                                                    |
|                                                                       |
| <strong>namespace: cpu-example</strong>                                            |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: cpu-demo-ctr</strong>                                              |
|                                                                       |
| <strong>image: vish/stress</strong>                                                |
|                                                                       |
| <strong>resources:</strong>                                                        |
|                                                                       |
| <strong>limits:</strong>                                                           |
|                                                                       |
| <strong>cpu: &quot;1&quot;</strong>                                                        |
|                                                                       |
| <strong>requests:</strong>                                                         |
|                                                                       |
| <strong>cpu: &quot;0.5&quot;</strong>                                                      |
|                                                                       |
| <strong>args:</strong>                                                             |
|                                                                       |
| <strong>- -cpus</strong>                                                           |
|                                                                       |
| <strong>- &quot;2&quot;</strong>                                                           |
+-----------------------------------------------------------------------+</p><p>In the configuration file, the <strong>args</strong> section provides arguments for the Container when it starts. The <strong>-cpus &quot;2&quot;</strong> argument tells the Container to attempt to use 2 cpus.</p><p>Create the Pod:</p><p><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/configure-pod-container/cpu-request-limit.yaml">https://k8s.io/docs/tasks/configure-pod-container/cpu-request-limit.yaml</a> --namespace=cpu-example</strong></p><p>Verify that the Pod&#x27;s Container is running:</p><p><strong>kubectl get pod cpu-demo --namespace=cpu-example</strong></p><p>View detailed information about the Pod:</p><p><strong>kubectl get pod cpu-demo --output=yaml --namespace=cpu-example</strong></p><p>The output shows that the one Container in the Pod has a CPU request of 500 millicpu and a CPU limit of 1 cpu.</p><p><strong>resources:</strong></p><p><strong>limits:</strong></p><p><strong>cpu: &quot;1&quot;</strong></p><p><strong>requests:</strong></p><p><strong>cpu: 500m</strong></p><p>Start a proxy so that you can call the heapster service:</p><p><strong>kubectl proxy</strong></p><p>In another command window, get the CPU usage rate from the heapster service:</p><p><strong>curl http://localhost:8001/api/v1/proxy/namespaces/kube-system/services/heapster/api/v1/model/namespaces/cpu-example/pods/cpu-demo/metrics/cpu/usage_rate</strong></p><p>The output shows that the Pod is using 974 millicpu, which is just a bit less than the limit of 1 cpu specified in the Pod&#x27;s configuration file.</p><p><strong>{</strong></p><p><strong>&quot;timestamp&quot;: &quot;2017-06-22T18:48:00Z&quot;,</strong></p><p><strong>&quot;value&quot;: 974</strong></p><p><strong>}</strong></p><p>Recall that by setting <strong>-cpu &quot;2&quot;</strong>, you configured the Container to attempt to use 2 cpus. But the Container is only being allowed to use about 1 cpu. The Container&#x27;s CPU use is being throttled, because the Container is attempting to use more CPU resources than its limit.</p><p><strong>Note:</strong> There&#x27;s another possible explanation for the CPU throttling. The Node might not have enough CPU resources available. Recall that the prerequisites for this exercise require that each of your Nodes has at least 1 cpu. If your Container is running on a Node that has only 1 cpu, the Container cannot use more than 1 cpu regardless of the CPU limit specified for the Container.</p><h4>CPU units</h4><p>The CPU resource is measured in cpu units. One cpu, in Kubernetes, is equivalent to:</p><ul><li>1 AWS vCPU</li><li>1 GCP Core</li><li>1 Azure vCore</li><li>1 Hyperthread on a bare-metal Intel processor with Hyperthreading</li></ul><p>Fractional values are allowed. A Container that requests 0.5 cpu is guaranteed half as much CPU as a Container that requests 1 cpu. You can use the suffix m to mean milli. For example 100m cpu, 100 millicpu, and 0.1 cpu are all the same. Precision finer than 1m is not allowed.</p><p>CPU is always requested as an absolute quantity, never as a relative quantity; 0.1 is the same amount of CPU on a single-core, dual-core, or 48-core machine.</p><p>Delete your Pod:</p><p><strong>kubectl delete pod cpu-demo --namespace=cpu-example</strong></p><h4>Specify a CPU request that is too big for your Nodes</h4><p>CPU requests and limits are associated with Containers, but it is useful to think of a Pod as having a CPU request and limit. The CPU request for a Pod is the sum of the CPU requests for all the Containers in the Pod. Likewise, the CPU limit for a Pod is the sum of the CPU limits for all the Containers in the Pod.</p><p>Pod scheduling is based on requests. A Pod is scheduled to run on a Node only if the Node has enough CPU resources available to satisfy the Pod&#x27;s CPU request.</p><p>In this exercise, you create a Pod that has a CPU request so big that it exceeds the capacity of any Node in your cluster. Here is the configuration file for a Pod that has one Container. The Container requests 100 cpu, which is likely to exceed the capacity of any Node in your cluster.</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>cpu-request                                                        |
| -limit-2.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/websit">https://raw.githubusercontent.com/kubernetes/websit</a> |
| e/master/docs/tasks/configure-pod-container/cpu-request-limit-2.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: cpu-demo-2</strong>                                                  |
|                                                                       |
| <strong>namespace: cpu-example</strong>                                            |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: cpu-demo-ctr-2</strong>                                            |
|                                                                       |
| <strong>image: vish/stress</strong>                                                |
|                                                                       |
| <strong>resources:</strong>                                                        |
|                                                                       |
| <strong>limits:</strong>                                                           |
|                                                                       |
| <strong>cpu: &quot;100&quot;</strong>                                                      |
|                                                                       |
| <strong>requests:</strong>                                                         |
|                                                                       |
| <strong>cpu: &quot;100&quot;</strong>                                                      |
|                                                                       |
| <strong>args:</strong>                                                             |
|                                                                       |
| <strong>- -cpus</strong>                                                           |
|                                                                       |
| <strong>- &quot;2&quot;</strong>                                                           |
+-----------------------------------------------------------------------+</p><p>Create the Pod:</p><p><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/configure-pod-container/cpu-request-limit-2.yaml">https://k8s.io/docs/tasks/configure-pod-container/cpu-request-limit-2.yaml</a> --namespace=cpu-example</strong></p><p>View the Pod&#x27;s status:</p><p><strong>kubectl get pod cpu-demo-2 --namespace=cpu-example</strong></p><p>The output shows that the Pod&#x27;s status is Pending. That is, the Pod has not been scheduled to run on any Node, and it will remain in the Pending state indefinitely:</p><p><strong>kubectl get pod cpu-demo-2 --namespace=cpu-example</strong></p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>cpu-demo-2 0/1 Pending 0 7m</strong></p><p>View detailed information about the Pod, including events:</p><p><strong>kubectl describe pod cpu-demo-2 --namespace=cpu-example</strong></p><p>The output shows that the Container cannot be scheduled because of insufficient CPU resources on the Nodes:</p><p><strong>Events:</strong></p><p><strong>Reason Message</strong></p><p><strong>------ -------</strong></p><p><strong>FailedScheduling No nodes are available that match all of the following predicates:: Insufficient cpu (3).</strong></p><p>Delete your Pod:</p><p><strong>kubectl delete pod cpu-demo-2 --namespace=cpu-example</strong></p><h4>If you don&#x27;t specify a CPU limit</h4><p>If you don&#x27;t specify a CPU limit for a Container, then one of these situations applies:</p><ul><li>The Container has no upper bound on the CPU resources it can use. The Container could use all of the CPU resources available on the Node where it is running.</li><li>The Container is running in a namespace that has a default CPU limit, and the Container is automatically assigned the default limit. Cluster administrators can use a <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#limitrange-v1-core/">LimitRange</a> to specify a default value for the CPU limit.</li></ul><h4>Motivation for CPU requests and limits</h4><p>By configuring the CPU requests and limits of the Containers that run in your cluster, you can make efficient use of the CPU resources available on your cluster&#x27;s Nodes. By keeping a Pod&#x27;s CPU request low, you give the Pod a good chance of being scheduled. By having a CPU limit that is greater than the CPU request, you accomplish two things:</p><ul><li>The Pod can have bursts of activity where it makes use of CPU resources that happen to be available.</li><li>The amount of CPU resources a Pod can use during a burst is limited to some reasonable amount.</li></ul><h4>Clean up</h4><p>Delete your namespace:</p><p><strong>kubectl delete namespace cpu-example</strong></p><h4>What&#x27;s next</h4><h5><strong>For app developers</strong></h5><ul><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/">Assign Memory Resources to Containers and Pods</a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/">Configure Quality of Service for Pods</a></li></ul><h5><strong>For cluster administrators</strong></h5><ul><li><a href="https://kubernetes.io/docs/tasks/administer-cluster/memory-default-namespace/">Configure Default Memory Requests and Limits for a Namespace</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-cluster/cpu-default-namespace/">Configure Default CPU Requests and Limits for a Namespace</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-cluster/memory-constraint-namespace/">Configure Minimum and Maximum Memory Constraints for a Namespace</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-cluster/cpu-constraint-namespace/">Configure Minimum and Maximum CPU Constraints for a Namespace</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-cluster/quota-memory-cpu-namespace/">Configure Memory and CPU Quotas for a Namespace</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-cluster/quota-pod-namespace/">Configure a Pod Quota for a Namespace</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-cluster/quota-api-object/">Configure Quotas for API Objects</a></li></ul><h3>Configure Quality of Service for Pods</h3><p>This page shows how to configure Pods so that they will be assigned particular Quality of Service (QoS) classes. Kubernetes uses QoS classes to make decisions about scheduling and evicting Pods.</p><ul><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/#qos-classes"><strong>QoS classes</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/#create-a-namespace"><strong>Create a namespace</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/#create-a-pod-that-gets-assigned-a-qos-class-of-guaranteed"><strong>Create a Pod that gets assigned a QoS class of Guaranteed</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/#create-a-pod-that-gets-assigned-a-qos-class-of-burstable"><strong>Create a Pod that gets assigned a QoS class of Burstable</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/#create-a-pod-that-gets-assigned-a-qos-class-of-besteffort"><strong>Create a Pod that gets assigned a QoS class of BestEffort</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/#create-a-pod-that-has-two-containers"><strong>Create a Pod that has two Containers</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/#clean-up"><strong>Clean up</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/#whats-next"><strong>What&#x27;s next</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/#for-app-developers"><strong>For app developers</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/#for-cluster-administrators"><strong>For cluster administrators</strong></a></li></ul></li></ul><h4>Before you begin</h4><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using<a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <strong>kubectl version</strong>.</p><h4>QoS classes</h4><p>When Kubernetes creates a Pod it assigns one of these QoS classes to the Pod:</p><ul><li>Guaranteed</li><li>Burstable</li><li>BestEffort</li></ul><h4>Create a namespace</h4><p>Create a namespace so that the resources you create in this exercise are isolated from the rest of your cluster.</p><p><strong>kubectl create namespace qos-example</strong></p><h4>Create a Pod that gets assigned a QoS class of Guaranteed</h4><p>For a Pod to be given a QoS class of Guaranteed:</p><ul><li>Every Container in the Pod must have a memory limit and a memory request, and they must be the same.</li><li>Every Container in the Pod must have a cpu limit and a cpu request, and they must be the same.</li></ul><p>Here is the configuration file for a Pod that has one Container. The Container has a memory limit and a memory request, both equal to 200 MiB. The Container has a cpu limit and a cpu request, both equal to 700 millicpu:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>qos-pod.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kuber">https://raw.githubusercontent.com/kuber</a>           |
| netes/website/master/docs/tasks/configure-pod-container/qos-pod.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: qos-demo</strong>                                                    |
|                                                                       |
| <strong>namespace: qos-example</strong>                                            |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: qos-demo-ctr</strong>                                              |
|                                                                       |
| <strong>image: nginx</strong>                                                      |
|                                                                       |
| <strong>resources:</strong>                                                        |
|                                                                       |
| <strong>limits:</strong>                                                           |
|                                                                       |
| <strong>memory: &quot;200Mi&quot;</strong>                                                 |
|                                                                       |
| <strong>cpu: &quot;700m&quot;</strong>                                                     |
|                                                                       |
| <strong>requests:</strong>                                                         |
|                                                                       |
| <strong>memory: &quot;200Mi&quot;</strong>                                                 |
|                                                                       |
| <strong>cpu: &quot;700m&quot;</strong>                                                     |
+-----------------------------------------------------------------------+</p><p>Create the Pod:</p><p><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/configure-pod-container/qos-pod.yaml">https://k8s.io/docs/tasks/configure-pod-container/qos-pod.yaml</a> --namespace=qos-example</strong></p><p>View detailed information about the Pod:</p><p><strong>kubectl get pod qos-demo --namespace=qos-example --output=yaml</strong></p><p>The output shows that Kubernetes gave the Pod a QoS class of Guaranteed. The output also verifies that the Pod&#x27;s Container has a memory request that matches its memory limit, and it has a cpu request that matches its cpu limit.</p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>resources:</strong></p><p><strong>limits:</strong></p><p><strong>cpu: 700m</strong></p><p><strong>memory: 200Mi</strong></p><p><strong>requests:</strong></p><p><strong>cpu: 700m</strong></p><p><strong>memory: 200Mi</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>qosClass: Guaranteed</strong></p><p><strong>Note:</strong> If a Container specifies its own memory limit, but does not specify a memory request, Kubernetes automatically assigns a memory request that matches the limit. Similarly, if a Container specifies its own cpu limit, but does not specify a cpu request, Kubernetes automatically assigns a cpu request that matches the limit.</p><p>Delete your Pod:</p><p><strong>kubectl delete pod qos-demo --namespace=qos-example</strong></p><h4>Create a Pod that gets assigned a QoS class of Burstable</h4><p>A Pod is given a QoS class of Burstable if:</p><ul><li>The Pod does not meet the criteria for QoS class Guaranteed.</li><li>At least one Container in the Pod has a memory or cpu request.</li></ul><p>Here is the configuration file for a Pod that has one Container. The Container has a memory limit of 200 MiB and a memory request of 100 MiB.</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>qos-pod-2.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kuberne">https://raw.githubusercontent.com/kuberne</a>       |
| tes/website/master/docs/tasks/configure-pod-container/qos-pod-2.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: qos-demo-2</strong>                                                  |
|                                                                       |
| <strong>namespace: qos-example</strong>                                            |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: qos-demo-2-ctr</strong>                                            |
|                                                                       |
| <strong>image: nginx</strong>                                                      |
|                                                                       |
| <strong>resources:</strong>                                                        |
|                                                                       |
| <strong>limits:</strong>                                                           |
|                                                                       |
| <strong>memory: &quot;200Mi&quot;</strong>                                                 |
|                                                                       |
| <strong>requests:</strong>                                                         |
|                                                                       |
| <strong>memory: &quot;100Mi&quot;</strong>                                                 |
+-----------------------------------------------------------------------+</p><p>Create the Pod:</p><p><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/configure-pod-container/qos-pod-2.yaml">https://k8s.io/docs/tasks/configure-pod-container/qos-pod-2.yaml</a> --namespace=qos-example</strong></p><p>View detailed information about the Pod:</p><p><strong>kubectl get pod qos-demo-2 --namespace=qos-example --output=yaml</strong></p><p>The output shows that Kubernetes gave the Pod a QoS class of Burstable.</p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- image: nginx</strong></p><p><strong>imagePullPolicy: Always</strong></p><p><strong>name: qos-demo-2-ctr</strong></p><p><strong>resources:</strong></p><p><strong>limits:</strong></p><p><strong>memory: 200Mi</strong></p><p><strong>requests:</strong></p><p><strong>memory: 100Mi</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>qosClass: Burstable</strong></p><p>Delete your Pod:</p><p><strong>kubectl delete pod qos-demo-2 --namespace=qos-example</strong></p><h4>Create a Pod that gets assigned a QoS class of BestEffort</h4><p>For a Pod to be given a QoS class of BestEffort, the Containers in the Pod must not have any memory or cpu limits or requests.</p><p>Here is the configuration file for a Pod that has one Container. The Container has no memory or cpu limits or requests:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>qos-pod-3.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kuberne">https://raw.githubusercontent.com/kuberne</a>       |
| tes/website/master/docs/tasks/configure-pod-container/qos-pod-3.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: qos-demo-3</strong>                                                  |
|                                                                       |
| <strong>namespace: qos-example</strong>                                            |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: qos-demo-3-ctr</strong>                                            |
|                                                                       |
| <strong>image: nginx</strong>                                                      |
+-----------------------------------------------------------------------+</p><p>Create the Pod:</p><p><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/configure-pod-container/qos-pod-3.yaml">https://k8s.io/docs/tasks/configure-pod-container/qos-pod-3.yaml</a> --namespace=qos-example</strong></p><p>View detailed information about the Pod:</p><p><strong>kubectl get pod qos-demo-3 --namespace=qos-example --output=yaml</strong></p><p>The output shows that Kubernetes gave the Pod a QoS class of BestEffort.</p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>resources: {}</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>qosClass: BestEffort</strong></p><p>Delete your Pod:</p><p><strong>kubectl delete pod qos-demo-3 --namespace=qos-example</strong></p><h4>Create a Pod that has two Containers</h4><p>Here is the configuration file for a Pod that has two Containers. One container specifies a memory request of 200 MiB. The other Container does not specify any requests or limits.</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>qos-pod-4.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kuberne">https://raw.githubusercontent.com/kuberne</a>       |
| tes/website/master/docs/tasks/configure-pod-container/qos-pod-4.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: qos-demo-4</strong>                                                  |
|                                                                       |
| <strong>namespace: qos-example</strong>                                            |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: qos-demo-4-ctr-1</strong>                                          |
|                                                                       |
| <strong>image: nginx</strong>                                                      |
|                                                                       |
| <strong>resources:</strong>                                                        |
|                                                                       |
| <strong>requests:</strong>                                                         |
|                                                                       |
| <strong>memory: &quot;200Mi&quot;</strong>                                                 |
|                                                                       |
| <strong>- name: qos-demo-4-ctr-2</strong>                                          |
|                                                                       |
| <strong>image: redis</strong>                                                      |
+-----------------------------------------------------------------------+</p><p>Notice that this Pod meets the criteria for QoS class Burstable. That is, it does not meet the criteria for QoS class Guaranteed, and one of its Containers has a memory request.</p><p>Create the Pod:</p><p><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/configure-pod-container/qos-pod-4.yaml">https://k8s.io/docs/tasks/configure-pod-container/qos-pod-4.yaml</a> --namespace=qos-example</strong></p><p>View detailed information about the Pod:</p><p><strong>kubectl get pod qos-demo-4 --namespace=qos-example --output=yaml</strong></p><p>The output shows that Kubernetes gave the Pod a QoS class of Burstable:</p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>name: qos-demo-4-ctr-1</strong></p><p><strong>resources:</strong></p><p><strong>requests:</strong></p><p><strong>memory: 200Mi</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>name: qos-demo-4-ctr-2</strong></p><p><strong>resources: {}</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>qosClass: Burstable</strong></p><p>Delete your Pod:</p><p><strong>kubectl delete pod qos-demo-4 --namespace=qos-example</strong></p><h4>Clean up</h4><p>Delete your namespace:</p><p><strong>kubectl delete namespace qos-example</strong></p><h4>What&#x27;s next</h4><h5><strong>For app developers</strong></h5><ul><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/">Assign Memory Resources to Containers and Pods</a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/">Assign CPU Resources to Containers and Pods</a></li></ul><h5><strong>For cluster administrators</strong></h5><ul><li><a href="https://kubernetes.io/docs/tasks/administer-cluster/memory-default-namespace/">Configure Default Memory Requests and Limits for a Namespace</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-cluster/cpu-default-namespace/">Configure Default CPU Requests and Limits for a Namespace</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-cluster/memory-constraint-namespace/">Configure Minimum and Maximum Memory Constraints for a Namespace</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-cluster/cpu-constraint-namespace/">Configure Minimum and Maximum CPU Constraints for a Namespace</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-cluster/quota-memory-cpu-namespace/">Configure Memory and CPU Quotas for a Namespace</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-cluster/quota-pod-namespace/">Configure a Pod Quota for a Namespace</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-cluster/quota-api-object/">Configure Quotas for API Objects</a></li></ul><p><strong><a href="https://kubernetes.io/docs/tasks/">Tasks</a></strong></p><p>Install Tools</p><p><a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">Install and Set Up kubectl</a></p><p><a href="https://kubernetes.io/docs/tasks/tools/install-minikube/">Install Minikube</a></p><p><a href="https://kubernetes.io/docs/setup/independent/install-kubeadm/">Installing kubeadm</a></p><p>Configure Pods and Containers</p><p><a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/">Assign Memory Resources to Containers and Pods</a></p><p><a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/">Assign CPU Resources to Containers and Pods</a></p><p><a href="https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/">Configure Quality of Service for Pods</a></p><p><a href="https://kubernetes.io/docs/tasks/configure-pod-container/extended-resource/">Assign Extended Resources to a Container</a></p><p><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-volume-storage/">Configure a Pod to Use a Volume for Storage</a></p><p><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/">Configure a Pod to Use a PersistentVolume for Storage</a></p><p><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-projected-volume-storage/">Configure a Pod to Use a Projected Volume for Storage</a></p><p><a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/">Configure a Security Context for a Pod or Container</a></p><p><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/">Configure Service Accounts for Pods</a></p><p><a href="https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/">Pull an Image from a Private Registry</a></p><p><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/">Configure Liveness and Readiness Probes</a></p><p><a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/">Assign Pods to Nodes</a></p><p><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-initialization/">Configure Pod Initialization</a></p><p><a href="https://kubernetes.io/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/">Attach Handlers to Container Lifecycle Events</a></p><p><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/">Configure a Pod to Use a ConfigMap</a></p><p><a href="https://kubernetes.io/docs/tasks/configure-pod-container/share-process-namespace/">Share Process Namespace between Containers in a Pod</a></p><p><a href="https://kubernetes.io/docs/tools/kompose/user-guide/">Translate a Docker Compose File to Kubernetes Resources</a></p><p>Inject Data Into Applications</p><p><a href="https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/">Define a Command and Arguments for a Container</a></p><p><a href="https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/">Define Environment Variables for a Container</a></p><p><a href="https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/">Expose Pod Information to Containers Through Environment Variables</a></p><p><a href="https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/">Expose Pod Information to Containers Through Files</a></p><p><a href="https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/">Distribute Credentials Securely Using Secrets</a></p><p><a href="https://kubernetes.io/docs/tasks/inject-data-application/podpreset/">Inject Information into Pods Using a PodPreset</a></p><p>Run Applications</p><p><a href="https://kubernetes.io/docs/tasks/run-application/run-stateless-application-deployment/">Run a Stateless Application Using a Deployment</a></p><p><a href="https://kubernetes.io/docs/tasks/run-application/run-single-instance-stateful-application/">Run a Single-Instance Stateful Application</a></p><p><a href="https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/">Run a Replicated Stateful Application</a></p><p><a href="https://kubernetes.io/docs/tasks/run-application/update-api-object-kubectl-patch/">Update API Objects in Place Using kubectl patch</a></p><p><a href="https://kubernetes.io/docs/tasks/run-application/scale-stateful-set/">Scale a StatefulSet</a></p><p><a href="https://kubernetes.io/docs/tasks/run-application/delete-stateful-set/">Delete a StatefulSet</a></p><p><a href="https://kubernetes.io/docs/tasks/run-application/force-delete-stateful-set-pod/">Force Delete StatefulSet Pods</a></p><p><a href="https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/">Perform Rolling Update Using a Replication Controller</a></p><p><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">Horizontal Pod Autoscaler</a></p><p><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/">Horizontal Pod Autoscaler Walkthrough</a></p><p><a href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/">Specifying a Disruption Budget for your Application</a></p><p>Run Jobs</p><p><a href="https://kubernetes.io/docs/tasks/job/parallel-processing-expansion/">Parallel Processing using Expansions</a></p><p><a href="https://kubernetes.io/docs/tasks/job/coarse-parallel-processing-work-queue/">Coarse Parallel Processing Using a Work Queue</a></p><p><a href="https://kubernetes.io/docs/tasks/job/fine-parallel-processing-work-queue/">Fine Parallel Processing Using a Work Queue</a></p><p>Access Applications in a Cluster</p><p><a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/">Web UI (Dashboard)</a></p><p><a href="https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/">Accessing Clusters</a></p><p><a href="https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/">Configure Access to Multiple Clusters</a></p><p><a href="https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/">Use Port Forwarding to Access Applications in a Cluster</a></p><p><a href="https://kubernetes.io/docs/tasks/access-application-cluster/load-balance-access-application-cluster/">Provide Load-Balanced Access to an Application in a Cluster</a></p><p><a href="https://kubernetes.io/docs/tasks/access-application-cluster/service-access-application-cluster/">Use a Service to Access an Application in a Cluster</a></p><p><a href="https://kubernetes.io/docs/tasks/access-application-cluster/connecting-frontend-backend/">Connect a Front End to a Back End Using a Service</a></p><p><a href="https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/">Create an External Load Balancer</a></p><p><a href="https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/">Configure Your Cloud Provider\&#x27;s Firewalls</a></p><p><a href="https://kubernetes.io/docs/tasks/access-application-cluster/list-all-running-container-images/">List All Container Images Running in a Cluster</a></p><p><a href="https://kubernetes.io/docs/tasks/access-application-cluster/communicate-containers-same-pod-shared-volume/">Communicate Between Containers in the Same Pod Using a Shared Volume</a></p><p><a href="https://github.com/kubernetes/kubernetes/tree/release-1.5/examples/cluster-dns">Configuring DNS for a Cluster</a></p><p>Monitor, Log, and Debug</p><p><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/core-metrics-pipeline/">Core metrics pipeline</a></p><p><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/resource-usage-monitoring/">Tools for Monitoring Compute, Storage, and Network Resources</a></p><p><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/get-shell-running-container/">Get a Shell to a Running Container</a></p><p><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/monitor-node-health/">Monitor Node Health</a></p><p><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/logging-stackdriver/">Logging Using Stackdriver</a></p><p><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/events-stackdriver/">Events in Stackdriver</a></p><p><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/logging-elasticsearch-kibana/">Logging Using Elasticsearch and Kibana</a></p><p><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/determine-reason-pod-failure/">Determine the Reason for Pod Failure</a></p><p><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-init-containers/">Debug Init Containers</a></p><p><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-pod-replication-controller/">Debug Pods and Replication Controllers</a></p><p><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/">Debug Services</a></p><p><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-cluster/">Troubleshoot Clusters</a></p><p><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/">Troubleshoot Applications</a></p><p><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-stateful-set/">Debug a StatefulSet</a></p><p><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application-introspection/">Application Introspection and Debugging</a></p><p><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/">Auditing</a></p><p><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/local-debugging/">Developing and debugging services locally</a></p><p><a href="https://github.com/kubernetes/kubernetes/tree/release-1.5/examples/explorer">Use Explorer to Examine the Runtime Environment</a></p><p>Extend Kubernetes</p><p><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/http-proxy-access-api/">Use an HTTP Proxy to Access the Kubernetes API</a></p><p><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/">Extend the Kubernetes API with CustomResourceDefinitions</a></p><p><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-third-party-resource/">Extend the Kubernetes API with ThirdPartyResources</a></p><p><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/migrate-third-party-resource/">Migrate a ThirdPartyResource to CustomResourceDefinition</a></p><p><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/configure-aggregation-layer/">Configure the aggregation layer</a></p><p><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/setup-extension-api-server/">Setup an extension API server</a></p><p><a href="https://kubernetes.io/docs/tasks/service-catalog/install-service-catalog-using-helm/">Install Service Catalog using Helm</a></p><p><a href="https://kubernetes.io/docs/tasks/service-catalog/install-service-catalog-using-sc/">Install Service Catalog using SC</a></p><p>TLS</p><p><a href="https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/">Manage TLS Certificates in a Cluster</a></p><p><a href="https://kubernetes.io/docs/tasks/tls/certificate-rotation/">Certificate Rotation</a></p><p>Administer a Cluster</p><p>Upgrading or downgrading Kubernetes</p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/upgrade-downgrade/upgrade-1-6/">Cluster Management Guide for Version 1.6</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/upgrade-downgrade/kubeadm-upgrade-1-7/">Upgrading kubeadm clusters from 1.6 to 1.7</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/upgrade-downgrade/kubeadm-upgrade-1-8/">Upgrading kubeadm clusters from 1.7 to 1.8</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/upgrade-downgrade/kubeadm-upgrade-1-9/">Upgrading/downgrading kubeadm clusters between v1.8 to v1.9</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/upgrade-downgrade/kubeadm-upgrade-ha/">Upgrading kubeadm HA clusters from 1.9.x to 1.9.y</a></p><p>Manage Memory, CPU, and API Resources</p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/memory-default-namespace/">Configure Default Memory Requests and Limits for a Namespace</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/cpu-default-namespace/">Configure Default CPU Requests and Limits for a Namespace</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/memory-constraint-namespace/">Configure Minimum and Maximum Memory Constraints for a Namespace</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/cpu-constraint-namespace/">Configure Minimum and Maximum CPU Constraints for a Namespace</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/quota-memory-cpu-namespace/">Configure Memory and CPU Quotas for a Namespace</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/quota-pod-namespace/">Configure a Pod Quota for a Namespace</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/quota-api-object/">Configure Quotas for API Objects</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/extended-resource-node/">Advertise Extended Resources for a Node</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/">Control CPU Management Policies on the Node</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/access-cluster-api/">Access Clusters Using the Kubernetes API</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/access-cluster-services/">Access Services Running on Clusters</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/">Securing a Cluster</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/">Using Sysctls in a Kubernetes Cluster</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/">Encrypting Secret Data at Rest</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/">Operating etcd clusters for Kubernetes</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/static-pod/">Static Pods</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/cluster-management/">Cluster Management</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/namespaces/">Share a Cluster with Namespaces</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/namespaces-walkthrough/">Namespaces Walkthrough</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/dns-horizontal-autoscaling/">Autoscale the DNS Service in a Cluster</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/coredns/">Using CoreDNS for Service Discovery</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/">Safely Drain a Node while Respecting Application SLOs</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/">Configure Out Of Resource Handling</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/">Reserve Compute Resources for System Daemons</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/">Guaranteed Scheduling For Critical Add-On Pods</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/">Declare Network Policy</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/kms-provider/">Using a KMS provider for data encryption</a></p><p>Install Network Policy Provider</p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/calico-network-policy/">Use Calico for NetworkPolicy</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/cilium-network-policy/">Use Cilium for NetworkPolicy</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/kube-router-network-policy/">Use Kube-router for NetworkPolicy</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/romana-network-policy/">Romana for NetworkPolicy</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/weave-network-policy/">Weave Net for NetworkPolicy</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/">Reconfigure a Node\&#x27;s Kubelet in a Live Cluster</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/">Set Kubelet parameters via a config file</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/change-pv-reclaim-policy/">Change the Reclaim Policy of a PersistentVolume</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/limit-storage-consumption/">Limit Storage Consumption</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/change-default-storage-class/">Change the default StorageClass</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/">Kubernetes Cloud Controller Manager</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/developing-cloud-controller-manager/">Developing Cloud Controller Manager</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/highly-available-master/">Set up High-Availability Kubernetes Masters</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/">Configure Multiple Schedulers</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/ip-masq-agent/">IP Masquerade Agent User Guide</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/">Customizing DNS Service</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/">Debugging DNS Resolution</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/pvc-protection/">Persistent Volume Claim Protection</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/storage-object-in-use-protection/">Storage Object in Use Protection</a></p><p>Federation - Run an App on Multiple Clusters</p><p><a href="https://kubernetes.io/docs/tasks/federation/federation-service-discovery/">Cross-cluster Service Discovery using Federated Services</a></p><p><a href="https://kubernetes.io/docs/tasks/federation/set-up-cluster-federation-kubefed/">Set up Cluster Federation with Kubefed</a></p><p><a href="https://kubernetes.io/docs/tasks/federation/set-up-coredns-provider-federation/">Set up CoreDNS as DNS provider for Cluster Federation</a></p><p><a href="https://kubernetes.io/docs/tasks/federation/set-up-placement-policies-federation/">Set up placement policies in Federation</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-federation/cluster/">Federated Cluster</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-federation/configmap/">Federated ConfigMap</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-federation/daemonset/">Federated DaemonSet</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-federation/deployment/">Federated Deployment</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-federation/events/">Federated Events</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-federation/hpa/">Federated Horizontal Pod Autoscalers (HPA)</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-federation/ingress/">Federated Ingress</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-federation/job/">Federated Jobs</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-federation/namespaces/">Federated Namespaces</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-federation/replicaset/">Federated ReplicaSets</a></p><p><a href="https://kubernetes.io/docs/tasks/administer-federation/secret/">Federated Secrets</a></p><p>Manage Cluster Daemons</p><p><a href="https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/">Perform a Rolling Update on a DaemonSet</a></p><p><a href="https://kubernetes.io/docs/tasks/manage-daemon/rollback-daemon-set/">Performing a Rollback on a DaemonSet</a></p><p><a href="https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/">Manage GPUs</a></p><p><a href="https://kubernetes.io/docs/tasks/manage-hugepages/scheduling-hugepages/">Manage HugePages</a></p><p><a href="https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/">Extend kubectl with plugins</a></p><p><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/">Troubleshooting</a></p><p><a href="https://kubernetes.io/editdocs#docs/tasks/configure-pod-container/extended-resource.md">Edit This Page</a></p><h3>Assign Extended Resources to a Container</h3><p>This page shows how to assign extended resources to a Container.</p><p><strong>FEATURE STATE:</strong> <strong>Kubernetes v1.10</strong> <a href="https://kubernetes.io/docs/tasks/configure-pod-container/extended-resource/">stable</a></p><ul><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/extended-resource/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/extended-resource/#assign-an-extended-resource-to-a-pod"><strong>Assign an extended resource to a Pod</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/extended-resource/#attempt-to-create-a-second-pod"><strong>Attempt to create a second Pod</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/extended-resource/#clean-up"><strong>Clean up</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/extended-resource/#whats-next"><strong>What&#x27;s next</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/extended-resource/#for-application-developers"><strong>For application developers</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/extended-resource/#for-cluster-administrators"><strong>For cluster administrators</strong></a></li></ul></li></ul><h4>Before you begin</h4><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using<a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <strong>kubectl version</strong>.</p><p>Before you do this exercise, do the exercise in <a href="https://kubernetes.io/docs/tasks/administer-cluster/extended-resource-node/">Advertise Extended Resources for a Node</a>. That will configure one of your Nodes to advertise a dongle resource.</p><h4>Assign an extended resource to a Pod</h4><p>To request an extended resource, include the <strong>resources:requests</strong> field in your Container manifest. Extended resources are fully qualified with any domain outside of <strong>*<!-- -->.kubernetes.io/</strong>. Valid extended resource names have the form <strong>example.com/foo</strong> where <strong>example.com</strong> is replaced with your organization&#x27;s domain and <strong>foo</strong> is a descriptive resource name.</p><p>Here is the configuration file for a Pod that has one Container:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>extended-resour                                                    |
| ce-pod.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/website/">https://raw.githubusercontent.com/kubernetes/website/</a> |
| master/docs/tasks/configure-pod-container/extended-resource-pod.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: extended-resource-demo</strong>                                      |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: extended-resource-demo-ctr</strong>                                |
|                                                                       |
| <strong>image: nginx</strong>                                                      |
|                                                                       |
| <strong>resources:</strong>                                                        |
|                                                                       |
| <strong>requests:</strong>                                                         |
|                                                                       |
| <strong>example.com/dongle: 3</strong>                                             |
|                                                                       |
| <strong>limits:</strong>                                                           |
|                                                                       |
| <strong>example.com/dongle: 3</strong>                                             |
+-----------------------------------------------------------------------+</p><p>In the configuration file, you can see that the Container requests 3 dongles.</p><p>Create a Pod:</p><p><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/configure-pod-container/extended-resource-pod.yaml">https://k8s.io/docs/tasks/configure-pod-container/extended-resource-pod.yaml</a></strong></p><p>Verify that the Pod is running:</p><p><strong>kubectl get pod extended-resource-demo</strong></p><p>Describe the Pod:</p><p><strong>kubectl describe pod extended-resource-demo</strong></p><p>The output shows dongle requests:</p><p><strong>Limits:</strong></p><p><strong>example.com/dongle: 3</strong></p><p><strong>Requests:</strong></p><p><strong>example.com/dongle: 3</strong></p><h4>Attempt to create a second Pod</h4><p>Here is the configuration file for a Pod that has one Container. The Container requests two dongles.</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>extended-resource-p                                                |
| od-2.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/website/ma">https://raw.githubusercontent.com/kubernetes/website/ma</a> |
| ster/docs/tasks/configure-pod-container/extended-resource-pod-2.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: extended-resource-demo-2</strong>                                    |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: extended-resource-demo-2-ctr</strong>                              |
|                                                                       |
| <strong>image: nginx</strong>                                                      |
|                                                                       |
| <strong>resources:</strong>                                                        |
|                                                                       |
| <strong>requests:</strong>                                                         |
|                                                                       |
| <strong>example.com/dongle: 2</strong>                                             |
|                                                                       |
| <strong>limits:</strong>                                                           |
|                                                                       |
| <strong>example.com/dongle: 2</strong>                                             |
+-----------------------------------------------------------------------+</p><p>Kubernetes will not be able to satisfy the request for two dongles, because the first Pod used three of the four available dongles.</p><p>Attempt to create a Pod:</p><p><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/configure-pod-container/extended-resource-pod-2.yaml">https://k8s.io/docs/tasks/configure-pod-container/extended-resource-pod-2.yaml</a></strong></p><p>Describe the Pod</p><p><strong>kubectl describe pod extended-resource-demo-2</strong></p><p>The output shows that the Pod cannot be scheduled, because there is no Node that has 2 dongles available:</p><p><strong>Conditions:</strong></p><p><strong>Type Status</strong></p><p><strong>PodScheduled False</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>Events:</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>.<!-- -->.. Warning FailedScheduling pod (extended-resource-demo-2) failed to fit in any node</strong></p><p><strong>fit failure summary on nodes : Insufficient example.com/dongle (1)</strong></p><p>View the Pod status:</p><p><strong>kubectl get pod extended-resource-demo-2</strong></p><p>The output shows that the Pod was created, but not scheduled to run on a Node. It has a status of Pending:</p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>extended-resource-demo-2 0/1 Pending 0 6m</strong></p><h4>Clean up</h4><p>Delete the Pod that you created for this exercise:</p><p><strong>kubectl delete pod extended-resource-demo-2</strong></p><h4>What&#x27;s next</h4><h5><strong>For application developers</strong></h5><ul><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/">Assign Memory Resources to Containers and Pods</a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/">Assign CPU Resources to Containers and Pods</a></li></ul><h5><strong>For cluster administrators</strong></h5><ul><li><a href="https://kubernetes.io/docs/tasks/administer-cluster/extended-resource-node/">Advertise Extended Resources for a Node</a></li></ul><h3>Configure a Pod to Use a Volume for Storage</h3><p>This page shows how to configure a Pod to use a Volume for storage.</p><p>A Container&#x27;s file system lives only as long as the Container does, so when a Container terminates and restarts, changes to the filesystem are lost. For more consistent storage that is independent of the Container, you can use a <a href="https://kubernetes.io/docs/concepts/storage/volumes/">Volume</a>. This is especially important for stateful applications, such as key-value stores and databases. For example, Redis is a key-value cache and store.</p><ul><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-volume-storage/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-volume-storage/#configure-a-volume-for-a-pod"><strong>Configure a volume for a Pod</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-volume-storage/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Before you begin</h4><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using<a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <strong>kubectl version</strong>.</p><h4>Configure a volume for a Pod</h4><p>In this exercise, you create a Pod that runs one Container. This Pod has a Volume of type <a href="https://kubernetes.io/docs/concepts/storage/volumes/#emptydir">emptyDir</a>that lasts for the life of the Pod, even if the Container terminates and restarts. Here is the configuration file for the Pod:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>pod-redis.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kuberne">https://raw.githubusercontent.com/kuberne</a>       |
| tes/website/master/docs/tasks/configure-pod-container/pod-redis.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: redis</strong>                                                       |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: redis</strong>                                                     |
|                                                                       |
| <strong>image: redis</strong>                                                      |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- name: redis-storage</strong>                                             |
|                                                                       |
| <strong>mountPath: /data/redis</strong>                                            |
|                                                                       |
| <strong>volumes:</strong>                                                          |
|                                                                       |
| <strong>- name: redis-storage</strong>                                             |
|                                                                       |
| <strong>emptyDir: {}</strong>                                                      |
+-----------------------------------------------------------------------+</p><ol><li>Create the Pod:</li><li><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/configure-pod-container/pod-redis.yaml">https://k8s.io/docs/tasks/configure-pod-container/pod-redis.yaml</a></strong></li><li>Verify that the Pod&#x27;s Container is running, and then watch for changes to the Pod:</li><li><strong>kubectl get pod redis --watch</strong></li></ol><p>The output looks like this:</p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>redis 1/1 Running 0 13s</strong></p><ol><li>In another terminal, get a shell to the running Container:</li><li><strong>kubectl exec -it redis -- /bin/bash</strong></li><li>In your shell, go to <strong>/data/redis</strong>, and create a file:</li><li><strong>root@redis:/data# cd /data/redis/</strong></li><li><strong>root@redis:/data/redis# echo Hello &gt; test-file</strong></li><li>In your shell, list the running processes:</li><li><strong>root@redis:/data/redis# ps aux</strong></li></ol><p>The output is similar to this:</p><p><strong>USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND</strong></p><p><strong>redis 1 0.1 0.1 33308 3828 ? Ssl 00:46 0:00 redis-server <!-- -->*<!-- -->:6379</strong></p><p><strong>root 12 0.0 0.0 20228 3020 ? Ss 00:47 0:00 /bin/bash</strong></p><p><strong>root 15 0.0 0.0 17500 2072 ? R+ 00:48 0:00 ps aux</strong></p><ol><li>In your shell, kill the redis process:</li><li><strong>root@redis:/data/redis# kill <code>&lt;pid&gt;</code></strong></li></ol><p>where <strong><code>&lt;pid&gt;</code></strong> is the redis process ID (PID).</p><ol><li>In your original terminal, watch for changes to the redis Pod. Eventually, you will see something like this:</li><li><strong>NAME READY STATUS RESTARTS AGE</strong></li><li><strong>redis 1/1 Running 0 13s</strong></li><li><strong>redis 0/1 Completed 0 6m</strong></li><li><strong>redis 1/1 Running 1 6m</strong></li></ol><p>At this point, the Container has terminated and restarted. This is because the redis Pod has a<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#podspec-v1-core">restartPolicy</a> of <strong>Always</strong>.</p><ol><li>Get a shell into the restarted Container:</li><li><strong>kubectl exec -it redis -- /bin/bash</strong></li><li>In your shell, goto <strong>/data/redis</strong>, and verify that <strong>test-file</strong> is still there.</li></ol><h4>What&#x27;s next</h4><ul><li>See <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#volume-v1-core">Volume</a>.</li><li>See <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#pod-v1-core">Pod</a>.</li><li>In addition to the local disk storage provided by <strong>emptyDir</strong>, Kubernetes supports many different network-attached storage solutions, including PD on GCE and EBS on EC2, which are preferred for critical data, and will handle details such as mounting and unmounting the devices on the nodes. See <a href="https://kubernetes.io/docs/concepts/storage/volumes/">Volumes</a> for more details.</li></ul><h3>Configure a Pod to Use a PersistentVolume for Storage</h3><p>This page shows how to configure a Pod to use a PersistentVolumeClaim for storage. Here is a summary of the process:</p><ol><li>A cluster administrator creates a PersistentVolume that is backed by physical storage. The administrator does not associate the volume with any Pod.</li><li>A cluster user creates a PersistentVolumeClaim, which gets automatically bound to a suitable PersistentVolume.</li><li>The user creates a Pod that uses the PersistentVolumeClaim as storage.</li></ol><ul><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-an-indexhtml-file-on-your-node"><strong>Create an index.html file on your Node</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolume"><strong>Create a PersistentVolume</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim"><strong>Create a PersistentVolumeClaim</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-pod"><strong>Create a Pod</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#access-control"><strong>Access control</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#whats-next"><strong>What&#x27;s next</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#reference"><strong>Reference</strong></a></li></ul></li></ul><h4>Before you begin</h4><ul><li>You need to have a Kubernetes cluster that has only one Node, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a single-node cluster, you can create one by using <a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>.</li><li>Familiarize yourself with the material in <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">Persistent Volumes</a>.</li></ul><h4>Create an index.html file on your Node</h4><p>Open a shell to the Node in your cluster. How you open a shell depends on how you set up your cluster. For example, if you are using Minikube, you can open a shell to your Node by entering <strong>minikube ssh</strong>.</p><p>In your shell, create a <strong>/mnt/data</strong> directory:</p><p><strong>mkdir /mnt/data</strong></p><p>In the <strong>/mnt/data</strong> directory, create an <strong>index.html</strong> file:</p><p><strong>echo \&#x27;Hello from Kubernetes storage\&#x27; &gt; /mnt/data/index.html</strong></p><h4>Create a PersistentVolume</h4><p>In this exercise, you create a hostPath PersistentVolume. Kubernetes supports hostPath for development and testing on a single-node cluster. A hostPath PersistentVolume uses a file or directory on the Node to emulate network-attached storage.</p><p>In a production cluster, you would not use hostPath. Instead a cluster administrator would provision a network resource like a Google Compute Engine persistent disk, an NFS share, or an Amazon Elastic Block Store volume. Cluster administrators can also use <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#storageclass-v1-storage">StorageClasses</a> to set up <a href="http://blog.kubernetes.io/2016/10/dynamic-provisioning-and-storage-in-kubernetes.html">dynamic provisioning</a>.</p><p>Here is the configuration file for the hostPath PersistentVolume:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>t                                                                  |
| ask-pv-volume.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/w">https://raw.githubusercontent.com/kubernetes/w</a> |
| ebsite/master/docs/tasks/configure-pod-container/task-pv-volume.yaml) |
+=======================================================================+
| <strong>kind: PersistentVolume</strong>                                            |
|                                                                       |
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: task-pv-volume</strong>                                              |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>type: local</strong>                                                       |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>storageClassName: manual</strong>                                          |
|                                                                       |
| <strong>capacity:</strong>                                                         |
|                                                                       |
| <strong>storage: 10Gi</strong>                                                     |
|                                                                       |
| <strong>accessModes:</strong>                                                      |
|                                                                       |
| <strong>- ReadWriteOnce</strong>                                                   |
|                                                                       |
| <strong>hostPath:</strong>                                                         |
|                                                                       |
| <strong>path: &quot;/mnt/data&quot;</strong>                                               |
+-----------------------------------------------------------------------+</p><p>The configuration file specifies that the volume is at <strong>/mnt/data</strong> on the cluster&#x27;s Node. The configuration also specifies a size of 10 gibibytes and an access mode of <strong>ReadWriteOnce</strong>, which means the volume can be mounted as read-write by a single Node. It defines the <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#class">StorageClass name</a><strong>manual</strong> for the PersistentVolume, which will be used to bind PersistentVolumeClaim requests to this PersistentVolume.</p><p>Create the PersistentVolume:</p><p><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/configure-pod-container/task-pv-volume.yaml">https://k8s.io/docs/tasks/configure-pod-container/task-pv-volume.yaml</a></strong></p><p>View information about the PersistentVolume:</p><p><strong>kubectl get pv task-pv-volume</strong></p><p>The output shows that the PersistentVolume has a <strong>STATUS</strong> of <strong>Available</strong>. This means it has not yet been bound to a PersistentVolumeClaim.</p><p><strong>NAME CAPACITY ACCESSMODES RECLAIMPOLICY STATUS CLAIM STORAGECLASS REASON AGE</strong></p><p><strong>task-pv-volume 10Gi RWO Retain Available manual 4s</strong></p><h4>Create a PersistentVolumeClaim</h4><p>The next step is to create a PersistentVolumeClaim. Pods use PersistentVolumeClaims to request physical storage. In this exercise, you create a PersistentVolumeClaim that requests a volume of at least three gibibytes that can provide read-write access for at least one Node.</p><p>Here is the configuration file for the PersistentVolumeClaim:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<em>                                                                    |
| </em>task-pv-claim.yaml** ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/">https://raw.githubusercontent.com/kubernetes/</a> |
| website/master/docs/tasks/configure-pod-container/task-pv-claim.yaml) |
+=======================================================================+
| <strong>kind: PersistentVolumeClaim</strong>                                       |
|                                                                       |
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: task-pv-claim</strong>                                               |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>storageClassName: manual</strong>                                          |
|                                                                       |
| <strong>accessModes:</strong>                                                      |
|                                                                       |
| <strong>- ReadWriteOnce</strong>                                                   |
|                                                                       |
| <strong>resources:</strong>                                                        |
|                                                                       |
| <strong>requests:</strong>                                                         |
|                                                                       |
| <strong>storage: 3Gi</strong>                                                      |
+-----------------------------------------------------------------------+</p><p>Create the PersistentVolumeClaim:</p><p><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/configure-pod-container/task-pv-claim.yaml">https://k8s.io/docs/tasks/configure-pod-container/task-pv-claim.yaml</a></strong></p><p>After you create the PersistentVolumeClaim, the Kubernetes control plane looks for a PersistentVolume that satisfies the claim&#x27;s requirements. If the control plane finds a suitable PersistentVolume with the same StorageClass, it binds the claim to the volume.</p><p>Look again at the PersistentVolume:</p><p><strong>kubectl get pv task-pv-volume</strong></p><p>Now the output shows a <strong>STATUS</strong> of <strong>Bound</strong>.</p><p><strong>NAME CAPACITY ACCESSMODES RECLAIMPOLICY STATUS CLAIM STORAGECLASS REASON AGE</strong></p><p><strong>task-pv-volume 10Gi RWO Retain Bound default/task-pv-claim manual 2m</strong></p><p>Look at the PersistentVolumeClaim:</p><p><strong>kubectl get pvc task-pv-claim</strong></p><p>The output shows that the PersistentVolumeClaim is bound to your PersistentVolume, <strong>task-pv-volume</strong>.</p><p><strong>NAME STATUS VOLUME CAPACITY ACCESSMODES STORAGECLASS AGE</strong></p><p><strong>task-pv-claim Bound task-pv-volume 10Gi RWO manual 30s</strong></p><h4>Create a Pod</h4><p>The next step is to create a Pod that uses your PersistentVolumeClaim as a volume.</p><p>Here is the configuration file for the Pod:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>task-pv-pod.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernete">https://raw.githubusercontent.com/kubernete</a>   |
| s/website/master/docs/tasks/configure-pod-container/task-pv-pod.yaml) |
+=======================================================================+
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: task-pv-pod</strong>                                                 |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>volumes:</strong>                                                          |
|                                                                       |
| <strong>- name: task-pv-storage</strong>                                           |
|                                                                       |
| <strong>persistentVolumeClaim:</strong>                                            |
|                                                                       |
| <strong>claimName: task-pv-claim</strong>                                          |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: task-pv-container</strong>                                         |
|                                                                       |
| <strong>image: nginx</strong>                                                      |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- containerPort: 80</strong>                                               |
|                                                                       |
| <strong>name: &quot;http-server&quot;</strong>                                             |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- mountPath: &quot;/usr/share/nginx/html&quot;</strong>                            |
|                                                                       |
| <strong>name: task-pv-storage</strong>                                             |
+-----------------------------------------------------------------------+</p><p>Notice that the Pod&#x27;s configuration file specifies a PersistentVolumeClaim, but it does not specify a PersistentVolume. From the Pod&#x27;s point of view, the claim is a volume.</p><p>Create the Pod:</p><p><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/configure-pod-container/task-pv-pod.yaml">https://k8s.io/docs/tasks/configure-pod-container/task-pv-pod.yaml</a></strong></p><p>Verify that the Container in the Pod is running;</p><p><strong>kubectl get pod task-pv-pod</strong></p><p>Get a shell to the Container running in your Pod:</p><p><strong>kubectl exec -it task-pv-pod -- /bin/bash</strong></p><p>In your shell, verify that nginx is serving the <strong>index.html</strong> file from the hostPath volume:</p><p><strong>root@task-pv-pod:/# apt-get update</strong></p><p><strong>root@task-pv-pod:/# apt-get install curl</strong></p><p><strong>root@task-pv-pod:/# curl localhost</strong></p><p>The output shows the text that you wrote to the <strong>index.html</strong> file on the hostPath volume:</p><p><strong>Hello from Kubernetes storage</strong></p><h4>Access control</h4><p>Storage configured with a group ID (GID) allows writing only by Pods using the same GID. Mismatched or missing GIDs cause permission denied errors. To reduce the need for coordination with users, an administrator can annotate a PersistentVolume with a GID. Then the GID is automatically added to any Pod that uses the PersistentVolume.</p><p>Use the <strong>pv.beta.kubernetes.io/gid</strong> annotation as follows:</p><p><strong>kind: PersistentVolume</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>metadata:</strong></p><p><strong>name: pv1</strong></p><p><strong>annotations:</strong></p><p><strong>pv.beta.kubernetes.io/gid: &quot;1234&quot;</strong></p><p>When a Pod consumes a PersistentVolume that has a GID annotation, the annotated GID is applied to all Containers in the Pod in the same way that GIDs specified in the Pod&#x27;s security context are. Every GID, whether it originates from a PersistentVolume annotation or the Pod&#x27;s specification, is applied to the first process run in each Container.</p><p><strong>Note</strong>: When a Pod consumes a PersistentVolume, the GIDs associated with the PersistentVolume are not present on the Pod resource itself.</p><h4>What&#x27;s next</h4><ul><li>Learn more about <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">PersistentVolumes</a>.</li><li>Read the <a href="https://git.k8s.io/community/contributors/design-proposals/storage/persistent-storage.md">Persistent Storage design document</a>.</li></ul><h5><strong>Reference</strong></h5><ul><li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#persistentvolume-v1-core">PersistentVolume</a></li><li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#persistentvolumespec-v1-core">PersistentVolumeSpec</a></li><li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#persistentvolumeclaim-v1-core">PersistentVolumeClaim</a></li><li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#persistentvolumeclaimspec-v1-core">PersistentVolumeClaimSpec</a></li></ul><h3>Configure a Pod to Use a Projected Volume for Storage</h3><p>This page shows how to use a <a href="https://kubernetes.io/docs/concepts/storage/volumes/#projected"><strong>projected</strong></a> volume to mount several existing volume sources into the same directory. Currently, <strong>secret</strong>, <strong>configMap</strong>, and <strong>downwardAPI</strong> volumes can be projected.</p><ul><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-projected-volume-storage/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-projected-volume-storage/#configure-a-projected-volume-for-a-pod"><strong>Configure a projected volume for a pod</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-projected-volume-storage/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Before you begin</h4><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using<a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <strong>kubectl version</strong>.</p><h4>Configure a projected volume for a pod</h4><p>In this exercise, you create username and password Secrets from local files. You then create a Pod that runs one Container, using a <a href="https://kubernetes.io/docs/concepts/storage/volumes/#projected"><strong>projected</strong></a> Volume to mount the Secrets into the same shared directory.</p><p>Here is the configuration file for the Pod:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>proje                                                              |
| cted-volume.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/web">https://raw.githubusercontent.com/kubernetes/web</a> |
| site/master/docs/tasks/configure-pod-container/projected-volume.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: test-projected-volume</strong>                                       |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: test-projected-volume</strong>                                     |
|                                                                       |
| <strong>image: busybox</strong>                                                    |
|                                                                       |
| <strong>args:</strong>                                                             |
|                                                                       |
| <strong>- sleep</strong>                                                           |
|                                                                       |
| <strong>- &quot;86400&quot;</strong>                                                       |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- name: all-in-one</strong>                                                |
|                                                                       |
| <strong>mountPath: &quot;/projected-volume&quot;</strong>                                  |
|                                                                       |
| <strong>readOnly: true</strong>                                                    |
|                                                                       |
| <strong>volumes:</strong>                                                          |
|                                                                       |
| <strong>- name: all-in-one</strong>                                                |
|                                                                       |
| <strong>projected:</strong>                                                        |
|                                                                       |
| <strong>sources:</strong>                                                          |
|                                                                       |
| <strong>- secret:</strong>                                                         |
|                                                                       |
| <strong>name: user</strong>                                                        |
|                                                                       |
| <strong>- secret:</strong>                                                         |
|                                                                       |
| <strong>name: pass</strong>                                                        |
+-----------------------------------------------------------------------+</p><ol><li>Create the Secrets:</li><li><strong># Create files containing the username and password:</strong></li><li><strong>echo -n &quot;admin&quot; &gt; ./username.txt</strong></li><li><strong>echo -n &quot;1f2d1e2e67df&quot; &gt; ./password.txt</strong></li><li><strong># Package these files into secrets:</strong></li><li><strong>kubectl create secret generic user --from-file=./username.txt</strong></li><li><strong>kubectl create secret generic pass --from-file=./password.txt</strong></li><li>Create the Pod:</li><li><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/configure-pod-container/projected-volume.yaml">https://k8s.io/docs/tasks/configure-pod-container/projected-volume.yaml</a></strong></li><li>Verify that the Pod&#x27;s Container is running, and then watch for changes to the Pod:</li><li><strong>kubectl get --watch pod test-projected-volume</strong></li></ol><p>The output looks like this:</p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>test-projected-volume 1/1 Running 0 14s</strong></p><ol><li>In another terminal, get a shell to the running Container:</li><li><strong>kubectl exec -it test-projected-volume -- /bin/sh</strong></li><li>In your shell, verify that the <strong>projected-volume</strong> directory contains your projected sources:</li><li><strong>/ # ls /projected-volume/</strong></li></ol><h4>What&#x27;s next</h4><ul><li>Learn more about <a href="https://kubernetes.io/docs/concepts/storage/volumes/#projected"><strong>projected</strong></a> volumes.</li><li>Read the <a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/all-in-one-volume.md">all-in-one volume</a> design document.</li></ul><h3>Configure a Security Context for a Pod or Container</h3><p>A security context defines privilege and access control settings for a Pod or Container. Security context settings include:</p><ul><li>Discretionary Access Control: Permission to access an object, like a file, is based on <a href="https://wiki.archlinux.org/index.php/users_and_groups">user ID (UID) and group ID (GID)</a>.</li><li><a href="https://en.wikipedia.org/wiki/Security-Enhanced_Linux">Security Enhanced Linux (SELinux)</a>: Objects are assigned security labels.</li><li>Running as privileged or unprivileged.</li><li><a href="https://linux-audit.com/linux-capabilities-hardening-linux-binaries-by-removing-setuid/">Linux Capabilities</a>: Give a process some privileges, but not all the privileges of the root user.</li><li><a href="https://kubernetes.io/docs/tutorials/clusters/apparmor/">AppArmor</a>: Use program profiles to restrict the capabilities of individual programs.</li><li><a href="https://en.wikipedia.org/wiki/Seccomp">Seccomp</a>: Limit a process&#x27;s access to open file descriptors.</li><li>AllowPrivilegeEscalation: Controls whether a process can gain more privileges than its parent process. This bool directly controls whether the <a href="https://www.kernel.org/doc/Documentation/prctl/no_new_privs.txt"><strong>no_new_privs</strong></a> flag gets set on the container process. AllowPrivilegeEscalation is true always when the container is: 1) run as Privileged OR 2) has <strong>CAP_SYS_ADMIN</strong>.</li></ul><p>For more information about security mechanisms in Linux, see <a href="https://www.linux.com/learn/overview-linux-kernel-security-features">Overview of Linux Kernel Security Features</a></p><ul><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod"><strong>Set the security context for a Pod</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container"><strong>Set the security context for a Container</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container"><strong>Set capabilities for a Container</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#assign-selinux-labels-to-a-container"><strong>Assign SELinux labels to a Container</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#discussion"><strong>Discussion</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Before you begin</h4><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using<a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <strong>kubectl version</strong>.</p><h4>Set the security context for a Pod</h4><p>To specify security settings for a Pod, include the <strong>securityContext</strong> field in the Pod specification. The <strong>securityContext</strong> field is a <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#podsecuritycontext-v1-core">PodSecurityContext</a> object. The security settings that you specify for a Pod apply to all Containers in the Pod. Here is a configuration file for a Pod that has a <strong>securityContext</strong> and an <strong>emptyDir</strong> volume:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>secur                                                              |
| ity-context.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/web">https://raw.githubusercontent.com/kubernetes/web</a> |
| site/master/docs/tasks/configure-pod-container/security-context.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: security-context-demo</strong>                                       |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>securityContext:</strong>                                                  |
|                                                                       |
| <strong>runAsUser: 1000</strong>                                                   |
|                                                                       |
| <strong>fsGroup: 2000</strong>                                                     |
|                                                                       |
| <strong>volumes:</strong>                                                          |
|                                                                       |
| <strong>- name: sec-ctx-vol</strong>                                               |
|                                                                       |
| <strong>emptyDir: {}</strong>                                                      |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: sec-ctx-demo</strong>                                              |
|                                                                       |
| <strong>image: gcr.io/google-samples/node-hello:1.0</strong>                       |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- name: sec-ctx-vol</strong>                                               |
|                                                                       |
| <strong>mountPath: /data/demo</strong>                                             |
|                                                                       |
| <strong>securityContext:</strong>                                                  |
|                                                                       |
| <strong>allowPrivilegeEscalation: false</strong>                                   |
+-----------------------------------------------------------------------+</p><p>In the configuration file, the <strong>runAsUser</strong> field specifies that for any Containers in the Pod, the first process runs with user ID 1000. The <strong>fsGroup</strong> field specifies that group ID 2000 is associated with all Containers in the Pod. Group ID 2000 is also associated with the volume mounted at <strong>/data/demo</strong> and with any files created in that volume.</p><p>Create the Pod:</p><p><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/configure-pod-container/security-context.yaml">https://k8s.io/docs/tasks/configure-pod-container/security-context.yaml</a></strong></p><p>Verify that the Pod&#x27;s Container is running:</p><p><strong>kubectl get pod security-context-demo</strong></p><p>Get a shell to the running Container:</p><p><strong>kubectl exec -it security-context-demo -- sh</strong></p><p>In your shell, list the running processes:</p><p><strong>ps aux</strong></p><p>The output shows that the processes are running as user 1000, which is the value of <strong>runAsUser</strong>:</p><p><strong>USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND</strong></p><p><strong>1000 1 0.0 0.0 4336 724 ? Ss 18:16 0:00 /bin/sh -c node server.js</strong></p><p><strong>1000 5 0.2 0.6 772124 22768 ? Sl 18:16 0:00 node server.js</strong></p><p><strong>.<!-- -->..</strong></p><p>In your shell, navigate to <strong>/data</strong>, and list the one directory:</p><p><strong>cd /data</strong></p><p><strong>ls -l</strong></p><p>The output shows that the <strong>/data/demo</strong> directory has group ID 2000, which is the value of <strong>fsGroup</strong>.</p><p><strong>drwxrwsrwx 2 root 2000 4096 Jun 6 20:08 demo</strong></p><p>In your shell, navigate to <strong>/data/demo</strong>, and create a file:</p><p><strong>cd demo</strong></p><p><strong>echo hello &gt; testfile</strong></p><p>List the file in the <strong>/data/demo</strong> directory:</p><p><strong>ls -l</strong></p><p>The output shows that <strong>testfile</strong> has group ID 2000, which is the value of <strong>fsGroup</strong>.</p><p><strong>-rw-r--r-- 1 1000 2000 6 Jun 6 20:08 testfile</strong></p><p>Exit your shell:</p><p><strong>exit</strong></p><h4>Set the security context for a Container</h4><p>To specify security settings for a Container, include the <strong>securityContext</strong> field in the Container manifest. The <strong>securityContext</strong> field is a <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#securitycontext-v1-core">SecurityContext</a> object. Security settings that you specify for a Container apply only to the individual Container, and they override settings made at the Pod level when there is overlap. Container settings do not affect the Pod&#x27;s Volumes.</p><p>Here is the configuration file for a Pod that has one Container. Both the Pod and the Container have a <strong>securityContext</strong> field:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>security-                                                          |
| context-2.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/websi">https://raw.githubusercontent.com/kubernetes/websi</a> |
| te/master/docs/tasks/configure-pod-container/security-context-2.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: security-context-demo-2</strong>                                     |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>securityContext:</strong>                                                  |
|                                                                       |
| <strong>runAsUser: 1000</strong>                                                   |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: sec-ctx-demo-2</strong>                                            |
|                                                                       |
| <strong>image: gcr.io/google-samples/node-hello:1.0</strong>                       |
|                                                                       |
| <strong>securityContext:</strong>                                                  |
|                                                                       |
| <strong>runAsUser: 2000</strong>                                                   |
|                                                                       |
| <strong>allowPrivilegeEscalation: false</strong>                                   |
+-----------------------------------------------------------------------+</p><p>Create the Pod:</p><p><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/configure-pod-container/security-context-2.yaml">https://k8s.io/docs/tasks/configure-pod-container/security-context-2.yaml</a></strong></p><p>Verify that the Pod&#x27;s Container is running:</p><p><strong>kubectl get pod security-context-demo-2</strong></p><p>Get a shell into the running Container:</p><p><strong>kubectl exec -it security-context-demo-2 -- sh</strong></p><p>In your shell, list the running processes:</p><p><strong>ps aux</strong></p><p>The output shows that the processes are running as user 2000. This is the value of <strong>runAsUser</strong>specified for the Container. It overrides the value 1000 that is specified for the Pod.</p><p><strong>USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND</strong></p><p><strong>2000 1 0.0 0.0 4336 764 ? Ss 20:36 0:00 /bin/sh -c node server.js</strong></p><p><strong>2000 8 0.1 0.5 772124 22604 ? Sl 20:36 0:00 node server.js</strong></p><p><strong>.<!-- -->..</strong></p><p>Exit your shell:</p><p><strong>exit</strong></p><h4>Set capabilities for a Container</h4><p>With <a href="http://man7.org/linux/man-pages/man7/capabilities.7.html">Linux capabilities</a>, you can grant certain privileges to a process without granting all the privileges of the root user. To add or remove Linux capabilities for a Container, include the <strong>capabilities</strong> field in the <strong>securityContext</strong> section of the Container manifest.</p><p>First, see what happens when you don&#x27;t include a <strong>capabilities</strong> field. Here is configuration file that does not add or remove any Container capabilities:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>security-                                                          |
| context-3.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/websi">https://raw.githubusercontent.com/kubernetes/websi</a> |
| te/master/docs/tasks/configure-pod-container/security-context-3.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: security-context-demo-3</strong>                                     |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: sec-ctx-3</strong>                                                 |
|                                                                       |
| <strong>image: gcr.io/google-samples/node-hello:1.0</strong>                       |
+-----------------------------------------------------------------------+</p><p>Create the Pod:</p><p><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/configure-pod-container/security-context-3.yaml">https://k8s.io/docs/tasks/configure-pod-container/security-context-3.yaml</a></strong></p><p>Verify that the Pod&#x27;s Container is running:</p><p><strong>kubectl get pod security-context-demo-3</strong></p><p>Get a shell into the running Container:</p><p><strong>kubectl exec -it security-context-demo-3 -- sh</strong></p><p>In your shell, list the running processes:</p><p><strong>ps aux</strong></p><p>The output shows the process IDs (PIDs) for the Container:</p><p><strong>USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND</strong></p><p><strong>root 1 0.0 0.0 4336 796 ? Ss 18:17 0:00 /bin/sh -c node server.js</strong></p><p><strong>root 5 0.1 0.5 772124 22700 ? Sl 18:17 0:00 node server.js</strong></p><p>In your shell, view the status for process 1:</p><p><strong>cd /proc/1</strong></p><p><strong>cat status</strong></p><p>The output shows the capabilities bitmap for the process:</p><p><strong>.<!-- -->..</strong></p><p><strong>CapPrm: 00000000a80425fb</strong></p><p><strong>CapEff: 00000000a80425fb</strong></p><p><strong>.<!-- -->..</strong></p><p>Make a note of the capabilities bitmap, and then exit your shell:</p><p><strong>exit</strong></p><p>Next, run a Container that is the same as the preceding container, except that it has additional capabilities set.</p><p>Here is the configuration file for a Pod that runs one Container. The configuration adds the <strong>CAP_NET_ADMIN</strong> and <strong>CAP_SYS_TIME</strong> capabilities:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>security-                                                          |
| context-4.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/websi">https://raw.githubusercontent.com/kubernetes/websi</a> |
| te/master/docs/tasks/configure-pod-container/security-context-4.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: security-context-demo-4</strong>                                     |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: sec-ctx-4</strong>                                                 |
|                                                                       |
| <strong>image: gcr.io/google-samples/node-hello:1.0</strong>                       |
|                                                                       |
| <strong>securityContext:</strong>                                                  |
|                                                                       |
| <strong>capabilities:</strong>                                                     |
|                                                                       |
| <strong>add: <!-- -->[&quot;NET_ADMIN&quot;, &quot;SYS_TIME&quot;]</strong>                              |
+-----------------------------------------------------------------------+</p><p>Create the Pod:</p><p><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/configure-pod-container/security-context-4.yaml">https://k8s.io/docs/tasks/configure-pod-container/security-context-4.yaml</a></strong></p><p>Get a shell into the running Container:</p><p><strong>kubectl exec -it security-context-demo-4 -- sh</strong></p><p>In your shell, view the capabilities for process 1:</p><p><strong>cd /proc/1</strong></p><p><strong>cat status</strong></p><p>The output shows capabilities bitmap for the process:</p><p><strong>.<!-- -->..</strong></p><p><strong>CapPrm: 00000000aa0435fb</strong></p><p><strong>CapEff: 00000000aa0435fb</strong></p><p><strong>.<!-- -->..</strong></p><p>Compare the capabilities of the two Containers:</p><p><strong>00000000a80425fb</strong></p><p><strong>00000000aa0435fb</strong></p><p>In the capability bitmap of the first container, bits 12 and 25 are clear. In the second container, bits 12 and 25 are set. Bit 12 is <strong>CAP_NET_ADMIN</strong>, and bit 25 is <strong>CAP_SYS_TIME</strong>. See <a href="https://github.com/torvalds/linux/blob/master/include/uapi/linux/capability.h">capability.h</a> for definitions of the capability constants.</p><p><strong>Note:</strong> Linux capability constants have the form <strong>CAP_XXX</strong>. But when you list capabilities in your Container manifest, you must omit the <strong>CAP<!-- -->_</strong> portion of the constant. For example, to add <strong>CAP_SYS_TIME</strong>, include <strong>SYS_TIME</strong> in your list of capabilities.</p><h4>Assign SELinux labels to a Container</h4><p>To assign SELinux labels to a Container, include the <strong>seLinuxOptions</strong> field in the <strong>securityContext</strong>section of your Pod or Container manifest. The <strong>seLinuxOptions</strong> field is an <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#selinuxoptions-v1-core">SELinuxOptions</a> object. Here&#x27;s an example that applies an SELinux level:</p><p><strong>.<!-- -->..</strong></p><p><strong>securityContext:</strong></p><p><strong>seLinuxOptions:</strong></p><p><strong>level: &quot;s0:c123,c456&quot;</strong></p><p><strong>Note:</strong> To assign SELinux labels, the SELinux security module must be loaded on the host operating system.</p><h4>Discussion</h4><p>The security context for a Pod applies to the Pod&#x27;s Containers and also to the Pod&#x27;s Volumes when applicable. Specifically <strong>fsGroup</strong> and <strong>seLinuxOptions</strong> are applied to Volumes as follows:</p><ul><li><strong>fsGroup</strong>: Volumes that support ownership management are modified to be owned and writable by the GID specified in <strong>fsGroup</strong>. See the <a href="https://git.k8s.io/community/contributors/design-proposals/storage/volume-ownership-management.md">Ownership Management design document</a> for more details.</li><li><strong>seLinuxOptions</strong>: Volumes that support SELinux labeling are relabeled to be accessible by the label specified under <strong>seLinuxOptions</strong>. Usually you only need to set the <strong>level</strong> section. This sets the <a href="https://selinuxproject.org/page/NB_MLS">Multi-Category Security (MCS)</a> label given to all Containers in the Pod as well as the Volumes.</li></ul><p><strong>Warning:</strong> After you specify an MCS label for a Pod, all Pods with the same label can access the Volume. If you need inter-Pod protection, you must assign a unique MCS label to each Pod.</p><h4>What&#x27;s next</h4><ul><li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#podsecuritycontext-v1-core">PodSecurityContext</a></li><li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#securitycontext-v1-core">SecurityContext</a></li><li><a href="https://opensource.com/business/15/3/docker-security-tuning">Tuning Docker with the newest security enhancements</a></li><li><a href="https://git.k8s.io/community/contributors/design-proposals/auth/security_context.md">Security Contexts design document</a></li><li><a href="https://git.k8s.io/community/contributors/design-proposals/storage/volume-ownership-management.md">Ownership Management design document</a></li><li><a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/">Pod Security Policies</a></li><li><a href="https://git.k8s.io/community/contributors/design-proposals/auth/no-new-privs.md">AllowPrivilegeEscalation design document</a></li></ul><h3>Configure Service Accounts for Pods</h3><p>A service account provides an identity for processes that run in a Pod.</p><p>This is a user introduction to Service Accounts. See also the <a href="https://kubernetes.io/docs/admin/service-accounts-admin/"><em>Cluster Admin Guide to Service Accounts</em></a>.</p><p><strong>Note:</strong> This document describes how service accounts behave in a cluster set up as recommended by the Kubernetes project. Your cluster administrator may have customized the behavior in your cluster, in which case this documentation may not apply.</p><p>When you (a human) access the cluster (for example, using <strong>kubectl</strong>), you are authenticated by the apiserver as a particular User Account (currently this is usually <strong>admin</strong>, unless your cluster administrator has customized your cluster). Processes in containers inside pods can also contact the apiserver. When they do, they are authenticated as a particular Service Account (for example, <strong>default</strong>).</p><h4>Use the Default Service Account to access the API server.</h4><p>When you create a pod, if you do not specify a service account, it is automatically assigned the <strong>default</strong> service account in the same namespace. If you get the raw json or yaml for a pod you have created (for example, <strong>kubectl get pods/podname -o yaml</strong>), you can see the <strong>spec.serviceAccountName</strong> field has been <a href="https://kubernetes.io/docs/user-guide/working-with-resources/#resources-are-automatically-modified">automatically set</a>.</p><p>You can access the API from inside a pod using automatically mounted service account credentials, as described in <a href="https://kubernetes.io/docs/user-guide/accessing-the-cluster/#accessing-the-api-from-a-pod">Accessing the Cluster</a>. The API permissions a service account has depend on the <a href="https://kubernetes.io/docs/admin/authorization/#a-quick-note-on-service-accounts">authorization plugin and policy</a> in use.</p><p>In version 1.6+, you can opt out of automounting API credentials for a service account by setting <strong>automountServiceAccountToken: false</strong> on the service account:</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: ServiceAccount</strong></p><p><strong>metadata:</strong></p><p><strong>name: build-robot</strong></p><p><strong>automountServiceAccountToken: false</strong></p><p><strong>.<!-- -->..</strong></p><p>In version 1.6+, you can also opt out of automounting API credentials for a particular pod:</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: my-pod</strong></p><p><strong>spec:</strong></p><p><strong>serviceAccountName: build-robot</strong></p><p><strong>automountServiceAccountToken: false</strong></p><p><strong>.<!-- -->..</strong></p><p>The pod spec takes precedence over the service account if both specify a <strong>automountServiceAccountToken</strong> value.</p><h4>Use Multiple Service Accounts.</h4><p>Every namespace has a default service account resource called <strong>default</strong>. You can list this and any other serviceAccount resources in the namespace with this command:</p><p><strong>$ kubectl get serviceAccounts</strong></p><p><strong>NAME SECRETS AGE</strong></p><p><strong>default 1 1d</strong></p><p>You can create additional ServiceAccount objects like this:</p><p><strong>$ cat &gt; /tmp/serviceaccount.yaml &lt;&lt;EOF</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>kind: ServiceAccount</strong></p><p><strong>metadata:</strong></p><p><strong>name: build-robot</strong></p><p><strong>EOF</strong></p><p><strong>$ kubectl create -f /tmp/serviceaccount.yaml</strong></p><p><strong>serviceaccount &quot;build-robot&quot; created</strong></p><p>If you get a complete dump of the service account object, like this:</p><p><strong>$ kubectl get serviceaccounts/build-robot -o yaml</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>kind: ServiceAccount</strong></p><p><strong>metadata:</strong></p><p><strong>creationTimestamp: 2015-06-16T00:12:59Z</strong></p><p><strong>name: build-robot</strong></p><p><strong>namespace: default</strong></p><p><strong>resourceVersion: &quot;272500&quot;</strong></p><p><strong>selfLink: /api/v1/namespaces/default/serviceaccounts/build-robot</strong></p><p><strong>uid: 721ab723-13bc-11e5-aec2-42010af0021e</strong></p><p><strong>secrets:</strong></p><p><strong>- name: build-robot-token-bvbk5</strong></p><p>then you will see that a token has automatically been created and is referenced by the service account.</p><p>You may use authorization plugins to <a href="https://kubernetes.io/docs/admin/authorization/#a-quick-note-on-service-accounts">set permissions on service accounts</a>.</p><p>To use a non-default service account, simply set the <strong>spec.serviceAccountName</strong> field of a pod to the name of the service account you wish to use.</p><p>The service account has to exist at the time the pod is created, or it will be rejected.</p><p>You cannot update the service account of an already created pod.</p><p>You can clean up the service account from this example like this:</p><p><strong>$ kubectl delete serviceaccount/build-robot</strong></p><h4>Manually create a service account API token.</h4><p>Suppose we have an existing service account named &quot;build-robot&quot; as mentioned above, and we create a new secret manually.</p><p><strong>$ cat &gt; /tmp/build-robot-secret.yaml &lt;&lt;EOF</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Secret</strong></p><p><strong>metadata:</strong></p><p><strong>name: build-robot-secret</strong></p><p><strong>annotations:</strong></p><p><strong>kubernetes.io/service-account.name: build-robot</strong></p><p><strong>type: kubernetes.io/service-account-token</strong></p><p><strong>EOF</strong></p><p><strong>$ kubectl create -f /tmp/build-robot-secret.yaml</strong></p><p><strong>secret &quot;build-robot-secret&quot; created</strong></p><p>Now you can confirm that the newly built secret is populated with an API token for the &quot;build-robot&quot; service account.</p><p>Any tokens for non-existent service accounts will be cleaned up by the token controller.</p><p><strong>$ kubectl describe secrets/build-robot-secret</strong></p><p><strong>Name: build-robot-secret</strong></p><p><strong>Namespace: default</strong></p><p><strong>Labels: <code>&lt;none&gt;</code></strong></p><p><strong>Annotations: kubernetes.io/service-account.name=build-robot</strong></p><p><strong>kubernetes.io/service-account.uid=da68f9c6-9d26-11e7-b84e-002dc52800da</strong></p><p><strong>Type: kubernetes.io/service-account-token</strong></p><p><strong>Data</strong></p><p><strong>====</strong></p><p><strong>ca.crt: 1338 bytes</strong></p><p><strong>namespace: 7 bytes</strong></p><p><strong>token: <!-- -->.<!-- -->..</strong></p><p><strong>Note:</strong> The content of <strong>token</strong> is elided here.</p><h4>Add ImagePullSecrets to a service account</h4><p>First, create an imagePullSecret, as described <a href="https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod">here</a>. Next, verify it has been created. For example:</p><p><strong>$ kubectl get secrets myregistrykey</strong></p><p><strong>NAME TYPE DATA AGE</strong></p><p><strong>myregistrykey   kubernetes.io/.dockerconfigjson   1       1d</strong></p><p>Next, modify the default service account for the namespace to use this secret as an imagePullSecret.</p><p><strong>kubectl patch serviceaccount default -p \&#x27;{<!-- -->\<!-- -->&quot;imagePullSecrets<!-- -->\<!-- -->&quot;: <!-- -->[{<!-- -->\<!-- -->&quot;name<!-- -->\<!-- -->&quot;: <!-- -->\<!-- -->&quot;acrkey<!-- -->\<!-- -->&quot;}]<!-- -->}\&#x27;</strong></p><p>Interactive version requiring manual edit:</p><p><strong>$ kubectl get serviceaccounts default -o yaml &gt; ./sa.yaml</strong></p><p><strong>$ cat sa.yaml</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>kind: ServiceAccount</strong></p><p><strong>metadata:</strong></p><p><strong>creationTimestamp: 2015-08-07T22:02:39Z</strong></p><p><strong>name: default</strong></p><p><strong>namespace: default</strong></p><p><strong>resourceVersion: &quot;243024&quot;</strong></p><p><strong>selfLink: /api/v1/namespaces/default/serviceaccounts/default</strong></p><p><strong>uid: 052fb0f4-3d50-11e5-b066-42010af0d7b6</strong></p><p><strong>secrets:</strong></p><p><strong>- name: default-token-uudge</strong></p><p><strong>$ vi sa.yaml</strong></p><p><strong>[editor session not shown]</strong></p><p><strong>[delete line with key &quot;resourceVersion&quot;]</strong></p><p><strong>[add lines with &quot;imagePullSecret:&quot;]</strong></p><p><strong>$ cat sa.yaml</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>kind: ServiceAccount</strong></p><p><strong>metadata:</strong></p><p><strong>creationTimestamp: 2015-08-07T22:02:39Z</strong></p><p><strong>name: default</strong></p><p><strong>namespace: default</strong></p><p><strong>selfLink: /api/v1/namespaces/default/serviceaccounts/default</strong></p><p><strong>uid: 052fb0f4-3d50-11e5-b066-42010af0d7b6</strong></p><p><strong>secrets:</strong></p><p><strong>- name: default-token-uudge</strong></p><p><strong>imagePullSecrets:</strong></p><p><strong>- name: myregistrykey</strong></p><p><strong>$ kubectl replace serviceaccount default -f ./sa.yaml</strong></p><p><strong>serviceaccounts/default</strong></p><p>Now, any new pods created in the current namespace will have this added to their spec:</p><p><strong>spec:</strong></p><p><strong>imagePullSecrets:</strong></p><p><strong>- name: myregistrykey</strong></p><h3>Pull an Image from a Private Registry</h3><p>This page shows how to create a Pod that uses a Secret to pull an image from a private Docker registry or repository.</p><ul><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/#log-in-to-docker"><strong>Log in to Docker</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/#create-a-secret-in-the-cluster-that-holds-your-authorization-token"><strong>Create a Secret in the cluster that holds your authorization token</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/#inspecting-the-secret-regcred"><strong>Inspecting the Secret regcred</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/#create-a-pod-that-uses-your-secret"><strong>Create a Pod that uses your Secret</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Before you begin</h4><ul><li>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using <a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</li><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <strong>kubectl version</strong>.</p><ul><li>To do this exercise, you need a <a href="https://docs.docker.com/docker-id/">Docker ID</a> and password.</li></ul><h4>Log in to Docker</h4><p>On your laptop, you must authenticate with a registry in order to pull a private image:</p><p><strong>docker login</strong></p><p>When prompted, enter your Docker username and password.</p><p>The login process creates or updates a <strong>config.json</strong> file that holds an authorization token.</p><p>View the <strong>config.json</strong> file:</p><p><strong>cat <!-- -->~<!-- -->/.docker/config.json</strong></p><p>The output contains a section similar to this:</p><p><strong>{</strong></p><p><strong>&quot;auths&quot;: {</strong></p><p><strong>&quot;<a href="https://index.docker.io/v1/%22">https://index.docker.io/v1/&quot;</a>: {</strong></p><p><strong>&quot;auth&quot;: &quot;c3R<!-- -->.<!-- -->..zE2&quot;</strong></p><p><strong>}</strong></p><p><strong>}</strong></p><p><strong>}</strong></p><p><strong>Note:</strong> If you use a Docker credentials store, you won&#x27;t see that <strong>auth</strong> entry but a <strong>credsStore</strong>entry with the name of the store as value.</p><h4>Create a Secret in the cluster that holds your authorization token</h4><p>A Kubernetes cluster uses the Secret of <strong>docker-registry</strong> type to authenticate with a container registry to pull a private image.</p><p>Create this Secret, naming it <strong>regcred</strong>:</p><p><strong>kubectl create secret docker-registry regcred --docker-server=<code>&lt;your-registry-server&gt; --docker-username=&lt;your-name&gt; --docker-password=&lt;your-pword&gt; --docker-email=&lt;your-email&gt;</code></strong></p><p>where:</p><ul><li><strong><code>&lt;your-registry-server&gt;</code></strong> is your Private Docker Registry FQDN.</li><li><strong><code>&lt;your-name&gt;</code></strong> is your Docker username.</li><li><strong><code>&lt;your-pword&gt;</code></strong> is your Docker password.</li><li><strong><code>&lt;your-email&gt;</code></strong> is your Docker email.</li></ul><p>You have successfully set your Docker credentials in the cluster as a Secret called <strong>regcred</strong>.</p><h4>Inspecting the Secret regcred</h4><p>To understand the contents of the <strong>regcred</strong> Secret you just created, start by viewing the Secret in YAML format:</p><p><strong>kubectl get secret regcred --output=yaml</strong></p><p>The output is similar to this:</p><p><strong>apiVersion: v1</strong></p><p><strong>data:</strong></p><p><strong>.dockerconfigjson: eyJodHRwczovL2luZGV4L <!-- -->.<!-- -->.. J0QUl6RTIifX0=</strong></p><p><strong>kind: Secret</strong></p><p><strong>metadata:</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>name: regcred</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>type: kubernetes.io/dockerconfigjson</strong></p><p>The value of the <strong>.dockerconfigjson</strong> field is a base64 representation of your Docker credentials.</p><p>To understand what is in the <strong>.dockerconfigjson</strong> field, convert the secret data to a readable format:</p><p><strong>kubectl get secret regcred --output=&quot;jsonpath={.data.<!-- -->\<!-- -->.dockerconfigjson}&quot; | base64 -d</strong></p><p>The output is similar to this:</p><p><strong>{&quot;auths&quot;:{&quot;yourprivateregistry.com&quot;:{&quot;username&quot;:&quot;janedoe&quot;,&quot;password&quot;:&quot;xxxxxxxxxxx&quot;,&quot;email&quot;:&quot;<a href="mailto:jdoe@example.com">jdoe@example.com</a>&quot;,&quot;auth&quot;:&quot;c3R<!-- -->.<!-- -->..zE2&quot;}}}</strong></p><p>Notice that the Secret data contains the authorization token similar to your local <strong>~<!-- -->/.docker/config.json</strong> file.</p><p>You have successfully set your Docker credentials as a Secret called <strong>regcred</strong> in the cluster.</p><h4>Create a Pod that uses your Secret</h4><p>Here is a configuration file for a Pod that needs access to your Docker credentials in <strong>regcred</strong>:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>pri                                                                |
| vate-reg-pod.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/we">https://raw.githubusercontent.com/kubernetes/we</a> |
| bsite/master/docs/tasks/configure-pod-container/private-reg-pod.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: private-reg</strong>                                                 |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: private-reg-container</strong>                                     |
|                                                                       |
| <strong>image: <code>&lt;your-private-image&gt;</code></strong>                                     |
|                                                                       |
| <strong>imagePullSecrets:</strong>                                                 |
|                                                                       |
| <strong>- name: regcred</strong>                                                   |
+-----------------------------------------------------------------------+</p><p>Download the above file:</p><p><strong>wget -O my-private-reg-pod.yaml <a href="https://k8s.io/docs/tasks/configure-pod-container/private-reg-pod.yaml">https://k8s.io/docs/tasks/configure-pod-container/private-reg-pod.yaml</a></strong></p><p>In file <strong>my-private-reg-pod.yaml</strong>, replace <strong><code>&lt;your-private-image&gt;</code></strong> with the path to an image in a private registry such as:</p><p><strong>janedoe/jdoe-private:v1</strong></p><p>To pull the image from the private registry, Kubernetes needs credentials. The <strong>imagePullSecrets</strong>field in the configuration file specifies that Kubernetes should get the credentials from a Secret named <strong>regcred</strong>.</p><p>Create a Pod that uses your Secret, and verify that the Pod is running:</p><p><strong>kubectl create -f my-private-reg-pod.yaml</strong></p><p><strong>kubectl get pod private-reg</strong></p><h4>What&#x27;s next</h4><ul><li>Learn more about <a href="https://kubernetes.io/docs/concepts/configuration/secret/">Secrets</a>.</li><li>Learn more about <a href="https://kubernetes.io/docs/concepts/containers/images/#using-a-private-registry">using a private registry</a>.</li><li>See <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#-em-secret-docker-registry-em-">kubectl create secret docker-registry</a>.</li><li>See <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#secret-v1-core">Secret</a>.</li><li>See the <strong>imagePullSecrets</strong> field of <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#podspec-v1-core">PodSpec</a>.</li></ul><h3>Configure Liveness and Readiness Probes</h3><p>This page shows how to configure liveness and readiness probes for Containers.</p><p>The <a href="https://kubernetes.io/docs/admin/kubelet/">kubelet</a> uses liveness probes to know when to restart a Container. For example, liveness probes could catch a deadlock, where an application is running, but unable to make progress. Restarting a Container in such a state can help to make the application more available despite bugs.</p><p>The kubelet uses readiness probes to know when a Container is ready to start accepting traffic. A Pod is considered ready when all of its Containers are ready. One use of this signal is to control which Pods are used as backends for Services. When a Pod is not ready, it is removed from Service load balancers.</p><ul><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#define-a-liveness-command"><strong>Define a liveness command</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#define-a-liveness-http-request"><strong>Define a liveness HTTP request</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#define-a-tcp-liveness-probe"><strong>Define a TCP liveness probe</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#use-a-named-port"><strong>Use a named port</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#define-readiness-probes"><strong>Define readiness probes</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes"><strong>Configure Probes</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#whats-next"><strong>What&#x27;s next</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#reference"><strong>Reference</strong></a></li></ul></li></ul><h4>Before you begin</h4><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using<a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <strong>kubectl version</strong>.</p><h4>Define a liveness command</h4><p>Many applications running for long periods of time eventually transition to broken states, and cannot recover except by being restarted. Kubernetes provides liveness probes to detect and remedy such situations.</p><p>In this exercise, you create a Pod that runs a Container based on the <strong>k8s.gcr.io/busybox</strong> image. Here is the configuration file for the Pod:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<em>                                                                    |
| </em>exec-liveness.yaml** ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/">https://raw.githubusercontent.com/kubernetes/</a> |
| website/master/docs/tasks/configure-pod-container/exec-liveness.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>test: liveness</strong>                                                    |
|                                                                       |
| <strong>name: liveness-exec</strong>                                               |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: liveness</strong>                                                  |
|                                                                       |
| <strong>image: k8s.gcr.io/busybox</strong>                                         |
|                                                                       |
| <strong>args:</strong>                                                             |
|                                                                       |
| <strong>- /bin/sh</strong>                                                         |
|                                                                       |
| <strong>- -c</strong>                                                              |
|                                                                       |
| <strong>- touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600</strong>    |
|                                                                       |
| <strong>livenessProbe:</strong>                                                    |
|                                                                       |
| <strong>exec:</strong>                                                             |
|                                                                       |
| <strong>command:</strong>                                                          |
|                                                                       |
| <strong>- cat</strong>                                                             |
|                                                                       |
| <strong>- /tmp/healthy</strong>                                                    |
|                                                                       |
| <strong>initialDelaySeconds: 5</strong>                                            |
|                                                                       |
| <strong>periodSeconds: 5</strong>                                                  |
+-----------------------------------------------------------------------+</p><p>In the configuration file, you can see that the Pod has a single Container. The <strong>periodSeconds</strong> field specifies that the kubelet should perform a liveness probe every 5 seconds. The <strong>initialDelaySeconds</strong> field tells the kubelet that it should wait 5 second before performing the first probe. To perform a probe, the kubelet executes the command <strong>cat /tmp/healthy</strong> in the Container. If the command succeeds, it returns 0, and the kubelet considers the Container to be alive and healthy. If the command returns a non-zero value, the kubelet kills the Container and restarts it.</p><p>When the Container starts, it executes this command:</p><p><strong>/bin/sh -c &quot;touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600&quot;</strong></p><p>For the first 30 seconds of the Container&#x27;s life, there is a <strong>/tmp/healthy</strong> file. So during the first 30 seconds, the command <strong>cat /tmp/healthy</strong> returns a success code. After 30 seconds, <strong>cat /tmp/healthy</strong> returns a failure code.</p><p>Create the Pod:</p><p><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/configure-pod-container/exec-liveness.yaml">https://k8s.io/docs/tasks/configure-pod-container/exec-liveness.yaml</a></strong></p><p>Within 30 seconds, view the Pod events:</p><p><strong>kubectl describe pod liveness-exec</strong></p><p>The output indicates that no liveness probes have failed yet:</p><p><strong>FirstSeen LastSeen Count From SubobjectPath Type Reason Message</strong></p><p><strong>--------- -------- ----- ---- ------------- -------- ------ -------</strong></p><p><strong>24s 24s 1 {default-scheduler } Normal Scheduled Successfully assigned liveness-exec to worker0</strong></p><p><strong>23s 23s 1 {kubelet worker0} spec.containers{liveness} Normal Pulling pulling image &quot;k8s.gcr.io/busybox&quot;</strong></p><p><strong>23s 23s 1 {kubelet worker0} spec.containers{liveness} Normal Pulled Successfully pulled image &quot;k8s.gcr.io/busybox&quot;</strong></p><p><strong>23s 23s 1 {kubelet worker0} spec.containers{liveness} Normal Created Created container with docker id 86849c15382e; Security:<!-- -->[seccomp=unconfined]</strong></p><p><strong>23s 23s 1 {kubelet worker0} spec.containers{liveness} Normal Started Started container with docker id 86849c15382e</strong></p><p>After 35 seconds, view the Pod events again:</p><p><strong>kubectl describe pod liveness-exec</strong></p><p>At the bottom of the output, there are messages indicating that the liveness probes have failed, and the containers have been killed and recreated.</p><p><strong>FirstSeen LastSeen Count From SubobjectPath Type Reason Message</strong></p><p><strong>--------- -------- ----- ---- ------------- -------- ------ -------</strong></p><p><strong>37s 37s 1 {default-scheduler } Normal Scheduled Successfully assigned liveness-exec to worker0</strong></p><p><strong>36s 36s 1 {kubelet worker0} spec.containers{liveness} Normal Pulling pulling image &quot;k8s.gcr.io/busybox&quot;</strong></p><p><strong>36s 36s 1 {kubelet worker0} spec.containers{liveness} Normal Pulled Successfully pulled image &quot;k8s.gcr.io/busybox&quot;</strong></p><p><strong>36s 36s 1 {kubelet worker0} spec.containers{liveness} Normal Created Created container with docker id 86849c15382e; Security:<!-- -->[seccomp=unconfined]</strong></p><p><strong>36s 36s 1 {kubelet worker0} spec.containers{liveness} Normal Started Started container with docker id 86849c15382e</strong></p><p><strong>2s 2s 1 {kubelet worker0} spec.containers{liveness} Warning Unhealthy Liveness probe failed: cat: can\&#x27;t open \&#x27;/tmp/healthy\&#x27;: No such file or directory</strong></p><p>Wait another 30 seconds, and verify that the Container has been restarted:</p><p><strong>kubectl get pod liveness-exec</strong></p><p>The output shows that <strong>RESTARTS</strong> has been incremented:</p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>liveness-exec 1/1 Running 1 1m</strong></p><h4>Define a liveness HTTP request</h4><p>Another kind of liveness probe uses an HTTP GET request. Here is the configuration file for a Pod that runs a container based on the <strong>k8s.gcr.io/liveness</strong> image.</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<em>                                                                    |
| </em>http-liveness.yaml** ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/">https://raw.githubusercontent.com/kubernetes/</a> |
| website/master/docs/tasks/configure-pod-container/http-liveness.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>test: liveness</strong>                                                    |
|                                                                       |
| <strong>name: liveness-http</strong>                                               |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: liveness</strong>                                                  |
|                                                                       |
| <strong>image: k8s.gcr.io/liveness</strong>                                        |
|                                                                       |
| <strong>args:</strong>                                                             |
|                                                                       |
| <strong>- /server</strong>                                                         |
|                                                                       |
| <strong>livenessProbe:</strong>                                                    |
|                                                                       |
| <strong>httpGet:</strong>                                                          |
|                                                                       |
| <strong>path: /healthz</strong>                                                    |
|                                                                       |
| <strong>port: 8080</strong>                                                        |
|                                                                       |
| <strong>httpHeaders:</strong>                                                      |
|                                                                       |
| <strong>- name: X-Custom-Header</strong>                                           |
|                                                                       |
| <strong>value: Awesome</strong>                                                    |
|                                                                       |
| <strong>initialDelaySeconds: 3</strong>                                            |
|                                                                       |
| <strong>periodSeconds: 3</strong>                                                  |
+-----------------------------------------------------------------------+</p><p>In the configuration file, you can see that the Pod has a single Container. The <strong>periodSeconds</strong> field specifies that the kubelet should perform a liveness probe every 3 seconds. The <strong>initialDelaySeconds</strong> field tells the kubelet that it should wait 3 seconds before performing the first probe. To perform a probe, the kubelet sends an HTTP GET request to the server that is running in the Container and listening on port 8080. If the handler for the server&#x27;s <strong>/healthz</strong> path returns a success code, the kubelet considers the Container to be alive and healthy. If the handler returns a failure code, the kubelet kills the Container and restarts it.</p><p>Any code greater than or equal to 200 and less than 400 indicates success. Any other code indicates failure.</p><p>You can see the source code for the server in <a href="https://github.com/kubernetes/kubernetes/blob/master/test/images/liveness/server.go">server.go</a>.</p><p>For the first 10 seconds that the Container is alive, the <strong>/healthz</strong> handler returns a status of 200. After that, the handler returns a status of 500.</p><p><strong>http.HandleFunc(&quot;/healthz&quot;, func(w http.ResponseWriter, r <!-- -->*<!-- -->http.Request) {</strong></p><p><strong>duration := time.Now().Sub(started)</strong></p><p><strong>if duration.Seconds() &gt; 10 {</strong></p><p><strong>w.WriteHeader(500)</strong></p><p><strong>w.Write([]byte(fmt.Sprintf(&quot;error: %v&quot;, duration.Seconds())))</strong></p><p><strong>} else {</strong></p><p><strong>w.WriteHeader(200)</strong></p><p><strong>w.Write([]byte(&quot;ok&quot;))</strong></p><p><strong>}</strong></p><p><strong>})</strong></p><p>The kubelet starts performing health checks 3 seconds after the Container starts. So the first couple of health checks will succeed. But after 10 seconds, the health checks will fail, and the kubelet will kill and restart the Container.</p><p>To try the HTTP liveness check, create a Pod:</p><p><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/configure-pod-container/http-liveness.yaml">https://k8s.io/docs/tasks/configure-pod-container/http-liveness.yaml</a></strong></p><p>After 10 seconds, view Pod events to verify that liveness probes have failed and the Container has been restarted:</p><p><strong>kubectl describe pod liveness-http</strong></p><h4>Define a TCP liveness probe</h4><p>A third type of liveness probe uses a TCP Socket. With this configuration, the kubelet will attempt to open a socket to your container on the specified port. If it can establish a connection, the container is considered healthy, if it can&#x27;t it is considered a failure.</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>tcp-liveness-read                                                  |
| iness.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/website/m">https://raw.githubusercontent.com/kubernetes/website/m</a> |
| aster/docs/tasks/configure-pod-container/tcp-liveness-readiness.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: goproxy</strong>                                                     |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: goproxy</strong>                                                      |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: goproxy</strong>                                                   |
|                                                                       |
| <strong>image: k8s.gcr.io/goproxy:0.1</strong>                                     |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- containerPort: 8080</strong>                                             |
|                                                                       |
| <strong>readinessProbe:</strong>                                                   |
|                                                                       |
| <strong>tcpSocket:</strong>                                                        |
|                                                                       |
| <strong>port: 8080</strong>                                                        |
|                                                                       |
| <strong>initialDelaySeconds: 5</strong>                                            |
|                                                                       |
| <strong>periodSeconds: 10</strong>                                                 |
|                                                                       |
| <strong>livenessProbe:</strong>                                                    |
|                                                                       |
| <strong>tcpSocket:</strong>                                                        |
|                                                                       |
| <strong>port: 8080</strong>                                                        |
|                                                                       |
| <strong>initialDelaySeconds: 15</strong>                                           |
|                                                                       |
| <strong>periodSeconds: 20</strong>                                                 |
+-----------------------------------------------------------------------+</p><p>As you can see, configuration for a TCP check is quite similar to an HTTP check. This example uses both readiness and liveness probes. The kubelet will send the first readiness probe 5 seconds after the container starts. This will attempt to connect to the <strong>goproxy</strong> container on port 8080. If the probe succeeds, the pod will be marked as ready. The kubelet will continue to run this check every 10 seconds.</p><p>In addition to the readiness probe, this configuration includes a liveness probe. The kubelet will run the first liveness probe 15 seconds after the container starts. Just like the readiness probe, this will attempt to connect to the <strong>goproxy</strong> container on port 8080. If the liveness probe fails, the container will be restarted.</p><h4>Use a named port</h4><p>You can use a named <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#containerport-v1-core">ContainerPort</a> for HTTP or TCP liveness checks:</p><p><strong>ports:</strong></p><p><strong>- name: liveness-port</strong></p><p><strong>containerPort: 8080</strong></p><p><strong>hostPort: 8080</strong></p><p><strong>livenessProbe:</strong></p><p><strong>httpGet:</strong></p><p><strong>path: /healthz</strong></p><p><strong>port: liveness-port</strong></p><h4>Define readiness probes</h4><p>Sometimes, applications are temporarily unable to serve traffic. For example, an application might need to load large data or configuration files during startup. In such cases, you don&#x27;t want to kill the application, but you don&#x27;t want to send it requests either. Kubernetes provides readiness probes to detect and mitigate these situations. A pod with containers reporting that they are not ready does not receive traffic through Kubernetes Services.</p><p>Readiness probes are configured similarly to liveness probes. The only difference is that you use the <strong>readinessProbe</strong> field instead of the <strong>livenessProbe</strong> field.</p><p><strong>readinessProbe:</strong></p><p><strong>exec:</strong></p><p><strong>command:</strong></p><p><strong>- cat</strong></p><p><strong>- /tmp/healthy</strong></p><p><strong>initialDelaySeconds: 5</strong></p><p><strong>periodSeconds: 5</strong></p><p>Configuration for HTTP and TCP readiness probes also remains identical to liveness probes.</p><p>Readiness and liveness probes can be used in parallel for the same container. Using both can ensure that traffic does not reach a container that is not ready for it, and that containers are restarted when they fail.</p><h4>Configure Probes</h4><p><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#probe-v1-core">Probes</a> have a number of fields that you can use to more precisely control the behavior of liveness and readiness checks:</p><ul><li><strong>initialDelaySeconds</strong>: Number of seconds after the container has started before liveness or readiness probes are initiated.</li><li><strong>periodSeconds</strong>: How often (in seconds) to perform the probe. Default to 10 seconds. Minimum value is 1.</li><li><strong>timeoutSeconds</strong>: Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1.</li><li><strong>successThreshold</strong>: Minimum consecutive successes for the probe to be considered successful after having failed. Defaults to 1. Must be 1 for liveness. Minimum value is 1.</li><li><strong>failureThreshold</strong>: When a Pod starts and the probe fails, Kubernetes will try <strong>failureThreshold</strong> times before giving up. Giving up in case of liveness probe means restarting the Pod. In case of readiness probe the Pod will be marked Unready. Defaults to 3. Minimum value is 1.</li></ul><p><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#httpgetaction-v1-core">HTTP probes</a> have additional fields that can be set on <strong>httpGet</strong>:</p><ul><li><strong>host</strong>: Host name to connect to, defaults to the pod IP. You probably want to set &quot;Host&quot; in httpHeaders instead.</li><li><strong>scheme</strong>: Scheme to use for connecting to the host (HTTP or HTTPS). Defaults to HTTP.</li><li><strong>path</strong>: Path to access on the HTTP server.</li><li><strong>httpHeaders</strong>: Custom headers to set in the request. HTTP allows repeated headers.</li><li><strong>port</strong>: Name or number of the port to access on the container. Number must be in the range 1 to 65535.</li></ul><p>For an HTTP probe, the kubelet sends an HTTP request to the specified path and port to perform the check. The kubelet sends the probe to the pod&#x27;s IP address, unless the address is overridden by the optional <strong>host</strong> field in <strong>httpGet</strong>. If <strong>scheme</strong> field is set to <strong>HTTPS</strong>, the kubelet sends an HTTPS request skipping the certificate verification. In most scenarios, you do not want to set the <strong>host</strong> field. Here&#x27;s one scenario where you would set it. Suppose the Container listens on 127.0.0.1 and the Pod&#x27;s <strong>hostNetwork</strong> field is true. Then <strong>host</strong>, under <strong>httpGet</strong>, should be set to 127.0.0.1. If your pod relies on virtual hosts, which is probably the more common case, you should not use <strong>host</strong>, but rather set the <strong>Host</strong> header in <strong>httpHeaders</strong>.</p><h4>What&#x27;s next</h4><ul><li>Learn more about <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes">Container Probes</a>.</li></ul><h5><strong>Reference</strong></h5><ul><li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#pod-v1-core">Pod</a></li><li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#container-v1-core">Container</a></li><li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#probe-v1-core">Probe</a></li></ul><h3>Assign Pods to Nodes</h3><p>This page shows how to assign a Kubernetes Pod to a particular node in a Kubernetes cluster.</p><ul><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/#add-a-label-to-a-node"><strong>Add a label to a node</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/#create-a-pod-that-gets-scheduled-to-your-chosen-node"><strong>Create a pod that gets scheduled to your chosen node</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Before you begin</h4><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using<a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <strong>kubectl version</strong>.</p><h4>Add a label to a node</h4><ol><li>List the nodes in your cluster:</li><li><strong>kubectl get nodes</strong></li></ol><p>The output is similar to this:</p><p><strong>NAME STATUS AGE VERSION</strong></p><p><strong>worker0 Ready 1d v1.6.0+fff5156</strong></p><p><strong>worker1 Ready 1d v1.6.0+fff5156</strong></p><p><strong>worker2 Ready 1d v1.6.0+fff5156</strong></p><ol><li>Chose one of your nodes, and add a label to it:</li><li><strong>kubectl label nodes <code>&lt;your-node-name&gt;</code> disktype=ssd</strong></li></ol><p>where <strong><code>&lt;your-node-name&gt;</code></strong> is the name of your chosen node.</p><ol><li>Verify that your chosen node has a <strong>disktype=ssd</strong> label:</li><li><strong>kubectl get nodes --show-labels</strong></li></ol><p>The output is similar to this:</p><p><strong>NAME STATUS AGE VERSION LABELS</strong></p><p><strong>worker0 Ready 1d v1.6.0+fff5156 <!-- -->.<!-- -->..,disktype=ssd,kubernetes.io/hostname=worker0</strong></p><p><strong>worker1 Ready 1d v1.6.0+fff5156 <!-- -->.<!-- -->..,kubernetes.io/hostname=worker1</strong></p><p><strong>worker2 Ready 1d v1.6.0+fff5156 <!-- -->.<!-- -->..,kubernetes.io/hostname=worker2</strong></p><p>In the preceding output, you can see that the <strong>worker0</strong> node has a <strong>disktype=ssd</strong> label.</p><h4>Create a pod that gets scheduled to your chosen node</h4><p>This pod configuration file describes a pod that has a node selector, <strong>disktype: ssd</strong>. This means that the pod will get scheduled on a node that has a <strong>disktype=ssd</strong> label.</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>pod.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/k">https://raw.githubusercontent.com/k</a>                   |
| ubernetes/website/master/docs/tasks/configure-pod-container/pod.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: nginx</strong>                                                       |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>env: test</strong>                                                         |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: nginx</strong>                                                     |
|                                                                       |
| <strong>image: nginx</strong>                                                      |
|                                                                       |
| <strong>imagePullPolicy: IfNotPresent</strong>                                     |
|                                                                       |
| <strong>nodeSelector:</strong>                                                     |
|                                                                       |
| <strong>disktype: ssd</strong>                                                     |
+-----------------------------------------------------------------------+</p><ol><li>Use the configuration file to create a pod that will get scheduled on your chosen node:</li><li><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/configure-pod-container/pod.yaml">https://k8s.io/docs/tasks/configure-pod-container/pod.yaml</a></strong></li><li>Verify that the pod is running on your chosen node:</li><li><strong>kubectl get pods --output=wide</strong></li></ol><p>The output is similar to this:</p><p><strong>NAME READY STATUS RESTARTS AGE IP NODE</strong></p><p><strong>nginx 1/1 Running 0 13s 10.200.0.4 worker0</strong></p><h4>What&#x27;s next</h4><p>Learn more about <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/">labels and selectors</a>.</p><h3>Configure Pod Initialization</h3><p>This page shows how to use an Init Container to initialize a Pod before an application Container runs.</p><ul><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-initialization/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-initialization/#create-a-pod-that-has-an-init-container"><strong>Create a Pod that has an Init Container</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-initialization/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Before you begin</h4><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using<a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <strong>kubectl version</strong>.</p><h4>Create a Pod that has an Init Container</h4><p>In this exercise you create a Pod that has one application Container and one Init Container. The init container runs to completion before the application container starts.</p><p>Here is the configuration file for the Pod:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>ini                                                                |
| t-containers.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/we">https://raw.githubusercontent.com/kubernetes/we</a> |
| bsite/master/docs/tasks/configure-pod-container/init-containers.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: init-demo</strong>                                                   |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: nginx</strong>                                                     |
|                                                                       |
| <strong>image: nginx</strong>                                                      |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- containerPort: 80</strong>                                               |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- name: workdir</strong>                                                   |
|                                                                       |
| <strong>mountPath: /usr/share/nginx/html</strong>                                  |
|                                                                       |
| <strong><em># These containers are run during pod initialization</em></strong>           |
|                                                                       |
| <strong>initContainers:</strong>                                                   |
|                                                                       |
| <strong>- name: install</strong>                                                   |
|                                                                       |
| <strong>image: busybox</strong>                                                    |
|                                                                       |
| <strong>command:</strong>                                                          |
|                                                                       |
| <strong>- wget</strong>                                                            |
|                                                                       |
| <strong>- &quot;-O&quot;</strong>                                                          |
|                                                                       |
| <strong>- &quot;/work-dir/index.html&quot;</strong>                                        |
|                                                                       |
| <strong>- <a href="http://kubernetes.io">http://kubernetes.io</a></strong>                                            |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- name: workdir</strong>                                                   |
|                                                                       |
| <strong>mountPath: &quot;/work-dir&quot;</strong>                                          |
|                                                                       |
| <strong>dnsPolicy: Default</strong>                                                |
|                                                                       |
| <strong>volumes:</strong>                                                          |
|                                                                       |
| <strong>- name: workdir</strong>                                                   |
|                                                                       |
| <strong>emptyDir: {}</strong>                                                      |
+-----------------------------------------------------------------------+</p><p>In the configuration file, you can see that the Pod has a Volume that the init container and the application container share.</p><p>The init container mounts the shared Volume at <strong>/work-dir</strong>, and the application container mounts the shared Volume at <strong>/usr/share/nginx/html</strong>. The init container runs the following command and then terminates:</p><p><strong>wget -O /work-dir/index.html <a href="http://kubernetes.io">http://kubernetes.io</a></strong></p><p>Notice that the init container writes the <strong>index.html</strong> file in the root directory of the nginx server.</p><p>Create the Pod:</p><p><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/configure-pod-container/init-containers.yaml">https://k8s.io/docs/tasks/configure-pod-container/init-containers.yaml</a></strong></p><p>Verify that the nginx container is running:</p><p><strong>kubectl get pod init-demo</strong></p><p>The output shows that the nginx container is running:</p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>init-demo 1/1 Running 0 1m</strong></p><p>Get a shell into the nginx container running in the init-demo Pod:</p><p><strong>kubectl exec -it init-demo -- /bin/bash</strong></p><p>In your shell, send a GET request to the nginx server:</p><p><strong>root@nginx:<!-- -->~<!-- --># apt-get update</strong></p><p><strong>root@nginx:<!-- -->~<!-- --># apt-get install curl</strong></p><p><strong>root@nginx:<!-- -->~<!-- --># curl localhost</strong></p><p>The output shows that nginx is serving the web page that was written by the init container:</p><div class="MuiContainer-root MuiContainer-maxWidthLg"><pre class="Code__Pre-gy960v-0 UDybk prism-code language-html" style="color:#9CDCFE;background-color:#1E1E1E"><div class="MuiGrid-root MuiGrid-container MuiGrid-justify-xs-flex-end"><button class="Code__CopyCode-gy960v-1 llUIua">Copy</button></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">**`</span><span class="token doctype">&lt;!Doctype html&gt;</span><span class="token plain">`**</span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">**`</span><span class="token tag punctuation" style="color:rgb(212, 212, 212)">&lt;</span><span class="token tag" style="color:rgb(78, 201, 176)">html</span><span class="token tag" style="color:rgb(78, 201, 176)"> </span><span class="token tag attr-name" style="color:rgb(156, 220, 254)">id</span><span class="token tag attr-value punctuation" style="color:rgb(212, 212, 212)">=</span><span class="token tag attr-value punctuation" style="color:rgb(212, 212, 212)">&quot;</span><span class="token tag attr-value" style="color:rgb(206, 145, 120)">home</span><span class="token tag attr-value punctuation" style="color:rgb(212, 212, 212)">&quot;</span><span class="token tag punctuation" style="color:rgb(212, 212, 212)">&gt;</span><span class="token plain">`**</span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">**`</span><span class="token tag punctuation" style="color:rgb(212, 212, 212)">&lt;</span><span class="token tag" style="color:rgb(78, 201, 176)">head</span><span class="token tag punctuation" style="color:rgb(212, 212, 212)">&gt;</span><span class="token plain">`**</span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">**\...**</span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">&quot;url&quot;: &quot;http://kubernetes.io/&quot;</span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain"></span><span class="token tag punctuation" style="color:rgb(212, 212, 212)">&lt;/</span><span class="token tag" style="color:rgb(78, 201, 176)">script</span><span class="token tag punctuation" style="color:rgb(212, 212, 212)">&gt;</span><span class="token plain"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">**`</span><span class="token tag punctuation" style="color:rgb(212, 212, 212)">&lt;/</span><span class="token tag" style="color:rgb(78, 201, 176)">head</span><span class="token tag punctuation" style="color:rgb(212, 212, 212)">&gt;</span><span class="token plain"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">**`</span><span class="token tag punctuation" style="color:rgb(212, 212, 212)">&lt;</span><span class="token tag" style="color:rgb(78, 201, 176)">body</span><span class="token tag punctuation" style="color:rgb(212, 212, 212)">&gt;</span><span class="token plain"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">**\...**</span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">**`</span><span class="token tag punctuation" style="color:rgb(212, 212, 212)">&lt;</span><span class="token tag" style="color:rgb(78, 201, 176)">p</span><span class="token tag punctuation" style="color:rgb(212, 212, 212)">&gt;</span><span class="token plain">Kubernetes is open source giving you the freedom to take advantage \...</span><span class="token tag punctuation" style="color:rgb(212, 212, 212)">&lt;/</span><span class="token tag" style="color:rgb(78, 201, 176)">p</span><span class="token tag punctuation" style="color:rgb(212, 212, 212)">&gt;</span><span class="token plain">`**</span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">**\...**</span></div></pre></div><h4>What&#x27;s next</h4><ul><li>Learn more about <a href="https://kubernetes.io/docs/tasks/access-application-cluster/communicate-containers-same-pod-shared-volume/">communicating between Containers running in the same Pod</a>.</li><li>Learn more about <a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/">Init Containers</a>.</li><li>Learn more about <a href="https://kubernetes.io/docs/concepts/storage/volumes/">Volumes</a>.</li><li>Learn more about <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-init-containers/">Debugging Init Containers</a></li></ul><h3>Attach Handlers to Container Lifecycle Events</h3><p>This page shows how to attach handlers to Container lifecycle events. Kubernetes supports the postStart and preStop events. Kubernetes sends the postStart event immediately after a Container is started, and it sends the preStop event immediately before the Container is terminated.</p><ul><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/#define-poststart-and-prestop-handlers"><strong>Define postStart and preStop handlers</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/#discussion"><strong>Discussion</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/#whats-next"><strong>What&#x27;s next</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/#reference"><strong>Reference</strong></a></li></ul></li></ul><h4>Before you begin</h4><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using<a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <strong>kubectl version</strong>.</p><h4>Define postStart and preStop handlers</h4><p>In this exercise, you create a Pod that has one Container. The Container has handlers for the postStart and preStop events.</p><p>Here is the configuration file for the Pod:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>lifec                                                              |
| ycle-events.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/web">https://raw.githubusercontent.com/kubernetes/web</a> |
| site/master/docs/tasks/configure-pod-container/lifecycle-events.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: lifecycle-demo</strong>                                              |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: lifecycle-demo-container</strong>                                  |
|                                                                       |
| <strong>image: nginx</strong>                                                      |
|                                                                       |
| <strong>lifecycle:</strong>                                                        |
|                                                                       |
| <strong>postStart:</strong>                                                        |
|                                                                       |
| <strong>exec:</strong>                                                             |
|                                                                       |
| <strong>command: <!-- -->[&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo Hello from the postStart     |
| handler &gt; /usr/share/message&quot;]</strong>                                   |
|                                                                       |
| <strong>preStop:</strong>                                                          |
|                                                                       |
| <strong>exec:</strong>                                                             |
|                                                                       |
| <strong>command: <!-- -->[&quot;/usr/sbin/nginx&quot;,&quot;-s&quot;,&quot;quit&quot;]</strong>                  |
+-----------------------------------------------------------------------+</p><p>In the configuration file, you can see that the postStart command writes a <strong>message</strong> file to the Container&#x27;s <strong>/usr/share</strong> directory. The preStop command shuts down nginx gracefully. This is helpful if the Container is being terminated because of a failure.</p><p>Create the Pod:</p><p><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/configure-pod-container/lifecycle-events.yaml">https://k8s.io/docs/tasks/configure-pod-container/lifecycle-events.yaml</a></strong></p><p>Verify that the Container in the Pod is running:</p><p><strong>kubectl get pod lifecycle-demo</strong></p><p>Get a shell into the Container running in your Pod:</p><p><strong>kubectl exec -it lifecycle-demo -- /bin/bash</strong></p><p>In your shell, verify that the <strong>postStart</strong> handler created the <strong>message</strong> file:</p><p><strong>root@lifecycle-demo:/# cat /usr/share/message</strong></p><p>The output shows the text written by the postStart handler:</p><p><strong>Hello from the postStart handler</strong></p><h4>Discussion</h4><p>Kubernetes sends the postStart event immediately after the Container is created. There is no guarantee, however, that the postStart handler is called before the Container&#x27;s entrypoint is called. The postStart handler runs asynchronously relative to the Container&#x27;s code, but Kubernetes&#x27; management of the container blocks until the postStart handler completes. The Container&#x27;s status is not set to RUNNING until the postStart handler completes.</p><p>Kubernetes sends the preStop event immediately before the Container is terminated. Kubernetes&#x27; management of the Container blocks until the preStop handler completes, unless the Pod&#x27;s grace period expires. For more details, see <a href="https://kubernetes.io/docs/user-guide/pods/#termination-of-pods">Termination of Pods</a>.</p><p><strong>Note</strong>: Kubernetes only sends the preStop event when a Pod is terminated. This means that the preStop hook is not invoked when the Pod is completed. This limitation is tracked in <a href="https://github.com/kubernetes/kubernetes/issues/55807">issue #55087</a>.</p><h4>What&#x27;s next</h4><ul><li>Learn more about <a href="https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/">Container lifecycle hooks</a>.</li><li>Learn more about the <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/">lifecycle of a Pod</a>.</li></ul><h5><strong>Reference</strong></h5><ul><li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#lifecycle-v1-core">Lifecycle</a></li><li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#container-v1-core">Container</a></li><li>See <strong>terminationGracePeriodSeconds</strong> in <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#podspec-v1-core">PodSpec</a></li></ul><h3>Configure a Pod to Use a ConfigMap</h3><p>ConfigMaps allow you to decouple configuration artifacts from image content to keep containerized applications portable. This page provides a series of usage examples demonstrating how to create ConfigMaps and configure Pods using data stored in ConfigMaps.</p><ul><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#create-a-configmap"><strong>Create a ConfigMap</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#create-configmaps-from-directories"><strong>Create ConfigMaps from directories</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#create-configmaps-from-files"><strong>Create ConfigMaps from files</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#define-the-key-to-use-when-creating-a-configmap-from-a-file"><strong>Define the key to use when creating a ConfigMap from a file</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#create-configmaps-from-literal-values"><strong>Create ConfigMaps from literal values</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#define-pod-environment-variables-using-configmap-data"><strong>Define Pod environment variables using ConfigMap data</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#define-a-pod-environment-variable-with-data-from-a-single-configmap"><strong>Define a Pod environment variable with data from a single ConfigMap</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#define-pod-environment-variables-with-data-from-multiple-configmaps"><strong>Define Pod environment variables with data from multiple ConfigMaps</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#configure-all-key-value-pairs-in-a-configmap-as-pod-environment-variables"><strong>Configure all key-value pairs in a ConfigMap as Pod environment variables</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#use-configmap-defined-environment-variables-in-pod-commands"><strong>Use ConfigMap-defined environment variables in Pod commands</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#add-configmap-data-to-a-volume"><strong>Add ConfigMap data to a Volume</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#populate-a-volume-with-data-stored-in-a-configmap"><strong>Populate a Volume with data stored in a ConfigMap</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#add-configmap-data-to-a-specific-path-in-the-volume"><strong>Add ConfigMap data to a specific path in the Volume</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#project-keys-to-specific-paths-and-file-permissions"><strong>Project keys to specific paths and file permissions</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#mounted-configmaps-are-updated-automatically"><strong>Mounted ConfigMaps are updated automatically</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#understanding-configmaps-and-pods"><strong>Understanding ConfigMaps and Pods</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#restrictions"><strong>Restrictions</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Before you begin</h4><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using<a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <strong>kubectl version</strong>.</p><h4>Create a ConfigMap</h4><p>Use the <strong>kubectl create configmap</strong> command to create configmaps from <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#create-configmaps-from-directories">directories</a>, <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#create-configmaps-from-files">files</a>, or <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#create-configmaps-from-literal-values">literal values</a>:</p><p><strong>kubectl create configmap <code>&lt;map-name&gt; &lt;data-source&gt;</code></strong></p><p>where <code style="background-color:lightgray">&lt;map-name&gt; is the name you want to assign to the ConfigMap and &lt;data-source&gt;</code> is the directory, file, or literal value to draw the data from.</p><p>The data source corresponds to a key-value pair in the ConfigMap, where</p><ul><li>key = the file name or the key you provided on the command line, and</li><li>value = the file contents or the literal value you provided on the command line.</li></ul><p>You can use <a href="https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands/#describe"><strong>kubectl describe</strong></a> or <a href="https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands/#get"><strong>kubectl get</strong></a> to retrieve information about a ConfigMap.</p><h5><strong>Create ConfigMaps from directories</strong></h5><p>You can use <strong>kubectl create configmap</strong> to create a ConfigMap from multiple files in the same directory.</p><p>For example:</p><p><strong>kubectl create configmap game-config --from-file=<a href="https://k8s.io/docs/tasks/configure-pod-container/configmap/kubectl">https://k8s.io/docs/tasks/configure-pod-container/configmap/kubectl</a></strong></p><p>combines the contents of the <strong>docs/tasks/configure-pod-container/configmap/kubectl/</strong>directory</p><p><strong>ls docs/tasks/configure-pod-container/configmap/kubectl/</strong></p><p><strong>game.properties</strong></p><p><strong>ui.properties</strong></p><p>into the following ConfigMap:</p><p><strong>kubectl describe configmaps game-config</strong></p><p><strong>Name: game-config</strong></p><p><strong>Namespace: default</strong></p><p><strong>Labels: <code>&lt;none&gt;</code></strong></p><p><strong>Annotations: <code>&lt;none&gt;</code></strong></p><p><strong>Data</strong></p><p><strong>====</strong></p><p><strong>game.properties: 158 bytes</strong></p><p><strong>ui.properties: 83 bytes</strong></p><p>The <strong>game.properties</strong> and <strong>ui.properties</strong> files in the <strong>docs/tasks/configure-pod-container/configmap/kubectl/</strong> directory are represented in the <strong>data</strong> section of the ConfigMap.</p><p><strong>kubectl get configmaps game-config -o yaml</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>data:</strong></p><p><strong>game.properties: |</strong></p><p><strong>enemies=aliens</strong></p><p><strong>lives=3</strong></p><p><strong>enemies.cheat=true</strong></p><p><strong>enemies.cheat.level=noGoodRotten</strong></p><p><strong>secret.code.passphrase=UUDDLRLRBABAS</strong></p><p><strong>secret.code.allowed=true</strong></p><p><strong>secret.code.lives=30</strong></p><p><strong>ui.properties: |</strong></p><p><strong>color.good=purple</strong></p><p><strong>color.bad=yellow</strong></p><p><strong>allow.textmode=true</strong></p><p><strong>how.nice.to.look=fairlyNice</strong></p><p><strong>kind: ConfigMap</strong></p><p><strong>metadata:</strong></p><p><strong>creationTimestamp: 2016-02-18T18:52:05Z</strong></p><p><strong>name: game-config</strong></p><p><strong>namespace: default</strong></p><p><strong>resourceVersion: &quot;516&quot;</strong></p><p><strong>selfLink: /api/v1/namespaces/default/configmaps/game-config</strong></p><p><strong>uid: b4952dc3-d670-11e5-8cd0-68f728db1985</strong></p><h5><strong>Create ConfigMaps from files</strong></h5><p>You can use <strong>kubectl create configmap</strong> to create a ConfigMap from an individual file, or from multiple files.</p><p>For example,</p><p><strong>kubectl create configmap game-config-2 --from-file=<a href="https://k8s.io/docs/tasks/configure-pod-container/configmap/kubectl/game.properties">https://k8s.io/docs/tasks/configure-pod-container/configmap/kubectl/game.properties</a></strong></p><p>would produce the following ConfigMap:</p><p><strong>kubectl describe configmaps game-config-2</strong></p><p><strong>Name: game-config-2</strong></p><p><strong>Namespace: default</strong></p><p><strong>Labels: <code>&lt;none&gt;</code></strong></p><p><strong>Annotations: <code>&lt;none&gt;</code></strong></p><p><strong>Data</strong></p><p><strong>====</strong></p><p><strong>game.properties: 158 bytes</strong></p><p>You can pass in the <strong>--from-file</strong> argument multiple times to create a ConfigMap from multiple data sources.</p><p><strong>kubectl create configmap game-config-2 --from-file=<a href="https://k8s.io/docs/tasks/configure-pod-container/configmap/kubectl/game.properties">https://k8s.io/docs/tasks/configure-pod-container/configmap/kubectl/game.properties</a> --from-file=<a href="https://k8s.io/docs/tasks/configure-pod-container/configmap/kubectl/ui.properties">https://k8s.io/docs/tasks/configure-pod-container/configmap/kubectl/ui.properties</a></strong></p><p><strong>kubectl describe configmaps game-config-2</strong></p><p><strong>Name: game-config-2</strong></p><p><strong>Namespace: default</strong></p><p><strong>Labels: <code>&lt;none&gt;</code></strong></p><p><strong>Annotations: <code>&lt;none&gt;</code></strong></p><p><strong>Data</strong></p><p><strong>====</strong></p><p><strong>game.properties: 158 bytes</strong></p><p><strong>ui.properties: 83 bytes</strong></p><p>Use the option <strong>--from-env-file</strong> to create a ConfigMap from an env-file, for example:</p><p><strong><em># Env-files contain a list of environment variables.</em></strong></p><p><strong><em># These syntax rules apply:</em></strong></p><p><strong><em># Each line in an env file has to be in VAR=VAL format.</em></strong></p><p><strong><em># Lines beginning with # (i.e. comments) are ignored.</em></strong></p><p><strong><em># Blank lines are ignored.</em></strong></p><p><strong><em># There is no special handling of quotation marks (i.e. they will be part of the ConfigMap value)).</em></strong></p><p><strong>cat docs/tasks/configure-pod-container/game-env-file.properties</strong></p><p><strong>enemies=aliens</strong></p><p><strong>lives=3</strong></p><p><strong>allowed=&quot;true&quot;</strong></p><p><strong><em># This comment and the empty line above it are ignored</em></strong></p><p><strong>kubectl create configmap game-config-env-file <!-- -->\</strong></p><p><strong>--from-env-file=docs/tasks/configure-pod-container/game-env-file.properties</strong></p><p>would produce the following ConfigMap:</p><p><strong>kubectl get configmap game-config-env-file -o yaml</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>data:</strong></p><p><strong>allowed: \&#x27;&quot;true&quot;\&#x27;</strong></p><p><strong>enemies: aliens</strong></p><p><strong>lives: &quot;3&quot;</strong></p><p><strong>kind: ConfigMap</strong></p><p><strong>metadata:</strong></p><p><strong>creationTimestamp: 2017-12-27T18:36:28Z</strong></p><p><strong>name: game-config-env-file</strong></p><p><strong>namespace: default</strong></p><p><strong>resourceVersion: &quot;809965&quot;</strong></p><p><strong>selfLink: /api/v1/namespaces/default/configmaps/game-config-env-file</strong></p><p><strong>uid: d9d1ca5b-eb34-11e7-887b-42010a8002b8</strong></p><p>When passing <strong>--from-env-file</strong> multiple times to create a ConfigMap from multiple data sources, only the last env-file is used:</p><p><strong>kubectl create configmap config-multi-env-files <!-- -->\</strong></p><p><strong>--from-env-file=docs/tasks/configure-pod-container/game-env-file.properties <!-- -->\</strong></p><p><strong>--from-env-file=docs/tasks/configure-pod-container/ui-env-file.properties</strong></p><p>would produce the following ConfigMap:</p><p><strong>kubectl get configmap config-multi-env-files -o yaml</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>data:</strong></p><p><strong>color: purple</strong></p><p><strong>how: fairlyNice</strong></p><p><strong>textmode: &quot;true&quot;</strong></p><p><strong>kind: ConfigMap</strong></p><p><strong>metadata:</strong></p><p><strong>creationTimestamp: 2017-12-27T18:38:34Z</strong></p><p><strong>name: config-multi-env-files</strong></p><p><strong>namespace: default</strong></p><p><strong>resourceVersion: &quot;810136&quot;</strong></p><p><strong>selfLink: /api/v1/namespaces/default/configmaps/config-multi-env-files</strong></p><p><strong>uid: 252c4572-eb35-11e7-887b-42010a8002b8</strong></p><h6><strong>Define the key to use when creating a ConfigMap from a file</strong></h6><p>You can define a key other than the file name to use in the <strong>data</strong> section of your ConfigMap when using the <strong>--from-file</strong> argument:</p><p><strong>kubectl create configmap game-config-3 --from-file=<code>&lt;my-key-name&gt;=&lt;path-to-file&gt;</code></strong></p><p>where <code style="background-color:lightgray">&lt;my-key-name&gt;</code>is the key you want to use in the ConfigMap and <code style="background-color:lightgray">&lt;path-to-file&gt;</code> is the location of the data source file you want the key to represent.</p><p>For example:</p><p><strong>kubectl create configmap game-config-3 --from-file=game-special-key=<a href="https://k8s.io/docs/tasks/configure-pod-container/configmap/kubectl/game.properties">https://k8s.io/docs/tasks/configure-pod-container/configmap/kubectl/game.properties</a></strong></p><p><strong>kubectl get configmaps game-config-3 -o yaml</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>data:</strong></p><p><strong>game-special-key: |</strong></p><p><strong>enemies=aliens</strong></p><p><strong>lives=3</strong></p><p><strong>enemies.cheat=true</strong></p><p><strong>enemies.cheat.level=noGoodRotten</strong></p><p><strong>secret.code.passphrase=UUDDLRLRBABAS</strong></p><p><strong>secret.code.allowed=true</strong></p><p><strong>secret.code.lives=30</strong></p><p><strong>kind: ConfigMap</strong></p><p><strong>metadata:</strong></p><p><strong>creationTimestamp: 2016-02-18T18:54:22Z</strong></p><p><strong>name: game-config-3</strong></p><p><strong>namespace: default</strong></p><p><strong>resourceVersion: &quot;530&quot;</strong></p><p><strong>selfLink: /api/v1/namespaces/default/configmaps/game-config-3</strong></p><p><strong>uid: 05f8da22-d671-11e5-8cd0-68f728db1985</strong></p><h5><strong>Create ConfigMaps from literal values</strong></h5><p>You can use <strong>kubectl create configmap</strong> with the <strong>--from-literal</strong> argument to define a literal value from the command line:</p><p><strong>kubectl create configmap special-config --from-literal=special.how=very --from-literal=special.type=charm</strong></p><p>You can pass in multiple key-value pairs. Each pair provided on the command line is represented as a separate entry in the <strong>data</strong> section of the ConfigMap.</p><p><strong>kubectl get configmaps special-config -o yaml</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>data:</strong></p><p><strong>special.how: very</strong></p><p><strong>special.type: charm</strong></p><p><strong>kind: ConfigMap</strong></p><p><strong>metadata:</strong></p><p><strong>creationTimestamp: 2016-02-18T19:14:38Z</strong></p><p><strong>name: special-config</strong></p><p><strong>namespace: default</strong></p><p><strong>resourceVersion: &quot;651&quot;</strong></p><p><strong>selfLink: /api/v1/namespaces/default/configmaps/special-config</strong></p><p><strong>uid: dadce046-d673-11e5-8cd0-68f728db1985</strong></p><h4>Define Pod environment variables using ConfigMap data</h4><h5><strong>Define a Pod environment variable with data from a single ConfigMap</strong></h5><ol><li>Define an environment variable as a key-value pair in a ConfigMap:</li><li><strong>kubectl create configmap special-config --from-literal=special.how=very</strong></li><li>Assign the <strong>special.how</strong> value defined in the ConfigMap to the <strong>SPECIAL_LEVEL_KEY</strong>environment variable in the Pod specification.</li><li><strong>kubectl edit pod dapi-test-pod</strong></li><li><strong>apiVersion: v1</strong></li><li><strong>kind: Pod</strong></li><li><strong>metadata:</strong></li><li><strong>name: dapi-test-pod</strong></li><li><strong>spec:</strong></li><li><strong>containers:</strong></li><li><strong>- name: test-container</strong></li><li><strong>image: k8s.gcr.io/busybox</strong></li><li><strong>command: <!-- -->[ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;env&quot; ]</strong></li><li><strong>env:</strong></li><li><strong><em># Define the environment variable</em></strong></li><li><strong>- name: SPECIAL_LEVEL_KEY</strong></li><li><strong>valueFrom:</strong></li><li><strong>configMapKeyRef:</strong></li><li><strong><em># The ConfigMap containing the value you want to assign to SPECIAL_LEVEL_KEY</em></strong></li><li><strong>name: special-config</strong></li><li><strong><em># Specify the key associated with the value</em></strong></li><li><strong>key: special.how</strong></li><li><strong>restartPolicy: Never</strong></li><li>Save the changes to the Pod specification. Now, the Pod&#x27;s output includes <strong>SPECIAL_LEVEL_KEY=very</strong>.</li></ol><h5><strong>Define Pod environment variables with data from multiple ConfigMaps</strong></h5><ol><li>As with the previous example, create the ConfigMaps first.</li><li><strong>apiVersion: v1</strong></li><li><strong>kind: ConfigMap</strong></li><li><strong>metadata:</strong></li><li><strong>name: special-config</strong></li><li><strong>namespace: default</strong></li><li><strong>data:</strong></li><li><strong>special.how: very</strong></li><li><strong>apiVersion: v1</strong></li><li><strong>kind: ConfigMap</strong></li><li><strong>metadata:</strong></li><li><strong>name: env-config</strong></li><li><strong>namespace: default</strong></li><li><strong>data:</strong></li><li><strong>log_level: INFO</strong></li><li>Define the environment variables in the Pod specification.</li><li><strong>apiVersion: v1</strong></li><li><strong>kind: Pod</strong></li><li><strong>metadata:</strong></li><li><strong>name: dapi-test-pod</strong></li><li><strong>spec:</strong></li><li><strong>containers:</strong></li><li><strong>- name: test-container</strong></li><li><strong>image: k8s.gcr.io/busybox</strong></li><li><strong>command: <!-- -->[ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;env&quot; ]</strong></li><li><strong>env:</strong></li><li><strong>- name: SPECIAL_LEVEL_KEY</strong></li><li><strong>valueFrom:</strong></li><li><strong>configMapKeyRef:</strong></li><li><strong>name: special-config</strong></li><li><strong>key: special.how</strong></li><li><strong>- name: LOG_LEVEL</strong></li><li><strong>valueFrom:</strong></li><li><strong>configMapKeyRef:</strong></li><li><strong>name: env-config</strong></li><li><strong>key: log_level</strong></li><li><strong>restartPolicy: Never</strong></li><li>Save the changes to the Pod specification. Now, the Pod&#x27;s output includes <strong>SPECIAL_LEVEL_KEY=very</strong> and <strong>LOG_LEVEL=info</strong>.</li></ol><h4>Configure all key-value pairs in a ConfigMap as Pod environment variables</h4><p><strong>Note:</strong> This functionality is available to users running Kubernetes v1.6 and later.</p><ol><li>Create a ConfigMap containing multiple key-value pairs.</li><li><strong>apiVersion: v1</strong></li><li><strong>kind: ConfigMap</strong></li><li><strong>metadata:</strong></li><li><strong>name: special-config</strong></li><li><strong>namespace: default</strong></li><li><strong>data:</strong></li><li><strong>SPECIAL_LEVEL: very</strong></li><li><strong>SPECIAL_TYPE: charm</strong></li><li>Use <strong>envFrom</strong> to define all of the ConfigMap&#x27;s data as Pod environment variables. The key from the ConfigMap becomes the environment variable name in the Pod.</li><li><strong>apiVersion: v1</strong></li><li><strong>kind: Pod</strong></li><li><strong>metadata:</strong></li><li><strong>name: dapi-test-pod</strong></li><li><strong>spec:</strong></li><li><strong>containers:</strong></li><li><strong>- name: test-container</strong></li><li><strong>image: k8s.gcr.io/busybox</strong></li><li><strong>command: <!-- -->[ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;env&quot; ]</strong></li><li><strong>envFrom:</strong></li><li><strong>- configMapRef:</strong></li><li><strong>name: special-config</strong></li><li><strong>restartPolicy: Never</strong></li><li>Save the changes to the Pod specification. Now, the Pod&#x27;s output includes <strong>SPECIAL_LEVEL=very</strong> and <strong>SPECIAL_TYPE=charm</strong>.</li></ol><h4>Use ConfigMap-defined environment variables in Pod commands</h4><p>You can use ConfigMap-defined environment variables in the <strong>command</strong> section of the Pod specification using the <strong>$(VAR_NAME)</strong> Kubernetes substitution syntax.</p><p>For example:</p><p>The following Pod specification</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: dapi-test-pod</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: test-container</strong></p><p><strong>image: k8s.gcr.io/busybox</strong></p><p><strong>command: <!-- -->[ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo $(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)&quot; ]</strong></p><p><strong>env:</strong></p><p><strong>- name: SPECIAL_LEVEL_KEY</strong></p><p><strong>valueFrom:</strong></p><p><strong>configMapKeyRef:</strong></p><p><strong>name: special-config</strong></p><p><strong>key: SPECIAL_LEVEL</strong></p><p><strong>- name: SPECIAL_TYPE_KEY</strong></p><p><strong>valueFrom:</strong></p><p><strong>configMapKeyRef:</strong></p><p><strong>name: special-config</strong></p><p><strong>key: SPECIAL_TYPE</strong></p><p><strong>restartPolicy: Never</strong></p><p>produces the following output in the <strong>test-container</strong> container:</p><p><strong>very charm</strong></p><h4>Add ConfigMap data to a Volume</h4><p>As explained in <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#create-configmaps-from-files">Create ConfigMaps from files</a>, when you create a ConfigMap using <strong>--from-file</strong>, the filename becomes a key stored in the <strong>data</strong> section of the ConfigMap. The file contents become the key&#x27;s value.</p><p>The examples in this section refer to a ConfigMap named special-config, shown below.</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: ConfigMap</strong></p><p><strong>metadata:</strong></p><p><strong>name: special-config</strong></p><p><strong>namespace: default</strong></p><p><strong>data:</strong></p><p><strong>special.level: very</strong></p><p><strong>special.type: charm</strong></p><h5><strong>Populate a Volume with data stored in a ConfigMap</strong></h5><p>Add the ConfigMap name under the <strong>volumes</strong> section of the Pod specification. This adds the ConfigMap data to the directory specified as <strong>volumeMounts.mountPath</strong> (in this case, <strong>/etc/config</strong>). The <strong>command</strong> section references the <strong>special.level</strong> item stored in the ConfigMap.</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: dapi-test-pod</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: test-container</strong></p><p><strong>image: k8s.gcr.io/busybox</strong></p><p><strong>command: <!-- -->[ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;ls /etc/config/&quot; ]</strong></p><p><strong>volumeMounts:</strong></p><p><strong>- name: config-volume</strong></p><p><strong>mountPath: /etc/config</strong></p><p><strong>volumes:</strong></p><p><strong>- name: config-volume</strong></p><p><strong>configMap:</strong></p><p><strong><em># Provide the name of the ConfigMap containing the files you want</em></strong></p><p><strong><em># to add to the container</em></strong></p><p><strong>name: special-config</strong></p><p><strong>restartPolicy: Never</strong></p><p>When the pod runs, the command (<strong>&quot;ls /etc/config/&quot;</strong>) produces the output below:</p><p><strong>special.level</strong></p><p><strong>special.type</strong></p><p><strong>Caution:</strong> If there are some files in the <strong>/etc/config/</strong> directory, they will be deleted.</p><h5><strong>Add ConfigMap data to a specific path in the Volume</strong></h5><p>Use the <strong>path</strong> field to specify the desired file path for specific ConfigMap items. In this case, the <strong>special.level</strong> item will be mounted in the <strong>config-volume</strong> volume at <strong>/etc/config/keys</strong>.</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: dapi-test-pod</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: test-container</strong></p><p><strong>image: k8s.gcr.io/busybox</strong></p><p><strong>command: <!-- -->[ &quot;/bin/sh&quot;,&quot;-c&quot;,&quot;cat /etc/config/keys&quot; ]</strong></p><p><strong>volumeMounts:</strong></p><p><strong>- name: config-volume</strong></p><p><strong>mountPath: /etc/config</strong></p><p><strong>volumes:</strong></p><p><strong>- name: config-volume</strong></p><p><strong>configMap:</strong></p><p><strong>name: special-config</strong></p><p><strong>items:</strong></p><p><strong>- key: special.level</strong></p><p><strong>path: keys</strong></p><p><strong>restartPolicy: Never</strong></p><p>When the pod runs, the command (<strong>&quot;cat /etc/config/keys&quot;</strong>) produces the output below:</p><p><strong>very</strong></p><h5><strong>Project keys to specific paths and file permissions</strong></h5><p>You can project keys to specific paths and specific permissions on a per-file basis. The <a href="https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets-as-files-from-a-pod">Secrets</a> user guide explains the syntax.</p><h5><strong>Mounted ConfigMaps are updated automatically</strong></h5><p>When a ConfigMap already being consumed in a volume is updated, projected keys are eventually updated as well. Kubelet is checking whether the mounted ConfigMap is fresh on every periodic sync. However, it is using its local ttl-based cache for getting the current value of the ConfigMap. As a result, the total delay from the moment when the ConfigMap is updated to the moment when new keys are projected to the pod can be as long as kubelet sync period + ttl of ConfigMaps cache in kubelet.</p><p><strong>Note:</strong> A container using a ConfigMap as a <a href="https://kubernetes.io/docs/concepts/storage/volumes/#using-subpath">subPath</a> volume will not receive ConfigMap updates.</p><h4>Understanding ConfigMaps and Pods</h4><p>The ConfigMap API resource stores configuration data as key-value pairs. The data can be consumed in pods or provide the configurations for system components such as controllers. ConfigMap is similar to <a href="https://kubernetes.io/docs/concepts/configuration/secret/">Secrets</a>, but provides a means of working with strings that don&#x27;t contain sensitive information. Users and system components alike can store configuration data in ConfigMap.</p><p><strong>Note:</strong> ConfigMaps should reference properties files, not replace them. Think of the ConfigMap as representing something similar to the Linux <strong>/etc</strong> directory and its contents. For example, if you create a <a href="https://kubernetes.io/docs/concepts/storage/volumes/">Kubernetes Volume</a> from a ConfigMap, each data item in the ConfigMap is represented by an individual file in the volume.</p><p>The ConfigMap&#x27;s <strong>data</strong> field contains the configuration data. As shown in the example below, this can be simple -- like individual properties defined using <strong>--from-literal</strong> -- or complex -- like configuration files or JSON blobs defined using <strong>--from-file</strong>.</p><p><strong>kind: ConfigMap</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>metadata:</strong></p><p><strong>creationTimestamp: 2016-02-18T19:14:38Z</strong></p><p><strong>name: example-config</strong></p><p><strong>namespace: default</strong></p><p><strong>data:</strong></p><p><strong><em># example of a simple property defined using --from-literal</em></strong></p><p><strong>example.property.1: hello</strong></p><p><strong>example.property.2: world</strong></p><p><strong><em># example of a complex property defined using --from-file</em></strong></p><p><strong>example.property.file: |-</strong></p><p><strong>property.1=value-1</strong></p><p><strong>property.2=value-2</strong></p><p><strong>property.3=value-3</strong></p><h5><strong>Restrictions</strong></h5><ol><li>You must create a ConfigMap before referencing it in a Pod specification (unless you mark the ConfigMap as &quot;optional&quot;). If you reference a ConfigMap that doesn&#x27;t exist, the Pod won&#x27;t start. Likewise, references to keys that don&#x27;t exist in the ConfigMap will prevent the pod from starting.</li><li>If you use <strong>envFrom</strong> to define environment variables from ConfigMaps, keys that are considered invalid will be skipped. The pod will be allowed to start, but the invalid names will be recorded in the event log (<strong>InvalidVariableNames</strong>). The log message lists each skipped key. For example:</li><li><strong>kubectl get events</strong></li><li><strong>LASTSEEN FIRSTSEEN COUNT NAME KIND SUBOBJECT TYPE REASON SOURCE MESSAGE</strong></li><li><strong>0s 0s 1 dapi-test-pod Pod Warning InvalidEnvironmentVariableNames {kubelet, 127.0.0.1} Keys <!-- -->[1badkey, 2alsobad]<!-- --> from the EnvFrom configMap default/myconfig were skipped since they are considered invalid environment variable names.</strong></li><li>ConfigMaps reside in a specific <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/">namespace</a>. A ConfigMap can only be referenced by pods residing in the same namespace.</li><li>Kubelet doesn&#x27;t support the use of ConfigMaps for pods not found on the API server. This includes pods created via the Kubelet&#x27;s --manifest-url flag, --config flag, or the Kubelet REST API.</li></ol><p><strong>Note:</strong> These are not commonly-used ways to create pods.</p><h4>What&#x27;s next</h4><ul><li>Follow a real world example of <a href="https://kubernetes.io/docs/tutorials/configuration/configure-redis-using-configmap/">Configuring Redis using a ConfigMap</a>.</li></ul><h3>Share Process Namespace between Containers in a Pod</h3><p><strong>FEATURE STATE:</strong> <strong>Kubernetes v1.10</strong> <a href="https://kubernetes.io/docs/tasks/configure-pod-container/share-process-namespace/">alpha</a></p><p>This page shows how to configure process namespace sharing for a pod. When process namespace sharing is enabled, processes in a container are visible to all other containers in that pod.</p><p>You can use this feature to configure cooperating containers, such as a log handler sidecar container, or to troubleshoot container images that don&#x27;t include debugging utilities like a shell.</p><ul><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/share-process-namespace/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/share-process-namespace/#configure-a-pod"><strong>Configure a Pod</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/share-process-namespace/#understanding-process-namespace-sharing"><strong>Understanding Process Namespace Sharing</strong></a></li></ul><h4>Before you begin</h4><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using<a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>Your Kubernetes server must be version v1.10. To check the version, enter <strong>kubectl version</strong>.</p><p>A special <strong>alpha</strong> feature gate <strong>PodShareProcessNamespace</strong> must be set to true across the system: <strong>--feature-gates=PodShareProcessNamespace=true</strong>.</p><h4>Configure a Pod</h4><p>Process Namespace Sharing is enabled using the <strong>ShareProcessNamespace</strong> field of <strong>v1.PodSpec</strong>. For example:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>share-process-names                                                |
| pace.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/website/ma">https://raw.githubusercontent.com/kubernetes/website/ma</a> |
| ster/docs/tasks/configure-pod-container/share-process-namespace.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: nginx</strong>                                                       |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>shareProcessNamespace: true</strong>                                       |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: nginx</strong>                                                     |
|                                                                       |
| <strong>image: nginx</strong>                                                      |
|                                                                       |
| <strong>- name: shell</strong>                                                     |
|                                                                       |
| <strong>image: busybox</strong>                                                    |
|                                                                       |
| <strong>securityContext:</strong>                                                  |
|                                                                       |
| <strong>capabilities:</strong>                                                     |
|                                                                       |
| <strong>add:</strong>                                                              |
|                                                                       |
| <strong>- SYS_PTRACE</strong>                                                      |
|                                                                       |
| <strong>stdin: true</strong>                                                       |
|                                                                       |
| <strong>tty: true</strong>                                                         |
+-----------------------------------------------------------------------+</p><ol><li>Create the pod <strong>nginx</strong> on your cluster:</li><li><strong>$ kubectl create -f <a href="https://k8s.io/docs/tasks/configure-pod-container/share-process-namespace.yaml">https://k8s.io/docs/tasks/configure-pod-container/share-process-namespace.yaml</a></strong></li><li>Attach to the <strong>shell</strong> container and run <strong>ps</strong>:</li><li><strong>$ kubectl attach -it nginx -c shell</strong></li><li><strong>If you don\&#x27;t see a command prompt, try pressing enter.</strong></li><li><strong>/ # ps ax</strong></li><li><strong>PID USER TIME COMMAND</strong></li><li><strong>1 root 0:00 /pause</strong></li><li><strong>8 root 0:00 nginx: master process nginx -g daemon off;</strong></li><li><strong>14 101 0:00 nginx: worker process</strong></li><li><strong>15 root 0:00 sh</strong></li><li><strong>21 root 0:00 ps ax</strong></li></ol><p>You can signal processes in other containers. For example, send <strong>SIGHUP</strong> to nginx to restart the worker process. This requires the <strong>SYS_PTRACE</strong> capability.</p><p><strong>/ # kill -HUP 8</strong></p><p><strong>/ # ps ax</strong></p><p><strong>PID USER TIME COMMAND</strong></p><p><strong>1 root 0:00 /pause</strong></p><p><strong>8 root 0:00 nginx: master process nginx -g daemon off;</strong></p><p><strong>15 root 0:00 sh</strong></p><p><strong>22 101 0:00 nginx: worker process</strong></p><p><strong>23 root 0:00 ps ax</strong></p><p>It&#x27;s even possible to access another container image using the <strong>/proc/$pid/root</strong> link.</p><p><strong>/ # head /proc/8/root/etc/nginx/nginx.conf</strong></p><p><strong>user nginx;</strong></p><p><strong>worker_processes 1;</strong></p><p><strong>error_log /var/log/nginx/error.log warn;</strong></p><p><strong>pid /var/run/nginx.pid;</strong></p><p><strong>events {</strong></p><p><strong>worker_connections 1024;</strong></p><h4>Understanding Process Namespace Sharing</h4><p>Pods share many resources so it makes sense they would also share a process namespace. Some container images may expect to be isolated from other containers, though, so it&#x27;s important to understand these differences:</p><ol><li><strong>The container process no longer has PID 1.</strong> Some container images refuse to start without PID 1 (for example, containers using <strong>systemd</strong>) or run commands like <strong>kill -HUP 1</strong> to signal the container process. In pods with a shared process namespace, <strong>kill -HUP 1</strong> will signal the pod sandbox. (<strong>/pause</strong> in the above example.)</li><li><strong>Processes are visible to other containers in the pod.</strong> This includes all information visible in <strong>/proc</strong>, such as passwords that were passed as arguments or environment variables. These are protected only by regular Unix permissions.</li><li><strong>Container filesystems are visible to other containers in the pod through the /proc/$pid/root link.</strong> This makes debugging easier, but it also means that filesystem secrets are protected only by filesystem permissions.</li></ol><h3>Translate a Docker Compose File to Kubernetes Resources</h3><ul><li><a href="https://kubernetes.io/docs/tools/kompose/user-guide/#kubernetes--compose--kompose"><strong>Kubernetes + Compose = Kompose</strong></a></li><li><a href="https://kubernetes.io/docs/tools/kompose/user-guide/#installation"><strong>Installation</strong></a><ul><li><a href="https://kubernetes.io/docs/tools/kompose/user-guide/#github-release"><strong>GitHub release</strong></a></li><li><a href="https://kubernetes.io/docs/tools/kompose/user-guide/#go"><strong>Go</strong></a></li><li><a href="https://kubernetes.io/docs/tools/kompose/user-guide/#centos"><strong>CentOS</strong></a></li><li><a href="https://kubernetes.io/docs/tools/kompose/user-guide/#fedora"><strong>Fedora</strong></a></li><li><a href="https://kubernetes.io/docs/tools/kompose/user-guide/#macos"><strong>macOS</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tools/kompose/user-guide/#user-guide"><strong>User Guide</strong></a><ul><li><a href="https://kubernetes.io/docs/tools/kompose/user-guide/#kompose-convert"><strong>kompose convert</strong></a><ul><li><a href="https://kubernetes.io/docs/tools/kompose/user-guide/#kubernetes"><strong>Kubernetes</strong></a></li><li><a href="https://kubernetes.io/docs/tools/kompose/user-guide/#openshift"><strong>OpenShift</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tools/kompose/user-guide/#kompose-up"><strong>kompose up</strong></a><ul><li><a href="https://kubernetes.io/docs/tools/kompose/user-guide/#kubernetes-1"><strong>Kubernetes</strong></a></li><li><a href="https://kubernetes.io/docs/tools/kompose/user-guide/#openshift-1"><strong>OpenShift</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tools/kompose/user-guide/#kompose-down"><strong>kompose down</strong></a></li><li><a href="https://kubernetes.io/docs/tools/kompose/user-guide/#build-and-push-docker-images"><strong>Build and Push Docker Images</strong></a></li><li><a href="https://kubernetes.io/docs/tools/kompose/user-guide/#alternative-conversions"><strong>Alternative Conversions</strong></a></li><li><a href="https://kubernetes.io/docs/tools/kompose/user-guide/#labels"><strong>Labels</strong></a></li><li><a href="https://kubernetes.io/docs/tools/kompose/user-guide/#restart"><strong>Restart</strong></a><ul><li><a href="https://kubernetes.io/docs/tools/kompose/user-guide/#warning-about-deployment-configs"><strong>Warning about Deployment Config&#x27;s</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tools/kompose/user-guide/#docker-compose-versions"><strong>Docker Compose Versions</strong></a></li></ul></li></ul><h4>Kubernetes + Compose = Kompose</h4><p>What&#x27;s Kompose? It&#x27;s a conversion tool for all things compose (namely Docker Compose) to container orchestrators (Kubernetes or OpenShift).</p><p>More information can be found on the Kompose website at <a href="http://kompose.io/">http://kompose.io</a>.</p><p>In three simple steps, we&#x27;ll take you from Docker Compose to Kubernetes.</p><p><strong>1. Take a sample docker-compose.yaml file</strong></p><p><strong>version: &quot;2&quot;</strong></p><p><strong>services:</strong></p><p><strong>redis-master:</strong></p><p><strong>image: k8s.gcr.io/redis:e2e</strong></p><p><strong>ports:</strong></p><p><strong>- &quot;6379&quot;</strong></p><p><strong>redis-slave:</strong></p><p><strong>image: gcr.io/google_samples/gb-redisslave:v1</strong></p><p><strong>ports:</strong></p><p><strong>- &quot;6379&quot;</strong></p><p><strong>environment:</strong></p><p><strong>- GET_HOSTS_FROM=dns</strong></p><p><strong>frontend:</strong></p><p><strong>image: gcr.io/google-samples/gb-frontend:v4</strong></p><p><strong>ports:</strong></p><p><strong>- &quot;80:80&quot;</strong></p><p><strong>environment:</strong></p><p><strong>- GET_HOSTS_FROM=dns</strong></p><p><strong>labels:</strong></p><p><strong>kompose.service.type: LoadBalancer</strong></p><p><strong>2. Run kompose up in the same directory</strong></p><p><strong>$ kompose up</strong></p><p><strong>We are going to create Kubernetes Deployments, Services and PersistentVolumeClaims for your Dockerized application.</strong></p><p><strong>If you need different kind of resources, use the \&#x27;kompose convert\&#x27; and \&#x27;kubectl create -f\&#x27; commands instead.</strong></p><p><strong>INFO Successfully created Service: redis</strong></p><p><strong>INFO Successfully created Service: web</strong></p><p><strong>INFO Successfully created Deployment: redis</strong></p><p><strong>INFO Successfully created Deployment: web</strong></p><p><strong>Your application has been deployed to Kubernetes. You can run \&#x27;kubectl get deployment,svc,pods,pvc\&#x27; for details.</strong></p><p><strong>Alternatively, you can run kompose convert and deploy with kubectl</strong></p><p><strong>2.1. Run kompose convert in the same directory</strong></p><p><strong>$ kompose convert</strong></p><p><strong>INFO Kubernetes file &quot;frontend-service.yaml&quot; created</strong></p><p><strong>INFO Kubernetes file &quot;redis-master-service.yaml&quot; created</strong></p><p><strong>INFO Kubernetes file &quot;redis-slave-service.yaml&quot; created</strong></p><p><strong>INFO Kubernetes file &quot;frontend-deployment.yaml&quot; created</strong></p><p><strong>INFO Kubernetes file &quot;redis-master-deployment.yaml&quot; created</strong></p><p><strong>INFO Kubernetes file &quot;redis-slave-deployment.yaml&quot; created</strong></p><p><strong>2.2. And start it on Kubernetes!</strong></p><p><strong>$ kubectl create -f frontend-service.yaml,redis-master-service.yaml,redis-slave-service.yaml,frontend-deployment.yaml,redis-master-deployment.yaml,redis-slave-deployment.yaml</strong></p><p><strong>service &quot;frontend&quot; created</strong></p><p><strong>service &quot;redis-master&quot; created</strong></p><p><strong>service &quot;redis-slave&quot; created</strong></p><p><strong>deployment &quot;frontend&quot; created</strong></p><p><strong>deployment &quot;redis-master&quot; created</strong></p><p><strong>deployment &quot;redis-slave&quot; created</strong></p><p><strong>3. View the newly deployed service</strong></p><p>Now that your service has been deployed, let&#x27;s access it.</p><p>If you&#x27;re already using <strong>minikube</strong> for your development process:</p><p><strong>$ minikube service frontend</strong></p><p>Otherwise, let&#x27;s look up what IP your service is using!</p><p><strong>$ kubectl describe svc frontend</strong></p><p><strong>Name: frontend</strong></p><p><strong>Namespace: default</strong></p><p><strong>Labels: service=frontend</strong></p><p><strong>Selector: service=frontend</strong></p><p><strong>Type: LoadBalancer</strong></p><p><strong>IP: 10.0.0.183</strong></p><p><strong>LoadBalancer Ingress: 123.45.67.89</strong></p><p><strong>Port: 80 80/TCP</strong></p><p><strong>NodePort: 80 31144/TCP</strong></p><p><strong>Endpoints: 172.17.0.4:80</strong></p><p><strong>Session Affinity: None</strong></p><p><strong>No events.</strong></p><p>If you&#x27;re using a cloud provider, your IP will be listed next to <strong>LoadBalancer Ingress</strong>.</p><p><strong>$ curl <a href="http://123.45.67.89">http://123.45.67.89</a></strong></p><h5><strong>Installation</strong></h5><p>We have multiple ways to install Kompose. Our preferred method is downloading the binary from the latest GitHub release.</p><p><strong>GitHub release</strong></p><p>Kompose is released via GitHub on a three-week cycle, you can see all current releases on the <a href="https://github.com/kubernetes/kompose/releases">GitHub release page</a>.</p><p><strong><em># Linux</em></strong></p><p><strong>curl -L <a href="https://github.com/kubernetes/kompose/releases/download/v1.1.0/kompose-linux-amd64">https://github.com/kubernetes/kompose/releases/download/v1.1.0/kompose-linux-amd64</a> -o kompose</strong></p><p><strong><em># macOS</em></strong></p><p><strong>curl -L <a href="https://github.com/kubernetes/kompose/releases/download/v1.1.0/kompose-darwin-amd64">https://github.com/kubernetes/kompose/releases/download/v1.1.0/kompose-darwin-amd64</a> -o kompose</strong></p><p><strong><em># Windows</em></strong></p><p><strong>curl -L <a href="https://github.com/kubernetes/kompose/releases/download/v1.1.0/kompose-windows-amd64.exe">https://github.com/kubernetes/kompose/releases/download/v1.1.0/kompose-windows-amd64.exe</a> -o kompose.exe</strong></p><p><strong>chmod +x kompose</strong></p><p><strong>sudo mv ./kompose /usr/local/bin/kompose</strong></p><p>Alternatively, you can download the <a href="https://github.com/kubernetes/kompose/releases">tarball</a>.</p><p><strong>Go</strong></p><p>Installing using <strong>go get</strong> pulls from the master branch with the latest development changes.</p><p><strong>go get -u github.com/kubernetes/kompose</strong></p><p><strong>CentOS</strong></p><p>Kompose is in <a href="https://fedoraproject.org/wiki/EPEL">EPEL</a> CentOS repository. If you don&#x27;t have <a href="https://fedoraproject.org/wiki/EPEL">EPEL</a> repository already installed and enabled you can do it by running <strong>sudo yum install epel-release</strong></p><p>If you have <a href="https://fedoraproject.org/wiki/EPEL">EPEL</a> enabled in your system, you can install Kompose like any other package.</p><p><strong>sudo yum -y install kompose</strong></p><p><strong>Fedora</strong></p><p>Kompose is in Fedora 24, 25 and 26 repositories. You can install it just like any other package.</p><p><strong>sudo dnf -y install kompose</strong></p><p><strong>macOS</strong></p><p>On macOS you can install latest release via <a href="https://brew.sh/">Homebrew</a>:</p><p><strong>brew install kompose</strong></p><h5><strong>User Guide</strong></h5><ul><li>CLI<ul><li><a href="https://kubernetes.io/docs/tools/kompose/user-guide/#kompose-convert"><strong>kompose convert</strong></a></li><li><a href="https://kubernetes.io/docs/tools/kompose/user-guide/#kompose-up"><strong>kompose up</strong></a></li><li><a href="https://kubernetes.io/docs/tools/kompose/user-guide/#kompose-down"><strong>kompose down</strong></a></li></ul></li><li>Documentation<ul><li><a href="https://kubernetes.io/docs/tools/kompose/user-guide/#build-and-push-docker-images">Build and Push Docker Images</a></li><li><a href="https://kubernetes.io/docs/tools/kompose/user-guide/#alternative-conversions">Alternative Conversions</a></li><li><a href="https://kubernetes.io/docs/tools/kompose/user-guide/#labels">Labels</a></li><li><a href="https://kubernetes.io/docs/tools/kompose/user-guide/#restart">Restart</a></li><li><a href="https://kubernetes.io/docs/tools/kompose/user-guide/#docker-compose-versions">Docker Compose Versions</a></li></ul></li></ul><p>Kompose has support for two providers: OpenShift and Kubernetes. You can choose a targeted provider using global option <strong>--provider</strong>. If no provider is specified, Kubernetes is set by default.</p><h6>kompose convert</h6><p>Kompose supports conversion of V1, V2, and V3 Docker Compose files into Kubernetes and OpenShift objects.</p><p><strong>Kubernetes</strong></p><p><strong>$ kompose --file docker-voting.yml convert</strong></p><p><strong>WARN Unsupported key networks - ignoring</strong></p><p><strong>WARN Unsupported key build - ignoring</strong></p><p><strong>INFO Kubernetes file &quot;worker-svc.yaml&quot; created</strong></p><p><strong>INFO Kubernetes file &quot;db-svc.yaml&quot; created</strong></p><p><strong>INFO Kubernetes file &quot;redis-svc.yaml&quot; created</strong></p><p><strong>INFO Kubernetes file &quot;result-svc.yaml&quot; created</strong></p><p><strong>INFO Kubernetes file &quot;vote-svc.yaml&quot; created</strong></p><p><strong>INFO Kubernetes file &quot;redis-deployment.yaml&quot; created</strong></p><p><strong>INFO Kubernetes file &quot;result-deployment.yaml&quot; created</strong></p><p><strong>INFO Kubernetes file &quot;vote-deployment.yaml&quot; created</strong></p><p><strong>INFO Kubernetes file &quot;worker-deployment.yaml&quot; created</strong></p><p><strong>INFO Kubernetes file &quot;db-deployment.yaml&quot; created</strong></p><p><strong>$ ls</strong></p><p><strong>db-deployment.yaml docker-compose.yml docker-gitlab.yml redis-deployment.yaml result-deployment.yaml vote-deployment.yaml worker-deployment.yaml</strong></p><p><strong>db-svc.yaml docker-voting.yml redis-svc.yaml result-svc.yaml vote-svc.yaml worker-svc.yaml</strong></p><p>You can also provide multiple docker-compose files at the same time:</p><p><strong>$ kompose -f docker-compose.yml -f docker-guestbook.yml convert</strong></p><p><strong>INFO Kubernetes file &quot;frontend-service.yaml&quot; created</strong></p><p><strong>INFO Kubernetes file &quot;mlbparks-service.yaml&quot; created</strong></p><p><strong>INFO Kubernetes file &quot;mongodb-service.yaml&quot; created</strong></p><p><strong>INFO Kubernetes file &quot;redis-master-service.yaml&quot; created</strong></p><p><strong>INFO Kubernetes file &quot;redis-slave-service.yaml&quot; created</strong></p><p><strong>INFO Kubernetes file &quot;frontend-deployment.yaml&quot; created</strong></p><p><strong>INFO Kubernetes file &quot;mlbparks-deployment.yaml&quot; created</strong></p><p><strong>INFO Kubernetes file &quot;mongodb-deployment.yaml&quot; created</strong></p><p><strong>INFO Kubernetes file &quot;mongodb-claim0-persistentvolumeclaim.yaml&quot; created</strong></p><p><strong>INFO Kubernetes file &quot;redis-master-deployment.yaml&quot; created</strong></p><p><strong>INFO Kubernetes file &quot;redis-slave-deployment.yaml&quot; created</strong></p><p><strong>$ ls</strong></p><p><strong>mlbparks-deployment.yaml mongodb-service.yaml redis-slave-service.jsonmlbparks-service.yaml</strong></p><p><strong>frontend-deployment.yaml mongodb-claim0-persistentvolumeclaim.yaml redis-master-service.yaml</strong></p><p><strong>frontend-service.yaml mongodb-deployment.yaml redis-slave-deployment.yaml</strong></p><p><strong>redis-master-deployment.yaml</strong></p><p>When multiple docker-compose files are provided the configuration is merged. Any configuration that is common will be over ridden by subsequent file.</p><p><strong>OpenShift</strong></p><p><strong>$ kompose --provider openshift --file docker-voting.yml convert</strong></p><p><strong>WARN <!-- -->[worker]<!-- --> Service cannot be created because of missing port.</strong></p><p><strong>INFO OpenShift file &quot;vote-service.yaml&quot; created</strong></p><p><strong>INFO OpenShift file &quot;db-service.yaml&quot; created</strong></p><p><strong>INFO OpenShift file &quot;redis-service.yaml&quot; created</strong></p><p><strong>INFO OpenShift file &quot;result-service.yaml&quot; created</strong></p><p><strong>INFO OpenShift file &quot;vote-deploymentconfig.yaml&quot; created</strong></p><p><strong>INFO OpenShift file &quot;vote-imagestream.yaml&quot; created</strong></p><p><strong>INFO OpenShift file &quot;worker-deploymentconfig.yaml&quot; created</strong></p><p><strong>INFO OpenShift file &quot;worker-imagestream.yaml&quot; created</strong></p><p><strong>INFO OpenShift file &quot;db-deploymentconfig.yaml&quot; created</strong></p><p><strong>INFO OpenShift file &quot;db-imagestream.yaml&quot; created</strong></p><p><strong>INFO OpenShift file &quot;redis-deploymentconfig.yaml&quot; created</strong></p><p><strong>INFO OpenShift file &quot;redis-imagestream.yaml&quot; created</strong></p><p><strong>INFO OpenShift file &quot;result-deploymentconfig.yaml&quot; created</strong></p><p><strong>INFO OpenShift file &quot;result-imagestream.yaml&quot; created</strong></p><p>It also supports creating buildconfig for build directive in a service. By default, it uses the remote repo for the current git branch as the source repo, and the current branch as the source branch for the build. You can specify a different source repo and branch using <strong>--build-repo</strong> and <strong>--build-branch</strong> options respectively.</p><p><strong>$ kompose --provider openshift --file buildconfig/docker-compose.yml convert</strong></p><p><strong>WARN <!-- -->[foo]<!-- --> Service cannot be created because of missing port.</strong></p><p><strong>INFO OpenShift Buildconfig using <a href="mailto:git@github.com">git@github.com</a>:rtnpro/kompose.git::master as source.</strong></p><p><strong>INFO OpenShift file &quot;foo-deploymentconfig.yaml&quot; created</strong></p><p><strong>INFO OpenShift file &quot;foo-imagestream.yaml&quot; created</strong></p><p><strong>INFO OpenShift file &quot;foo-buildconfig.yaml&quot; created</strong></p><p><strong>Note</strong>: If you are manually pushing the Openshift artifacts using <strong>oc create -f</strong>, you need to ensure that you push the imagestream artifact before the buildconfig artifact, to workaround this Openshift issue: <a href="https://github.com/openshift/origin/issues/4518">https://github.com/openshift/origin/issues/4518</a> .</p><h6>kompose up</h6><p>Kompose supports a straightforward way to deploy your &quot;composed&quot; application to Kubernetes or OpenShift via <strong>kompose up</strong>.</p><p><strong>Kubernetes</strong></p><p><strong>$ kompose --file ./examples/docker-guestbook.yml up</strong></p><p><strong>We are going to create Kubernetes deployments and services for your Dockerized application.</strong></p><p><strong>If you need different kind of resources, use the \&#x27;kompose convert\&#x27; and \&#x27;kubectl create -f\&#x27; commands instead.</strong></p><p><strong>INFO Successfully created service: redis-master</strong></p><p><strong>INFO Successfully created service: redis-slave</strong></p><p><strong>INFO Successfully created service: frontend</strong></p><p><strong>INFO Successfully created deployment: redis-master</strong></p><p><strong>INFO Successfully created deployment: redis-slave</strong></p><p><strong>INFO Successfully created deployment: frontend</strong></p><p><strong>Your application has been deployed to Kubernetes. You can run \&#x27;kubectl get deployment,svc,pods\&#x27; for details.</strong></p><p><strong>$ kubectl get deployment,svc,pods</strong></p><p><strong>NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE</strong></p><p><strong>deploy/frontend 1 1 1 1 4m</strong></p><p><strong>deploy/redis-master 1 1 1 1 4m</strong></p><p><strong>deploy/redis-slave 1 1 1 1 4m</strong></p><p><strong>NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE</strong></p><p><strong>svc/frontend 10.0.174.12 <code>&lt;none&gt;</code> 80/TCP 4m</strong></p><p><strong>svc/kubernetes 10.0.0.1 <code>&lt;none&gt;</code> 443/TCP 13d</strong></p><p><strong>svc/redis-master 10.0.202.43 <code>&lt;none&gt;</code> 6379/TCP 4m</strong></p><p><strong>svc/redis-slave 10.0.1.85 <code>&lt;none&gt;</code> 6379/TCP 4m</strong></p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>po/frontend-2768218532-cs5t5 1/1 Running 0 4m</strong></p><p><strong>po/redis-master-1432129712-63jn8 1/1 Running 0 4m</strong></p><p><strong>po/redis-slave-2504961300-nve7b 1/1 Running 0 4m</strong></p><p>Note:</p><ul><li>You must have a running Kubernetes cluster with a pre-configured kubectl context.</li><li>Only deployments and services are generated and deployed to Kubernetes. If you need different kind of resources, use the &#x27;kompose convert&#x27; and &#x27;kubectl create -f&#x27; commands instead.</li></ul><p><strong>OpenShift</strong></p><p><strong>$ kompose --file ./examples/docker-guestbook.yml --provider openshift up</strong></p><p><strong>We are going to create OpenShift DeploymentConfigs and Services for your Dockerized application.</strong></p><p><strong>If you need different kind of resources, use the \&#x27;kompose convert\&#x27; and \&#x27;oc create -f\&#x27; commands instead.</strong></p><p><strong>INFO Successfully created service: redis-slave</strong></p><p><strong>INFO Successfully created service: frontend</strong></p><p><strong>INFO Successfully created service: redis-master</strong></p><p><strong>INFO Successfully created deployment: redis-slave</strong></p><p><strong>INFO Successfully created ImageStream: redis-slave</strong></p><p><strong>INFO Successfully created deployment: frontend</strong></p><p><strong>INFO Successfully created ImageStream: frontend</strong></p><p><strong>INFO Successfully created deployment: redis-master</strong></p><p><strong>INFO Successfully created ImageStream: redis-master</strong></p><p><strong>Your application has been deployed to OpenShift. You can run \&#x27;oc get dc,svc,is\&#x27; for details.</strong></p><p><strong>$ oc get dc,svc,is</strong></p><p><strong>NAME REVISION DESIRED CURRENT TRIGGERED BY</strong></p><p><strong>dc/frontend 0 1 0 config,image(frontend:v4)</strong></p><p><strong>dc/redis-master 0 1 0 config,image(redis-master:e2e)</strong></p><p><strong>dc/redis-slave 0 1 0 config,image(redis-slave:v1)</strong></p><p><strong>NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE</strong></p><p><strong>svc/frontend 172.30.46.64 <code>&lt;none&gt;</code> 80/TCP 8s</strong></p><p><strong>svc/redis-master 172.30.144.56 <code>&lt;none&gt;</code> 6379/TCP 8s</strong></p><p><strong>svc/redis-slave 172.30.75.245 <code>&lt;none&gt;</code> 6379/TCP 8s</strong></p><p><strong>NAME DOCKER REPO TAGS UPDATED</strong></p><p><strong>is/frontend 172.30.12.200:5000/fff/frontend</strong></p><p><strong>is/redis-master 172.30.12.200:5000/fff/redis-master</strong></p><p><strong>is/redis-slave 172.30.12.200:5000/fff/redis-slave v1</strong></p><p>Note:</p><ul><li>You must have a running OpenShift cluster with a pre-configured <strong>oc</strong> context (<strong>oc login</strong>)</li></ul><h6>kompose down</h6><p>Once you have deployed &quot;composed&quot; application to Kubernetes, <strong>$ kompose down</strong> will help you to take the application out by deleting its deployments and services. If you need to remove other resources, use the &#x27;kubectl&#x27; command.</p><p><strong>$ kompose --file docker-guestbook.yml down</strong></p><p><strong>INFO Successfully deleted service: redis-master</strong></p><p><strong>INFO Successfully deleted deployment: redis-master</strong></p><p><strong>INFO Successfully deleted service: redis-slave</strong></p><p><strong>INFO Successfully deleted deployment: redis-slave</strong></p><p><strong>INFO Successfully deleted service: frontend</strong></p><p><strong>INFO Successfully deleted deployment: frontend</strong></p><p>Note:</p><ul><li>You must have a running Kubernetes cluster with a pre-configured kubectl context.</li></ul><h6><strong>Build and Push Docker Images</strong></h6><p>Kompose supports both building and pushing Docker images. When using the <strong>build</strong> key within your Docker Compose file, your image will:</p><ul><li>Automatically be built with Docker using the <strong>image</strong> key specified within your file</li><li>Be pushed to the correct Docker repository using local credentials (located at <strong>.docker/config</strong>)</li></ul><p>Using an <a href="https://raw.githubusercontent.com/kubernetes/kompose/master/examples/buildconfig/docker-compose.yml">example Docker Compose file</a>:</p><p><strong>version: &quot;2&quot;</strong></p><p><strong>services:</strong></p><p><strong>foo:</strong></p><p><strong>build: &quot;./build&quot;</strong></p><p><strong>image: docker.io/foo/bar</strong></p><p>Using <strong>kompose up</strong> with a <strong>build</strong> key:</p><p><strong>$ kompose up</strong></p><p><strong>INFO Build key detected. Attempting to build and push image \&#x27;docker.io/foo/bar\&#x27;</strong></p><p><strong>INFO Building image \&#x27;docker.io/foo/bar\&#x27; from directory \&#x27;build\&#x27;</strong></p><p><strong>INFO Image \&#x27;docker.io/foo/bar\&#x27; from directory \&#x27;build\&#x27; built successfully</strong></p><p><strong>INFO Pushing image \&#x27;foo/bar:latest\&#x27; to registry \&#x27;docker.io\&#x27;</strong></p><p><strong>INFO Attempting authentication credentials \&#x27;<a href="https://index.docker.io/v1/">https://index.docker.io/v1/</a></strong></p><p><strong>INFO Successfully pushed image \&#x27;foo/bar:latest\&#x27; to registry \&#x27;docker.io\&#x27;</strong></p><p><strong>INFO We are going to create Kubernetes Deployments, Services and PersistentVolumeClaims for your Dockerized application. If you need different kind of resources, use the \&#x27;kompose convert\&#x27; and \&#x27;kubectl create -f\&#x27; commands instead.</strong></p><p><strong>INFO Deploying application in &quot;default&quot; namespace</strong></p><p><strong>INFO Successfully created Service: foo</strong></p><p><strong>INFO Successfully created Deployment: foo</strong></p><p><strong>Your application has been deployed to Kubernetes. You can run \&#x27;kubectl get deployment,svc,pods,pvc\&#x27; for details.</strong></p><p>In order to disable the functionality, or choose to use BuildConfig generation (with OpenShift) <strong>--build (local|build-config|none)</strong> can be passed.</p><p><strong><em># Disable building/pushing Docker images</em></strong></p><p><strong>$ kompose up --build none</strong></p><p><strong><em># Generate Build Config artifacts for OpenShift</em></strong></p><p><strong>$ kompose up --provider openshift --build build-config</strong></p><h6><strong>Alternative Conversions</strong></h6><p>The default <strong>kompose</strong> transformation will generate Kubernetes <a href="http://kubernetes.io/docs/user-guide/deployments/">Deployments</a> and <a href="http://kubernetes.io/docs/concepts/services-networking/service/">Services</a>, in yaml format. You have alternative option to generate json with <strong>-j</strong>. Also, you can alternatively generate <a href="http://kubernetes.io/docs/user-guide/replication-controller/">Replication Controllers</a> objects, <a href="http://kubernetes.io/docs/admin/daemons/">Daemon Sets</a>, or <a href="https://github.com/helm/helm">Helm</a> charts.</p><p><strong>$ kompose convert -j</strong></p><p><strong>INFO Kubernetes file &quot;redis-svc.json&quot; created</strong></p><p><strong>INFO Kubernetes file &quot;web-svc.json&quot; created</strong></p><p><strong>INFO Kubernetes file &quot;redis-deployment.json&quot; created</strong></p><p><strong>INFO Kubernetes file &quot;web-deployment.json&quot; created</strong></p><p>The <strong>*<!-- -->-deployment.json</strong> files contain the Deployment objects.</p><p><strong>$ kompose convert --replication-controller</strong></p><p><strong>INFO Kubernetes file &quot;redis-svc.yaml&quot; created</strong></p><p><strong>INFO Kubernetes file &quot;web-svc.yaml&quot; created</strong></p><p><strong>INFO Kubernetes file &quot;redis-replicationcontroller.yaml&quot; created</strong></p><p><strong>INFO Kubernetes file &quot;web-replicationcontroller.yaml&quot; created</strong></p><p>The <strong>*<!-- -->-replicationcontroller.yaml</strong> files contain the Replication Controller objects. If you want to specify replicas (default is 1), use <strong>--replicas</strong> flag: <strong>$ kompose convert --replication-controller --replicas 3</strong></p><p><strong>$ kompose convert --daemon-set</strong></p><p><strong>INFO Kubernetes file &quot;redis-svc.yaml&quot; created</strong></p><p><strong>INFO Kubernetes file &quot;web-svc.yaml&quot; created</strong></p><p><strong>INFO Kubernetes file &quot;redis-daemonset.yaml&quot; created</strong></p><p><strong>INFO Kubernetes file &quot;web-daemonset.yaml&quot; created</strong></p><p>The <strong>*<!-- -->-daemonset.yaml</strong> files contain the Daemon Set objects</p><p>If you want to generate a Chart to be used with <a href="https://github.com/kubernetes/helm">Helm</a> simply do:</p><p><strong>$ kompose convert -c</strong></p><p><strong>INFO Kubernetes file &quot;web-svc.yaml&quot; created</strong></p><p><strong>INFO Kubernetes file &quot;redis-svc.yaml&quot; created</strong></p><p><strong>INFO Kubernetes file &quot;web-deployment.yaml&quot; created</strong></p><p><strong>INFO Kubernetes file &quot;redis-deployment.yaml&quot; created</strong></p><p><strong>chart created in &quot;./docker-compose/&quot;</strong></p><p><strong>$ tree docker-compose/</strong></p><p><strong>docker-compose</strong></p><p><strong>├── Chart.yaml</strong></p><p><strong>├── README.md</strong></p><p><strong>└── templates</strong></p><p><strong>├── redis-deployment.yaml</strong></p><p><strong>├── redis-svc.yaml</strong></p><p><strong>├── web-deployment.yaml</strong></p><p><strong>└── web-svc.yaml</strong></p><p>The chart structure is aimed at providing a skeleton for building your Helm charts.</p><h6><strong>Labels</strong></h6><p><strong>kompose</strong> supports Kompose-specific labels within the <strong>docker-compose.yml</strong> file in order to explicitly define a service&#x27;s behavior upon conversion.</p><ul><li>kompose.service.type defines the type of service to be created.</li></ul><p>For example:</p><p><strong>version: &quot;2&quot;</strong></p><p><strong>services:</strong></p><p><strong>nginx:</strong></p><p><strong>image: nginx</strong></p><p><strong>dockerfile: foobar</strong></p><p><strong>build: ./foobar</strong></p><p><strong>cap_add:</strong></p><p><strong>- ALL</strong></p><p><strong>container_name: foobar</strong></p><p><strong>labels:</strong></p><p><strong>kompose.service.type: nodeport</strong></p><ul><li>kompose.service.expose defines if the service needs to be made accessible from outside the cluster or not. If the value is set to &quot;true&quot;, the provider sets the endpoint automatically, and for any other value, the value is set as the hostname. If multiple ports are defined in a service, the first one is chosen to be the exposed.<ul><li>For the Kubernetes provider, an ingress resource is created and it is assumed that an ingress controller has already been configured.</li><li>For the OpenShift provider, a route is created.</li></ul></li></ul><p>For example:</p><p><strong>version: &quot;2&quot;</strong></p><p><strong>services:</strong></p><p><strong>web:</strong></p><p><strong>image: tuna/docker-counter23</strong></p><p><strong>ports:</strong></p><p><strong>- &quot;5000:5000&quot;</strong></p><p><strong>links:</strong></p><p><strong>- redis</strong></p><p><strong>labels:</strong></p><p><strong>kompose.service.expose: &quot;counter.example.com&quot;</strong></p><p><strong>redis:</strong></p><p><strong>image: redis:3.0</strong></p><p><strong>ports:</strong></p><p><strong>- &quot;6379&quot;</strong></p><p>The currently supported options are:</p><p>  Key                      Value</p><hr/><p>  kompose.service.type     nodeport / clusterip / loadbalancer
kompose.service.expose   true / hostname</p><p><strong>Note</strong>: <strong>kompose.service.type</strong> label should be defined with <strong>ports</strong> only, otherwise <strong>kompose</strong> will fail.</p><h6><strong>Restart</strong></h6><p>If you want to create normal pods without controllers you can use <strong>restart</strong> construct of docker-compose to define that. Follow table below to see what happens on the <strong>restart</strong> value.</p><p>  <strong>docker-compose</strong> <strong>restart</strong>   object created      Pod <strong>restartPolicy</strong></p><hr/><p>  <strong>&quot;&quot;</strong>                         controller object   <strong>Always</strong>
<strong>always</strong>                       controller object   <strong>Always</strong>
<strong>on-failure</strong>                   Pod                 <strong>OnFailure</strong>
<strong>no</strong>                           Pod                 <strong>Never</strong></p><p><strong>Note</strong>: controller object could be <strong>deployment</strong> or <strong>replicationcontroller</strong>, etc.</p><p>For e.g. <strong>pival</strong> service will become pod down here. This container calculated value of <strong>pi</strong>.</p><p><strong>version: \&#x27;2\&#x27;</strong></p><p><strong>services:</strong></p><p><strong>pival:</strong></p><p><strong>image: perl</strong></p><p><strong>command: <!-- -->[&quot;perl&quot;, &quot;-Mbignum=bpi&quot;, &quot;-wle&quot;, &quot;print bpi(2000)&quot;]</strong></p><p><strong>restart: &quot;on-failure&quot;</strong></p><p><strong>Warning about Deployment Config&#x27;s</strong></p><p>If the Docker Compose file has a volume specified for a service, the Deployment (Kubernetes) or DeploymentConfig (OpenShift) strategy is changed to &quot;Recreate&quot; instead of &quot;RollingUpdate&quot; (default). This is done to avoid multiple instances of a service from accessing a volume at the same time.</p><p>If the Docker Compose file has service name with <strong>_</strong> in it (eg.<strong>web_service</strong>), then it will be replaced by <strong>-</strong> and the service name will be renamed accordingly (eg.<strong>web-service</strong>). Kompose does this because &quot;Kubernetes&quot; doesn&#x27;t allow <strong>_</strong> in object name.</p><p>Please note that changing service name might break some <strong>docker-compose</strong> files.</p><h6>Docker Compose Versions</h6><p>Kompose supports Docker Compose versions: 1, 2 and 3. We have limited support on versions 2.1 and 3.2 due to their experimental nature.</p><p>A full list on compatibility between all three versions is listed in our <a href="https://github.com/kubernetes/kompose/blob/master/docs/conversion.md">conversion document</a> including a list of all incompatible Docker Compose keys.</p><h2>Inject Data Into Applications</h2><h3>Define a Command and Arguments for a Container</h3><p>This page shows how to define commands and arguments when you run a container in a <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/">Pod</a>.</p><ul><li><a href="https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#define-a-command-and-arguments-when-you-create-a-pod"><strong>Define a command and arguments when you create a Pod</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#use-environment-variables-to-define-arguments"><strong>Use environment variables to define arguments</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#run-a-command-in-a-shell"><strong>Run a command in a shell</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#notes"><strong>Notes</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Before you begin</h4><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using<a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <strong>kubectl version</strong>.</p><h4>Define a command and arguments when you create a Pod</h4><p>When you create a Pod, you can define a command and arguments for the containers that run in the Pod. To define a command, include the <strong>command</strong> field in the configuration file. To define arguments for the command, include the <strong>args</strong> field in the configuration file. The command and arguments that you define cannot be changed after the Pod is created.</p><p>The command and arguments that you define in the configuration file override the default command and arguments provided by the container image. If you define args, but do not define a command, the default command is used with your new arguments.</p><p>In this exercise, you create a Pod that runs one container. The configuration file for the Pod defines a command and two arguments:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>commands.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubern">https://raw.githubusercontent.com/kubern</a>         |
| etes/website/master/docs/tasks/inject-data-application/commands.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: command-demo</strong>                                                |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>purpose: demonstrate-command</strong>                                      |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: command-demo-container</strong>                                    |
|                                                                       |
| <strong>image: debian</strong>                                                     |
|                                                                       |
| <strong>command: <!-- -->[&quot;printenv&quot;]</strong>                                         |
|                                                                       |
| <strong>args: <!-- -->[&quot;HOSTNAME&quot;, &quot;KUBERNETES_PORT&quot;]</strong>                       |
|                                                                       |
| <strong>restartPolicy: OnFailure</strong>                                          |
+-----------------------------------------------------------------------+</p><ol><li>Create a Pod based on the YAML configuration file:</li><li><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/inject-data-application/commands.yaml">https://k8s.io/docs/tasks/inject-data-application/commands.yaml</a></strong></li><li>List the running Pods:</li><li><strong>kubectl get pods</strong></li></ol><p>The output shows that the container that ran in the command-demo Pod has completed.</p><ol><li>To see the output of the command that ran in the container, view the logs from the Pod:</li><li><strong>kubectl logs command-demo</strong></li></ol><p>The output shows the values of the HOSTNAME and KUBERNETES_PORT environment variables:</p><p><strong>command-demo</strong></p><p><strong>tcp://10.3.240.1:443</strong></p><h4>Use environment variables to define arguments</h4><p>In the preceding example, you defined the arguments directly by providing strings. As an alternative to providing strings directly, you can define arguments by using environment variables:</p><p><strong>env:</strong></p><p><strong>- name: MESSAGE</strong></p><p><strong>value: &quot;hello world&quot;</strong></p><p><strong>command: <!-- -->[&quot;/bin/echo&quot;]</strong></p><p><strong>args: <!-- -->[&quot;$(MESSAGE)&quot;]</strong></p><p>This means you can define an argument for a Pod using any of the techniques available for defining environment variables, including <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/">ConfigMaps</a> and <a href="https://kubernetes.io/docs/concepts/configuration/secret/">Secrets</a>.</p><p><strong>Note:</strong> The environment variable appears in parentheses, <strong>&quot;$(VAR)&quot;</strong>. This is required for the variable to be expanded in the <strong>command</strong> or <strong>args</strong> field.</p><h4>Run a command in a shell</h4><p>In some cases, you need your command to run in a shell. For example, your command might consist of several commands piped together, or it might be a shell script. To run your command in a shell, wrap it like this:</p><p><strong>command: <!-- -->[&quot;/bin/sh&quot;]</strong></p><p><strong>args: <!-- -->[&quot;-c&quot;, &quot;while true; do echo hello; sleep 10;done&quot;]</strong></p><h4>Notes</h4><p>This table summarizes the field names used by Docker and Kubernetes.</p><p>  Description                           Docker field name   Kubernetes field name</p><hr/><p>  The command run by the container      Entrypoint          command
The arguments passed to the command   Cmd                 args</p><p>When you override the default Entrypoint and Cmd, these rules apply:</p><ul><li>If you do not supply <strong>command</strong> or <strong>args</strong> for a Container, the defaults defined in the Docker image are used.</li><li>If you supply a <strong>command</strong> but no <strong>args</strong> for a Container, only the supplied <strong>command</strong> is used. The default EntryPoint and the default Cmd defined in the Docker image are ignored.</li><li>If you supply only <strong>args</strong> for a Container, the default Entrypoint defined in the Docker image is run with the <strong>args</strong> that you supplied.</li><li>If you supply a <strong>command</strong> and <strong>args</strong>, the default Entrypoint and the default Cmd defined in the Docker image are ignored. Your <strong>command</strong> is run with your <strong>args</strong>.</li></ul><p>Here are some examples:</p><p>  Image Entrypoint   Image Cmd         Container command   Container args    Command run</p><hr/><p>  <strong>[/ep-1]</strong>      <strong>[foo bar]</strong>   <code style="background-color:lightgray">&lt;not set&gt;         &lt;not set&gt;</code>       <strong>[ep-1 foo bar]</strong>
<strong>[/ep-1]</strong>      <strong>[foo bar]</strong>   <strong>[/ep-2]</strong>       <code style="background-color:lightgray">&lt;not set&gt;</code>       <strong>[ep-2]</strong>
<strong>[/ep-1]</strong>      <strong>[foo bar]</strong>   <code style="background-color:lightgray">&lt;not set&gt;</code>         <strong>[zoo boo]</strong>   <strong>[ep-1 zoo boo]</strong>
<strong>[/ep-1]</strong>      <strong>[foo bar]</strong>   <strong>[/ep-2]</strong>       <strong>[zoo boo]</strong>   <strong>[ep-2 zoo boo]</strong></p><h4>What&#x27;s next</h4><ul><li>Learn more about <a href="https://kubernetes.io/docs/user-guide/containers/">containers and commands</a>.</li><li>Learn more about <a href="https://kubernetes.io/docs/tasks/">configuring pods and containers</a>.</li><li>Learn more about <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/get-shell-running-container/">running commands in a container</a>.</li><li>See <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#container-v1-core">Container</a>.</li></ul><h3>Define Environment Variables for a Container</h3><p>This page shows how to define environment variables when you run a container in a Kubernetes Pod.</p><ul><li><a href="https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/#define-an-environment-variable-for-a-container"><strong>Define an environment variable for a container</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Before you begin</h4><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using<a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <strong>kubectl version</strong>.</p><h4>Define an environment variable for a container</h4><p>When you create a Pod, you can set environment variables for the containers that run in the Pod. To set environment variables, include the <strong>env</strong> or <strong>envFrom</strong> field in the configuration file.</p><p>In this exercise, you create a Pod that runs one container. The configuration file for the Pod defines an environment variable with name <strong>DEMO_GREETING</strong> and value <strong>&quot;Hello from the environment&quot;</strong>. Here is the configuration file for the Pod:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>envars.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kube">https://raw.githubusercontent.com/kube</a>             |
| rnetes/website/master/docs/tasks/inject-data-application/envars.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: envar-demo</strong>                                                  |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>purpose: demonstrate-envars</strong>                                       |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: envar-demo-container</strong>                                      |
|                                                                       |
| <strong>image: gcr.io/google-samples/node-hello:1.0</strong>                       |
|                                                                       |
| <strong>env:</strong>                                                              |
|                                                                       |
| <strong>- name: DEMO_GREETING</strong>                                             |
|                                                                       |
| <strong>value: &quot;Hello from the environment&quot;</strong>                             |
|                                                                       |
| <strong>- name: DEMO_FAREWELL</strong>                                             |
|                                                                       |
| <strong>value: &quot;Such a sweet sorrow&quot;</strong>                                    |
+-----------------------------------------------------------------------+</p><ol><li>Create a Pod based on the YAML configuration file:</li><li><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/inject-data-application/envars.yaml">https://k8s.io/docs/tasks/inject-data-application/envars.yaml</a></strong></li><li>List the running Pods:</li><li><strong>kubectl get pods -l purpose=demonstrate-envars</strong></li></ol><p>The output is similar to this:</p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>envar-demo 1/1 Running 0 9s</strong></p><ol><li>Get a shell to the container running in your Pod:</li><li><strong>kubectl exec -it envar-demo -- /bin/bash</strong></li><li>In your shell, run the <strong>printenv</strong> command to list the environment variables.</li><li><strong>root@envar-demo:/# printenv</strong></li></ol><p>The output is similar to this:</p><p><strong>NODE_VERSION=4.4.2</strong></p><p><strong>EXAMPLE_SERVICE_PORT_8080_TCP_ADDR=10.3.245.237</strong></p><p><strong>HOSTNAME=envar-demo</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>DEMO_GREETING=Hello from the environment</strong></p><p><strong>DEMO_FAREWELL=Such a sweet sorrow</strong></p><ol><li>To exit the shell, enter <strong>exit</strong>.</li></ol><p><strong>Note:</strong> The environment variables set using the <strong>env</strong> or <strong>envFrom</strong> field will override any environment variables specified in the container image.</p><h4>What&#x27;s next</h4><ul><li>Learn more about <a href="https://kubernetes.io/docs/tasks/configure-pod-container/environment-variable-expose-pod-information/">environment variables</a>.</li><li>Learn about <a href="https://kubernetes.io/docs/user-guide/secrets/#using-secrets-as-environment-variables">using secrets as environment variables</a>.</li><li>See <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#envvarsource-v1-core">EnvVarSource</a>.</li></ul><h2>Expose Pod Information to Containers Through Environment Variables</h2><p>This page shows how a Pod can use environment variables to expose information about itself to Containers running in the Pod. Environment variables can expose Pod fields and Container fields.</p><ul><li><a href="https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/#the-downward-api"><strong>The Downward API</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/#use-pod-fields-as-values-for-environment-variables"><strong>Use Pod fields as values for environment variables</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/#use-container-fields-as-values-for-environment-variables"><strong>Use Container fields as values for environment variables</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h3>Before you begin</h3><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using<a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <strong>kubectl version</strong>.</p><h3>The Downward API</h3><p>There are two ways to expose Pod and Container fields to a running Container:</p><ul><li>Environment variables</li><li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#downwardapivolumefile-v1-core">DownwardAPIVolumeFiles</a></li></ul><p>Together, these two ways of exposing Pod and Container fields are called the Downward API.</p><h3>Use Pod fields as values for environment variables</h3><p>In this exercise, you create a Pod that has one Container. Here is the configuration file for the Pod:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>dap                                                                |
| i-envars-pod.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/we">https://raw.githubusercontent.com/kubernetes/we</a> |
| bsite/master/docs/tasks/inject-data-application/dapi-envars-pod.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: dapi-envars-fieldref</strong>                                        |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: test-container</strong>                                            |
|                                                                       |
| <strong>image: k8s.gcr.io/busybox</strong>                                         |
|                                                                       |
| <strong>command: <!-- -->[ &quot;sh&quot;, &quot;-c&quot;]</strong>                                      |
|                                                                       |
| <strong>args:</strong>                                                             |
|                                                                       |
| <strong>- while true; do</strong>                                                  |
|                                                                       |
| <strong>echo -en \&#x27;<!-- -->\<!-- -->n\&#x27;;</strong>                                                 |
|                                                                       |
| <strong>printenv MY_NODE_NAME MY_POD_NAME MY_POD_NAMESPACE;</strong>               |
|                                                                       |
| <strong>printenv MY_POD_IP MY_POD_SERVICE_ACCOUNT;</strong>                        |
|                                                                       |
| <strong>sleep 10;</strong>                                                         |
|                                                                       |
| <strong>done;</strong>                                                             |
|                                                                       |
| <strong>env:</strong>                                                              |
|                                                                       |
| <strong>- name: MY_NODE_NAME</strong>                                              |
|                                                                       |
| <strong>valueFrom:</strong>                                                        |
|                                                                       |
| <strong>fieldRef:</strong>                                                         |
|                                                                       |
| <strong>fieldPath: spec.nodeName</strong>                                          |
|                                                                       |
| <strong>- name: MY_POD_NAME</strong>                                               |
|                                                                       |
| <strong>valueFrom:</strong>                                                        |
|                                                                       |
| <strong>fieldRef:</strong>                                                         |
|                                                                       |
| <strong>fieldPath: metadata.name</strong>                                          |
|                                                                       |
| <strong>- name: MY_POD_NAMESPACE</strong>                                          |
|                                                                       |
| <strong>valueFrom:</strong>                                                        |
|                                                                       |
| <strong>fieldRef:</strong>                                                         |
|                                                                       |
| <strong>fieldPath: metadata.namespace</strong>                                     |
|                                                                       |
| <strong>- name: MY_POD_IP</strong>                                                 |
|                                                                       |
| <strong>valueFrom:</strong>                                                        |
|                                                                       |
| <strong>fieldRef:</strong>                                                         |
|                                                                       |
| <strong>fieldPath: status.podIP</strong>                                           |
|                                                                       |
| <strong>- name: MY_POD_SERVICE_ACCOUNT</strong>                                    |
|                                                                       |
| <strong>valueFrom:</strong>                                                        |
|                                                                       |
| <strong>fieldRef:</strong>                                                         |
|                                                                       |
| <strong>fieldPath: spec.serviceAccountName</strong>                                |
|                                                                       |
| <strong>restartPolicy: Never</strong>                                              |
+-----------------------------------------------------------------------+</p><p>In the configuration file, you can see five environment variables. The <strong>env</strong> field is an array of <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#envvar-v1-core">EnvVars</a>. The first element in the array specifies that the <strong>MY_NODE_NAME</strong> environment variable gets its value from the Pod&#x27;s <strong>spec.nodeName</strong> field. Similarly, the other environment variables get their names from Pod fields.</p><p><strong>Note:</strong> The fields in this example are Pod fields. They are not fields of the Container in the Pod.</p><p>Create the Pod:</p><p><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/inject-data-application/dapi-envars-pod.yaml">https://k8s.io/docs/tasks/inject-data-application/dapi-envars-pod.yaml</a></strong></p><p>Verify that the Container in the Pod is running:</p><p><strong>kubectl get pods</strong></p><p>View the Container&#x27;s logs:</p><p><strong>kubectl logs dapi-envars-fieldref</strong></p><p>The output shows the values of selected environment variables:</p><p><strong>minikube</strong></p><p><strong>dapi-envars-fieldref</strong></p><p><strong>default</strong></p><p><strong>172.17.0.4</strong></p><p><strong>default</strong></p><p>To see why these values are in the log, look at the <strong>command</strong> and <strong>args</strong> fields in the configuration file. When the Container starts, it writes the values of five environment variables to stdout. It repeats this every ten seconds.</p><p>Next, get a shell into the Container that is running in your Pod:</p><p><strong>kubectl exec -it dapi-envars-fieldref -- sh</strong></p><p>In your shell, view the environment variables:</p><p><strong>/# printenv</strong></p><p>The output shows that certain environment variables have been assigned the values of Pod fields:</p><p><strong>MY_POD_SERVICE_ACCOUNT=default</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>MY_POD_NAMESPACE=default</strong></p><p><strong>MY_POD_IP=172.17.0.4</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>MY_NODE_NAME=minikube</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>MY_POD_NAME=dapi-envars-fieldref</strong></p><h3>Use Container fields as values for environment variables</h3><p>In the preceding exercise, you used Pod fields as the values for environment variables. In this next exercise, you use Container fields as the values for environment variables. Here is the configuration file for a Pod that has one container:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>dapi-envars-con                                                    |
| tainer.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/website/">https://raw.githubusercontent.com/kubernetes/website/</a> |
| master/docs/tasks/inject-data-application/dapi-envars-container.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: dapi-envars-resourcefieldref</strong>                                |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: test-container</strong>                                            |
|                                                                       |
| <strong>image: k8s.gcr.io/busybox:1.24</strong>                                    |
|                                                                       |
| <strong>command: <!-- -->[ &quot;sh&quot;, &quot;-c&quot;]</strong>                                      |
|                                                                       |
| <strong>args:</strong>                                                             |
|                                                                       |
| <strong>- while true; do</strong>                                                  |
|                                                                       |
| <strong>echo -en \&#x27;<!-- -->\<!-- -->n\&#x27;;</strong>                                                 |
|                                                                       |
| <strong>printenv MY_CPU_REQUEST MY_CPU_LIMIT;</strong>                             |
|                                                                       |
| <strong>printenv MY_MEM_REQUEST MY_MEM_LIMIT;</strong>                             |
|                                                                       |
| <strong>sleep 10;</strong>                                                         |
|                                                                       |
| <strong>done;</strong>                                                             |
|                                                                       |
| <strong>resources:</strong>                                                        |
|                                                                       |
| <strong>requests:</strong>                                                         |
|                                                                       |
| <strong>memory: &quot;32Mi&quot;</strong>                                                  |
|                                                                       |
| <strong>cpu: &quot;125m&quot;</strong>                                                     |
|                                                                       |
| <strong>limits:</strong>                                                           |
|                                                                       |
| <strong>memory: &quot;64Mi&quot;</strong>                                                  |
|                                                                       |
| <strong>cpu: &quot;250m&quot;</strong>                                                     |
|                                                                       |
| <strong>env:</strong>                                                              |
|                                                                       |
| <strong>- name: MY_CPU_REQUEST</strong>                                            |
|                                                                       |
| <strong>valueFrom:</strong>                                                        |
|                                                                       |
| <strong>resourceFieldRef:</strong>                                                 |
|                                                                       |
| <strong>containerName: test-container</strong>                                     |
|                                                                       |
| <strong>resource: requests.cpu</strong>                                            |
|                                                                       |
| <strong>- name: MY_CPU_LIMIT</strong>                                              |
|                                                                       |
| <strong>valueFrom:</strong>                                                        |
|                                                                       |
| <strong>resourceFieldRef:</strong>                                                 |
|                                                                       |
| <strong>containerName: test-container</strong>                                     |
|                                                                       |
| <strong>resource: limits.cpu</strong>                                              |
|                                                                       |
| <strong>- name: MY_MEM_REQUEST</strong>                                            |
|                                                                       |
| <strong>valueFrom:</strong>                                                        |
|                                                                       |
| <strong>resourceFieldRef:</strong>                                                 |
|                                                                       |
| <strong>containerName: test-container</strong>                                     |
|                                                                       |
| <strong>resource: requests.memory</strong>                                         |
|                                                                       |
| <strong>- name: MY_MEM_LIMIT</strong>                                              |
|                                                                       |
| <strong>valueFrom:</strong>                                                        |
|                                                                       |
| <strong>resourceFieldRef:</strong>                                                 |
|                                                                       |
| <strong>containerName: test-container</strong>                                     |
|                                                                       |
| <strong>resource: limits.memory</strong>                                           |
|                                                                       |
| <strong>restartPolicy: Never</strong>                                              |
+-----------------------------------------------------------------------+</p><p>In the configuration file, you can see four environment variables. The <strong>env</strong> field is an array of<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#envvar-v1-core">EnvVars</a>. The first element in the array specifies that the <strong>MY_CPU_REQUEST</strong> environment variable gets its value from the <strong>requests.cpu</strong> field of a Container named <strong>test-container</strong>. Similarly, the other environment variables get their values from Container fields.</p><p>Create the Pod:</p><p><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/inject-data-application/dapi-envars-container.yaml">https://k8s.io/docs/tasks/inject-data-application/dapi-envars-container.yaml</a></strong></p><p>Verify that the Container in the Pod is running:</p><p><strong>kubectl get pods</strong></p><p>View the Container&#x27;s logs:</p><p><strong>kubectl logs dapi-envars-resourcefieldref</strong></p><p>The output shows the values of selected environment variables:</p><p><strong>1</strong></p><p><strong>1</strong></p><p><strong>33554432</strong></p><p><strong>67108864</strong></p><h3>What&#x27;s next</h3><ul><li><a href="https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/">Defining Environment Variables for a Container</a></li><li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#podspec-v1-core">PodSpec</a></li><li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#container-v1-core">Container</a></li><li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#envvar-v1-core">EnvVar</a></li><li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#envvarsource-v1-core">EnvVarSource</a></li><li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#objectfieldselector-v1-core">ObjectFieldSelector</a></li><li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#resourcefieldselector-v1-core">ResourceFieldSelector</a></li></ul><h3>Expose Pod Information to Containers Through Files</h3><p>This page shows how a Pod can use a DownwardAPIVolumeFile to expose information about itself to Containers running in the Pod. A DownwardAPIVolumeFile can expose Pod fields and Container fields.</p><ul><li><a href="https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/#the-downward-api"><strong>The Downward API</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/#store-pod-fields"><strong>Store Pod fields</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/#store-container-fields"><strong>Store Container fields</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/#capabilities-of-the-downward-api"><strong>Capabilities of the Downward API</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/#project-keys-to-specific-paths-and-file-permissions"><strong>Project keys to specific paths and file permissions</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/#motivation-for-the-downward-api"><strong>Motivation for the Downward API</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Before you begin</h4><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using<a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <strong>kubectl version</strong>.</p><h4>The Downward API</h4><p>There are two ways to expose Pod and Container fields to a running Container:</p><ul><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/environment-variable-expose-pod-information/">Environment variables</a></li><li>DownwardAPIVolumeFiles</li></ul><p>Together, these two ways of exposing Pod and Container fields are called the Downward API.</p><h4>Store Pod fields</h4><p>In this exercise, you create a Pod that has one Container. Here is the configuration file for the Pod:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>dapi-volume.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernete">https://raw.githubusercontent.com/kubernete</a>   |
| s/website/master/docs/tasks/inject-data-application/dapi-volume.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: kubernetes-downwardapi-volume-example</strong>                       |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>zone: us-est-coast</strong>                                                |
|                                                                       |
| <strong>cluster: test-cluster1</strong>                                            |
|                                                                       |
| <strong>rack: rack-22</strong>                                                     |
|                                                                       |
| <strong>annotations:</strong>                                                      |
|                                                                       |
| <strong>build: two</strong>                                                        |
|                                                                       |
| <strong>builder: john-doe</strong>                                                 |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: client-container</strong>                                          |
|                                                                       |
| <strong>image: k8s.gcr.io/busybox</strong>                                         |
|                                                                       |
| <strong>command: <!-- -->[&quot;sh&quot;, &quot;-c&quot;]</strong>                                       |
|                                                                       |
| <strong>args:</strong>                                                             |
|                                                                       |
| <strong>- while true; do</strong>                                                  |
|                                                                       |
| <strong>if [<!-- -->[ -e /etc/podinfo/labels ]<!-- -->]; then</strong>                         |
|                                                                       |
| <strong>echo -en \&#x27;<!-- -->\<!-- -->n<!-- -->\<!-- -->n\&#x27;; cat /etc/podinfo/labels; fi;</strong>                 |
|                                                                       |
| <strong>if [<!-- -->[ -e /etc/podinfo/annotations ]<!-- -->]; then</strong>                    |
|                                                                       |
| <strong>echo -en \&#x27;<!-- -->\<!-- -->n<!-- -->\<!-- -->n\&#x27;; cat /etc/podinfo/annotations; fi;</strong>            |
|                                                                       |
| <strong>sleep 5;</strong>                                                          |
|                                                                       |
| <strong>done;</strong>                                                             |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- name: podinfo</strong>                                                   |
|                                                                       |
| <strong>mountPath: /etc/podinfo</strong>                                           |
|                                                                       |
| <strong>readOnly: false</strong>                                                   |
|                                                                       |
| <strong>volumes:</strong>                                                          |
|                                                                       |
| <strong>- name: podinfo</strong>                                                   |
|                                                                       |
| <strong>downwardAPI:</strong>                                                      |
|                                                                       |
| <strong>items:</strong>                                                            |
|                                                                       |
| <strong>- path: &quot;labels&quot;</strong>                                                |
|                                                                       |
| <strong>fieldRef:</strong>                                                         |
|                                                                       |
| <strong>fieldPath: metadata.labels</strong>                                        |
|                                                                       |
| <strong>- path: &quot;annotations&quot;</strong>                                           |
|                                                                       |
| <strong>fieldRef:</strong>                                                         |
|                                                                       |
| <strong>fieldPath: metadata.annotations</strong>                                   |
+-----------------------------------------------------------------------+</p><p>In the configuration file, you can see that the Pod has a <strong>downwardAPI</strong> Volume, and the Container mounts the Volume at <strong>/etc/podinfo</strong>.</p><p>Look at the <strong>items</strong> array under <strong>downwardAPI</strong>. Each element of the array is a<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#downwardapivolumefile-v1-core">DownwardAPIVolumeFile</a>. The first element specifies that the value of the Pod&#x27;s <strong>metadata.labels</strong>field should be stored in a file named <strong>labels</strong>. The second element specifies that the value of the Pod&#x27;s <strong>annotations</strong> field should be stored in a file named <strong>annotations</strong>.</p><p><strong>Note:</strong> The fields in this example are Pod fields. They are not fields of the Container in the Pod.</p><p>Create the Pod:</p><p><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/inject-data-application/dapi-volume.yaml">https://k8s.io/docs/tasks/inject-data-application/dapi-volume.yaml</a></strong></p><p>Verify that Container in the Pod is running:</p><p><strong>kubectl get pods</strong></p><p>View the Container&#x27;s logs:</p><p><strong>kubectl logs kubernetes-downwardapi-volume-example</strong></p><p>The output shows the contents of the <strong>labels</strong> file and the <strong>annotations</strong> file:</p><p><strong>cluster=&quot;test-cluster1&quot;</strong></p><p><strong>rack=&quot;rack-22&quot;</strong></p><p><strong>zone=&quot;us-est-coast&quot;</strong></p><p><strong>build=&quot;two&quot;</strong></p><p><strong>builder=&quot;john-doe&quot;</strong></p><p>Get a shell into the Container that is running in your Pod:</p><p><strong>kubectl exec -it kubernetes-downwardapi-volume-example -- sh</strong></p><p>In your shell, view the <strong>labels</strong> file:</p><p><strong>/# cat /etc/podinfo/labels</strong></p><p>The output shows that all of the Pod&#x27;s labels have been written to the <strong>labels</strong> file:</p><p><strong>cluster=&quot;test-cluster1&quot;</strong></p><p><strong>rack=&quot;rack-22&quot;</strong></p><p><strong>zone=&quot;us-est-coast&quot;</strong></p><p>Similarly, view the <strong>annotations</strong> file:</p><p><strong>/# cat /etc/podinfo/annotations</strong></p><p>View the files in the <strong>/etc/podinfo</strong> directory:</p><p><strong>/# ls -laR /etc/podinfo</strong></p><p>In the output, you can see that the <strong>labels</strong> and <strong>annotations</strong> files are in a temporary subdirectory: in this example, <strong>..2982_06_02_21_47_53.299460680</strong>. In the <strong>/etc/podinfo</strong> directory, <strong>..data</strong> is a symbolic link to the temporary subdirectory. Also in the <strong>/etc/podinfo</strong> directory, <strong>labels</strong> and <strong>annotations</strong> are symbolic links.</p><p><strong>drwxr-xr-x <!-- -->.<!-- -->.. Feb 6 21:47 ..2982_06_02_21_47_53.299460680</strong></p><p><strong>lrwxrwxrwx <!-- -->.<!-- -->.. Feb 6 21:47 ..data -&gt; ..2982_06_02_21_47_53.299460680</strong></p><p><strong>lrwxrwxrwx <!-- -->.<!-- -->.. Feb 6 21:47 annotations -&gt; ..data/annotations</strong></p><p><strong>lrwxrwxrwx <!-- -->.<!-- -->.. Feb 6 21:47 labels -&gt; ..data/labels</strong></p><p><strong>/etc/..2982_06_02_21_47_53.299460680:</strong></p><p><strong>total 8</strong></p><p><strong>-rw-r--r-- <!-- -->.<!-- -->.. Feb 6 21:47 annotations</strong></p><p><strong>-rw-r--r-- <!-- -->.<!-- -->.. Feb 6 21:47 labels</strong></p><p>Using symbolic links enables dynamic atomic refresh of the metadata; updates are written to a new temporary directory, and the <strong>..data</strong> symlink is updated atomically using <a href="http://man7.org/linux/man-pages/man2/rename.2.html">rename(2)</a>.</p><p><strong>Note:</strong> A container using Downward API as a <a href="https://kubernetes.io/docs/concepts/storage/volumes/#using-subpath">subPath</a> volume mount will not receive Downward API updates.</p><p>Exit the shell:</p><p><strong>/# exit</strong></p><h4>Store Container fields</h4><p>The preceding exercise, you stored Pod fields in a DownwardAPIVolumeFile. In this next exercise, you store Container fields. Here is the configuration file for a Pod that has one Container:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>dapi-volume-res                                                    |
| ources.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/website/">https://raw.githubusercontent.com/kubernetes/website/</a> |
| master/docs/tasks/inject-data-application/dapi-volume-resources.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: kubernetes-downwardapi-volume-example-2</strong>                     |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: client-container</strong>                                          |
|                                                                       |
| <strong>image: k8s.gcr.io/busybox:1.24</strong>                                    |
|                                                                       |
| <strong>command: <!-- -->[&quot;sh&quot;, &quot;-c&quot;]</strong>                                       |
|                                                                       |
| <strong>args:</strong>                                                             |
|                                                                       |
| <strong>- while true; do</strong>                                                  |
|                                                                       |
| <strong>echo -en \&#x27;<!-- -->\<!-- -->n\&#x27;;</strong>                                                 |
|                                                                       |
| <strong>if [<!-- -->[ -e /etc/podinfo/cpu_limit ]<!-- -->]; then</strong>                      |
|                                                                       |
| <strong>echo -en \&#x27;<!-- -->\<!-- -->n\&#x27;; cat /etc/podinfo/cpu_limit; fi;</strong>                 |
|                                                                       |
| <strong>if [<!-- -->[ -e /etc/podinfo/cpu_request ]<!-- -->]; then</strong>                    |
|                                                                       |
| <strong>echo -en \&#x27;<!-- -->\<!-- -->n\&#x27;; cat /etc/podinfo/cpu_request; fi;</strong>               |
|                                                                       |
| <strong>if [<!-- -->[ -e /etc/podinfo/mem_limit ]<!-- -->]; then</strong>                      |
|                                                                       |
| <strong>echo -en \&#x27;<!-- -->\<!-- -->n\&#x27;; cat /etc/podinfo/mem_limit; fi;</strong>                 |
|                                                                       |
| <strong>if [<!-- -->[ -e /etc/podinfo/mem_request ]<!-- -->]; then</strong>                    |
|                                                                       |
| <strong>echo -en \&#x27;<!-- -->\<!-- -->n\&#x27;; cat /etc/podinfo/mem_request; fi;</strong>               |
|                                                                       |
| <strong>sleep 5;</strong>                                                          |
|                                                                       |
| <strong>done;</strong>                                                             |
|                                                                       |
| <strong>resources:</strong>                                                        |
|                                                                       |
| <strong>requests:</strong>                                                         |
|                                                                       |
| <strong>memory: &quot;32Mi&quot;</strong>                                                  |
|                                                                       |
| <strong>cpu: &quot;125m&quot;</strong>                                                     |
|                                                                       |
| <strong>limits:</strong>                                                           |
|                                                                       |
| <strong>memory: &quot;64Mi&quot;</strong>                                                  |
|                                                                       |
| <strong>cpu: &quot;250m&quot;</strong>                                                     |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- name: podinfo</strong>                                                   |
|                                                                       |
| <strong>mountPath: /etc/podinfo</strong>                                           |
|                                                                       |
| <strong>readOnly: false</strong>                                                   |
|                                                                       |
| <strong>volumes:</strong>                                                          |
|                                                                       |
| <strong>- name: podinfo</strong>                                                   |
|                                                                       |
| <strong>downwardAPI:</strong>                                                      |
|                                                                       |
| <strong>items:</strong>                                                            |
|                                                                       |
| <strong>- path: &quot;cpu_limit&quot;</strong>                                             |
|                                                                       |
| <strong>resourceFieldRef:</strong>                                                 |
|                                                                       |
| <strong>containerName: client-container</strong>                                   |
|                                                                       |
| <strong>resource: limits.cpu</strong>                                              |
|                                                                       |
| <strong>- path: &quot;cpu_request&quot;</strong>                                           |
|                                                                       |
| <strong>resourceFieldRef:</strong>                                                 |
|                                                                       |
| <strong>containerName: client-container</strong>                                   |
|                                                                       |
| <strong>resource: requests.cpu</strong>                                            |
|                                                                       |
| <strong>- path: &quot;mem_limit&quot;</strong>                                             |
|                                                                       |
| <strong>resourceFieldRef:</strong>                                                 |
|                                                                       |
| <strong>containerName: client-container</strong>                                   |
|                                                                       |
| <strong>resource: limits.memory</strong>                                           |
|                                                                       |
| <strong>- path: &quot;mem_request&quot;</strong>                                           |
|                                                                       |
| <strong>resourceFieldRef:</strong>                                                 |
|                                                                       |
| <strong>containerName: client-container</strong>                                   |
|                                                                       |
| <strong>resource: requests.memory</strong>                                         |
+-----------------------------------------------------------------------+</p><p>In the configuration file, you can see that the Pod has a <strong>downwardAPI</strong> Volume, and the Container mounts the Volume at <strong>/etc/podinfo</strong>.</p><p>Look at the <strong>items</strong> array under <strong>downwardAPI</strong>. Each element of the array is a DownwardAPIVolumeFile.</p><p>The first element specifies that in the Container named <strong>client-container</strong>, the value of the <strong>limits.cpu</strong> field should be stored in a file named <strong>cpu_limit</strong>.</p><p>Create the Pod:</p><p><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/inject-data-application/dapi-volume-resources.yaml">https://k8s.io/docs/tasks/inject-data-application/dapi-volume-resources.yaml</a></strong></p><p>Get a shell into the Container that is running in your Pod:</p><p><strong>kubectl exec -it kubernetes-downwardapi-volume-example-2 -- sh</strong></p><p>In your shell, view the <strong>cpu_limit</strong> file:</p><p><strong>/# cat /etc/podinfo/cpu_limit</strong></p><p>You can use similar commands to view the <strong>cpu_request</strong>, <strong>mem_limit</strong> and <strong>mem_request</strong> files.</p><h4>Capabilities of the Downward API</h4><p>The following information is available to Containers through environment variables and DownwardAPIVolumeFiles:</p><ul><li>The Node&#x27;s name</li><li>The Node&#x27;s IP</li><li>The Pod&#x27;s name</li><li>The Pod&#x27;s namespace</li><li>The Pod&#x27;s IP address</li><li>The Pod&#x27;s service account name</li><li>The Pod&#x27;s UID</li><li>A Container&#x27;s CPU limit</li><li>A Container&#x27;s CPU request</li><li>A Container&#x27;s memory limit</li><li>A Container&#x27;s memory request</li></ul><p>In addition, the following information is available through DownwardAPIVolumeFiles.</p><ul><li>The Pod&#x27;s labels</li><li>The Pod&#x27;s annotations</li></ul><p><strong>Note:</strong> If CPU and memory limits are not specified for a Container, the Downward API defaults to the node allocatable value for CPU and memory.</p><h4>Project keys to specific paths and file permissions</h4><p>You can project keys to specific paths and specific permissions on a per-file basis. For more information, see <a href="https://kubernetes.io/docs/concepts/configuration/secret/">Secrets</a>.</p><h4>Motivation for the Downward API</h4><p>It is sometimes useful for a Container to have information about itself, without being overly coupled to Kubernetes. The Downward API allows containers to consume information about themselves or the cluster without using the Kubernetes client or API server.</p><p>An example is an existing application that assumes a particular well-known environment variable holds a unique identifier. One possibility is to wrap the application, but that is tedious and error prone, and it violates the goal of low coupling. A better option would be to use the Pod&#x27;s name as an identifier, and inject the Pod&#x27;s name into the well-known environment variable.</p><h4>What&#x27;s next</h4><ul><li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#podspec-v1-core">PodSpec</a></li><li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#volume-v1-core">Volume</a></li><li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#downwardapivolumesource-v1-core">DownwardAPIVolumeSource</a></li><li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#downwardapivolumefile-v1-core">DownwardAPIVolumeFile</a></li><li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#resourcefieldselector-v1-core">ResourceFieldSelector</a></li></ul><h3>Distribute Credentials Securely Using Secrets</h3><p>This page shows how to securely inject sensitive data, such as passwords and encryption keys, into Pods.</p><ul><li><a href="https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/#convert-your-secret-data-to-a-base-64-representation"><strong>Convert your secret data to a base-64 representation</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/#create-a-secret"><strong>Create a Secret</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/#create-a-pod-that-has-access-to-the-secret-data-through-a-volume"><strong>Create a Pod that has access to the secret data through a Volume</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/#create-a-pod-that-has-access-to-the-secret-data-through-environment-variables"><strong>Create a Pod that has access to the secret data through environment variables</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/#whats-next"><strong>What&#x27;s next</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/#reference"><strong>Reference</strong></a></li></ul></li></ul><h4>Before you begin</h4><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using<a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <strong>kubectl version</strong>.</p><h4>Convert your secret data to a base-64 representation</h4><p>Suppose you want to have two pieces of secret data: a username <strong>my-app</strong> and a password <strong>39528$vdg7Jb</strong>. First, use <a href="https://www.base64encode.org/">Base64 encoding</a> to convert your username and password to a base-64 representation. Here&#x27;s a Linux example:</p><p><strong>echo -n \&#x27;my-app\&#x27; | base64</strong></p><p><strong>echo -n \&#x27;39528$vdg7Jb\&#x27; | base64</strong></p><p>The output shows that the base-64 representation of your username is <strong>bXktYXBw</strong>, and the base-64 representation of your password is <strong>Mzk1MjgkdmRnN0pi</strong>.</p><h4>Create a Secret</h4><p>Here is a configuration file you can use to create a Secret that holds your username and password:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>secret.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kube">https://raw.githubusercontent.com/kube</a>             |
| rnetes/website/master/docs/tasks/inject-data-application/secret.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Secret</strong>                                                      |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: test-secret</strong>                                                 |
|                                                                       |
| <strong>data:</strong>                                                             |
|                                                                       |
| <strong>username: bXktYXBw</strong>                                                |
|                                                                       |
| <strong>password: Mzk1MjgkdmRnN0pi</strong>                                        |
+-----------------------------------------------------------------------+</p><ol><li>Create the Secret</li><li><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/inject-data-application/secret.yaml">https://k8s.io/docs/tasks/inject-data-application/secret.yaml</a></strong></li></ol><p><strong>Note:</strong> If you want to skip the Base64 encoding step, you can create a Secret by using the <strong>kubectl create secret</strong> command:</p><p><strong>kubectl create secret generic test-secret --from-literal=username=\&#x27;my-app\&#x27; --from-literal=password=\&#x27;39528$vdg7Jb\&#x27;</strong></p><ol><li>View information about the Secret:</li><li><strong>kubectl get secret test-secret</strong></li></ol><p>Output:</p><p><strong>NAME TYPE DATA AGE</strong></p><p><strong>test-secret Opaque 2 1m</strong></p><ol><li>View more detailed information about the Secret:</li><li><strong>kubectl describe secret test-secret</strong></li></ol><p>Output:</p><p><strong>Name: test-secret</strong></p><p><strong>Namespace: default</strong></p><p><strong>Labels: <code>&lt;none&gt;</code></strong></p><p><strong>Annotations: <code>&lt;none&gt;</code></strong></p><p><strong>Type: Opaque</strong></p><p><strong>Data</strong></p><p><strong>====</strong></p><p><strong>password: 13 bytes</strong></p><p><strong>username: 7 bytes</strong></p><h4>Create a Pod that has access to the secret data through a Volume</h4><p>Here is a configuration file you can use to create a Pod:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>secret-pod.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernet">https://raw.githubusercontent.com/kubernet</a>     |
| es/website/master/docs/tasks/inject-data-application/secret-pod.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: secret-test-pod</strong>                                             |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: test-container</strong>                                            |
|                                                                       |
| <strong>image: nginx</strong>                                                      |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong><em># name must match the volume name below</em></strong>                        |
|                                                                       |
| <strong>- name: secret-volume</strong>                                             |
|                                                                       |
| <strong>mountPath: /etc/secret-volume</strong>                                     |
|                                                                       |
| <strong><em># The secret data is exposed to Containers in the Pod through a   |
| Volume.</em></strong>                                                            |
|                                                                       |
| <strong>volumes:</strong>                                                          |
|                                                                       |
| <strong>- name: secret-volume</strong>                                             |
|                                                                       |
| <strong>secret:</strong>                                                           |
|                                                                       |
| <strong>secretName: test-secret</strong>                                           |
+-----------------------------------------------------------------------+</p><ol><li>Create the Pod:</li><li><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/inject-data-application/secret-pod.yaml">https://k8s.io/docs/tasks/inject-data-application/secret-pod.yaml</a></strong></li><li>Verify that your Pod is running:</li><li><strong>kubectl get pod secret-test-pod</strong></li></ol><p>Output:</p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>secret-test-pod 1/1 Running 0 42m</strong></p><ol><li>Get a shell into the Container that is running in your Pod:</li><li><strong>kubectl exec -it secret-test-pod -- /bin/bash</strong></li><li>The secret data is exposed to the Container through a Volume mounted under <strong>/etc/secret-volume</strong>. In your shell, go to the directory where the secret data is exposed:</li><li><strong>root@secret-test-pod:/# cd /etc/secret-volume</strong></li><li>In your shell, list the files in the <strong>/etc/secret-volume</strong> directory:</li><li><strong>root@secret-test-pod:/etc/secret-volume# ls</strong></li></ol><p>The output shows two files, one for each piece of secret data:</p><p><strong>password username</strong></p><ol><li>In your shell, display the contents of the <strong>username</strong> and <strong>password</strong> files:</li><li><strong>root@secret-test-pod:/etc/secret-volume# cat username; echo; cat password; echo</strong></li></ol><p>The output is your username and password:</p><p><strong>my-app</strong></p><p><strong>39528$vdg7Jb</strong></p><h4>Create a Pod that has access to the secret data through environment variables</h4><p>Here is a configuration file you can use to create a Pod:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>secret-                                                            |
| envars-pod.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/webs">https://raw.githubusercontent.com/kubernetes/webs</a> |
| ite/master/docs/tasks/inject-data-application/secret-envars-pod.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: secret-envars-test-pod</strong>                                      |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: envars-test-container</strong>                                     |
|                                                                       |
| <strong>image: nginx</strong>                                                      |
|                                                                       |
| <strong>env:</strong>                                                              |
|                                                                       |
| <strong>- name: SECRET_USERNAME</strong>                                           |
|                                                                       |
| <strong>valueFrom:</strong>                                                        |
|                                                                       |
| <strong>secretKeyRef:</strong>                                                     |
|                                                                       |
| <strong>name: test-secret</strong>                                                 |
|                                                                       |
| <strong>key: username</strong>                                                     |
|                                                                       |
| <strong>- name: SECRET_PASSWORD</strong>                                           |
|                                                                       |
| <strong>valueFrom:</strong>                                                        |
|                                                                       |
| <strong>secretKeyRef:</strong>                                                     |
|                                                                       |
| <strong>name: test-secret</strong>                                                 |
|                                                                       |
| <strong>key: password</strong>                                                     |
+-----------------------------------------------------------------------+</p><ol><li>Create the Pod:</li><li><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/inject-data-application/secret-envars-pod.yaml">https://k8s.io/docs/tasks/inject-data-application/secret-envars-pod.yaml</a></strong></li><li>Verify that your Pod is running:</li><li><strong>kubectl get pod secret-envars-test-pod</strong></li></ol><p>Output:</p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>secret-envars-test-pod 1/1 Running 0 4m</strong></p><ol><li>Get a shell into the Container that is running in your Pod:</li><li><strong>kubectl exec -it secret-envars-test-pod -- /bin/bash</strong></li><li>In your shell, display the environment variables:</li><li><strong>root@secret-envars-test-pod:/# printenv</strong></li></ol><p>The output includes your username and password:</p><p><strong>.<!-- -->..</strong></p><p><strong>SECRET_USERNAME=my-app</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>SECRET_PASSWORD=39528$vdg7Jb</strong></p><h4>What&#x27;s next</h4><ul><li>Learn more about <a href="https://kubernetes.io/docs/concepts/configuration/secret/">Secrets</a>.</li><li>Learn about <a href="https://kubernetes.io/docs/concepts/storage/volumes/">Volumes</a>.</li></ul><h5><strong>Reference</strong></h5><ul><li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#secret-v1-core">Secret</a></li><li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#volume-v1-core">Volume</a></li><li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#pod-v1-core">Pod</a></li></ul><h3>Inject Information into Pods Using a PodPreset</h3><p>You can use a <strong>podpreset</strong> object to inject information like secrets, volume mounts, and environment variables etc into pods at creation time. This task shows some examples on using the <strong>PodPreset</strong>resource. You can get an overview of PodPresets at <a href="https://kubernetes.io/docs/concepts/workloads/pods/podpreset/">Understanding Pod Presets</a>.</p><ul><li><a href="https://kubernetes.io/docs/tasks/inject-data-application/podpreset/#create-a-pod-preset"><strong>Create a Pod Preset</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/inject-data-application/podpreset/#simple-pod-spec-example"><strong>Simple Pod Spec Example</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/inject-data-application/podpreset/#pod-spec-with-configmap-example"><strong>Pod Spec with ConfigMap Example</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/inject-data-application/podpreset/#replicaset-with-pod-spec-example"><strong>ReplicaSet with Pod Spec Example</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/inject-data-application/podpreset/#multiple-podpreset-example"><strong>Multiple PodPreset Example</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/inject-data-application/podpreset/#conflict-example"><strong>Conflict Example</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/inject-data-application/podpreset/#deleting-a-pod-preset"><strong>Deleting a Pod Preset</strong></a></li></ul><h4>Create a Pod Preset</h4><h5><strong>Simple Pod Spec Example</strong></h5><p>This is a simple example to show how a Pod spec is modified by the Pod Preset.</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>podpr                                                              |
| eset-preset.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/web">https://raw.githubusercontent.com/kubernetes/web</a> |
| site/master/docs/tasks/inject-data-application/podpreset-preset.yaml) |
+=======================================================================+
| <strong>apiVersion: settings.k8s.io/v1alpha1</strong>                              |
|                                                                       |
| <strong>kind: PodPreset</strong>                                                   |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: allow-database</strong>                                              |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>matchLabels:</strong>                                                      |
|                                                                       |
| <strong>role: frontend</strong>                                                    |
|                                                                       |
| <strong>env:</strong>                                                              |
|                                                                       |
| <strong>- name: DB_PORT</strong>                                                   |
|                                                                       |
| <strong>value: &quot;6379&quot;</strong>                                                   |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- mountPath: /cache</strong>                                               |
|                                                                       |
| <strong>name: cache-volume</strong>                                                |
|                                                                       |
| <strong>volumes:</strong>                                                          |
|                                                                       |
| <strong>- name: cache-volume</strong>                                              |
|                                                                       |
| <strong>emptyDir: {}</strong>                                                      |
+-----------------------------------------------------------------------+</p><p>Create the PodPreset:</p><p><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/inject-data-application/podpreset-preset.yaml">https://k8s.io/docs/tasks/inject-data-application/podpreset-preset.yaml</a></strong></p><p>Examine the created PodPreset:</p><p><strong>$ kubectl get podpreset</strong></p><p><strong>NAME AGE</strong></p><p><strong>allow-database 1m</strong></p><p>The new PodPreset will act upon any pod that has label <strong>role: frontend</strong>.</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<em>                                                                    |
| </em>podpreset-pod.yaml** ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/">https://raw.githubusercontent.com/kubernetes/</a> |
| website/master/docs/tasks/inject-data-application/podpreset-pod.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: website</strong>                                                     |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: website</strong>                                                      |
|                                                                       |
| <strong>role: frontend</strong>                                                    |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: website</strong>                                                   |
|                                                                       |
| <strong>image: nginx</strong>                                                      |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- containerPort: 80</strong>                                               |
+-----------------------------------------------------------------------+</p><p>Create a pod:</p><p><strong>$ kubectl create -f <a href="https://k8s.io/docs/tasks/inject-data-application/podpreset-pod.yaml">https://k8s.io/docs/tasks/inject-data-application/podpreset-pod.yaml</a></strong></p><p>List the running Pods:</p><p><strong>$ kubectl get pods</strong></p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>website 1/1 Running 0 4m</strong></p><p><strong>Pod spec after admission controller:</strong></p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>podpr                                                              |
| eset-merged.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/web">https://raw.githubusercontent.com/kubernetes/web</a> |
| site/master/docs/tasks/inject-data-application/podpreset-merged.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: website</strong>                                                     |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: website</strong>                                                      |
|                                                                       |
| <strong>role: frontend</strong>                                                    |
|                                                                       |
| <strong>annotations:</strong>                                                      |
|                                                                       |
| <strong>podpreset.admission.kubernetes.io/podpreset-allow-database:         |
| &quot;resource version&quot;</strong>                                                |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: website</strong>                                                   |
|                                                                       |
| <strong>image: nginx</strong>                                                      |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- mountPath: /cache</strong>                                               |
|                                                                       |
| <strong>name: cache-volume</strong>                                                |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- containerPort: 80</strong>                                               |
|                                                                       |
| <strong>env:</strong>                                                              |
|                                                                       |
| <strong>- name: DB_PORT</strong>                                                   |
|                                                                       |
| <strong>value: &quot;6379&quot;</strong>                                                   |
|                                                                       |
| <strong>volumes:</strong>                                                          |
|                                                                       |
| <strong>- name: cache-volume</strong>                                              |
|                                                                       |
| <strong>emptyDir: {}</strong>                                                      |
+-----------------------------------------------------------------------+</p><p>To see above output, run the following command:</p><p><strong>$ kubectl get pod website -o yaml</strong></p><h5><strong>Pod Spec with </strong>ConfigMap<strong> Example</strong></h5><p>This is an example to show how a Pod spec is modified by the Pod Preset that defines a <strong>ConfigMap</strong>for Environment Variables.</p><p><strong>User submitted pod spec:</strong></p><p>+-----------------------------------------------------------------------+
| <!-- -->[<em>                                                                    |
| </em>podpreset-pod.yaml** ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/">https://raw.githubusercontent.com/kubernetes/</a> |
| website/master/docs/tasks/inject-data-application/podpreset-pod.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: website</strong>                                                     |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: website</strong>                                                      |
|                                                                       |
| <strong>role: frontend</strong>                                                    |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: website</strong>                                                   |
|                                                                       |
| <strong>image: nginx</strong>                                                      |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- containerPort: 80</strong>                                               |
+-----------------------------------------------------------------------+</p><p><strong>User submitted ConfigMap:</strong></p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>podpreset-c                                                        |
| onfigmap.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/websit">https://raw.githubusercontent.com/kubernetes/websit</a> |
| e/master/docs/tasks/inject-data-application/podpreset-configmap.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: ConfigMap</strong>                                                   |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: etcd-env-config</strong>                                             |
|                                                                       |
| <strong>data:</strong>                                                             |
|                                                                       |
| <strong>number_of_members: &quot;1&quot;</strong>                                          |
|                                                                       |
| <strong>initial_cluster_state: new</strong>                                        |
|                                                                       |
| <strong>initial_cluster_token: DUMMY_ETCD_INITIAL_CLUSTER_TOKEN</strong>           |
|                                                                       |
| <strong>discovery_token: DUMMY_ETCD_DISCOVERY_TOKEN</strong>                       |
|                                                                       |
| <strong>discovery_url: http://etcd_discovery:2379</strong>                         |
|                                                                       |
| <strong>etcdctl_peers: http://etcd:2379</strong>                                   |
|                                                                       |
| <strong>duplicate_key: FROM_CONFIG_MAP</strong>                                    |
|                                                                       |
| <strong>REPLACE_ME: &quot;a value&quot;</strong>                                           |
+-----------------------------------------------------------------------+</p><p><strong>Example Pod Preset:</strong></p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>podpreset                                                          |
| -allow-db.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/websi">https://raw.githubusercontent.com/kubernetes/websi</a> |
| te/master/docs/tasks/inject-data-application/podpreset-allow-db.yaml) |
+=======================================================================+
| <strong>apiVersion: settings.k8s.io/v1alpha1</strong>                              |
|                                                                       |
| <strong>kind: PodPreset</strong>                                                   |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: allow-database</strong>                                              |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>matchLabels:</strong>                                                      |
|                                                                       |
| <strong>role: frontend</strong>                                                    |
|                                                                       |
| <strong>env:</strong>                                                              |
|                                                                       |
| <strong>- name: DB_PORT</strong>                                                   |
|                                                                       |
| <strong>value: &quot;6379&quot;</strong>                                                   |
|                                                                       |
| <strong>- name: duplicate_key</strong>                                             |
|                                                                       |
| <strong>value: FROM_ENV</strong>                                                   |
|                                                                       |
| <strong>- name: expansion</strong>                                                 |
|                                                                       |
| <strong>value: $(REPLACE_ME)</strong>                                             |
|                                                                       |
| <strong>envFrom:</strong>                                                          |
|                                                                       |
| <strong>- configMapRef:</strong>                                                   |
|                                                                       |
| <strong>name: etcd-env-config</strong>                                             |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- mountPath: /cache</strong>                                               |
|                                                                       |
| <strong>name: cache-volume</strong>                                                |
|                                                                       |
| <strong>- mountPath: /etc/app/config.json</strong>                                 |
|                                                                       |
| <strong>readOnly: true</strong>                                                    |
|                                                                       |
| <strong>name: secret-volume</strong>                                               |
|                                                                       |
| <strong>volumes:</strong>                                                          |
|                                                                       |
| <strong>- name: cache-volume</strong>                                              |
|                                                                       |
| <strong>emptyDir: {}</strong>                                                      |
|                                                                       |
| <strong>- name: secret-volume</strong>                                             |
|                                                                       |
| <strong>secret:</strong>                                                           |
|                                                                       |
| <strong>secretName: config-details</strong>                                        |
+-----------------------------------------------------------------------+</p><p><strong>Pod spec after admission controller:</strong></p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>podpreset-allow-db-merg                                            |
| ed.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/website/mast">https://raw.githubusercontent.com/kubernetes/website/mast</a> |
| er/docs/tasks/inject-data-application/podpreset-allow-db-merged.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: website</strong>                                                     |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: website</strong>                                                      |
|                                                                       |
| <strong>role: frontend</strong>                                                    |
|                                                                       |
| <strong>annotations:</strong>                                                      |
|                                                                       |
| <strong>podpreset.admission.kubernetes.io/podpreset-allow-database:         |
| &quot;resource version&quot;</strong>                                                |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: website</strong>                                                   |
|                                                                       |
| <strong>image: nginx</strong>                                                      |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- mountPath: /cache</strong>                                               |
|                                                                       |
| <strong>name: cache-volume</strong>                                                |
|                                                                       |
| <strong>- mountPath: /etc/app/config.json</strong>                                 |
|                                                                       |
| <strong>readOnly: true</strong>                                                    |
|                                                                       |
| <strong>name: secret-volume</strong>                                               |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- containerPort: 80</strong>                                               |
|                                                                       |
| <strong>env:</strong>                                                              |
|                                                                       |
| <strong>- name: DB_PORT</strong>                                                   |
|                                                                       |
| <strong>value: &quot;6379&quot;</strong>                                                   |
|                                                                       |
| <strong>- name: duplicate_key</strong>                                             |
|                                                                       |
| <strong>value: FROM_ENV</strong>                                                   |
|                                                                       |
| <strong>- name: expansion</strong>                                                 |
|                                                                       |
| <strong>value: $(REPLACE_ME)</strong>                                             |
|                                                                       |
| <strong>envFrom:</strong>                                                          |
|                                                                       |
| <strong>- configMapRef:</strong>                                                   |
|                                                                       |
| <strong>name: etcd-env-config</strong>                                             |
|                                                                       |
| <strong>volumes:</strong>                                                          |
|                                                                       |
| <strong>- name: cache-volume</strong>                                              |
|                                                                       |
| <strong>emptyDir: {}</strong>                                                      |
|                                                                       |
| <strong>- name: secret-volume</strong>                                             |
|                                                                       |
| <strong>secret:</strong>                                                           |
|                                                                       |
| <strong>secretName: config-details</strong>                                        |
+-----------------------------------------------------------------------+</p><h5><strong>ReplicaSet with Pod Spec Example</strong></h5><p>The following example shows that only the pod spec is modified by the Pod Preset.</p><p><strong>User submitted ReplicaSet:</strong></p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>podpreset-rep                                                      |
| licaset.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/website">https://raw.githubusercontent.com/kubernetes/website</a> |
| /master/docs/tasks/inject-data-application/podpreset-replicaset.yaml) |
+=======================================================================+
| <strong>apiVersion: apps/v1</strong>                                               |
|                                                                       |
| <strong>kind: ReplicaSet</strong>                                                  |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: frontend</strong>                                                    |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>replicas: 3</strong>                                                       |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>matchLabels:</strong>                                                      |
|                                                                       |
| <strong>role: frontend</strong>                                                    |
|                                                                       |
| <strong>matchExpressions:</strong>                                                 |
|                                                                       |
| <strong>- {key: role, operator: In, values: <!-- -->[frontend]<!-- -->}</strong>                 |
|                                                                       |
| <strong>template:</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: guestbook</strong>                                                    |
|                                                                       |
| <strong>role: frontend</strong>                                                    |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: php-redis</strong>                                                 |
|                                                                       |
| <strong>image: gcr.io/google_samples/gb-frontend:v3</strong>                       |
|                                                                       |
| <strong>resources:</strong>                                                        |
|                                                                       |
| <strong>requests:</strong>                                                         |
|                                                                       |
| <strong>cpu: 100m</strong>                                                         |
|                                                                       |
| <strong>memory: 100Mi</strong>                                                     |
|                                                                       |
| <strong>env:</strong>                                                              |
|                                                                       |
| <strong>- name: GET_HOSTS_FROM</strong>                                            |
|                                                                       |
| <strong>value: dns</strong>                                                        |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- containerPort: 80</strong>                                               |
+-----------------------------------------------------------------------+</p><p><strong>Example Pod Preset:</strong></p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>podpr                                                              |
| eset-preset.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/web">https://raw.githubusercontent.com/kubernetes/web</a> |
| site/master/docs/tasks/inject-data-application/podpreset-preset.yaml) |
+=======================================================================+
| <strong>apiVersion: settings.k8s.io/v1alpha1</strong>                              |
|                                                                       |
| <strong>kind: PodPreset</strong>                                                   |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: allow-database</strong>                                              |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>matchLabels:</strong>                                                      |
|                                                                       |
| <strong>role: frontend</strong>                                                    |
|                                                                       |
| <strong>env:</strong>                                                              |
|                                                                       |
| <strong>- name: DB_PORT</strong>                                                   |
|                                                                       |
| <strong>value: &quot;6379&quot;</strong>                                                   |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- mountPath: /cache</strong>                                               |
|                                                                       |
| <strong>name: cache-volume</strong>                                                |
|                                                                       |
| <strong>volumes:</strong>                                                          |
|                                                                       |
| <strong>- name: cache-volume</strong>                                              |
|                                                                       |
| <strong>emptyDir: {}</strong>                                                      |
+-----------------------------------------------------------------------+</p><p><strong>Pod spec after admission controller:</strong></p><p>Note that the ReplicaSet spec was not changed, users have to check individual pods to validate that the PodPreset has been applied.</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>podpreset-replicaset-merged                                        |
| .yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/website/master">https://raw.githubusercontent.com/kubernetes/website/master</a> |
| /docs/tasks/inject-data-application/podpreset-replicaset-merged.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: frontend</strong>                                                    |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: guestbook</strong>                                                    |
|                                                                       |
| <strong>role: frontend</strong>                                                    |
|                                                                       |
| <strong>annotations:</strong>                                                      |
|                                                                       |
| <strong>podpreset.admission.kubernetes.io/podpreset-allow-database:         |
| &quot;resource version&quot;</strong>                                                |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: php-redis</strong>                                                 |
|                                                                       |
| <strong>image: gcr.io/google_samples/gb-frontend:v3</strong>                       |
|                                                                       |
| <strong>resources:</strong>                                                        |
|                                                                       |
| <strong>requests:</strong>                                                         |
|                                                                       |
| <strong>cpu: 100m</strong>                                                         |
|                                                                       |
| <strong>memory: 100Mi</strong>                                                     |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- mountPath: /cache</strong>                                               |
|                                                                       |
| <strong>name: cache-volume</strong>                                                |
|                                                                       |
| <strong>env:</strong>                                                              |
|                                                                       |
| <strong>- name: GET_HOSTS_FROM</strong>                                            |
|                                                                       |
| <strong>value: dns</strong>                                                        |
|                                                                       |
| <strong>- name: DB_PORT</strong>                                                   |
|                                                                       |
| <strong>value: &quot;6379&quot;</strong>                                                   |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- containerPort: 80</strong>                                               |
|                                                                       |
| <strong>volumes:</strong>                                                          |
|                                                                       |
| <strong>- name: cache-volume</strong>                                              |
|                                                                       |
| <strong>emptyDir: {}</strong>                                                      |
+-----------------------------------------------------------------------+</p><h5><strong>Multiple PodPreset Example</strong></h5><p>This is an example to show how a Pod spec is modified by multiple Pod Injection Policies.</p><p><strong>User submitted pod spec:</strong></p><p>+-----------------------------------------------------------------------+
| <!-- -->[<em>                                                                    |
| </em>podpreset-pod.yaml** ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/">https://raw.githubusercontent.com/kubernetes/</a> |
| website/master/docs/tasks/inject-data-application/podpreset-pod.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: website</strong>                                                     |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: website</strong>                                                      |
|                                                                       |
| <strong>role: frontend</strong>                                                    |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: website</strong>                                                   |
|                                                                       |
| <strong>image: nginx</strong>                                                      |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- containerPort: 80</strong>                                               |
+-----------------------------------------------------------------------+</p><p><strong>Example Pod Preset:</strong></p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>podpr                                                              |
| eset-preset.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/web">https://raw.githubusercontent.com/kubernetes/web</a> |
| site/master/docs/tasks/inject-data-application/podpreset-preset.yaml) |
+=======================================================================+
| <strong>apiVersion: settings.k8s.io/v1alpha1</strong>                              |
|                                                                       |
| <strong>kind: PodPreset</strong>                                                   |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: allow-database</strong>                                              |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>matchLabels:</strong>                                                      |
|                                                                       |
| <strong>role: frontend</strong>                                                    |
|                                                                       |
| <strong>env:</strong>                                                              |
|                                                                       |
| <strong>- name: DB_PORT</strong>                                                   |
|                                                                       |
| <strong>value: &quot;6379&quot;</strong>                                                   |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- mountPath: /cache</strong>                                               |
|                                                                       |
| <strong>name: cache-volume</strong>                                                |
|                                                                       |
| <strong>volumes:</strong>                                                          |
|                                                                       |
| <strong>- name: cache-volume</strong>                                              |
|                                                                       |
| <strong>emptyDir: {}</strong>                                                      |
+-----------------------------------------------------------------------+</p><p><strong>Another Pod Preset:</strong></p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>pod                                                                |
| preset-proxy.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/we">https://raw.githubusercontent.com/kubernetes/we</a> |
| bsite/master/docs/tasks/inject-data-application/podpreset-proxy.yaml) |
+=======================================================================+
| <strong>apiVersion: settings.k8s.io/v1alpha1</strong>                              |
|                                                                       |
| <strong>kind: PodPreset</strong>                                                   |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: proxy</strong>                                                       |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>matchLabels:</strong>                                                      |
|                                                                       |
| <strong>role: frontend</strong>                                                    |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- mountPath: /etc/proxy/configs</strong>                                   |
|                                                                       |
| <strong>name: proxy-volume</strong>                                                |
|                                                                       |
| <strong>volumes:</strong>                                                          |
|                                                                       |
| <strong>- name: proxy-volume</strong>                                              |
|                                                                       |
| <strong>emptyDir: {}</strong>                                                      |
+-----------------------------------------------------------------------+</p><p><strong>Pod spec after admission controller:</strong></p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>podpreset-multi-m                                                  |
| erged.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/website/m">https://raw.githubusercontent.com/kubernetes/website/m</a> |
| aster/docs/tasks/inject-data-application/podpreset-multi-merged.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: website</strong>                                                     |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: website</strong>                                                      |
|                                                                       |
| <strong>role: frontend</strong>                                                    |
|                                                                       |
| <strong>annotations:</strong>                                                      |
|                                                                       |
| <strong>podpreset.admission.kubernetes.io/podpreset-allow-database:         |
| &quot;resource version&quot;</strong>                                                |
|                                                                       |
| <strong>podpreset.admission.kubernetes.io/podpreset-proxy: &quot;resource       |
| version&quot;</strong>                                                           |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: website</strong>                                                   |
|                                                                       |
| <strong>image: nginx</strong>                                                      |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- mountPath: /cache</strong>                                               |
|                                                                       |
| <strong>name: cache-volume</strong>                                                |
|                                                                       |
| <strong>- mountPath: /etc/proxy/configs</strong>                                   |
|                                                                       |
| <strong>name: proxy-volume</strong>                                                |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- containerPort: 80</strong>                                               |
|                                                                       |
| <strong>env:</strong>                                                              |
|                                                                       |
| <strong>- name: DB_PORT</strong>                                                   |
|                                                                       |
| <strong>value: &quot;6379&quot;</strong>                                                   |
|                                                                       |
| <strong>volumes:</strong>                                                          |
|                                                                       |
| <strong>- name: cache-volume</strong>                                              |
|                                                                       |
| <strong>emptyDir: {}</strong>                                                      |
|                                                                       |
| <strong>- name: proxy-volume</strong>                                              |
|                                                                       |
| <strong>emptyDir: {}</strong>                                                      |
+-----------------------------------------------------------------------+</p><h5><strong>Conflict Example</strong></h5><p>This is an example to show how a Pod spec is not modified by the Pod Preset when there is a conflict.</p><p><strong>User submitted pod spec:</strong></p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>podpreset-conflic                                                  |
| t-pod.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/website/m">https://raw.githubusercontent.com/kubernetes/website/m</a> |
| aster/docs/tasks/inject-data-application/podpreset-conflict-pod.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: website</strong>                                                     |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: website</strong>                                                      |
|                                                                       |
| <strong>role: frontend</strong>                                                    |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: website</strong>                                                   |
|                                                                       |
| <strong>image: nginx</strong>                                                      |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- mountPath: /cache</strong>                                               |
|                                                                       |
| <strong>name: cache-volume</strong>                                                |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- containerPort: 80</strong>                                               |
|                                                                       |
| <strong>volumes:</strong>                                                          |
|                                                                       |
| <strong>- name: cache-volume</strong>                                              |
|                                                                       |
| <strong>emptyDir: {}</strong>                                                      |
+-----------------------------------------------------------------------+</p><p><strong>Example Pod Preset:</strong></p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>podpreset-conflict-pres                                            |
| et.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/website/mast">https://raw.githubusercontent.com/kubernetes/website/mast</a> |
| er/docs/tasks/inject-data-application/podpreset-conflict-preset.yaml) |
+=======================================================================+
| <strong>apiVersion: settings.k8s.io/v1alpha1</strong>                              |
|                                                                       |
| <strong>kind: PodPreset</strong>                                                   |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: allow-database</strong>                                              |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>matchLabels:</strong>                                                      |
|                                                                       |
| <strong>role: frontend</strong>                                                    |
|                                                                       |
| <strong>env:</strong>                                                              |
|                                                                       |
| <strong>- name: DB_PORT</strong>                                                   |
|                                                                       |
| <strong>value: &quot;6379&quot;</strong>                                                   |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- mountPath: /cache</strong>                                               |
|                                                                       |
| <strong>name: other-volume</strong>                                                |
|                                                                       |
| <strong>volumes:</strong>                                                          |
|                                                                       |
| <strong>- name: other-volume</strong>                                              |
|                                                                       |
| <strong>emptyDir: {}</strong>                                                      |
+-----------------------------------------------------------------------+</p><p><strong>Pod spec after admission controller will not change because of the conflict:</strong></p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>podpreset-conflic                                                  |
| t-pod.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/website/m">https://raw.githubusercontent.com/kubernetes/website/m</a> |
| aster/docs/tasks/inject-data-application/podpreset-conflict-pod.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: website</strong>                                                     |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: website</strong>                                                      |
|                                                                       |
| <strong>role: frontend</strong>                                                    |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: website</strong>                                                   |
|                                                                       |
| <strong>image: nginx</strong>                                                      |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- mountPath: /cache</strong>                                               |
|                                                                       |
| <strong>name: cache-volume</strong>                                                |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- containerPort: 80</strong>                                               |
|                                                                       |
| <strong>volumes:</strong>                                                          |
|                                                                       |
| <strong>- name: cache-volume</strong>                                              |
|                                                                       |
| <strong>emptyDir: {}</strong>                                                      |
+-----------------------------------------------------------------------+</p><p><strong>If we run kubectl describe<!-- -->.<!-- -->.. we can see the event:</strong></p><p><strong>$ kubectl describe <!-- -->.<!-- -->..</strong></p><p><strong>.<!-- -->...</strong></p><p><strong>Events:</strong></p><p><strong>FirstSeen LastSeen Count From SubobjectPath Reason Message</strong></p><p><strong>Tue, 07 Feb 2017 16:56:12 -0700 Tue, 07 Feb 2017 16:56:12 -0700 1 {podpreset.admission.kubernetes.io/podpreset-allow-database } conflict Conflict on pod preset. Duplicate mountPath /cache.</strong></p><h4>Deleting a Pod Preset</h4><p>Once you don&#x27;t need a pod preset anymore, you can delete it with <strong>kubectl</strong>:</p><p><strong>$ kubectl delete podpreset allow-database</strong></p><p><strong>podpreset &quot;allow-database&quot; deleted</strong></p><h2>Run Applications</h2><h3>Run a Stateless Application Using a Deployment</h3><p>This page shows how to run an application using a Kubernetes Deployment object.</p><ul><li><a href="https://kubernetes.io/docs/tasks/run-application/run-stateless-application-deployment/#objectives"><strong>Objectives</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/run-stateless-application-deployment/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/run-stateless-application-deployment/#creating-and-exploring-an-nginx-deployment"><strong>Creating and exploring an nginx deployment</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/run-stateless-application-deployment/#updating-the-deployment"><strong>Updating the deployment</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/run-stateless-application-deployment/#scaling-the-application-by-increasing-the-replica-count"><strong>Scaling the application by increasing the replica count</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/run-stateless-application-deployment/#deleting-a-deployment"><strong>Deleting a deployment</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/run-stateless-application-deployment/#replicationcontrollers--the-old-way"><strong>ReplicationControllers -- the Old Way</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/run-stateless-application-deployment/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Objectives</h4><ul><li>Create an nginx deployment.</li><li>Use kubectl to list information about the deployment.</li><li>Update the deployment.</li></ul><h4>Before you begin</h4><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using<a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>Your Kubernetes server must be version v1.9 or later. To check the version, enter <strong>kubectl version</strong>.</p><h4>Creating and exploring an nginx deployment</h4><p>You can run an application by creating a Kubernetes Deployment object, and you can describe a Deployment in a YAML file. For example, this YAML file describes a Deployment that runs the nginx:1.7.9 Docker image:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>deployment.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/">https://raw.githubusercontent.com/</a>             |
| kubernetes/website/master/docs/tasks/run-application/deployment.yaml) |
+=======================================================================+
| <strong>apiVersion: apps/v1 <em># for versions before 1.9.0 use               |
| apps/v1beta2</em></strong>                                                       |
|                                                                       |
| <strong>kind: Deployment</strong>                                                  |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: nginx-deployment</strong>                                            |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>matchLabels:</strong>                                                      |
|                                                                       |
| <strong>app: nginx</strong>                                                        |
|                                                                       |
| <strong>replicas: 2 <em># tells deployment to run 2 pods matching the         |
| template</em></strong>                                                           |
|                                                                       |
| <strong>template: <em># create pods using pod definition in this template</em></strong>  |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong><em># unlike pod-nginx.yaml, the name is not included in the meta     |
| data as a unique name is</em></strong>                                           |
|                                                                       |
| <strong><em># generated from the deployment name</em></strong>                           |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: nginx</strong>                                                        |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: nginx</strong>                                                     |
|                                                                       |
| <strong>image: nginx:1.7.9</strong>                                                |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- containerPort: 80</strong>                                               |
+-----------------------------------------------------------------------+</p><ol><li>Create a Deployment based on the YAML file:</li><li><strong>kubectl apply -f <a href="https://k8s.io/docs/tasks/run-application/deployment.yaml">https://k8s.io/docs/tasks/run-application/deployment.yaml</a></strong></li><li>Display information about the Deployment:</li><li><strong>kubectl describe deployment nginx-deployment</strong></li></ol><p>The output is similar to this:</p><p><strong>user@computer:<!-- -->~<!-- -->/website$ kubectl describe deployment nginx-deployment</strong></p><p><strong>Name: nginx-deployment</strong></p><p><strong>Namespace: default</strong></p><p><strong>CreationTimestamp: Tue, 30 Aug 2016 18:11:37 -0700</strong></p><p><strong>Labels: app=nginx</strong></p><p><strong>Annotations: deployment.kubernetes.io/revision=1</strong></p><p><strong>Selector: app=nginx</strong></p><p><strong>Replicas: 2 desired | 2 updated | 2 total | 2 available | 0 unavailable</strong></p><p><strong>StrategyType: RollingUpdate</strong></p><p><strong>MinReadySeconds: 0</strong></p><p><strong>RollingUpdateStrategy: 1 max unavailable, 1 max surge</strong></p><p><strong>Pod Template:</strong></p><p><strong>Labels: app=nginx</strong></p><p><strong>Containers:</strong></p><p><strong>nginx:</strong></p><p><strong>Image: nginx:1.7.9</strong></p><p><strong>Port: 80/TCP</strong></p><p><strong>Environment: <code>&lt;none&gt;</code></strong></p><p><strong>Mounts: <code>&lt;none&gt;</code></strong></p><p><strong>Volumes: <code>&lt;none&gt;</code></strong></p><p><strong>Conditions:</strong></p><p><strong>Type Status Reason</strong></p><p><strong>---- ------ ------</strong></p><p><strong>Available True MinimumReplicasAvailable</strong></p><p><strong>Progressing True NewReplicaSetAvailable</strong></p><p><strong>OldReplicaSets: <code>&lt;none&gt;</code></strong></p><p><strong>NewReplicaSet: nginx-deployment-1771418926 (2/2 replicas created)</strong></p><p><strong>No events.</strong></p><ol><li>List the pods created by the deployment:</li><li><strong>kubectl get pods -l app=nginx</strong></li></ol><p>The output is similar to this:</p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>nginx-deployment-1771418926-7o5ns 1/1 Running 0 16h</strong></p><p><strong>nginx-deployment-1771418926-r18az 1/1 Running 0 16h</strong></p><ol><li>Display information about a pod:</li><li><code>kubectl describe pod &lt;pod-name&gt;</code></li></ol><p>where <code style="background-color:lightgray">&lt;pod-name&gt;</code> is the name of one of your pods.</p><h4>Updating the deployment</h4><p>You can update the deployment by applying a new YAML file. This YAML file specifies that the deployment should be updated to use nginx 1.8.</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<em>                                                                    |
| </em>deployment-update.yaml** ]<!-- -->(<a href="https://raw.githubusercontent.com/kuberne">https://raw.githubusercontent.com/kuberne</a> |
| tes/website/master/docs/tasks/run-application/deployment-update.yaml) |
+=======================================================================+
| <strong>apiVersion: apps/v1 <em># for versions before 1.9.0 use               |
| apps/v1beta2</em></strong>                                                       |
|                                                                       |
| <strong>kind: Deployment</strong>                                                  |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: nginx-deployment</strong>                                            |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>matchLabels:</strong>                                                      |
|                                                                       |
| <strong>app: nginx</strong>                                                        |
|                                                                       |
| <strong>replicas: 2</strong>                                                       |
|                                                                       |
| <strong>template:</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: nginx</strong>                                                        |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: nginx</strong>                                                     |
|                                                                       |
| <strong>image: nginx:1.8 <em># Update the version of nginx from 1.7.9 to      |
| 1.8</em></strong>                                                                |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- containerPort: 80</strong>                                               |
+-----------------------------------------------------------------------+</p><ol><li>Apply the new YAML file:</li><li><strong>kubectl apply -f <a href="https://k8s.io/docs/tasks/run-application/deployment-update.yaml">https://k8s.io/docs/tasks/run-application/deployment-update.yaml</a></strong></li><li>Watch the deployment create pods with new names and delete the old pods:</li><li><strong>kubectl get pods -l app=nginx</strong></li></ol><h4>Scaling the application by increasing the replica count</h4><p>You can increase the number of pods in your Deployment by applying a new YAML file. This YAML file sets <strong>replicas</strong> to 4, which specifies that the Deployment should have four pods:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>deployment-scale.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubern">https://raw.githubusercontent.com/kubern</a> |
| etes/website/master/docs/tasks/run-application/deployment-scale.yaml) |
+=======================================================================+
| <strong>apiVersion: apps/v1 <em># for versions before 1.9.0 use               |
| apps/v1beta2</em></strong>                                                       |
|                                                                       |
| <strong>kind: Deployment</strong>                                                  |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: nginx-deployment</strong>                                            |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>matchLabels:</strong>                                                      |
|                                                                       |
| <strong>app: nginx</strong>                                                        |
|                                                                       |
| <strong>replicas: 4 <em># Update the replicas from 2 to 4</em></strong>                  |
|                                                                       |
| <strong>template:</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: nginx</strong>                                                        |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: nginx</strong>                                                     |
|                                                                       |
| <strong>image: nginx:1.8</strong>                                                  |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- containerPort: 80</strong>                                               |
+-----------------------------------------------------------------------+</p><ol><li>Apply the new YAML file:</li><li><strong>kubectl apply -f <a href="https://k8s.io/docs/tasks/run-application/deployment-scale.yaml">https://k8s.io/docs/tasks/run-application/deployment-scale.yaml</a></strong></li><li>Verify that the Deployment has four pods:</li><li><strong>kubectl get pods -l app=nginx</strong></li></ol><p>The output is similar to this:</p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>nginx-deployment-148880595-4zdqq 1/1 Running 0 25s</strong></p><p><strong>nginx-deployment-148880595-6zgi1 1/1 Running 0 25s</strong></p><p><strong>nginx-deployment-148880595-fxcez 1/1 Running 0 2m</strong></p><p><strong>nginx-deployment-148880595-rwovn 1/1 Running 0 2m</strong></p><h4>Deleting a deployment</h4><p>Delete the deployment by name:</p><p><strong>kubectl delete deployment nginx-deployment</strong></p><h4>ReplicationControllers -- the Old Way</h4><p>The preferred way to create a replicated application is to use a Deployment, which in turn uses a ReplicaSet. Before the Deployment and ReplicaSet were added to Kubernetes, replicated applications were configured by using a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/">ReplicationController</a>.</p><h4>What&#x27;s next</h4><ul><li>Learn more about <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployment objects</a>.</li></ul><h3>Run a Single-Instance Stateful Application</h3><p>This page shows you how to run a single-instance stateful application in Kubernetes using a PersistentVolume and a Deployment. The application is MySQL.</p><ul><li><a href="https://kubernetes.io/docs/tasks/run-application/run-single-instance-stateful-application/#objectives"><strong>Objectives</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/run-single-instance-stateful-application/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/run-single-instance-stateful-application/#deploy-mysql"><strong>Deploy MySQL</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/run-single-instance-stateful-application/#accessing-the-mysql-instance"><strong>Accessing the MySQL instance</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/run-single-instance-stateful-application/#updating"><strong>Updating</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/run-single-instance-stateful-application/#deleting-a-deployment"><strong>Deleting a deployment</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/run-single-instance-stateful-application/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Objectives</h4><ul><li>Create a PersistentVolume referencing a disk in your environment.</li><li>Create a MySQL Deployment.</li><li>Expose MySQL to other pods in the cluster at a known DNS name.</li></ul><h4>Before you begin</h4><ul><li>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using <a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</li><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <strong>kubectl version</strong>.</p><ul><li>You need to either have a dynamic PersistentVolume provisioner with a default <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/">StorageClass</a>, or <a href="https://kubernetes.io/docs/user-guide/persistent-volumes/#provisioning">statically provision PersistentVolumes</a> yourself to satisfy the <a href="https://kubernetes.io/docs/user-guide/persistent-volumes/#persistentvolumeclaims">PersistentVolumeClaims</a> used here.</li></ul><h4>Deploy MySQL</h4><p>You can run a stateful application by creating a Kubernetes Deployment and connecting it to an existing PersistentVolume using a PersistentVolumeClaim. For example, this YAML file describes a Deployment that runs MySQL and references the PersistentVolumeClaim. The file defines a volume mount for /var/lib/mysql, and then creates a PersistentVolumeClaim that looks for a 20G volume. This claim is satisfied by any existing volume that meets the requirements, or by a dynamic provisioner.</p><p>Note: The password is defined in the config yaml, and this is insecure. See <a href="https://kubernetes.io/docs/concepts/configuration/secret/">Kubernetes Secrets</a> for a secure solution.</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>mysql-deployment.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubern">https://raw.githubusercontent.com/kubern</a> |
| etes/website/master/docs/tasks/run-application/mysql-deployment.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Service</strong>                                                     |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: mysql</strong>                                                       |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- port: 3306</strong>                                                      |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>app: mysql</strong>                                                        |
|                                                                       |
| <strong>clusterIP: None</strong>                                                   |
|                                                                       |
| <strong>---</strong>                                                             |
|                                                                       |
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: PersistentVolumeClaim</strong>                                       |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: mysql-pv-claim</strong>                                              |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>accessModes:</strong>                                                      |
|                                                                       |
| <strong>- ReadWriteOnce</strong>                                                   |
|                                                                       |
| <strong>resources:</strong>                                                        |
|                                                                       |
| <strong>requests:</strong>                                                         |
|                                                                       |
| <strong>storage: 20Gi</strong>                                                     |
|                                                                       |
| <strong>---</strong>                                                             |
|                                                                       |
| <strong>apiVersion: apps/v1 <em># for versions before 1.9.0 use               |
| apps/v1beta2</em></strong>                                                       |
|                                                                       |
| <strong>kind: Deployment</strong>                                                  |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: mysql</strong>                                                       |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>matchLabels:</strong>                                                      |
|                                                                       |
| <strong>app: mysql</strong>                                                        |
|                                                                       |
| <strong>strategy:</strong>                                                         |
|                                                                       |
| <strong>type: Recreate</strong>                                                    |
|                                                                       |
| <strong>template:</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: mysql</strong>                                                        |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- image: mysql:5.6</strong>                                                |
|                                                                       |
| <strong>name: mysql</strong>                                                       |
|                                                                       |
| <strong>env:</strong>                                                              |
|                                                                       |
| <strong><em># Use secret in real usage</em></strong>                                     |
|                                                                       |
| <strong>- name: MYSQL_ROOT_PASSWORD</strong>                                       |
|                                                                       |
| <strong>value: password</strong>                                                   |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- containerPort: 3306</strong>                                             |
|                                                                       |
| <strong>name: mysql</strong>                                                       |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- name: mysql-persistent-storage</strong>                                  |
|                                                                       |
| <strong>mountPath: /var/lib/mysql</strong>                                         |
|                                                                       |
| <strong>volumes:</strong>                                                          |
|                                                                       |
| <strong>- name: mysql-persistent-storage</strong>                                  |
|                                                                       |
| <strong>persistentVolumeClaim:</strong>                                            |
|                                                                       |
| <strong>claimName: mysql-pv-claim</strong>                                         |
+-----------------------------------------------------------------------+</p><ol><li>Deploy the contents of the YAML file:</li><li><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/run-application/mysql-deployment.yaml">https://k8s.io/docs/tasks/run-application/mysql-deployment.yaml</a></strong></li><li>Display information about the Deployment:</li><li><strong>kubectl describe deployment mysql</strong></li><li><strong>Name: mysql</strong></li><li><strong>Namespace: default</strong></li><li><strong>CreationTimestamp: Tue, 01 Nov 2016 11:18:45 -0700</strong></li><li><strong>Labels: app=mysql</strong></li><li><strong>Annotations: deployment.kubernetes.io/revision=1</strong></li><li><strong>Selector: app=mysql</strong></li><li><strong>Replicas: 1 desired | 1 updated | 1 total | 0 available | 1 unavailable</strong></li><li><strong>StrategyType: Recreate</strong></li><li><strong>MinReadySeconds: 0</strong></li><li><strong>Pod Template:</strong></li><li><strong>Labels: app=mysql</strong></li><li><strong>Containers:</strong></li><li><strong>mysql:</strong></li><li><strong>Image: mysql:5.6</strong></li><li><strong>Port: 3306/TCP</strong></li><li><strong>Environment:</strong></li><li><strong>MYSQL_ROOT_PASSWORD: password</strong></li><li><strong>Mounts:</strong></li><li><strong>/var/lib/mysql from mysql-persistent-storage (rw)</strong></li><li><strong>Volumes:</strong></li><li><strong>mysql-persistent-storage:</strong></li><li><strong>Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)</strong></li><li><strong>ClaimName: mysql-pv-claim</strong></li><li><strong>ReadOnly: false</strong></li><li><strong>Conditions:</strong></li><li><strong>Type Status Reason</strong></li><li><strong>---- ------ ------</strong></li><li><strong>Available False MinimumReplicasUnavailable</strong></li><li><strong>Progressing True ReplicaSetUpdated</strong></li><li><strong>OldReplicaSets: <code>&lt;none&gt;</code></strong></li><li><strong>NewReplicaSet: mysql-63082529 (1/1 replicas created)</strong></li><li><strong>Events:</strong></li><li><strong>FirstSeen LastSeen Count From SubobjectPath Type Reason Message</strong></li><li><strong>--------- -------- ----- ---- ------------- -------- ------ -------</strong></li><li><strong>33s 33s 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set mysql-63082529 to 1</strong></li><li>List the pods created by the Deployment:</li><li><strong>kubectl get pods -l app=mysql</strong></li><li><strong>NAME READY STATUS RESTARTS AGE</strong></li><li><strong>mysql-63082529-2z3ki 1/1 Running 0 3m</strong></li><li>Inspect the PersistentVolumeClaim:</li><li><strong>kubectl describe pvc mysql-pv-claim</strong></li><li><strong>Name: mysql-pv-claim</strong></li><li><strong>Namespace: default</strong></li><li><strong>StorageClass:</strong></li><li><strong>Status: Bound</strong></li><li><strong>Volume: mysql-pv</strong></li><li><strong>Labels: <code>&lt;none&gt;</code></strong></li><li><strong>Annotations: pv.kubernetes.io/bind-completed=yes</strong></li><li><strong>pv.kubernetes.io/bound-by-controller=yes</strong></li><li><strong>Capacity: 20Gi</strong></li><li><strong>Access Modes: RWO</strong></li><li><strong>Events: <code>&lt;none&gt;</code></strong></li></ol><h4>Accessing the MySQL instance</h4><p>The preceding YAML file creates a service that allows other Pods in the cluster to access the database. The Service option <strong>clusterIP: None</strong> lets the Service DNS name resolve directly to the Pod&#x27;s IP address. This is optimal when you have only one Pod behind a Service and you don&#x27;t intend to increase the number of Pods.</p><p>Run a MySQL client to connect to the server:</p><p><strong>kubectl run -it --rm --image=mysql:5.6 --restart=Never mysql-client -- mysql -h mysql -ppassword</strong></p><p>This command creates a new Pod in the cluster running a MySQL client and connects it to the server through the Service. If it connects, you know your stateful MySQL database is up and running.</p><p><strong>Waiting for pod default/mysql-client-274442439-zyp6i to be running, status is Pending, pod ready: false</strong></p><p><strong>If you don\&#x27;t see a command prompt, try pressing enter.</strong></p><p><strong>mysql&gt;</strong></p><h4>Updating</h4><p>The image or any other part of the Deployment can be updated as usual with the <strong>kubectl apply</strong>command. Here are some precautions that are specific to stateful apps:</p><ul><li>Don&#x27;t scale the app. This setup is for single-instance apps only. The underlying PersistentVolume can only be mounted to one Pod. For clustered stateful apps, see the <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">StatefulSet documentation</a>.</li><li>Use <strong>strategy:</strong> <strong>type: Recreate</strong> in the Deployment configuration YAML file. This instructs Kubernetes to not use rolling updates. Rolling updates will not work, as you cannot have more than one Pod running at a time. The <strong>Recreate</strong> strategy will stop the first pod before creating a new one with the updated configuration.</li></ul><h4>Deleting a deployment</h4><p>Delete the deployed objects by name:</p><p><strong>kubectl delete deployment,svc mysql</strong></p><p><strong>kubectl delete pvc mysql-pv-claim</strong></p><p>If you manually provisioned a PersistentVolume, you also need to manually delete it, as well as release the underlying resource. If you used a dynamic provisioner, it automatically deletes the PersistentVolume when it sees that you deleted the PersistentVolumeClaim. Some dynamic provisioners (such as those for EBS and PD) also release the underlying resource upon deleting the PersistentVolume.</p><h4>What&#x27;s next</h4><ul><li>Learn more about <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployment objects</a>.</li><li>Learn more about <a href="https://kubernetes.io/docs/user-guide/deploying-applications/">Deploying applications</a></li><li><a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#run">kubectl run documentation</a></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/">Volumes</a> and <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">Persistent Volumes</a></li></ul><h3>Run a Replicated Stateful Application</h3><p>This page shows how to run a replicated stateful application using a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">StatefulSet</a> controller. The example is a MySQL single-master topology with multiple slaves running asynchronous replication.</p><p>Note that <strong>this is not a production configuration</strong>. In particular, MySQL settings remain on insecure defaults to keep the focus on general patterns for running stateful applications in Kubernetes.</p><ul><li><a href="https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/#objectives"><strong>Objectives</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/#deploy-mysql"><strong>Deploy MySQL</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/#configmap"><strong>ConfigMap</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/#services"><strong>Services</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/#statefulset"><strong>StatefulSet</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/#understanding-stateful-pod-initialization"><strong>Understanding stateful Pod initialization</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/#generating-configuration"><strong>Generating configuration</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/#cloning-existing-data"><strong>Cloning existing data</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/#starting-replication"><strong>Starting replication</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/#sending-client-traffic"><strong>Sending client traffic</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/#simulating-pod-and-node-downtime"><strong>Simulating Pod and Node downtime</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/#break-the-readiness-probe"><strong>Break the Readiness Probe</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/#delete-pods"><strong>Delete Pods</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/#drain-a-node"><strong>Drain a Node</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/#scaling-the-number-of-slaves"><strong>Scaling the number of slaves</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/#cleaning-up"><strong>Cleaning up</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Objectives</h4><ul><li>Deploy a replicated MySQL topology with a StatefulSet controller.</li><li>Send MySQL client traffic.</li><li>Observe resistance to downtime.</li><li>Scale the StatefulSet up and down.</li></ul><h4>Before you begin</h4><ul><li>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using <a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</li><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <strong>kubectl version</strong>.</p><ul><li>You need to either have a dynamic PersistentVolume provisioner with a default <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/">StorageClass</a>, or <a href="https://kubernetes.io/docs/user-guide/persistent-volumes/#provisioning">statically provision PersistentVolumes</a> yourself to satisfy the <a href="https://kubernetes.io/docs/user-guide/persistent-volumes/#persistentvolumeclaims">PersistentVolumeClaims</a> used here.</li><li>This tutorial assumes you are familiar with <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">PersistentVolumes</a> and <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">StatefulSets</a>, as well as other core concepts like <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/">Pods</a>, <a href="https://kubernetes.io/docs/concepts/services-networking/service/">Services</a>, and <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/">ConfigMaps</a>.</li><li>Some familiarity with MySQL helps, but this tutorial aims to present general patterns that should be useful for other systems.</li></ul><h4>Deploy MySQL</h4><p>The example MySQL deployment consists of a ConfigMap, two Services, and a StatefulSet.</p><h5><strong>ConfigMap</strong></h5><p>Create the ConfigMap from the following YAML configuration file:</p><p><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/run-application/mysql-configmap.yaml">https://k8s.io/docs/tasks/run-application/mysql-configmap.yaml</a></strong></p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>mysql-configmap.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kuber">https://raw.githubusercontent.com/kuber</a>   |
| netes/website/master/docs/tasks/run-application/mysql-configmap.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: ConfigMap</strong>                                                   |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: mysql</strong>                                                       |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: mysql</strong>                                                        |
|                                                                       |
| <strong>data:</strong>                                                             |
|                                                                       |
| <strong>master.cnf: |</strong>                                                    |
|                                                                       |
| <strong># Apply this config only on the master.</strong>                          |
|                                                                       |
| <strong>[mysqld]</strong>                                                        |
|                                                                       |
| <strong>log-bin</strong>                                                           |
|                                                                       |
| <strong>slave.cnf: |</strong>                                                     |
|                                                                       |
| <strong># Apply this config only on slaves.</strong>                              |
|                                                                       |
| <strong>[mysqld]</strong>                                                        |
|                                                                       |
| <strong>super-read-only</strong>                                                   |
+-----------------------------------------------------------------------+</p><p>This ConfigMap provides <strong>my.cnf</strong> overrides that let you independently control configuration on the MySQL master and slaves. In this case, you want the master to be able to serve replication logs to slaves and you want slaves to reject any writes that don&#x27;t come via replication.</p><p>There&#x27;s nothing special about the ConfigMap itself that causes different portions to apply to different Pods. Each Pod decides which portion to look at as it&#x27;s initializing, based on information provided by the StatefulSet controller.</p><h5><strong>Services</strong></h5><p>Create the Services from the following YAML configuration file:</p><p><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/run-application/mysql-services.yaml">https://k8s.io/docs/tasks/run-application/mysql-services.yaml</a></strong></p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>mysql-services.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kube">https://raw.githubusercontent.com/kube</a>     |
| rnetes/website/master/docs/tasks/run-application/mysql-services.yaml) |
+=======================================================================+
| <strong><em># Headless service for stable DNS entries of StatefulSet          |
| members.</em></strong>                                                           |
|                                                                       |
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Service</strong>                                                     |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: mysql</strong>                                                       |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: mysql</strong>                                                        |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- name: mysql</strong>                                                     |
|                                                                       |
| <strong>port: 3306</strong>                                                        |
|                                                                       |
| <strong>clusterIP: None</strong>                                                   |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>app: mysql</strong>                                                        |
|                                                                       |
| <strong>---</strong>                                                             |
|                                                                       |
| <strong><em># Client service for connecting to any MySQL instance for         |
| reads.</em></strong>                                                             |
|                                                                       |
| <strong><em># For writes, you must instead connect to the master:             |
| mysql-0.mysql.</em></strong>                                                     |
|                                                                       |
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Service</strong>                                                     |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: mysql-read</strong>                                                  |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: mysql</strong>                                                        |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- name: mysql</strong>                                                     |
|                                                                       |
| <strong>port: 3306</strong>                                                        |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>app: mysql</strong>                                                        |
+-----------------------------------------------------------------------+</p><p>The Headless Service provides a home for the DNS entries that the StatefulSet controller creates for each Pod that&#x27;s part of the set. Because the Headless Service is named <strong>mysql</strong>, the Pods are accessible by resolving <code style="background-color:lightgray">&lt;pod-name&gt;.mysql</code> from within any other Pod in the same Kubernetes cluster and namespace.</p><p>The Client Service, called <strong>mysql-read</strong>, is a normal Service with its own cluster IP that distributes connections across all MySQL Pods that report being Ready. The set of potential endpoints includes the MySQL master and all slaves.</p><p>Note that only read queries can use the load-balanced Client Service. Because there is only one MySQL master, clients should connect directly to the MySQL master Pod (through its DNS entry within the Headless Service) to execute writes.</p><h5><strong>StatefulSet</strong></h5><p>Finally, create the StatefulSet from the following YAML configuration file:</p><p><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/run-application/mysql-statefulset.yaml">https://k8s.io/docs/tasks/run-application/mysql-statefulset.yaml</a></strong></p><p>+-----------------------------------------------------------------------+
| <!-- -->[<em>                                                                    |
| </em>mysql-statefulset.yaml** ]<!-- -->(<a href="https://raw.githubusercontent.com/kuberne">https://raw.githubusercontent.com/kuberne</a> |
| tes/website/master/docs/tasks/run-application/mysql-statefulset.yaml) |
+=======================================================================+
| <strong>apiVersion: apps/v1</strong>                                               |
|                                                                       |
| <strong>kind: StatefulSet</strong>                                                 |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: mysql</strong>                                                       |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>matchLabels:</strong>                                                      |
|                                                                       |
| <strong>app: mysql</strong>                                                        |
|                                                                       |
| <strong>serviceName: mysql</strong>                                                |
|                                                                       |
| <strong>replicas: 3</strong>                                                       |
|                                                                       |
| <strong>template:</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: mysql</strong>                                                        |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>initContainers:</strong>                                                   |
|                                                                       |
| <strong>- name: init-mysql</strong>                                                |
|                                                                       |
| <strong>image: mysql:5.7</strong>                                                  |
|                                                                       |
| <strong>command:</strong>                                                          |
|                                                                       |
| <strong>- bash</strong>                                                            |
|                                                                       |
| <strong>- &quot;-c&quot;</strong>                                                          |
|                                                                       |
| <strong>- |</strong>                                                              |
|                                                                       |
| <strong>set -ex</strong>                                                           |
|                                                                       |
| <strong># Generate mysql server-id from pod ordinal index.</strong>               |
|                                                                       |
| <strong>[[ <code>hostname</code> =<!-- -->~<!-- --> -(<!-- -->[0-9]<!-- -->+)$ ]] || exit 1</strong>              |
|                                                                       |
| <strong>ordinal=${BASH_REMATCH<!-- -->[1]<!-- -->}</strong>                                     |
|                                                                       |
| <strong>echo <!-- -->[mysqld]<!-- --> &gt; /mnt/conf.d/server-id.cnf</strong>                      |
|                                                                       |
| <strong># Add an offset to avoid reserved server-id=0 value.</strong>             |
|                                                                       |
| <strong>echo server-id=$((100 + $ordinal)) &gt;&gt;                           |
| /mnt/conf.d/server-id.cnf</strong>                                           |
|                                                                       |
| <strong># Copy appropriate conf.d files from config-map to emptyDir.</strong>     |
|                                                                       |
| <strong>if [<!-- -->[ $ordinal -eq 0 ]<!-- -->]; then</strong>                                |
|                                                                       |
| <strong>cp /mnt/config-map/master.cnf /mnt/conf.d/</strong>                        |
|                                                                       |
| <strong>else</strong>                                                              |
|                                                                       |
| <strong>cp /mnt/config-map/slave.cnf /mnt/conf.d/</strong>                         |
|                                                                       |
| <strong>fi</strong>                                                                |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- name: conf</strong>                                                      |
|                                                                       |
| <strong>mountPath: /mnt/conf.d</strong>                                            |
|                                                                       |
| <strong>- name: config-map</strong>                                                |
|                                                                       |
| <strong>mountPath: /mnt/config-map</strong>                                        |
|                                                                       |
| <strong>- name: clone-mysql</strong>                                               |
|                                                                       |
| <strong>image: gcr.io/google-samples/xtrabackup:1.0</strong>                       |
|                                                                       |
| <strong>command:</strong>                                                          |
|                                                                       |
| <strong>- bash</strong>                                                            |
|                                                                       |
| <strong>- &quot;-c&quot;</strong>                                                          |
|                                                                       |
| <strong>- |</strong>                                                              |
|                                                                       |
| <strong>set -ex</strong>                                                           |
|                                                                       |
| <strong># Skip the clone if data already exists.</strong>                         |
|                                                                       |
| <strong>[<!-- -->[ -d /var/lib/mysql/mysql ]<!-- -->] &amp;&amp; exit 0</strong>                       |
|                                                                       |
| <strong># Skip the clone on master (ordinal index 0).</strong>                    |
|                                                                       |
| <strong>[[ <code>hostname</code> =<!-- -->~<!-- --> -(<!-- -->[0-9]<!-- -->+)$ ]] || exit 1</strong>              |
|                                                                       |
| <strong>ordinal=${BASH_REMATCH<!-- -->[1]<!-- -->}</strong>                                     |
|                                                                       |
| <strong>[<!-- -->[ $ordinal -eq 0 ]<!-- -->] &amp;&amp; exit 0</strong>                               |
|                                                                       |
| <strong># Clone data from previous peer.</strong>                                 |
|                                                                       |
| <strong>ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x |
| -C /var/lib/mysql</strong>                                                   |
|                                                                       |
| <strong># Prepare the backup.</strong>                                            |
|                                                                       |
| <strong>xtrabackup --prepare --target-dir=/var/lib/mysql</strong>                |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- name: data</strong>                                                      |
|                                                                       |
| <strong>mountPath: /var/lib/mysql</strong>                                         |
|                                                                       |
| <strong>subPath: mysql</strong>                                                    |
|                                                                       |
| <strong>- name: conf</strong>                                                      |
|                                                                       |
| <strong>mountPath: /etc/mysql/conf.d</strong>                                      |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: mysql</strong>                                                     |
|                                                                       |
| <strong>image: mysql:5.7</strong>                                                  |
|                                                                       |
| <strong>env:</strong>                                                              |
|                                                                       |
| <strong>- name: MYSQL_ALLOW_EMPTY_PASSWORD</strong>                                |
|                                                                       |
| <strong>value: &quot;1&quot;</strong>                                                      |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- name: mysql</strong>                                                     |
|                                                                       |
| <strong>containerPort: 3306</strong>                                               |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- name: data</strong>                                                      |
|                                                                       |
| <strong>mountPath: /var/lib/mysql</strong>                                         |
|                                                                       |
| <strong>subPath: mysql</strong>                                                    |
|                                                                       |
| <strong>- name: conf</strong>                                                      |
|                                                                       |
| <strong>mountPath: /etc/mysql/conf.d</strong>                                      |
|                                                                       |
| <strong>resources:</strong>                                                        |
|                                                                       |
| <strong>requests:</strong>                                                         |
|                                                                       |
| <strong>cpu: 500m</strong>                                                         |
|                                                                       |
| <strong>memory: 1Gi</strong>                                                       |
|                                                                       |
| <strong>livenessProbe:</strong>                                                    |
|                                                                       |
| <strong>exec:</strong>                                                             |
|                                                                       |
| <strong>command: <!-- -->[&quot;mysqladmin&quot;, &quot;ping&quot;]</strong>                             |
|                                                                       |
| <strong>initialDelaySeconds: 30</strong>                                           |
|                                                                       |
| <strong>periodSeconds: 10</strong>                                                 |
|                                                                       |
| <strong>timeoutSeconds: 5</strong>                                                 |
|                                                                       |
| <strong>readinessProbe:</strong>                                                   |
|                                                                       |
| <strong>exec:</strong>                                                             |
|                                                                       |
| <strong><em># Check we can execute queries over TCP (skip-networking is       |
| off).</em></strong>                                                              |
|                                                                       |
| <strong>command: <!-- -->[&quot;mysql&quot;, &quot;-h&quot;, &quot;127.0.0.1&quot;, &quot;-e&quot;, &quot;SELECT       |
| 1&quot;]</strong>                                                               |
|                                                                       |
| <strong>initialDelaySeconds: 5</strong>                                            |
|                                                                       |
| <strong>periodSeconds: 2</strong>                                                  |
|                                                                       |
| <strong>timeoutSeconds: 1</strong>                                                 |
|                                                                       |
| <strong>- name: xtrabackup</strong>                                                |
|                                                                       |
| <strong>image: gcr.io/google-samples/xtrabackup:1.0</strong>                       |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- name: xtrabackup</strong>                                                |
|                                                                       |
| <strong>containerPort: 3307</strong>                                               |
|                                                                       |
| <strong>command:</strong>                                                          |
|                                                                       |
| <strong>- bash</strong>                                                            |
|                                                                       |
| <strong>- &quot;-c&quot;</strong>                                                          |
|                                                                       |
| <strong>- |</strong>                                                              |
|                                                                       |
| <strong>set -ex</strong>                                                           |
|                                                                       |
| <strong>cd /var/lib/mysql</strong>                                                 |
|                                                                       |
| <strong># Determine binlog position of cloned data, if any.</strong>              |
|                                                                       |
| <strong>if [<!-- -->[ -f xtrabackup_slave_info ]<!-- -->]; then</strong>                       |
|                                                                       |
| <strong># XtraBackup already generated a partial &quot;CHANGE MASTER TO&quot;      |
| query</strong>                                                               |
|                                                                       |
| <strong># because we\&#x27;re cloning from an existing slave.</strong>                 |
|                                                                       |
| <strong>mv xtrabackup_slave_info change_master_to.sql.in</strong>                  |
|                                                                       |
| <strong># Ignore xtrabackup_binlog_info in this case (it\&#x27;s useless).</strong>    |
|                                                                       |
| <strong>rm -f xtrabackup_binlog_info</strong>                                      |
|                                                                       |
| <strong>elif [<!-- -->[ -f xtrabackup_binlog_info ]<!-- -->]; then</strong>                    |
|                                                                       |
| <strong># We\&#x27;re cloning directly from master. Parse binlog position.</strong>    |
|                                                                       |
| <strong>[[ <code>cat xtrabackup_binlog_info</code> =<!-- -->~<!-- -->                             |
| \^(.<!-- -->*<!-- -->?)[<!-- -->[:space:]<!-- -->]+(.<!-- -->*<!-- -->?)$ ]] || exit 1</strong>                   |
|                                                                       |
| <strong>rm xtrabackup_binlog_info</strong>                                         |
|                                                                       |
| <strong>echo &quot;CHANGE MASTER TO                                             |
| MASTER_LOG_FILE=\&#x27;${BASH_REMATCH<!-- -->[1]<!-- -->}\&#x27;,<!-- -->\</strong>                        |
|                                                                       |
| <strong>MASTER_LOG_POS=${BASH_REMATCH<!-- -->[2]<!-- -->}&quot; &gt; change_master_to.sql.in</strong> |
|                                                                       |
| <strong>fi</strong>                                                                |
|                                                                       |
| <strong># Check if we need to complete a clone by starting replication.</strong>  |
|                                                                       |
| <strong>if [<!-- -->[ -f change_master_to.sql.in ]<!-- -->]; then</strong>                     |
|                                                                       |
| <strong>echo &quot;Waiting for mysqld to be ready (accepting connections)&quot;</strong>   |
|                                                                       |
| <strong>until mysql -h 127.0.0.1 -e &quot;SELECT 1&quot;; do sleep 1; done</strong>        |
|                                                                       |
| <strong>echo &quot;Initializing replication from clone position&quot;</strong>             |
|                                                                       |
| <strong># In case of container restart, attempt this at-most-once.</strong>       |
|                                                                       |
| <strong>mv change_master_to.sql.in change_master_to.sql.orig</strong>              |
|                                                                       |
| <strong>mysql -h 127.0.0.1 &lt;&lt;EOF</strong>                                        |
|                                                                       |
| <strong>$(&lt;change_master_to.sql.orig),</strong>                                  |
|                                                                       |
| <strong>MASTER_HOST=\&#x27;mysql-0.mysql\&#x27;,</strong>                                    |
|                                                                       |
| <strong>MASTER_USER=\&#x27;root\&#x27;,</strong>                                             |
|                                                                       |
| <strong>MASTER_PASSWORD=\&#x27;\&#x27;,</strong>                                             |
|                                                                       |
| <strong>MASTER_CONNECT_RETRY=10;</strong>                                          |
|                                                                       |
| <strong>START SLAVE;</strong>                                                      |
|                                                                       |
| <strong>EOF</strong>                                                               |
|                                                                       |
| <strong>fi</strong>                                                                |
|                                                                       |
| <strong># Start a server to send backups when requested by peers.</strong>        |
|                                                                       |
| <strong>exec ncat --listen --keep-open --send-only --max-conns=1 3307   |
| -c <!-- -->\</strong>                                                               |
|                                                                       |
| <strong>&quot;xtrabackup --backup --slave-info --stream=xbstream             |
| --host=127.0.0.1 --user=root&quot;</strong>                                    |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- name: data</strong>                                                      |
|                                                                       |
| <strong>mountPath: /var/lib/mysql</strong>                                         |
|                                                                       |
| <strong>subPath: mysql</strong>                                                    |
|                                                                       |
| <strong>- name: conf</strong>                                                      |
|                                                                       |
| <strong>mountPath: /etc/mysql/conf.d</strong>                                      |
|                                                                       |
| <strong>resources:</strong>                                                        |
|                                                                       |
| <strong>requests:</strong>                                                         |
|                                                                       |
| <strong>cpu: 100m</strong>                                                         |
|                                                                       |
| <strong>memory: 100Mi</strong>                                                     |
|                                                                       |
| <strong>volumes:</strong>                                                          |
|                                                                       |
| <strong>- name: conf</strong>                                                      |
|                                                                       |
| <strong>emptyDir: {}</strong>                                                      |
|                                                                       |
| <strong>- name: config-map</strong>                                                |
|                                                                       |
| <strong>configMap:</strong>                                                        |
|                                                                       |
| <strong>name: mysql</strong>                                                       |
|                                                                       |
| <strong>volumeClaimTemplates:</strong>                                             |
|                                                                       |
| <strong>- metadata:</strong>                                                       |
|                                                                       |
| <strong>name: data</strong>                                                        |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>accessModes: <!-- -->[&quot;ReadWriteOnce&quot;]</strong>                                |
|                                                                       |
| <strong>resources:</strong>                                                        |
|                                                                       |
| <strong>requests:</strong>                                                         |
|                                                                       |
| <strong>storage: 10Gi</strong>                                                     |
+-----------------------------------------------------------------------+</p><p>You can watch the startup progress by running:</p><p><strong>kubectl get pods -l app=mysql --watch</strong></p><p>After a while, you should see all 3 Pods become Running:</p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>mysql-0 2/2 Running 0 2m</strong></p><p><strong>mysql-1 2/2 Running 0 1m</strong></p><p><strong>mysql-2 2/2 Running 0 1m</strong></p><p>Press <strong>Ctrl+C</strong> to cancel the watch. If you don&#x27;t see any progress, make sure you have a dynamic PersistentVolume provisioner enabled as mentioned in the <a href="https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/#before-you-begin">prerequisites</a>.</p><p>This manifest uses a variety of techniques for managing stateful Pods as part of a StatefulSet. The next section highlights some of these techniques to explain what happens as the StatefulSet creates Pods.</p><h4>Understanding stateful Pod initialization</h4><p>The StatefulSet controller starts Pods one at a time, in order by their ordinal index. It waits until each Pod reports being Ready before starting the next one.</p><p>In addition, the controller assigns each Pod a unique, stable name of the form <strong><code>&lt;statefulset-name&gt;-&lt;ordinal-index&gt;</code></strong>. In this case, that results in Pods named <strong>mysql-0</strong>, <strong>mysql-1</strong>, and <strong>mysql-2</strong>.</p><p>The Pod template in the above StatefulSet manifest takes advantage of these properties to perform orderly startup of MySQL replication.</p><h5><strong>Generating configuration</strong></h5><p>Before starting any of the containers in the Pod spec, the Pod first runs any <a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/">Init Containers</a> in the order defined.</p><p>The first Init Container, named <strong>init-mysql</strong>, generates special MySQL config files based on the ordinal index.</p><p>The script determines its own ordinal index by extracting it from the end of the Pod name, which is returned by the <strong>hostname</strong> command. Then it saves the ordinal (with a numeric offset to avoid reserved values) into a file called <strong>server-id.cnf</strong> in the MySQL <strong>conf.d</strong> directory. This translates the unique, stable identity provided by the StatefulSet controller into the domain of MySQL server IDs, which require the same properties.</p><p>The script in the <strong>init-mysql</strong> container also applies either <strong>master.cnf</strong> or <strong>slave.cnf</strong> from the ConfigMap by copying the contents into <strong>conf.d</strong>. Because the example topology consists of a single MySQL master and any number of slaves, the script simply assigns ordinal <strong>0</strong> to be the master, and everyone else to be slaves. Combined with the StatefulSet controller&#x27;s <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#deployment-and-scaling-guarantees/">deployment order guarantee</a>, this ensures the MySQL master is Ready before creating slaves, so they can begin replicating.</p><h5><strong>Cloning existing data</strong></h5><p>In general, when a new Pod joins the set as a slave, it must assume the MySQL master might already have data on it. It also must assume that the replication logs might not go all the way back to the beginning of time. These conservative assumptions are the key to allow a running StatefulSet to scale up and down over time, rather than being fixed at its initial size.</p><p>The second Init Container, named <strong>clone-mysql</strong>, performs a clone operation on a slave Pod the first time it starts up on an empty PersistentVolume. That means it copies all existing data from another running Pod, so its local state is consistent enough to begin replicating from the master.</p><p>MySQL itself does not provide a mechanism to do this, so the example uses a popular open-source tool called Percona XtraBackup. During the clone, the source MySQL server might suffer reduced performance. To minimize impact on the MySQL master, the script instructs each Pod to clone from the Pod whose ordinal index is one lower. This works because the StatefulSet controller always ensures Pod <strong>N</strong> is Ready before starting Pod <strong>N+1</strong>.</p><h5><strong>Starting replication</strong></h5><p>After the Init Containers complete successfully, the regular containers run. The MySQL Pods consist of a <strong>mysql</strong> container that runs the actual <strong>mysqld</strong> server, and an <strong>xtrabackup</strong> container that acts as a <a href="http://blog.kubernetes.io/2015/06/the-distributed-system-toolkit-patterns.html">sidecar</a>.</p><p>The <strong>xtrabackup</strong> sidecar looks at the cloned data files and determines if it&#x27;s necessary to initialize MySQL replication on the slave. If so, it waits for <strong>mysqld</strong> to be ready and then executes the <strong>CHANGE MASTER TO</strong> and <strong>START SLAVE</strong> commands with replication parameters extracted from the XtraBackup clone files.</p><p>Once a slave begins replication, it remembers its MySQL master and reconnects automatically if the server restarts or the connection dies. Also, because slaves look for the master at its stable DNS name (<strong>mysql-0.mysql</strong>), they automatically find the master even if it gets a new Pod IP due to being rescheduled.</p><p>Lastly, after starting replication, the <strong>xtrabackup</strong> container listens for connections from other Pods requesting a data clone. This server remains up indefinitely in case the StatefulSet scales up, or in case the next Pod loses its PersistentVolumeClaim and needs to redo the clone.</p><h4>Sending client traffic</h4><p>You can send test queries to the MySQL master (hostname <strong>mysql-0.mysql</strong>) by running a temporary container with the <strong>mysql:5.7</strong> image and running the <strong>mysql</strong> client binary.</p><p><strong>kubectl run mysql-client --image=mysql:5.7 -i --rm --restart=Never --<!-- -->\</strong></p><p><strong>mysql -h mysql-0.mysql &lt;&lt;EOF</strong></p><p><strong>CREATE DATABASE test;</strong></p><p><strong>CREATE TABLE test.messages (message VARCHAR(250));</strong></p><p><strong>INSERT INTO test.messages VALUES (\&#x27;hello\&#x27;);</strong></p><p><strong>EOF</strong></p><p>Use the hostname <strong>mysql-read</strong> to send test queries to any server that reports being Ready:</p><p><strong>kubectl run mysql-client --image=mysql:5.7 -i -t --rm --restart=Never --<!-- -->\</strong></p><p><strong>mysql -h mysql-read -e &quot;SELECT <!-- -->*<!-- --> FROM test.messages&quot;</strong></p><p>You should get output like this:</p><p><strong>Waiting for pod default/mysql-client to be running, status is Pending, pod ready: false</strong></p><p><strong>+---------+</strong></p><p><strong>| message |</strong></p><p><strong>+---------+</strong></p><p><strong>| hello |</strong></p><p><strong>+---------+</strong></p><p><strong>pod &quot;mysql-client&quot; deleted</strong></p><p>To demonstrate that the <strong>mysql-read</strong> Service distributes connections across servers, you can run <strong>SELECT @@server_id</strong> in a loop:</p><p><strong>kubectl run mysql-client-loop --image=mysql:5.7 -i -t --rm --restart=Never --<!-- -->\</strong></p><p><strong>bash -ic &quot;while sleep 1; do mysql -h mysql-read -e \&#x27;SELECT @@server_id,NOW()\&#x27;; done&quot;</strong></p><p>You should see the reported <strong>@@server_id</strong> change randomly, because a different endpoint might be selected upon each connection attempt:</p><p><strong>+-------------+---------------------+</strong></p><p><strong>| @@server_id | NOW() |</strong></p><p><strong>+-------------+---------------------+</strong></p><p><strong>| 100 | 2006-01-02 15:04:05 |</strong></p><p><strong>+-------------+---------------------+</strong></p><p><strong>+-------------+---------------------+</strong></p><p><strong>| @@server_id | NOW() |</strong></p><p><strong>+-------------+---------------------+</strong></p><p><strong>| 102 | 2006-01-02 15:04:06 |</strong></p><p><strong>+-------------+---------------------+</strong></p><p><strong>+-------------+---------------------+</strong></p><p><strong>| @@server_id | NOW() |</strong></p><p><strong>+-------------+---------------------+</strong></p><p><strong>| 101 | 2006-01-02 15:04:07 |</strong></p><p><strong>+-------------+---------------------+</strong></p><p>You can press <strong>Ctrl+C</strong> when you want to stop the loop, but it&#x27;s useful to keep it running in another window so you can see the effects of the following steps.</p><h4>Simulating Pod and Node downtime</h4><p>To demonstrate the increased availability of reading from the pool of slaves instead of a single server, keep the <strong>SELECT @@server_id</strong> loop from above running while you force a Pod out of the Ready state.</p><h5><strong>Break the Readiness Probe</strong></h5><p>The <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#define-readiness-probes">readiness probe</a> for the <strong>mysql</strong> container runs the command <strong>mysql -h 127.0.0.1 -e \&#x27;SELECT 1\&#x27;</strong> to make sure the server is up and able to execute queries.</p><p>One way to force this readiness probe to fail is to break that command:</p><p><strong>kubectl exec mysql-2 -c mysql -- mv /usr/bin/mysql /usr/bin/mysql.off</strong></p><p>This reaches into the actual container&#x27;s filesystem for Pod <strong>mysql-2</strong> and renames the <strong>mysql</strong>command so the readiness probe can&#x27;t find it. After a few seconds, the Pod should report one of its containers as not Ready, which you can check by running:</p><p><strong>kubectl get pod mysql-2</strong></p><p>Look for <strong>1/2</strong> in the <strong>READY</strong> column:</p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>mysql-2 1/2 Running 0 3m</strong></p><p>At this point, you should see your <strong>SELECT @@server_id</strong> loop continue to run, although it never reports <strong>102</strong> anymore. Recall that the <strong>init-mysql</strong> script defined <strong>server-id</strong> as <strong>100 + $ordinal</strong>, so server ID <strong>102</strong> corresponds to Pod <strong>mysql-2</strong>.</p><p>Now repair the Pod and it should reappear in the loop output after a few seconds:</p><p><strong>kubectl exec mysql-2 -c mysql -- mv /usr/bin/mysql.off /usr/bin/mysql</strong></p><h5><strong>Delete Pods</strong></h5><p>The StatefulSet also recreates Pods if they&#x27;re deleted, similar to what a ReplicaSet does for stateless Pods.</p><p><strong>kubectl delete pod mysql-2</strong></p><p>The StatefulSet controller notices that no <strong>mysql-2</strong> Pod exists anymore, and creates a new one with the same name and linked to the same PersistentVolumeClaim. You should see server ID <strong>102</strong>disappear from the loop output for a while and then return on its own.</p><h5><strong>Drain a Node</strong></h5><p>If your Kubernetes cluster has multiple Nodes, you can simulate Node downtime (such as when Nodes are upgraded) by issuing a <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#drain">drain</a>.</p><p>First determine which Node one of the MySQL Pods is on:</p><p><strong>kubectl get pod mysql-2 -o wide</strong></p><p>The Node name should show up in the last column:</p><p><strong>NAME READY STATUS RESTARTS AGE IP NODE</strong></p><p><strong>mysql-2 2/2 Running 0 15m 10.244.5.27 kubernetes-minion-group-9l2t</strong></p><p>Then drain the Node by running the following command, which cordons it so no new Pods may schedule there, and then evicts any existing Pods. Replace <strong><code>&lt;node-name&gt;</code></strong> with the name of the Node you found in the last step.</p><p>This might impact other applications on the Node, so it&#x27;s best to <strong>only do this in a test cluster</strong>.</p><p><strong>kubectl drain <code>&lt;node-name&gt;</code> --force --delete-local-data --ignore-daemonsets</strong></p><p>Now you can watch as the Pod reschedules on a different Node:</p><p><strong>kubectl get pod mysql-2 -o wide --watch</strong></p><p>It should look something like this:</p><p><strong>NAME READY STATUS RESTARTS AGE IP NODE</strong></p><p><strong>mysql-2 2/2 Terminating 0 15m 10.244.1.56 kubernetes-minion-group-9l2t</strong></p><p><strong>[.<!-- -->..]</strong></p><p><strong>mysql-2 0/2 Pending 0 0s <code>&lt;none&gt;</code> kubernetes-minion-group-fjlm</strong></p><p><strong>mysql-2 0/2 Init:0/2 0 0s <code>&lt;none&gt;</code> kubernetes-minion-group-fjlm</strong></p><p><strong>mysql-2 0/2 Init:1/2 0 20s 10.244.5.32 kubernetes-minion-group-fjlm</strong></p><p><strong>mysql-2 0/2 PodInitializing 0 21s 10.244.5.32 kubernetes-minion-group-fjlm</strong></p><p><strong>mysql-2 1/2 Running 0 22s 10.244.5.32 kubernetes-minion-group-fjlm</strong></p><p><strong>mysql-2 2/2 Running 0 30s 10.244.5.32 kubernetes-minion-group-fjlm</strong></p><p>And again, you should see server ID <strong>102</strong> disappear from the <strong>SELECT @@server_id</strong> loop output for a while and then return.</p><p>Now uncordon the Node to return it to a normal state:</p><p><strong>kubectl uncordon <code>&lt;node-name&gt;</code></strong></p><h4>Scaling the number of slaves</h4><p>With MySQL replication, you can scale your read query capacity by adding slaves. With StatefulSet, you can do this with a single command:</p><p><strong>kubectl scale statefulset mysql --replicas=5</strong></p><p>Watch the new Pods come up by running:</p><p><strong>kubectl get pods -l app=mysql --watch</strong></p><p>Once they&#x27;re up, you should see server IDs <strong>103</strong> and <strong>104</strong> start appearing in the <strong>SELECT @@server_id</strong> loop output.</p><p>You can also verify that these new servers have the data you added before they existed:</p><p><strong>kubectl run mysql-client --image=mysql:5.7 -i -t --rm --restart=Never --<!-- -->\</strong></p><p><strong>mysql -h mysql-3.mysql -e &quot;SELECT <!-- -->*<!-- --> FROM test.messages&quot;</strong></p><p><strong>Waiting for pod default/mysql-client to be running, status is Pending, pod ready: false</strong></p><p><strong>+---------+</strong></p><p><strong>| message |</strong></p><p><strong>+---------+</strong></p><p><strong>| hello |</strong></p><p><strong>+---------+</strong></p><p><strong>pod &quot;mysql-client&quot; deleted</strong></p><p>Scaling back down is also seamless:</p><p><strong>kubectl scale statefulset mysql --replicas=3</strong></p><p>Note, however, that while scaling up creates new PersistentVolumeClaims automatically, scaling down does not automatically delete these PVCs. This gives you the choice to keep those initialized PVCs around to make scaling back up quicker, or to extract data before deleting them.</p><p>You can see this by running:</p><p><strong>kubectl get pvc -l app=mysql</strong></p><p>Which shows that all 5 PVCs still exist, despite having scaled the StatefulSet down to 3:</p><p><strong>NAME STATUS VOLUME CAPACITY ACCESSMODES AGE</strong></p><p><strong>data-mysql-0 Bound pvc-8acbf5dc-b103-11e6-93fa-42010a800002 10Gi RWO 20m</strong></p><p><strong>data-mysql-1 Bound pvc-8ad39820-b103-11e6-93fa-42010a800002 10Gi RWO 20m</strong></p><p><strong>data-mysql-2 Bound pvc-8ad69a6d-b103-11e6-93fa-42010a800002 10Gi RWO 20m</strong></p><p><strong>data-mysql-3 Bound pvc-50043c45-b1c5-11e6-93fa-42010a800002 10Gi RWO 2m</strong></p><p><strong>data-mysql-4 Bound pvc-500a9957-b1c5-11e6-93fa-42010a800002 10Gi RWO 2m</strong></p><p>If you don&#x27;t intend to reuse the extra PVCs, you can delete them:</p><p><strong>kubectl delete pvc data-mysql-3</strong></p><p><strong>kubectl delete pvc data-mysql-4</strong></p><h4>Cleaning up</h4><ol><li>Cancel the <strong>SELECT @@server_id</strong> loop by pressing <strong>Ctrl+C</strong> in its terminal, or running the following from another terminal:</li><li><strong>kubectl delete pod mysql-client-loop --now</strong></li><li>Delete the StatefulSet. This also begins terminating the Pods.</li><li><strong>kubectl delete statefulset mysql</strong></li><li>Verify that the Pods disappear. They might take some time to finish terminating.</li><li><strong>kubectl get pods -l app=mysql</strong></li></ol><p>You&#x27;ll know the Pods have terminated when the above returns:</p><p><strong>No resources found.</strong></p><ol><li>Delete the ConfigMap, Services, and PersistentVolumeClaims.</li><li><strong>kubectl delete configmap,service,pvc -l app=mysql</strong></li><li>If you manually provisioned PersistentVolumes, you also need to manually delete them, as well as release the underlying resources. If you used a dynamic provisioner, it automatically deletes the PersistentVolumes when it sees that you deleted the PersistentVolumeClaims. Some dynamic provisioners (such as those for EBS and PD) also release the underlying resources upon deleting the PersistentVolumes.</li></ol><h4>What&#x27;s next</h4><ul><li>Look in the <a href="https://github.com/kubernetes/charts">Helm Charts repository</a> for other stateful application examples.</li></ul><h3>Update API Objects in Place Using kubectl patch</h3><p>This task shows how to use <strong>kubectl patch</strong> to update an API object in place. The exercises in this task demonstrate a strategic merge patch and a JSON merge patch.</p><ul><li><a href="https://kubernetes.io/docs/tasks/run-application/update-api-object-kubectl-patch/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/update-api-object-kubectl-patch/#use-a-strategic-merge-patch-to-update-a-deployment"><strong>Use a strategic merge patch to update a Deployment</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/run-application/update-api-object-kubectl-patch/#notes-on-the-strategic-merge-patch"><strong>Notes on the strategic merge patch</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/run-application/update-api-object-kubectl-patch/#use-a-json-merge-patch-to-update-a-deployment"><strong>Use a JSON merge patch to update a Deployment</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/update-api-object-kubectl-patch/#alternate-forms-of-the-kubectl-patch-command"><strong>Alternate forms of the kubectl patch command</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/update-api-object-kubectl-patch/#summary"><strong>Summary</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/update-api-object-kubectl-patch/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Before you begin</h4><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using<a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <strong>kubectl version</strong>.</p><h4>Use a strategic merge patch to update a Deployment</h4><p>Here&#x27;s the configuration file for a Deployment that has two replicas. Each replica is a Pod that has one container:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>deploym                                                            |
| ent-patch-demo.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/">https://raw.githubusercontent.com/kubernetes/</a> |
| website/master/docs/tasks/run-application/deployment-patch-demo.yaml) |
+=======================================================================+
| <strong>apiVersion: apps/v1 <em># for versions before 1.9.0 use               |
| apps/v1beta2</em></strong>                                                       |
|                                                                       |
| <strong>kind: Deployment</strong>                                                  |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: patch-demo</strong>                                                  |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>replicas: 2</strong>                                                       |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>matchLabels:</strong>                                                      |
|                                                                       |
| <strong>app: nginx</strong>                                                        |
|                                                                       |
| <strong>template:</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: nginx</strong>                                                        |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: patch-demo-ctr</strong>                                            |
|                                                                       |
| <strong>image: nginx</strong>                                                      |
|                                                                       |
| <strong>tolerations:</strong>                                                      |
|                                                                       |
| <strong>- effect: NoSchedule</strong>                                              |
|                                                                       |
| <strong>key: dedicated</strong>                                                    |
|                                                                       |
| <strong>value: test-team</strong>                                                  |
+-----------------------------------------------------------------------+</p><p>Create the Deployment:</p><p><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/run-application/deployment-patch-demo.yaml">https://k8s.io/docs/tasks/run-application/deployment-patch-demo.yaml</a></strong></p><p>View the Pods associated with your Deployment:</p><p><strong>kubectl get pods</strong></p><p>The output shows that the Deployment has two Pods. The <strong>1/1</strong> indicates that each Pod has one container:</p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>patch-demo-28633765-670qr 1/1 Running 0 23s</strong></p><p><strong>patch-demo-28633765-j5qs3 1/1 Running 0 23s</strong></p><p>Make a note of the names of the running Pods. Later, you will see that these Pods get terminated and replaced by new ones.</p><p>At this point, each Pod has one Container that runs the nginx image. Now suppose you want each Pod to have two containers: one that runs nginx and one that runs redis.</p><p>Create a file named <strong>patch-file-containers.yaml</strong> that has this content:</p><p><strong>spec:</strong></p><p><strong>template:</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: patch-demo-ctr-2</strong></p><p><strong>image: redis</strong></p><p>Patch your Deployment:</p><p><strong>kubectl patch deployment patch-demo --patch &quot;$(cat patch-file-containers.yaml)&quot;</strong></p><p>View the patched Deployment:</p><p><strong>kubectl get deployment patch-demo --output yaml</strong></p><p>The output shows that the PodSpec in the Deployment has two Containers:</p><p><strong>containers:</strong></p><p><strong>- image: redis</strong></p><p><strong>imagePullPolicy: Always</strong></p><p><strong>name: patch-demo-ctr-2</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>- image: nginx</strong></p><p><strong>imagePullPolicy: Always</strong></p><p><strong>name: patch-demo-ctr</strong></p><p><strong>.<!-- -->..</strong></p><p>View the Pods associated with your patched Deployment:</p><p><strong>kubectl get pods</strong></p><p>The output shows that the running Pods have different names from the Pods that were running previously. The Deployment terminated the old Pods and created two new Pods that comply with the updated Deployment spec. The <strong>2/2</strong> indicates that each Pod has two Containers:</p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>patch-demo-1081991389-2wrn5 2/2 Running 0 1m</strong></p><p><strong>patch-demo-1081991389-jmg7b 2/2 Running 0 1m</strong></p><p>Take a closer look at one of the patch-demo Pods:</p><p><strong>kubectl get pod <code>&lt;your-pod-name&gt;</code> --output yaml</strong></p><p>The output shows that the Pod has two Containers: one running nginx and one running redis:</p><p><strong>containers:</strong></p><p><strong>- image: redis</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>- image: nginx</strong></p><p><strong>.<!-- -->..</strong></p><h5><strong>Notes on the strategic merge patch</strong></h5><p>The patch you did in the preceding exercise is called a strategic merge patch. Notice that the patch did not replace the <strong>containers</strong> list. Instead it added a new Container to the list. In other words, the list in the patch was merged with the existing list. This is not always what happens when you use a strategic merge patch on a list. In some cases, the list is replaced, not merged.</p><p>With a strategic merge patch, a list is either replaced or merged depending on its patch strategy. The patch strategy is specified by the value of the <strong>patchStrategy</strong> key in a field tag in the Kubernetes source code. For example, the <strong>Containers</strong> field of <strong>PodSpec</strong> struct has a <strong>patchStrategy</strong> of <strong>merge</strong>:</p><p><strong>type PodSpec struct {</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>Containers []Container <code>json:&quot;containers&quot; patchStrategy:&quot;merge&quot; patchMergeKey:&quot;name&quot; \...</code></strong></p><p>You can also see the patch strategy in the <a href="https://raw.githubusercontent.com/kubernetes/kubernetes/master/api/openapi-spec/swagger.json">OpenApi spec</a>:</p><p><strong>&quot;io.k8s.api.core.v1.PodSpec&quot;: {</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>&quot;containers&quot;: {</strong></p><p><strong>&quot;description&quot;: &quot;List of containers belonging to the pod. <!-- -->.<!-- -->..</strong></p><p><strong>},</strong></p><p><strong>&quot;x-kubernetes-patch-merge-key&quot;: &quot;name&quot;,</strong></p><p><strong>&quot;x-kubernetes-patch-strategy&quot;: &quot;merge&quot;</strong></p><p><strong>},</strong></p><p>And you can see the patch strategy in the <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#podspec-v1-core">Kubernetes API documentation</a>.</p><p>Create a file named <strong>patch-file-tolerations.yaml</strong> that has this content:</p><p><strong>spec:</strong></p><p><strong>template:</strong></p><p><strong>spec:</strong></p><p><strong>tolerations:</strong></p><p><strong>- effect: NoSchedule</strong></p><p><strong>key: disktype</strong></p><p><strong>value: ssd</strong></p><p>Patch your Deployment:</p><p><strong>kubectl patch deployment patch-demo --patch &quot;$(cat patch-file-tolerations.yaml)&quot;</strong></p><p>View the patched Deployment:</p><p><strong>kubectl get deployment patch-demo --output yaml</strong></p><p>The output shows that the PodSpec in the Deployment has only one Toleration:</p><p><strong>tolerations:</strong></p><p><strong>- effect: NoSchedule</strong></p><p><strong>key: disktype</strong></p><p><strong>value: ssd</strong></p><p>Notice that the <strong>tolerations</strong> list in the PodSpec was replaced, not merged. This is because the Tolerations field of PodSpec does not have a <strong>patchStrategy</strong> key in its field tag. So the strategic merge patch uses the default patch strategy, which is <strong>replace</strong>.</p><p><strong>type PodSpec struct {</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>Tolerations []Toleration <code>json:&quot;tolerations,omitempty&quot; protobuf:&quot;bytes,22,opt,name=tolerations&quot;</code></strong></p><h4>Use a JSON merge patch to update a Deployment</h4><p>A strategic merge patch is different from a <a href="https://tools.ietf.org/html/rfc6902">JSON merge patch</a>. With a JSON merge patch, if you want to update a list, you have to specify the entire new list. And the new list completely replaces the existing list.</p><p>The <strong>kubectl patch</strong> command has a <strong>type</strong> parameter that you can set to one of these values:</p><hr/><p>  Parameter value   Merge type
json              <a href="https://tools.ietf.org/html/rfc6902">JSON Patch, RFC 6902</a>
merge             <a href="https://tools.ietf.org/html/rfc7386">JSON Merge Patch, RFC 7386</a>
strategic         Strategic merge patch</p><hr/><p>For a comparison of JSON patch and JSON merge patch, see <a href="http://erosb.github.io/post/json-patch-vs-merge-patch/">JSON Patch and JSON Merge Patch</a>.</p><p>The default value for the <strong>type</strong> parameter is <strong>strategic</strong>. So in the preceding exercise, you did a strategic merge patch.</p><p>Next, do a JSON merge patch on your same Deployment. Create a file named <strong>patch-file-2.yaml</strong>that has this content:</p><p><strong>spec:</strong></p><p><strong>template:</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: patch-demo-ctr-3</strong></p><p><strong>image: gcr.io/google-samples/node-hello:1.0</strong></p><p>In your patch command, set <strong>type</strong> to <strong>merge</strong>:</p><p><strong>kubectl patch deployment patch-demo --type merge --patch &quot;$(cat patch-file-2.yaml)&quot;</strong></p><p>View the patched Deployment:</p><p><strong>kubectl get deployment patch-demo --output yaml</strong></p><p>The <strong>containers</strong> list that you specified in the patch has only one Container. The output shows that your list of one Container replaced the existing <strong>containers</strong> list.</p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- image: gcr.io/google-samples/node-hello:1.0</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>name: patch-demo-ctr-3</strong></p><p>List the running Pods:</p><p><strong>kubectl get pods</strong></p><p>In the output, you can see that the existing Pods were terminated, and new Pods were created. The <strong>1/1</strong> indicates that each new Pod is running only one Container.</p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>patch-demo-1307768864-69308 1/1 Running 0 1m</strong></p><p><strong>patch-demo-1307768864-c86dc 1/1 Running 0 1m</strong></p><h4>Alternate forms of the kubectl patch command</h4><p>The <strong>kubectl patch</strong> command takes YAML or JSON. It can take the patch as a file or directly on the command line.</p><p>Create a file named <strong>patch-file.json</strong> that has this content:</p><p><strong>{</strong></p><p><strong>&quot;spec&quot;: {</strong></p><p><strong>&quot;template&quot;: {</strong></p><p><strong>&quot;spec&quot;: {</strong></p><p><strong>&quot;containers&quot;: [</strong></p><p><strong>{</strong></p><p><strong>&quot;name&quot;: &quot;patch-demo-ctr-2&quot;,</strong></p><p><strong>&quot;image&quot;: &quot;redis&quot;</strong></p><p><strong>}</strong></p><p><strong>]</strong></p><p><strong>}</strong></p><p><strong>}</strong></p><p><strong>}</strong></p><p><strong>}</strong></p><p>The following commands are equivalent:</p><p><strong>kubectl patch deployment patch-demo --patch &quot;$(cat patch-file.yaml)&quot;</strong></p><p><strong>kubectl patch deployment patch-demo --patch $\&#x27;spec:<!-- -->\<!-- -->n template:<!-- -->\<!-- -->n spec:<!-- -->\<!-- -->n containers:<!-- -->\<!-- -->n - name: patch-demo-ctr-2<!-- -->\<!-- -->n image: redis\&#x27;</strong></p><p><strong>kubectl patch deployment patch-demo --patch &quot;$(cat patch-file.json)&quot;</strong></p><p><strong>kubectl patch deployment patch-demo --patch \&#x27;{&quot;spec&quot;: {&quot;template&quot;: {&quot;spec&quot;: {&quot;containers&quot;: <!-- -->[{&quot;name&quot;: &quot;patch-demo-ctr-2&quot;,&quot;image&quot;: &quot;redis&quot;}]<!-- -->}}}}\&#x27;</strong></p><h4>Summary</h4><p>In this exercise, you used <strong>kubectl patch</strong> to change the live configuration of a Deployment object. You did not change the configuration file that you originally used to create the Deployment object. Other commands for updating API objects include <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#annotate">kubectl annotate</a>, <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#edit">kubectl edit</a>, <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#replace">kubectl replace</a>,<a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#scale">kubectl scale</a>, and <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#apply">kubectl apply</a>.</p><h4>What&#x27;s next</h4><ul><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/overview/">Kubernetes Object Management</a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/imperative-command/">Managing Kubernetes Objects Using Imperative Commands</a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/imperative-config/">Imperative Management of Kubernetes Objects Using Configuration Files</a></li><li><a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/">Declarative Management of Kubernetes Objects Using Configuration Files</a></li></ul><h3>Scale a StatefulSet</h3><p>This page shows how to scale a StatefulSet.</p><ul><li><a href="https://kubernetes.io/docs/tasks/run-application/scale-stateful-set/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/scale-stateful-set/#use-kubectl-to-scale-statefulsets"><strong>Use kubectl to scale StatefulSets</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/run-application/scale-stateful-set/#kubectl-scale"><strong>kubectl scale</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/scale-stateful-set/#alternative-kubectl-apply--kubectl-edit--kubectl-patch"><strong>Alternative: kubectl apply / kubectl edit / kubectl patch</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/run-application/scale-stateful-set/#troubleshooting"><strong>Troubleshooting</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/run-application/scale-stateful-set/#scaling-down-doesnt-work-right"><strong>Scaling down doesn&#x27;t work right</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/run-application/scale-stateful-set/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Before you begin</h4><ul><li>StatefulSets are only available in Kubernetes version 1.5 or later.</li><li><strong>Not all stateful applications scale nicely.</strong> You need to understand your StatefulSets well before continuing. If you&#x27;re unsure, remember that it might not be safe to scale your StatefulSets.</li><li>You should perform scaling only when you&#x27;re sure that your stateful application cluster is completely healthy.</li></ul><h4>Use kubectl to scale StatefulSets</h4><p>Make sure you have <strong>kubectl</strong> upgraded to Kubernetes version 1.5 or later before continuing. If you&#x27;re unsure, run <strong>kubectl version</strong> and check <strong>Client Version</strong> for which kubectl you&#x27;re using.</p><h5>kubectl scale</h5><p>First, find the StatefulSet you want to scale. Remember, you need to first understand if you can scale it or not.</p><p><strong>kubectl get statefulsets <code>&lt;stateful-set-name&gt;</code></strong></p><p>Change the number of replicas of your StatefulSet:</p><p><strong>kubectl scale statefulsets <code>&lt;stateful-set-name&gt; --replicas=&lt;new-replicas&gt;</code></strong></p><h5><strong>Alternative: </strong>kubectl apply<strong> / </strong>kubectl edit<strong> / </strong>kubectl patch</h5><p>Alternatively, you can do <a href="https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#in-place-updates-of-resources">in-place updates</a> on your StatefulSets.</p><p>If your StatefulSet was initially created with <strong>kubectl apply</strong> or <strong>kubectl create --save-config</strong>, update <strong>.spec.replicas</strong> of the StatefulSet manifests, and then do a <strong>kubectl apply</strong>:</p><p><strong>kubectl apply -f <code>&lt;stateful-set-file-updated&gt;</code></strong></p><p>Otherwise, edit that field with <strong>kubectl edit</strong>:</p><p><strong>kubectl edit statefulsets <code>&lt;stateful-set-name&gt;</code></strong></p><p>Or use <strong>kubectl patch</strong>:</p><p><strong>kubectl patch statefulsets <code>&lt;stateful-set-name&gt; -p \&#x27;{&quot;spec&quot;:{&quot;replicas&quot;:&lt;new-replicas&gt;</code>}}\&#x27;</strong></p><h4>Troubleshooting</h4><h5><strong>Scaling down doesn&#x27;t work right</strong></h5><p>You cannot scale down a StatefulSet when any of the stateful Pods it manages is unhealthy. Scaling down only takes place after those stateful Pods become running and ready.</p><p>With a StatefulSet of size &gt; 1, if there is an unhealthy Pod, there is no way for Kubernetes to know (yet) if it is due to a permanent fault or a transient one (upgrade/maintenance/node reboot). If the Pod is unhealthy due to a permanent fault, scaling without correcting the fault may lead to a state where the StatefulSet membership drops below a certain minimum number of &quot;replicas&quot; that are needed to function correctly. This may cause your StatefulSet to become unavailable.</p><p>If the Pod is unhealthy due to a transient fault and the Pod might become available again, the transient error may interfere with your scale-up/scale-down operation. Some distributed databases have issues when nodes join and leave at the same time. It is better to reason about scaling operations at the application level in these cases, and perform scaling only when you&#x27;re sure that your stateful application cluster is completely healthy.</p><h4>What&#x27;s next</h4><p>Learn more about <a href="https://kubernetes.io/docs/tasks/manage-stateful-set/deleting-a-statefulset/">deleting a StatefulSet</a>.</p><h3>Delete a StatefulSet</h3><p>This task shows you how to delete a StatefulSet.</p><ul><li><a href="https://kubernetes.io/docs/tasks/run-application/delete-stateful-set/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/delete-stateful-set/#deleting-a-statefulset"><strong>Deleting a StatefulSet</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/run-application/delete-stateful-set/#persistent-volumes"><strong>Persistent Volumes</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/delete-stateful-set/#complete-deletion-of-a-statefulset"><strong>Complete deletion of a StatefulSet</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/delete-stateful-set/#force-deletion-of-statefulset-pods"><strong>Force deletion of StatefulSet pods</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/run-application/delete-stateful-set/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Before you begin</h4><ul><li>This task assumes you have an application running on your cluster represented by a StatefulSet.</li></ul><h4>Deleting a StatefulSet</h4><p>You can delete a StatefulSet in the same way you delete other resources in Kubernetes: use the <strong>kubectl delete</strong> command, and specify the StatefulSet either by file or by name.</p><p><strong>kubectl delete -f <code>&lt;file.yaml&gt;</code></strong></p><p><strong>kubectl delete statefulsets <code>&lt;statefulset-name&gt;</code></strong></p><p>You may need to delete the associated headless service separately after the StatefulSet itself is deleted.</p><p><strong>kubectl delete service <code>&lt;service-name&gt;</code></strong></p><p>Deleting a StatefulSet through kubectl will scale it down to 0, thereby deleting all pods that are a part of it. If you want to delete just the StatefulSet and not the pods, use <strong>--cascade=false</strong>.</p><p><strong>kubectl delete -f <code>&lt;file.yaml&gt;</code> --cascade=false</strong></p><p>By passing <strong>--cascade=false</strong> to <strong>kubectl delete</strong>, the Pods managed by the StatefulSet are left behind even after the StatefulSet object itself is deleted. If the pods have a label <strong>app=myapp</strong>, you can then delete them as follows:</p><p><strong>kubectl delete pods -l app=myapp</strong></p><h5><strong>Persistent Volumes</strong></h5><p>Deleting the Pods in a StatefulSet will not delete the associated volumes. This is to ensure that you have the chance to copy data off the volume before deleting it. Deleting the PVC after the pods have left the <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods">terminating state</a> might trigger deletion of the backing Persistent Volumes depending on the storage class and reclaim policy. You should never assume ability to access a volume after claim deletion.</p><p><strong>Note: Use caution when deleting a PVC, as it may lead to data loss.</strong></p><h5><strong>Complete deletion of a StatefulSet</strong></h5><p>To simply delete everything in a StatefulSet, including the associated pods, you can run a series of commands similar to the following:</p><p><strong>grace=$(kubectl get pods <code>&lt;stateful-set-pod&gt;</code> --template \&#x27;{{.spec.terminationGracePeriodSeconds}}\&#x27;)</strong></p><p><strong>kubectl delete statefulset -l app=myapp</strong></p><p><strong>sleep $grace</strong></p><p><strong>kubectl delete pvc -l app=myapp</strong></p><p>In the example above, the Pods have the label <strong>app=myapp</strong>; substitute your own label as appropriate.</p><h5><strong>Force deletion of StatefulSet pods</strong></h5><p>If you find that some pods in your StatefulSet are stuck in the &#x27;Terminating&#x27; or &#x27;Unknown&#x27; states for an extended period of time, you may need to manually intervene to forcefully delete the pods from the apiserver. This is a potentially dangerous task. Refer to <a href="https://kubernetes.io/docs/tasks/manage-stateful-set/delete-pods/">Deleting StatefulSet Pods</a> for details.</p><h4>What&#x27;s next</h4><p>Learn more about <a href="https://kubernetes.io/docs/tasks/run-application/force-delete-stateful-set-pod/">force deleting StatefulSet Pods</a>.</p><h3>Force Delete StatefulSet Pods</h3><p>This page shows how to delete Pods which are part of a stateful set, and explains the considerations to keep in mind when doing so.</p><ul><li><a href="https://kubernetes.io/docs/tasks/run-application/force-delete-stateful-set-pod/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/force-delete-stateful-set-pod/#statefulset-considerations"><strong>StatefulSet considerations</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/force-delete-stateful-set-pod/#delete-pods"><strong>Delete Pods</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/run-application/force-delete-stateful-set-pod/#force-deletion"><strong>Force Deletion</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/run-application/force-delete-stateful-set-pod/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Before you begin</h4><ul><li>This is a fairly advanced task and has the potential to violate some of the properties inherent to StatefulSet.</li><li>Before proceeding, make yourself familiar with the considerations enumerated below.</li></ul><h4>StatefulSet considerations</h4><p>In normal operation of a StatefulSet, there is <strong>never</strong> a need to force delete a StatefulSet Pod. The StatefulSet controller is responsible for creating, scaling and deleting members of the StatefulSet. It tries to ensure that the specified number of Pods from ordinal 0 through N-1 are alive and ready. StatefulSet ensures that, at any time, there is at most one Pod with a given identity running in a cluster. This is referred to as at most one semantics provided by a StatefulSet.</p><p>Manual force deletion should be undertaken with caution, as it has the potential to violate the at most one semantics inherent to StatefulSet. StatefulSets may be used to run distributed and clustered applications which have a need for a stable network identity and stable storage. These applications often have configuration which relies on an ensemble of a fixed number of members with fixed identities. Having multiple members with the same identity can be disastrous and may lead to data loss (e.g. split brain scenario in quorum-based systems).</p><h4>Delete Pods</h4><p>You can perform a graceful pod deletion with the following command:</p><p><strong>kubectl delete pods <code>&lt;pod&gt;</code></strong></p><p>For the above to lead to graceful termination, the Pod <strong>must not</strong> specify a <strong>pod.Spec.TerminationGracePeriodSeconds</strong> of 0. The practice of setting a <strong>pod.Spec.TerminationGracePeriodSeconds</strong> of 0 seconds is unsafe and strongly discouraged for StatefulSet Pods. Graceful deletion is safe and will ensure that the <a href="https://kubernetes.io/docs/user-guide/pods/#termination-of-pods">Pod shuts down gracefully</a> before the kubelet deletes the name from the apiserver.</p><p>Kubernetes (versions 1.5 or newer) will not delete Pods just because a Node is unreachable. The Pods running on an unreachable Node enter the &#x27;Terminating&#x27; or &#x27;Unknown&#x27; state after a <a href="https://kubernetes.io/docs/admin/node/#node-condition">timeout</a>. Pods may also enter these states when the user attempts graceful deletion of a Pod on an unreachable Node. The only ways in which a Pod in such a state can be removed from the apiserver are as follows:</p><ul><li>The Node object is deleted (either by you, or by the <a href="https://kubernetes.io/docs/admin/node">Node Controller</a>).</li><li>The kubelet on the unresponsive Node starts responding, kills the Pod and removes the entry from the apiserver.</li><li>Force deletion of the Pod by the user.</li></ul><p>The recommended best practice is to use the first or second approach. If a Node is confirmed to be dead (e.g. permanently disconnected from the network, powered down, etc), then delete the Node object. If the Node is suffering from a network partition, then try to resolve this or wait for it to resolve. When the partition heals, the kubelet will complete the deletion of the Pod and free up its name in the apiserver.</p><p>Normally, the system completes the deletion once the Pod is no longer running on a Node, or the Node is deleted by an administrator. You may override this by force deleting the Pod.</p><h5><strong>Force Deletion</strong></h5><p>Force deletions <strong>do not</strong> wait for confirmation from the kubelet that the Pod has been terminated. Irrespective of whether a force deletion is successful in killing a Pod, it will immediately free up the name from the apiserver. This would let the StatefulSet controller create a replacement Pod with that same identity; this can lead to the duplication of a still-running Pod, and if said Pod can still communicate with the other members of the StatefulSet, will violate the at most one semantics that StatefulSet is designed to guarantee.</p><p>When you force delete a StatefulSet pod, you are asserting that the Pod in question will never again make contact with other Pods in the StatefulSet and its name can be safely freed up for a replacement to be created.</p><p>If you want to delete a Pod forcibly using kubectl version &gt;= 1.5, do the following:</p><p><strong>kubectl delete pods <code>&lt;pod&gt;</code> --grace-period=0 --force</strong></p><p>If you&#x27;re using any version of kubectl &lt;= 1.4, you should omit the <strong>--force</strong> option and use:</p><p><strong>kubectl delete pods <code>&lt;pod&gt;</code> --grace-period=0</strong></p><p>Always perform force deletion of StatefulSet Pods carefully and with complete knowledge of the risks involved.</p><h4>What&#x27;s next</h4><p>Learn more about <a href="https://kubernetes.io/docs/tasks/manage-stateful-set/debugging-a-statefulset/">debugging a StatefulSet</a>.</p><h3>Perform Rolling Update Using a Replication Controller</h3><ul><li><a href="https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/#overview"><strong>Overview</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/#passing-a-configuration-file"><strong>Passing a configuration file</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/#examples"><strong>Examples</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/#updating-the-container-image"><strong>Updating the container image</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/#examples-1"><strong>Examples</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/#required-and-optional-fields"><strong>Required and optional fields</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/#walkthrough"><strong>Walkthrough</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/#troubleshooting"><strong>Troubleshooting</strong></a></li></ul><h4>Overview</h4><p><strong>Note</strong>: The preferred way to create a replicated application is to use a <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#deployment-v1beta1-apps">Deployment</a>, which in turn uses a <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#replicaset-v1beta1-extensions">ReplicaSet</a>. For more information, see <a href="https://kubernetes.io/docs/tasks/run-application/run-stateless-application-deployment/">Running a Stateless Application Using a Deployment</a>.</p><p>To update a service without an outage, <strong>kubectl</strong> supports what is called <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#rolling-update">rolling update</a>, which updates one pod at a time, rather than taking down the entire service at the same time. See the <a href="https://git.k8s.io/community/contributors/design-proposals/cli/simple-rolling-update.md">rolling update design document</a> for more information.</p><p>Note that <strong>kubectl rolling-update</strong> only supports Replication Controllers. However, if you deploy applications with Replication Controllers, consider switching them to <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployments</a>. A Deployment is a higher-level controller that automates rolling updates of applications declaratively, and therefore is recommended. If you still want to keep your Replication Controllers and use <strong>kubectl rolling-update</strong>, keep reading:</p><p>A rolling update applies changes to the configuration of pods being managed by a replication controller. The changes can be passed as a new replication controller configuration file; or, if only updating the image, a new container image can be specified directly.</p><p>A rolling update works by:</p><ol><li>Creating a new replication controller with the updated configuration.</li><li>Increasing/decreasing the replica count on the new and old controllers until the correct number of replicas is reached.</li><li>Deleting the original replication controller.</li></ol><p>Rolling updates are initiated with the <strong>kubectl rolling-update</strong> command:</p><p><strong>$ kubectl rolling-update NAME <!-- -->\</strong></p><p><strong>(<!-- -->[NEW_NAME]<!-- --> --image=IMAGE | -f FILE)</strong></p><h4>Passing a configuration file</h4><p>To initiate a rolling update using a configuration file, pass the new file to <strong>kubectl rolling-update</strong>:</p><p><strong>$ kubectl rolling-update NAME -f FILE</strong></p><p>The configuration file must:</p><ul><li>Specify a different <strong>metadata.name</strong> value.</li><li>Overwrite at least one common label in its <strong>spec.selector</strong> field.</li><li>Use the same <strong>metadata.namespace</strong>.</li></ul><p>Replication controller configuration files are described in <a href="https://kubernetes.io/docs/tutorials/stateless-application/run-stateless-ap-replication-controller/">Creating Replication Controllers</a>.</p><h5><strong>Examples</strong></h5><p><strong>// Update pods of frontend-v1 using new replication controller data in frontend-v2.json.</strong></p><p><strong>$ kubectl rolling-update frontend-v1 -f frontend-v2.json</strong></p><p><strong>// Update pods of frontend-v1 using JSON data passed into stdin.</strong></p><p><strong>$ cat frontend-v2.json | kubectl rolling-update frontend-v1 -f -</strong></p><h4>Updating the container image</h4><p>To update only the container image, pass a new image name and tag with the <strong>--image</strong> flag and (optionally) a new controller name:</p><p><strong>$ kubectl rolling-update NAME <!-- -->[NEW_NAME]<!-- --> --image=IMAGE:TAG</strong></p><p>The <strong>--image</strong> flag is only supported for single-container pods. Specifying <strong>--image</strong> with multi-container pods returns an error.</p><p>If no <strong>NEW_NAME</strong> is specified, a new replication controller is created with a temporary name. Once the rollout is complete, the old controller is deleted, and the new controller is updated to use the original name.</p><p>The update will fail if <strong>IMAGE:TAG</strong> is identical to the current value. For this reason, we recommend the use of versioned tags as opposed to values such as <strong>:latest</strong>. Doing a rolling update from <strong>image:latest</strong> to a new <strong>image:latest</strong> will fail, even if the image at that tag has changed. Moreover, the use of <strong>:latest</strong> is not recommended, see <a href="https://kubernetes.io/docs/concepts/configuration/overview/#container-images">Best Practices for Configuration</a> for more information.</p><h5><strong>Examples</strong></h5><p><strong>// Update the pods of frontend-v1 to frontend-v2</strong></p><p><strong>$ kubectl rolling-update frontend-v1 frontend-v2 --image=image:v2</strong></p><p><strong>// Update the pods of frontend, keeping the replication controller name</strong></p><p><strong>$ kubectl rolling-update frontend --image=image:v2</strong></p><h4>Required and optional fields</h4><p>Required fields are:</p><ul><li><strong>NAME</strong>: The name of the replication controller to update.</li></ul><p>as well as either:</p><ul><li><strong>-f FILE</strong>: A replication controller configuration file, in either JSON or YAML format. The configuration file must specify a new top-level <strong>id</strong> value and include at least one of the existing <strong>spec.selector</strong> key:value pairs. See the <a href="https://kubernetes.io/docs/tutorials/stateless-application/run-stateless-ap-replication-controller/#replication-controller-configuration-file">Run Stateless AP Replication Controller</a> page for details. \
\
or: \</li><li><strong>--image IMAGE:TAG</strong>: The name and tag of the image to update to. Must be different than the current image:tag currently specified.</li></ul><p>Optional fields are:</p><ul><li><strong>NEW_NAME</strong>: Only used in conjunction with <strong>--image</strong> (not with <strong>-f FILE</strong>). The name to assign to the new replication controller.</li><li><strong>--poll-interval DURATION</strong>: The time between polling the controller status after update. Valid units are <strong>ns</strong> (nanoseconds), <strong>us</strong> or <strong>µs</strong> (microseconds), <strong>ms</strong> (milliseconds), <strong>s</strong> (seconds), <strong>m</strong>(minutes), or <strong>h</strong> (hours). Units can be combined (e.g. <strong>1m30s</strong>). The default is <strong>3s</strong>.</li><li><strong>--timeout DURATION</strong>: The maximum time to wait for the controller to update a pod before exiting. Default is <strong>5m0s</strong>. Valid units are as described for <strong>--poll-interval</strong> above.</li><li><strong>--update-period DURATION</strong>: The time to wait between updating pods. Default is <strong>1m0s</strong>. Valid units are as described for <strong>--poll-interval</strong> above.</li></ul><p>Additional information about the <strong>kubectl rolling-update</strong> command is available from the <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#rolling-update"><strong>kubectl</strong> reference</a>.</p><h4>Walkthrough</h4><p>Let&#x27;s say you were running version 1.7.9 of nginx:</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: ReplicationController</strong></p><p><strong>metadata:</strong></p><p><strong>name: my-nginx</strong></p><p><strong>spec:</strong></p><p><strong>replicas: 5</strong></p><p><strong>template:</strong></p><p><strong>metadata:</strong></p><p><strong>labels:</strong></p><p><strong>app: nginx</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: nginx</strong></p><p><strong>image: nginx:1.7.9</strong></p><p><strong>ports:</strong></p><p><strong>- containerPort: 80</strong></p><p>To update to version 1.9.1, you can use <a href="https://git.k8s.io/community/contributors/design-proposals/cli/simple-rolling-update.md"><strong>kubectl rolling-update --image</strong></a> to specify the new image:</p><p><strong>$ kubectl rolling-update my-nginx --image=nginx:1.9.1</strong></p><p><strong>Created my-nginx-ccba8fbd8cc8160970f63f9a2696fc46</strong></p><p>In another window, you can see that <strong>kubectl</strong> added a <strong>deployment</strong> label to the pods, whose value is a hash of the configuration, to distinguish the new pods from the old:</p><p><strong>$ kubectl get pods -l app=nginx -L deployment</strong></p><p><strong>NAME READY STATUS RESTARTS AGE DEPLOYMENT</strong></p><p><strong>my-nginx-ccba8fbd8cc8160970f63f9a2696fc46-k156z 1/1 Running 0 1m ccba8fbd8cc8160970f63f9a2696fc46</strong></p><p><strong>my-nginx-ccba8fbd8cc8160970f63f9a2696fc46-v95yh 1/1 Running 0 35s ccba8fbd8cc8160970f63f9a2696fc46</strong></p><p><strong>my-nginx-divi2 1/1 Running 0 2h 2d1d7a8f682934a254002b56404b813e</strong></p><p><strong>my-nginx-o0ef1 1/1 Running 0 2h 2d1d7a8f682934a254002b56404b813e</strong></p><p><strong>my-nginx-q6all 1/1 Running 0 8m 2d1d7a8f682934a254002b56404b813e</strong></p><p><strong>kubectl rolling-update</strong> reports progress as it progresses:</p><p><strong>Scaling up my-nginx-ccba8fbd8cc8160970f63f9a2696fc46 from 0 to 3, scaling down my-nginx from 3 to 0 (keep 3 pods available, don\&#x27;t exceed 4 pods)</strong></p><p><strong>Scaling my-nginx-ccba8fbd8cc8160970f63f9a2696fc46 up to 1</strong></p><p><strong>Scaling my-nginx down to 2</strong></p><p><strong>Scaling my-nginx-ccba8fbd8cc8160970f63f9a2696fc46 up to 2</strong></p><p><strong>Scaling my-nginx down to 1</strong></p><p><strong>Scaling my-nginx-ccba8fbd8cc8160970f63f9a2696fc46 up to 3</strong></p><p><strong>Scaling my-nginx down to 0</strong></p><p><strong>Update succeeded. Deleting old controller: my-nginx</strong></p><p><strong>Renaming my-nginx-ccba8fbd8cc8160970f63f9a2696fc46 to my-nginx</strong></p><p><strong>replicationcontroller &quot;my-nginx&quot; rolling updated</strong></p><p>If you encounter a problem, you can stop the rolling update midway and revert to the previous version using <strong>--rollback</strong>:</p><p><strong>$ kubectl rolling-update my-nginx --rollback</strong></p><p><strong>Setting &quot;my-nginx&quot; replicas to 1</strong></p><p><strong>Continuing update with existing controller my-nginx.</strong></p><p><strong>Scaling up nginx from 1 to 1, scaling down my-nginx-ccba8fbd8cc8160970f63f9a2696fc46 from 1 to 0 (keep 1 pods available, don\&#x27;t exceed 2 pods)</strong></p><p><strong>Scaling my-nginx-ccba8fbd8cc8160970f63f9a2696fc46 down to 0</strong></p><p><strong>Update succeeded. Deleting my-nginx-ccba8fbd8cc8160970f63f9a2696fc46</strong></p><p><strong>replicationcontroller &quot;my-nginx&quot; rolling updated</strong></p><p>This is one example where the immutability of containers is a huge asset.</p><p>If you need to update more than just the image (e.g., command arguments, environment variables), you can create a new replication controller, with a new name and distinguishing label value, such as:</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: ReplicationController</strong></p><p><strong>metadata:</strong></p><p><strong>name: my-nginx-v4</strong></p><p><strong>spec:</strong></p><p><strong>replicas: 5</strong></p><p><strong>selector:</strong></p><p><strong>app: nginx</strong></p><p><strong>deployment: v4</strong></p><p><strong>template:</strong></p><p><strong>metadata:</strong></p><p><strong>labels:</strong></p><p><strong>app: nginx</strong></p><p><strong>deployment: v4</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: nginx</strong></p><p><strong>image: nginx:1.9.2</strong></p><p><strong>args: <!-- -->[&quot;nginx&quot;, &quot;-T&quot;]</strong></p><p><strong>ports:</strong></p><p><strong>- containerPort: 80</strong></p><p>and roll it out:</p><p><strong>$ kubectl rolling-update my-nginx -f ./nginx-rc.yaml</strong></p><p><strong>Created my-nginx-v4</strong></p><p><strong>Scaling up my-nginx-v4 from 0 to 5, scaling down my-nginx from 4 to 0 (keep 4 pods available, don\&#x27;t exceed 5 pods)</strong></p><p><strong>Scaling my-nginx-v4 up to 1</strong></p><p><strong>Scaling my-nginx down to 3</strong></p><p><strong>Scaling my-nginx-v4 up to 2</strong></p><p><strong>Scaling my-nginx down to 2</strong></p><p><strong>Scaling my-nginx-v4 up to 3</strong></p><p><strong>Scaling my-nginx down to 1</strong></p><p><strong>Scaling my-nginx-v4 up to 4</strong></p><p><strong>Scaling my-nginx down to 0</strong></p><p><strong>Scaling my-nginx-v4 up to 5</strong></p><p><strong>Update succeeded. Deleting old controller: my-nginx</strong></p><p><strong>replicationcontroller &quot;my-nginx-v4&quot; rolling updated</strong></p><h4>Troubleshooting</h4><p>If the <strong>timeout</strong> duration is reached during a rolling update, the operation will fail with some pods belonging to the new replication controller, and some to the original controller.</p><p>To continue the update from where it failed, retry using the same command.</p><p>To roll back to the original state before the attempted update, append the <strong>--rollback=true</strong> flag to the original command. This will revert all changes.</p><h3>Horizontal Pod Autoscaler</h3><ul><li><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#what-is-the-horizontal-pod-autoscaler"><strong>What is the Horizontal Pod Autoscaler?</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#how-does-the-horizontal-pod-autoscaler-work"><strong>How does the Horizontal Pod Autoscaler work?</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#api-object"><strong>API Object</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-horizontal-pod-autoscaler-in-kubectl"><strong>Support for Horizontal Pod Autoscaler in kubectl</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#autoscaling-during-rolling-update"><strong>Autoscaling during rolling update</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-cooldowndelay"><strong>Support for cooldown/delay</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-multiple-metrics"><strong>Support for multiple metrics</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-custom-metrics"><strong>Support for custom metrics</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#requirements"><strong>Requirements</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#further-reading"><strong>Further reading</strong></a></li></ul><p>This document describes the current state of the Horizontal Pod Autoscaler in Kubernetes.</p><h4>What is the Horizontal Pod Autoscaler?</h4><p>The Horizontal Pod Autoscaler automatically scales the number of pods in a replication controller, deployment or replica set based on observed CPU utilization (or, with <a href="https://git.k8s.io/community/contributors/design-proposals/instrumentation/custom-metrics-api.md">custom metrics</a> support, on some other application-provided metrics). Note that Horizontal Pod Autoscaling does not apply to objects that can&#x27;t be scaled, for example, DaemonSets.</p><p>The Horizontal Pod Autoscaler is implemented as a Kubernetes API resource and a controller. The resource determines the behavior of the controller. The controller periodically adjusts the number of replicas in a replication controller or deployment to match the observed average CPU utilization to the target specified by user.</p><h4>How does the Horizontal Pod Autoscaler work?</h4><p>The Horizontal Pod Autoscaler is implemented as a control loop, with a period controlled by the controller manager&#x27;s <strong>--horizontal-pod-autoscaler-sync-period</strong> flag (with a default value of 30 seconds).</p><p>During each period, the controller manager queries the resource utilization against the metrics specified in each HorizontalPodAutoscaler definition. The controller manager obtains the metrics from either the resource metrics API (for per-pod resource metrics), or the custom metrics API (for all other metrics).</p><ul><li>For per-pod resource metrics (like CPU), the controller fetches the metrics from the resource metrics API for each pod targeted by the HorizontalPodAutoscaler. Then, if a target utilization value is set, the controller calculates the utilization value as a percentage of the equivalent resource request on the containers in each pod. If a target raw value is set, the raw metric values are used directly. The controller then takes the mean of the utilization or the raw value (depending on the type of target specified) across all targeted pods, and produces a ratio used to scale the number of desired replicas.</li></ul><p>Please note that if some of the pod&#x27;s containers do not have the relevant resource request set, CPU utilization for the pod will not be defined and the autoscaler will not take any action for that metric. See the <a href="https://git.k8s.io/community/contributors/design-proposals/autoscaling/horizontal-pod-autoscaler.md#autoscaling-algorithm">autoscaling algorithm design document</a> for further details about how the autoscaling algorithm works.</p><ul><li>For per-pod custom metrics, the controller functions similarly to per-pod resource metrics, except that it works with raw values, not utilization values.</li><li>For object metrics, a single metric is fetched (which describes the object in question), and compared to the target value, to produce a ratio as above.</li></ul><p>The HorizontalPodAutoscaler controller can fetch metrics in two different ways: direct Heapster access, and REST client access.</p><p>When using direct Heapster access, the HorizontalPodAutoscaler queries Heapster directly through the API server&#x27;s service proxy subresource. Heapster needs to be deployed on the cluster and running in the kube-system namespace.</p><p>See <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-custom-metrics">Support for custom metrics</a> for more details on REST client access.</p><p>The autoscaler accesses corresponding replication controller, deployment or replica set by scale sub-resource. Scale is an interface that allows you to dynamically set the number of replicas and examine each of their current states. More details on scale sub-resource can be found <a href="https://git.k8s.io/community/contributors/design-proposals/autoscaling/horizontal-pod-autoscaler.md#scale-subresource">here</a>.</p><h4>API Object</h4><p>The Horizontal Pod Autoscaler is an API resource in the Kubernetes <strong>autoscaling</strong> API group. The current stable version, which only includes support for CPU autoscaling, can be found in the <strong>autoscaling/v1</strong> API version.</p><p>The beta version, which includes support for scaling on memory and custom metrics, can be found in <strong>autoscaling/v2beta1</strong>. The new fields introduced in <strong>autoscaling/v2beta1</strong> are preserved as annotations when working with <strong>autoscaling/v1</strong>.</p><p>More details about the API object can be found at <a href="https://git.k8s.io/community/contributors/design-proposals/autoscaling/horizontal-pod-autoscaler.md#horizontalpodautoscaler-object">HorizontalPodAutoscaler Object</a>.</p><h4>Support for Horizontal Pod Autoscaler in kubectl</h4><p>Horizontal Pod Autoscaler, like every API resource, is supported in a standard way by <strong>kubectl</strong>. We can create a new autoscaler using <strong>kubectl create</strong> command. We can list autoscalers by <strong>kubectl get hpa</strong> and get detailed description by <strong>kubectl describe hpa</strong>. Finally, we can delete an autoscaler using <strong>kubectl delete hpa</strong>.</p><p>In addition, there is a special <strong>kubectl autoscale</strong> command for easy creation of a Horizontal Pod Autoscaler. For instance, executing <strong>kubectl autoscale rc foo --min=2 --max=5 --cpu-percent=80</strong> will create an autoscaler for replication controller foo, with target CPU utilization set to <strong>80%</strong> and the number of replicas between 2 and 5. The detailed documentation of <strong>kubectl autoscale</strong> can be found <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#autoscale">here</a>.</p><h4>Autoscaling during rolling update</h4><p>Currently in Kubernetes, it is possible to perform a <a href="https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/">rolling update</a> by managing replication controllers directly, or by using the deployment object, which manages the underlying replica sets for you. Horizontal Pod Autoscaler only supports the latter approach: the Horizontal Pod Autoscaler is bound to the deployment object, it sets the size for the deployment object, and the deployment is responsible for setting sizes of underlying replica sets.</p><p>Horizontal Pod Autoscaler does not work with rolling update using direct manipulation of replication controllers, i.e. you cannot bind a Horizontal Pod Autoscaler to a replication controller and do rolling update (e.g. using <strong>kubectl rolling-update</strong>). The reason this doesn&#x27;t work is that when rolling update creates a new replication controller, the Horizontal Pod Autoscaler will not be bound to the new replication controller.</p><h4>Support for cooldown/delay</h4><p>When managing the scale of a group of replicas using the Horizontal Pod Autoscaler, it is possible that the number of replicas keeps fluctuating frequently due to the dynamic nature of the metrics evaluated. This is sometimes referred to as thrashing.</p><p>Starting from v1.6, a cluster operator can mitigate this problem by tuning the global HPA settings exposed as flags for the <strong>kube-controller-manager</strong> component:</p><ul><li><strong>--horizontal-pod-autoscaler-downscale-delay</strong>: The value for this option is a duration that specifies how long the autoscaler has to wait before another downscale operation can be performed after the current one has completed. The default value is 5 minutes (<strong>5m0s</strong>).</li><li><strong>--horizontal-pod-autoscaler-upscale-delay</strong>: The value for this option is a duration that specifies how long the autoscaler has to wait before another upscale operation can be performed after the current one has completed. The default value is 3 minutes (<strong>3m0s</strong>).</li></ul><p><strong>Note</strong>: When tuning these parameter values, a cluster operator should be aware of the possible consequences. If the delay (cooldown) value is set too long, there could be complaints that the Horizontal Pod Autoscaler is not responsive to workload changes. However, if the delay value is set too short, the scale of the replicas set may keep thrashing as usual.</p><h4>Support for multiple metrics</h4><p>Kubernetes 1.6 adds support for scaling based on multiple metrics. You can use the <strong>autoscaling/v2beta1</strong> API version to specify multiple metrics for the Horizontal Pod Autoscaler to scale on. Then, the Horizontal Pod Autoscaler controller will evaluate each metric, and propose a new scale based on that metric. The largest of the proposed scales will be used as the new scale.</p><h4>Support for custom metrics</h4><p><strong>Note</strong>: Kubernetes 1.2 added alpha support for scaling based on application-specific metrics using special annotations. Support for these annotations was removed in Kubernetes 1.6 in favor of the new autoscaling API. While the old method for collecting custom metrics is still available, these metrics will not be available for use by the Horizontal Pod Autoscaler, and the former annotations for specifying which custom metrics to scale on are no longer honored by the Horizontal Pod Autoscaler controller.</p><p>Kubernetes 1.6 adds support for making use of custom metrics in the Horizontal Pod Autoscaler. You can add custom metrics for the Horizontal Pod Autoscaler to use in the <strong>autoscaling/v2beta1</strong>API. Kubernetes then queries the new custom metrics API to fetch the values of the appropriate custom metrics.</p><h5><strong>Requirements</strong></h5><p>To use custom metrics with your Horizontal Pod Autoscaler, you must set the necessary configurations when deploying your cluster:</p><ul><li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/configure-aggregation-layer/">Enable the API aggregation layer</a> if you have not already done so.</li><li>Register your resource metrics API, your custom metrics API and, optionally, external metrics API with the API aggregation layer. All of these API servers must be running on your cluster.<ul><li>Resource Metrics API: You can use Heapster&#x27;s implementation of the resource metrics API, by running Heapster with its <strong>--api-server</strong> flag set to true.</li><li>Custom Metrics API: This must be provided by a separate component. To get started with boilerplate code, see the <a href="https://github.com/kubernetes-incubator/custom-metrics-apiserver">kubernetes-incubator/custom-metrics-apiserver</a> and the <a href="https://github.com/kubernetes/metrics">k8s.io/metrics</a> repositories.</li><li>External Metrics API: Starting from Kubernetes 1.10 you can use this API if you need to autoscale on metrics not related to any Kubernetes object. Similarly to Custom Metrics APIthis must be provided by a separate component.</li></ul></li><li>Set the appropriate flags for kube-controller-manager:<ul><li><strong>--horizontal-pod-autoscaler-use-rest-clients</strong> should be true.</li><li><strong>--kubeconfig <code>&lt;path-to-kubeconfig&gt; OR --master &lt;ip-address-of-apiserver&gt;</code></strong></li></ul></li></ul><p>Note that either the <strong>--master</strong> or <strong>--kubeconfig</strong> flag can be used; <strong>--master</strong> will override <strong>--kubeconfig</strong> if both are specified. These flags specify the location of the API aggregation layer, allowing the controller manager to communicate to the API server.</p><p>In Kubernetes 1.7, the standard aggregation layer that Kubernetes provides runs in-process with the kube-apiserver, so the target IP address can be found with <strong>kubectl get pods --selector k8s-app=kube-apiserver --namespace kube-system -o jsonpath=\&#x27;{.items<!-- -->[0]<!-- -->.status.podIP}\&#x27;</strong>.</p><h4>Further reading</h4><ul><li>Design documentation: <a href="https://git.k8s.io/community/contributors/design-proposals/autoscaling/horizontal-pod-autoscaler.md">Horizontal Pod Autoscaling</a>.</li><li>kubectl autoscale command: <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#autoscale">kubectl autoscale</a>.</li><li>Usage example of <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/">Horizontal Pod Autoscaler</a>.</li></ul><h3>Horizontal Pod Autoscaler Walkthrough</h3><ul><li><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#prerequisites"><strong>Prerequisites</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#step-one-run--expose-php-apache-server"><strong>Step One: Run &amp; expose php-apache server</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#step-two-create-horizontal-pod-autoscaler"><strong>Step Two: Create Horizontal Pod Autoscaler</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#step-three-increase-load"><strong>Step Three: Increase load</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#step-four-stop-load"><strong>Step Four: Stop load</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics"><strong>Autoscaling on multiple metrics and custom metrics</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-metrics-not-related-to-kubernetes-objects"><strong>Autoscaling on metrics not related to Kubernetes objects</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#appendix-horizontal-pod-autoscaler-status-conditions"><strong>Appendix: Horizontal Pod Autoscaler Status Conditions</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#appendix-other-possible-scenarios"><strong>Appendix: Other possible scenarios</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#creating-the-autoscaler-declaratively"><strong>Creating the autoscaler declaratively</strong></a></li></ul></li></ul><p>Horizontal Pod Autoscaler automatically scales the number of pods in a replication controller, deployment or replica set based on observed CPU utilization (or, with beta support, on some other, application-provided metrics).</p><p>This document walks you through an example of enabling Horizontal Pod Autoscaler for the php-apache server. For more information on how Horizontal Pod Autoscaler behaves, see the <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">Horizontal Pod Autoscaler user guide</a>.</p><h4>Prerequisites</h4><p>This example requires a running Kubernetes cluster and kubectl, version 1.2 or later. <a href="https://github.com/kubernetes/heapster">Heapster</a>monitoring needs to be deployed in the cluster as Horizontal Pod Autoscaler uses it to collect metrics (if you followed <a href="https://kubernetes.io/docs/getting-started-guides/gce/">getting started on GCE guide</a>, heapster monitoring will be turned-on by default).</p><p>To specify multiple resource metrics for a Horizontal Pod Autoscaler, you must have a Kubernetes cluster and kubectl at version 1.6 or later. Furthermore, in order to make use of custom metrics, your cluster must be able to communicate with the API server providing the custom metrics API. Finally, to use metrics not related to any Kubernetes object you must have a Kubernetes cluster at version 1.10 or later, and you must be able to communicate with the API server that provides the external metrics API. See the <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-custom-metrics">Horizontal Pod Autoscaler user guide</a> for more details.</p><h4>Step One: Run &amp; expose php-apache server</h4><p>To demonstrate Horizontal Pod Autoscaler we will use a custom docker image based on the php-apache image. The Dockerfile has the following content:</p><p><strong>FROM php:5-apache</strong></p><p><strong>ADD index.php /var/www/html/index.php</strong></p><p><strong>RUN chmod a+rx index.php</strong></p><p>It defines an index.php page which performs some CPU intensive computations:</p><p><strong>&lt;?php</strong></p><p><strong>$x = 0.0001;</strong></p><p><strong>for ($i = 0; $i &lt;= 1000000; $i++) {</strong></p><p><strong>$x += sqrt($x);</strong></p><p><strong>}</strong></p><p><strong>echo &quot;OK!&quot;;</strong></p><p><strong>?&gt;</strong></p><p>First, we will start a deployment running the image and expose it as a service:</p><p><strong>$ kubectl run php-apache --image=k8s.gcr.io/hpa-example --requests=cpu=200m --expose --port=80</strong></p><p><strong>service &quot;php-apache&quot; created</strong></p><p><strong>deployment &quot;php-apache&quot; created</strong></p><h4>Step Two: Create Horizontal Pod Autoscaler</h4><p>Now that the server is running, we will create the autoscaler using <a href="https://github.com/kubernetes/kubernetes/blob/master/docs/user-guide/kubectl/kubectl_autoscale.md">kubectl autoscale</a>. The following command will create a Horizontal Pod Autoscaler that maintains between 1 and 10 replicas of the Pods controlled by the php-apache deployment we created in the first step of these instructions. Roughly speaking, HPA will increase and decrease the number of replicas (via the deployment) to maintain an average CPU utilization across all Pods of 50% (since each pod requests 200 milli-cores by <a href="https://github.com/kubernetes/kubernetes/blob/master/docs/user-guide/kubectl/kubectl_run.md">kubectl run</a>, this means average CPU usage of 100 milli-cores). See <a href="https://git.k8s.io/community/contributors/design-proposals/autoscaling/horizontal-pod-autoscaler.md#autoscaling-algorithm">here</a> for more details on the algorithm.</p><p><strong>$ kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10</strong></p><p><strong>deployment &quot;php-apache&quot; autoscaled</strong></p><p>We may check the current status of autoscaler by running:</p><p><strong>$ kubectl get hpa</strong></p><p><strong>NAME REFERENCE TARGET MINPODS MAXPODS REPLICAS AGE</strong></p><p><strong>php-apache Deployment/php-apache/scale 0% / 50% 1 10 1 18s</strong></p><p>Please note that the current CPU consumption is 0% as we are not sending any requests to the server (the <strong>CURRENT</strong> column shows the average across all the pods controlled by the corresponding deployment).</p><h4>Step Three: Increase load</h4><p>Now, we will see how the autoscaler reacts to increased load. We will start a container, and send an infinite loop of queries to the php-apache service (please run it in a different terminal):</p><p><strong>$ kubectl run -i --tty load-generator --image=busybox /bin/sh</strong></p><p><strong>Hit enter for command prompt</strong></p><p><strong>$ while true; do wget -q -O- <a href="http://php-apache.default.svc.cluster.local">http://php-apache.default.svc.cluster.local</a>; done</strong></p><p>Within a minute or so, we should see the higher CPU load by executing:</p><p><strong>$ kubectl get hpa</strong></p><p><strong>NAME REFERENCE TARGET CURRENT MINPODS MAXPODS REPLICAS AGE</strong></p><p><strong>php-apache Deployment/php-apache/scale 305% / 50% 305% 1 10 1 3m</strong></p><p>Here, CPU consumption has increased to 305% of the request. As a result, the deployment was resized to 7 replicas:</p><p><strong>$ kubectl get deployment php-apache</strong></p><p><strong>NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE</strong></p><p><strong>php-apache 7 7 7 7 19m</strong></p><p><strong>Note</strong> Sometimes it may take a few minutes to stabilize the number of replicas. Since the amount of load is not controlled in any way it may happen that the final number of replicas will differ from this example.</p><h4>Step Four: Stop load</h4><p>We will finish our example by stopping the user load.</p><p>In the terminal where we created the container with <strong>busybox</strong> image, terminate the load generation by typing <strong><code>&lt;Ctrl&gt;</code> + C</strong>.</p><p>Then we will verify the result state (after a minute or so):</p><p><strong>$ kubectl get hpa</strong></p><p><strong>NAME REFERENCE TARGET MINPODS MAXPODS REPLICAS AGE</strong></p><p><strong>php-apache Deployment/php-apache/scale 0% / 50% 1 10 1 11m</strong></p><p><strong>$ kubectl get deployment php-apache</strong></p><p><strong>NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE</strong></p><p><strong>php-apache 1 1 1 1 27m</strong></p><p>Here CPU utilization dropped to 0, and so HPA autoscaled the number of replicas back down to 1.</p><p><strong>Note</strong> autoscaling the replicas may take a few minutes.</p><h4>Autoscaling on multiple metrics and custom metrics</h4><p>You can introduce additional metrics to use when autoscaling the <strong>php-apache</strong> Deployment by making use of the <strong>autoscaling/v2beta1</strong> API version.</p><p>First, get the YAML of your HorizontalPodAutoscaler in the <strong>autoscaling/v2beta1</strong> form:</p><p><strong>$ kubectl get hpa.v2beta1.autoscaling -o yaml &gt; /tmp/hpa-v2.yaml</strong></p><p>Open the <strong>/tmp/hpa-v2.yaml</strong> file in an editor, and you should see YAML which looks like this:</p><p><strong>apiVersion: autoscaling/v2beta1</strong></p><p><strong>kind: HorizontalPodAutoscaler</strong></p><p><strong>metadata:</strong></p><p><strong>name: php-apache</strong></p><p><strong>namespace: default</strong></p><p><strong>spec:</strong></p><p><strong>scaleTargetRef:</strong></p><p><strong>apiVersion: apps/v1beta1</strong></p><p><strong>kind: Deployment</strong></p><p><strong>name: php-apache</strong></p><p><strong>minReplicas: 1</strong></p><p><strong>maxReplicas: 10</strong></p><p><strong>metrics:</strong></p><p><strong>- type: Resource</strong></p><p><strong>resource:</strong></p><p><strong>name: cpu</strong></p><p><strong>targetAverageUtilization: 50</strong></p><p><strong>status:</strong></p><p><strong>observedGeneration: 1</strong></p><p><strong>lastScaleTime: <code>&lt;some-time&gt;</code></strong></p><p><strong>currentReplicas: 1</strong></p><p><strong>desiredReplicas: 1</strong></p><p><strong>currentMetrics:</strong></p><p><strong>- type: Resource</strong></p><p><strong>resource:</strong></p><p><strong>name: cpu</strong></p><p><strong>currentAverageUtilization: 0</strong></p><p><strong>currentAverageValue: 0</strong></p><p>Notice that the <strong>targetCPUUtilizationPercentage</strong> field has been replaced with an array called <strong>metrics</strong>. The CPU utilization metric is a resource metric, since it is represented as a percentage of a resource specified on pod containers. Notice that you can specify other resource metrics besides CPU. By default, the only other supported resource metric is memory. These resources do not change names from cluster to cluster, and should always be available, as long as Heapster is deployed.</p><p>You can also specify resource metrics in terms of direct values, instead of as percentages of the requested value. To do so, use the <strong>targetAverageValue</strong> field instead of the <strong>targetAverageUtilization</strong> field.</p><p>There are two other types of metrics, both of which are considered custom metrics: pod metrics and object metrics. These metrics may have names which are cluster specific, and require a more advanced cluster monitoring setup.</p><p>The first of these alternative metric types is pod metrics. These metrics describe pods, and are averaged together across pods and compared with a target value to determine the replica count. They work much like resource metrics, except that they only have the <strong>targetAverageValue</strong> field.</p><p>Pod metrics are specified using a metric block like this:</p><p><strong>type: Pods</strong></p><p><strong>pods:</strong></p><p><strong>metricName: packets-per-second</strong></p><p><strong>targetAverageValue: 1k</strong></p><p>The second alternative metric type is object metrics. These metrics describe a different object in the same namespace, instead of describing pods. Note that the metrics are not fetched from the object -- they simply describe it. Object metrics do not involve averaging, and look like this:</p><p><strong>type: Object</strong></p><p><strong>object:</strong></p><p><strong>metricName: requests-per-second</strong></p><p><strong>target:</strong></p><p><strong>apiVersion: extensions/v1beta1</strong></p><p><strong>kind: Ingress</strong></p><p><strong>name: main-route</strong></p><p><strong>targetValue: 2k</strong></p><p>If you provide multiple such metric blocks, the HorizontalPodAutoscaler will consider each metric in turn. The HorizontalPodAutoscaler will calculate proposed replica counts for each metric, and then choose the one with the highest replica count.</p><p>For example, if you had your monitoring system collecting metrics about network traffic, you could update the definition above using <strong>kubectl edit</strong> to look like this:</p><p><strong>apiVersion: autoscaling/v2beta1</strong></p><p><strong>kind: HorizontalPodAutoscaler</strong></p><p><strong>metadata:</strong></p><p><strong>name: php-apache</strong></p><p><strong>namespace: default</strong></p><p><strong>spec:</strong></p><p><strong>scaleTargetRef:</strong></p><p><strong>apiVersion: apps/v1beta1</strong></p><p><strong>kind: Deployment</strong></p><p><strong>name: php-apache</strong></p><p><strong>minReplicas: 1</strong></p><p><strong>maxReplicas: 10</strong></p><p><strong>metrics:</strong></p><p><strong>- type: Resource</strong></p><p><strong>resource:</strong></p><p><strong>name: cpu</strong></p><p><strong>targetAverageUtilization: 50</strong></p><p><strong>- type: Pods</strong></p><p><strong>pods:</strong></p><p><strong>metricName: packets-per-second</strong></p><p><strong>targetAverageValue: 1k</strong></p><p><strong>- type: Object</strong></p><p><strong>object:</strong></p><p><strong>metricName: requests-per-second</strong></p><p><strong>target:</strong></p><p><strong>apiVersion: extensions/v1beta1</strong></p><p><strong>kind: Ingress</strong></p><p><strong>name: main-route</strong></p><p><strong>targetValue: 10k</strong></p><p><strong>status:</strong></p><p><strong>observedGeneration: 1</strong></p><p><strong>lastScaleTime: <code>&lt;some-time&gt;</code></strong></p><p><strong>currentReplicas: 1</strong></p><p><strong>desiredReplicas: 1</strong></p><p><strong>currentMetrics:</strong></p><p><strong>- type: Resource</strong></p><p><strong>resource:</strong></p><p><strong>name: cpu</strong></p><p><strong>currentAverageUtilization: 0</strong></p><p><strong>currentAverageValue: 0</strong></p><p>Then, your HorizontalPodAutoscaler would attempt to ensure that each pod was consuming roughly 50% of its requested CPU, serving 1000 packets per second, and that all pods behind the main-route Ingress were serving a total of 10000 requests per second.</p><h5><strong>Autoscaling on metrics not related to Kubernetes objects</strong></h5><p>Applications running on Kubernetes may need to autoscale based on metrics that don&#x27;t have an obvious relationship to any object in the Kubernetes cluster, such as metrics describing a hosted service with no direct correlation to Kubernetes namespaces. In Kubernetes 1.10 and later, you can address this use case with external metrics.</p><p>Using external metrics requires a certain level of knowledge of your monitoring system, and it requires a cluster monitoring setup similar to one required for using custom metrics. With external metrics, you can autoscale based on any metric available in your monitoring system by providing a <strong>metricName</strong> field in your HorizontalPodAutoscaler manifest. Additionally you can use a <strong>metricSelector</strong> field to limit which metrics&#x27; time series you want to use for autoscaling. If multiple time series are matched by <strong>metricSelector</strong>, the sum of their values is used by the HorizontalPodAutoscaler.</p><p>For example if your application processes tasks from a hosted queue service, you could add the following section to your HorizontalPodAutoscaler manifest to specify that you need one worker per 30 outstanding tasks.</p><p><strong>- type: External</strong></p><p><strong>external:</strong></p><p><strong>metricName: queue_messages_ready</strong></p><p><strong>metricSelector:</strong></p><p><strong>matchLabels:</strong></p><p><strong>queue: worker_tasks</strong></p><p><strong>targetAverageValue: 30</strong></p><p>If your metric describes work or resources that can be divided between autoscaled pods the <strong>targetAverageValue</strong> field describes how much of that work each pod can handle. Instead of using the <strong>targetAverageValue</strong> field, you could use the <strong>targetValue</strong> to define a desired value of your external metric.</p><h4>Appendix: Horizontal Pod Autoscaler Status Conditions</h4><p>When using the <strong>autoscaling/v2beta1</strong> form of the HorizontalPodAutoscaler, you will be able to seestatus conditions set by Kubernetes on the HorizontalPodAutoscaler. These status conditions indicate whether or not the HorizontalPodAutoscaler is able to scale, and whether or not it is currently restricted in any way.</p><p>The conditions appear in the <strong>status.conditions</strong> field. To see the conditions affecting a HorizontalPodAutoscaler, we can use <strong>kubectl describe hpa</strong>:</p><p><strong>$ kubectl describe hpa cm-test</strong></p><p><strong>Name: cm-test</strong></p><p><strong>Namespace: prom</strong></p><p><strong>Labels: <code>&lt;none&gt;</code></strong></p><p><strong>Annotations: <code>&lt;none&gt;</code></strong></p><p><strong>CreationTimestamp: Fri, 16 Jun 2017 18:09:22 +0000</strong></p><p><strong>Reference: ReplicationController/cm-test</strong></p><p><strong>Metrics: ( current / target )</strong></p><p><strong>&quot;http_requests&quot; on pods: 66m / 500m</strong></p><p><strong>Min replicas: 1</strong></p><p><strong>Max replicas: 4</strong></p><p><strong>ReplicationController pods: 1 current / 1 desired</strong></p><p><strong>Conditions:</strong></p><p><strong>Type Status Reason Message</strong></p><p><strong>---- ------ ------ -------</strong></p><p><strong>AbleToScale True ReadyForNewScale the last scale time was sufficiently old as to warrant a new scale</strong></p><p><strong>ScalingActive True ValidMetricFound the HPA was able to successfully calculate a replica count from pods metric http_requests</strong></p><p><strong>ScalingLimited False DesiredWithinRange the desired replica count is within the acceptable range</strong></p><p><strong>Events:</strong></p><p>For this HorizontalPodAutoscaler, we can see several conditions in a healthy state. The first, <strong>AbleToScale</strong>, indicates whether or not the HPA is able to fetch and update scales, as well as whether or not any backoff-related conditions would prevent scaling. The second, <strong>ScalingActive</strong>, indicates whether or not the HPA is enabled (i.e. the replica count of the target is not zero) and is able to calculate desired scales. When it is <strong>False</strong>, it generally indicates problems with fetching metrics. Finally, the last condition, <strong>ScalingLimited</strong>, indicates that the desired scale was capped by the maximum or minimum of the HorizontalPodAutoscaler. This is an indication that you may wish to raise or lower the minimum or maximum replica count constraints on your HorizontalPodAutoscaler.</p><h4>Appendix: Other possible scenarios</h4><h5><strong>Creating the autoscaler declaratively</strong></h5><p>Instead of using <strong>kubectl autoscale</strong> command to create a HorizontalPodAutoscaler imperatively we can use the following file to create it declaratively:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>hpa-php-apache.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kube">https://raw.githubusercontent.com/kube</a>     |
| rnetes/website/master/docs/tasks/run-application/hpa-php-apache.yaml) |
+=======================================================================+
| <strong>apiVersion: autoscaling/v1</strong>                                        |
|                                                                       |
| <strong>kind: HorizontalPodAutoscaler</strong>                                     |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: php-apache</strong>                                                  |
|                                                                       |
| <strong>namespace: default</strong>                                                |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>scaleTargetRef:</strong>                                                   |
|                                                                       |
| <strong>apiVersion: apps/v1beta1</strong>                                          |
|                                                                       |
| <strong>kind: Deployment</strong>                                                  |
|                                                                       |
| <strong>name: php-apache</strong>                                                  |
|                                                                       |
| <strong>minReplicas: 1</strong>                                                    |
|                                                                       |
| <strong>maxReplicas: 10</strong>                                                   |
|                                                                       |
| <strong>targetCPUUtilizationPercentage: 50</strong>                                |
+-----------------------------------------------------------------------+</p><p>We will create the autoscaler by executing the following command:</p><p><strong>$ kubectl create -f <a href="https://k8s.io/docs/tasks/run-application/hpa-php-apache.yaml">https://k8s.io/docs/tasks/run-application/hpa-php-apache.yaml</a></strong></p><p><strong>horizontalpodautoscaler &quot;php-apache&quot; created</strong></p><h3>Specifying a Disruption Budget for your Application</h3><p>This page shows how to limit the number of concurrent disruptions that your application experiences, allowing for higher availability while permitting the cluster administrator to manage the clusters nodes.</p><ul><li><a href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/#protecting-an-application-with-a-poddisruptionbudget"><strong>Protecting an Application with a PodDisruptionBudget</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/#identify-an-application-to-protect"><strong>Identify an Application to Protect</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/#think-about-how-your-application-reacts-to-disruptions"><strong>Think about how your application reacts to disruptions</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget"><strong>Specifying a PodDisruptionBudget</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/#create-the-pdb-object"><strong>Create the PDB object</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/#check-the-status-of-the-pdb"><strong>Check the status of the PDB</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/#arbitrary-controllers-and-selectors"><strong>Arbitrary Controllers and Selectors</strong></a></li></ul><h4>Before you begin</h4><ul><li>You are the owner of an application running on a Kubernetes cluster that requires high availability.</li><li>You should know how to deploy <a href="https://kubernetes.io/docs/tasks/run-application/run-stateless-application-deployment/">Replicated Stateless Applications</a> and/or <a href="https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/">Replicated Stateful Applications</a>.</li><li>You should have read about <a href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/">Pod Disruptions</a>.</li><li>You should confirm with your cluster owner or service provider that they respect Pod Disruption Budgets.</li></ul><h4>Protecting an Application with a PodDisruptionBudget</h4><ol><li>Identify what application you want to protect with a PodDisruptionBudget (PDB).</li><li>Think about how your application reacts to disruptions.</li><li>Create a PDB definition as a YAML file.</li><li>Create the PDB object from the YAML file.</li></ol><h4>Identify an Application to Protect</h4><p>The most common use case when you want to protect an application specified by one of the built-in Kubernetes controllers:</p><ul><li>Deployment</li><li>ReplicationController</li><li>ReplicaSet</li><li>StatefulSet</li></ul><p>In this case, make a note of the controller&#x27;s <strong>.spec.selector</strong>; the same selector goes into the PDBs <strong>.spec.selector</strong>.</p><p>You can also use PDBs with pods which are not controlled by one of the above controllers, or arbitrary groups of pods, but there are some restrictions, described in <a href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/#arbitrary-controllers-and-selectors">Arbitrary Controllers and Selectors</a>.</p><h4>Think about how your application reacts to disruptions</h4><p>Decide how many instances can be down at the same time for a short period due to a voluntary disruption.</p><ul><li>Stateless frontends:<ul><li>Concern: don&#x27;t reduce serving capacity by more than 10%.<ul><li>Solution: use PDB with minAvailable 90% for example.</li></ul></li></ul></li><li>Single-instance Stateful Application:<ul><li>Concern: do not terminate this application without talking to me.<ul><li>Possible Solution 1: Do not use a PDB and tolerate occasional downtime.</li><li>Possible Solution 2: Set PDB with maxUnavailable=0. Have an understanding (outside of Kubernetes) that the cluster operator needs to consult you before termination. When the cluster operator contacts you, prepare for downtime, and then delete the PDB to indicate readiness for disruption. Recreate afterwards.</li></ul></li></ul></li><li>Multiple-instance Stateful application such as Consul, ZooKeeper, or etcd:<ul><li>Concern: Do not reduce number of instances below quorum, otherwise writes fail.<ul><li>Possible Solution 1: set maxUnavailable to 1 (works with varying scale of application).</li><li>Possible Solution 2: set minAvailable to quorum-size (e.g. 3 when scale is 5). (Allows more disruptions at once).</li></ul></li></ul></li><li>Restartable Batch Job:<ul><li>Concern: Job needs to complete in case of voluntary disruption.<ul><li>Possible solution: Do not create a PDB. The Job controller will create a replacement pod.</li></ul></li></ul></li></ul><h4>Specifying a PodDisruptionBudget</h4><p>A <strong>PodDisruptionBudget</strong> has three fields:</p><ul><li>A label selector <strong>.spec.selector</strong> to specify the set of pods to which it applies. This field is required.</li><li><strong>.spec.minAvailable</strong> which is a description of the number of pods from that set that must still be available after the eviction, even in the absence of the evicted pod. <strong>minAvailable</strong> can be either an absolute number or a percentage.</li><li><strong>.spec.maxUnavailable</strong> (available in Kubernetes 1.7 and higher) which is a description of the number of pods from that set that can be unavailable after the eviction. It can be either an absolute number or a percentage.</li></ul><p><strong>Note:</strong> For versions 1.8 and earlier: When creating a <strong>PodDisruptionBudget</strong> object using the <strong>kubectl</strong> command line tool, the <strong>minAvailable</strong> field has a default value of 1 if neither <strong>minAvailable</strong> nor <strong>maxUnavailable</strong> is specified.</p><p>You can specify only one of <strong>maxUnavailable</strong> and <strong>minAvailable</strong> in a single <strong>PodDisruptionBudget</strong>. <strong>maxUnavailable</strong> can only be used to control the eviction of pods that have an associated controller managing them. In the examples below, &quot;desired replicas&quot; is the <strong>scale</strong> of the controller managing the pods being selected by the <strong>PodDisruptionBudget</strong>.</p><p>Example 1: With a <strong>minAvailable</strong> of 5, evictions are be allowed as long as they leave behind 5 or more healthy pods among those selected by the PodDisruptionBudget&#x27;s <strong>selector</strong>.</p><p>Example 2: With a <strong>minAvailable</strong> of 30%, evictions are allowed as long as at least 30% of the number of desired replicas are healthy.</p><p>Example 3: With a <strong>maxUnavailable</strong> of 5, evictions are allowed as long as there are at most 5 unhealthy replicas among the total number of desired replicas.</p><p>Example 4: With a <strong>maxUnavailable</strong> of 30%, evictions are allowed as long as no more than 30% of the desired replicas are unhealthy.</p><p>In typical usage, a single budget would be used for a collection of pods managed by a controller---for example, the pods in a single ReplicaSet or StatefulSet.</p><p><strong>Note:</strong> A disruption budget does not truly guarantee that the specified number/percentage of pods will always be up. For example, a node that hosts a pod from the collection may fail when the collection is at the minimum size specified in the budget, thus bringing the number of available pods from the collection below the specified size. The budget can only protect against voluntary evictions, not all causes of unavailability.</p><p>A <strong>maxUnavailable</strong> of 0% (or 0) or a <strong>minAvailable</strong> of 100% (or equal to the number of replicas) may block node drains entirely. This is permitted as per the semantics of <strong>PodDisruptionBudget</strong>.</p><p>You can find examples of pod disruption budgets defined below. They match pods with the label<strong>app: zookeeper</strong>.</p><p>Example PDB Using minAvailable:</p><p><strong>apiVersion: policy/v1beta1</strong></p><p><strong>kind: PodDisruptionBudget</strong></p><p><strong>metadata:</strong></p><p><strong>name: zk-pdb</strong></p><p><strong>spec:</strong></p><p><strong>minAvailable: 2</strong></p><p><strong>selector:</strong></p><p><strong>matchLabels:</strong></p><p><strong>app: zookeeper</strong></p><p>Example PDB Using maxUnavailable (Kubernetes 1.7 or higher):</p><p><strong>apiVersion: policy/v1beta1</strong></p><p><strong>kind: PodDisruptionBudget</strong></p><p><strong>metadata:</strong></p><p><strong>name: zk-pdb</strong></p><p><strong>spec:</strong></p><p><strong>maxUnavailable: 1</strong></p><p><strong>selector:</strong></p><p><strong>matchLabels:</strong></p><p><strong>app: zookeeper</strong></p><p>For example, if the above <strong>zk-pdb</strong> object selects the pods of a StatefulSet of size 3, both specifications have the exact same meaning. The use of <strong>maxUnavailable</strong> is recommended as it automatically responds to changes in the number of replicas of the corresponding controller.</p><h4>Create the PDB object</h4><p>You can create the PDB object with a command like <strong>kubectl create -f mypdb.yaml</strong>.</p><p>You cannot update PDB objects. They must be deleted and re-created.</p><h4>Check the status of the PDB</h4><p>Use kubectl to check that your PDB is created.</p><p>Assuming you don&#x27;t actually have pods matching <strong>app: zookeeper</strong> in your namespace, then you&#x27;ll see something like this:</p><p><strong>$ kubectl get poddisruptionbudgets</strong></p><p><strong>NAME MIN-AVAILABLE ALLOWED-DISRUPTIONS AGE</strong></p><p><strong>zk-pdb 2 0 7s</strong></p><p>If there are matching pods (say, 3), then you would see something like this:</p><p><strong>$ kubectl get poddisruptionbudgets</strong></p><p><strong>NAME MIN-AVAILABLE ALLOWED-DISRUPTIONS AGE</strong></p><p><strong>zk-pdb 2 1 7s</strong></p><p>The non-zero value for <strong>ALLOWED-DISRUPTIONS</strong> means that the disruption controller has seen the pods, counted the matching pods, and update the status of the PDB.</p><p>You can get more information about the status of a PDB with this command:</p><p><strong>$ kubectl get poddisruptionbudgets zk-pdb -o yaml</strong></p><p><strong>apiVersion: policy/v1beta1</strong></p><p><strong>kind: PodDisruptionBudget</strong></p><p><strong>metadata:</strong></p><p><strong>creationTimestamp: 2017-08-28T02:38:26Z</strong></p><p><strong>generation: 1</strong></p><p><strong>name: zk-pdb</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>status:</strong></p><p><strong>currentHealthy: 3</strong></p><p><strong>desiredHealthy: 3</strong></p><p><strong>disruptedPods: null</strong></p><p><strong>disruptionsAllowed: 1</strong></p><p><strong>expectedPods: 3</strong></p><p><strong>observedGeneration: 1</strong></p><h4>Arbitrary Controllers and Selectors</h4><p>You can skip this section if you only use PDBs with the built-in application controllers (Deployment, ReplicationController, ReplicaSet, and StatefulSet), with the PDB selector matching the controller&#x27;s selector.</p><p>You can use a PDB with pods controlled by another type of controller, by an &quot;operator&quot;, or bare pods, but with these restrictions:</p><ul><li>only <strong>.spec.minAvailable</strong> can be used, not <strong>.spec.maxUnavailable</strong>.</li><li>only an integer value can be used with <strong>.spec.minAvailable</strong>, not a percentage.</li></ul><p>You can use a selector which selects a subset or superset of the pods belonging to a built-in controller. However, when there are multiple PDBs in a namespace, you must be careful not to create PDBs whose selectors overlap.</p><h2>Run Jobs</h2><h3>Parallel Processing using Expansions</h3><ul><li><a href="https://kubernetes.io/docs/tasks/job/parallel-processing-expansion/#example-multiple-job-objects-from-template-expansion"><strong>Example: Multiple Job Objects from Template Expansion</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/job/parallel-processing-expansion/#basic-template-expansion"><strong>Basic Template Expansion</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/job/parallel-processing-expansion/#multiple-template-parameters"><strong>Multiple Template Parameters</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/job/parallel-processing-expansion/#alternatives"><strong>Alternatives</strong></a></li></ul></li></ul><h4>Example: Multiple Job Objects from Template Expansion</h4><p>In this example, we will run multiple Kubernetes Jobs created from a common template. You may want to be familiar with the basic, non-parallel, use of <a href="https://kubernetes.io/docs/concepts/jobs/run-to-completion-finite-workloads/">Jobs</a> first.</p><h5><strong>Basic Template Expansion</strong></h5><p>First, download the following template of a job to a file called <strong>job.yaml</strong></p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>job.yaml</strong> ]<!-- -->(<a href="https://raw.git">https://raw.git</a>                                       |
| hubusercontent.com/kubernetes/website/master/docs/tasks/job/job.yaml) |
+=======================================================================+
| <strong>apiVersion: batch/v1</strong>                                              |
|                                                                       |
| <strong>kind: Job</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: process-item-$ITEM</strong>                                         |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>jobgroup: jobexample</strong>                                              |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>template:</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: jobexample</strong>                                                  |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>jobgroup: jobexample</strong>                                              |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: c</strong>                                                         |
|                                                                       |
| <strong>image: busybox</strong>                                                    |
|                                                                       |
| <strong>command: <!-- -->[&quot;sh&quot;, &quot;-c&quot;, &quot;echo Processing item $ITEM &amp;&amp; sleep   |
| 5&quot;]</strong>                                                               |
|                                                                       |
| <strong>restartPolicy: Never</strong>                                              |
+-----------------------------------------------------------------------+</p><p>Unlike a pod template, our job template is not a Kubernetes API type. It is just a yaml representation of a Job object that has some placeholders that need to be filled in before it can be used. The <strong>$ITEM</strong> syntax is not meaningful to Kubernetes.</p><p>In this example, the only processing the container does is to <strong>echo</strong> a string and sleep for a bit. In a real use case, the processing would be some substantial computation, such as rendering a frame of a movie, or processing a range of rows in a database. The <strong>$ITEM</strong> parameter would specify for example, the frame number or the row range.</p><p>This Job and its Pod template have a label: <strong>jobgroup=jobexample</strong>. There is nothing special to the system about this label. This label makes it convenient to operate on all the jobs in this group at once. We also put the same label on the pod template so that we can check on all Pods of these Jobs with a single command. After the job is created, the system will add more labels that distinguish one Job&#x27;s pods from another Job&#x27;s pods. Note that the label key <strong>jobgroup</strong> is not special to Kubernetes. You can pick your own label scheme.</p><p>Next, expand the template into multiple files, one for each item to be processed.</p><p><strong><em># Expand files into a temporary directory</em></strong></p><p><strong>$ mkdir ./jobs</strong></p><p><strong>$ for i in apple banana cherry</strong></p><p><strong>do</strong></p><p><strong>cat job.yaml | sed &quot;s/<!-- -->\<!-- -->$ITEM/$i/&quot; &gt; ./jobs/job-$i.yaml</strong></p><p><strong>done</strong></p><p>Check if it worked:</p><p><strong>$ ls jobs/</strong></p><p><strong>job-apple.yaml</strong></p><p><strong>job-banana.yaml</strong></p><p><strong>job-cherry.yaml</strong></p><p>Here, we used <strong>sed</strong> to replace the string <strong>$ITEM</strong> with the loop variable. You could use any type of template language (jinja2, erb) or write a program to generate the Job objects.</p><p>Next, create all the jobs with one kubectl command:</p><p><strong>$ kubectl create -f ./jobs</strong></p><p><strong>job &quot;process-item-apple&quot; created</strong></p><p><strong>job &quot;process-item-banana&quot; created</strong></p><p><strong>job &quot;process-item-cherry&quot; created</strong></p><p>Now, check on the jobs:</p><p><strong>$ kubectl get jobs -l jobgroup=jobexample</strong></p><p><strong>NAME DESIRED SUCCESSFUL AGE</strong></p><p><strong>process-item-apple 1 1 31s</strong></p><p><strong>process-item-banana 1 1 31s</strong></p><p><strong>process-item-cherry 1 1 31s</strong></p><p>Here we use the <strong>-l</strong> option to select all jobs that are part of this group of jobs. (There might be other unrelated jobs in the system that we do not care to see.)</p><p>We can check on the pods as well using the same label selector:</p><p><strong>$ kubectl get pods -l jobgroup=jobexample</strong></p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>process-item-apple-kixwv 0/1 Completed 0 4m</strong></p><p><strong>process-item-banana-wrsf7 0/1 Completed 0 4m</strong></p><p><strong>process-item-cherry-dnfu9 0/1 Completed 0 4m</strong></p><p>There is not a single command to check on the output of all jobs at once, but looping over all the pods is pretty easy:</p><p><strong>$ for p in $(kubectl get pods -l jobgroup=jobexample -o name)</strong></p><p><strong>do</strong></p><p><strong>kubectl logs $p</strong></p><p><strong>done</strong></p><p><strong>Processing item apple</strong></p><p><strong>Processing item banana</strong></p><p><strong>Processing item cherry</strong></p><h5><strong>Multiple Template Parameters</strong></h5><p>In the first example, each instance of the template had one parameter, and that parameter was also used as a label. However label keys are limited in <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set">what characters they can contain</a>.</p><p>This slightly more complex example uses the jinja2 template language to generate our objects. We will use a one-line python script to convert the template to a file.</p><p>First, copy and paste the following template of a Job object, into a file called <strong>job.yaml.jinja2</strong>:</p><p><strong>{%- set params = [{ &quot;name&quot;: &quot;apple&quot;, &quot;url&quot;: &quot;<a href="http://www.orangepippin.com/apples%22">http://www.orangepippin.com/apples&quot;</a>, },</strong></p><p><strong>{ &quot;name&quot;: &quot;banana&quot;, &quot;url&quot;: &quot;<a href="https://en.wikipedia.org/wiki/Banana%22">https://en.wikipedia.org/wiki/Banana&quot;</a>, },</strong></p><p><strong>{ &quot;name&quot;: &quot;raspberry&quot;, &quot;url&quot;: &quot;<a href="https://www.raspberrypi.org/%22">https://www.raspberrypi.org/&quot;</a> }]</strong></p><p><strong>%}</strong></p><p><strong>{%- for p in params %}</strong></p><p><strong>{%- set name = p<!-- -->[&quot;name&quot;]<!-- --> %}</strong></p><p><strong>{%- set url = p<!-- -->[&quot;url&quot;]<!-- --> %}</strong></p><p><strong>apiVersion: batch/v1</strong></p><p><strong>kind: Job</strong></p><p><strong>metadata:</strong></p><p><strong>name: jobexample-{{ name }}</strong></p><p><strong>labels:</strong></p><p><strong>jobgroup: jobexample</strong></p><p><strong>spec:</strong></p><p><strong>template:</strong></p><p><strong>metadata:</strong></p><p><strong>name: jobexample</strong></p><p><strong>labels:</strong></p><p><strong>jobgroup: jobexample</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: c</strong></p><p><strong>image: busybox</strong></p><p><strong>command: <!-- -->[&quot;sh&quot;, &quot;-c&quot;, &quot;echo Processing URL {{ url }} &amp;&amp; sleep 5&quot;]</strong></p><p><strong>restartPolicy: Never</strong></p><p><strong>---</strong></p><p><strong>{%- endfor %}</strong></p><p>The above template defines parameters for each job object using a list of python dicts (lines 1-4). Then a for loop emits one job yaml object for each set of parameters (remaining lines). We take advantage of the fact that multiple yaml documents can be concatenated with the <strong>---</strong> separator (second to last line). .) We can pipe the output directly to kubectl to create the objects.</p><p>You will need the jinja2 package if you do not already have it: <strong>pip install --user jinja2</strong>. Now, use this one-line python program to expand the template:</p><p><strong>alias render_template=\&#x27;python -c &quot;from jinja2 import Template; import sys; print(Template(sys.stdin.read()).render());&quot;\&#x27;</strong></p><p>The output can be saved to a file, like this:</p><p><strong>cat job.yaml.jinja2 | render_template &gt; jobs.yaml</strong></p><p>Or sent directly to kubectl, like this:</p><p><strong>cat job.yaml.jinja2 | render_template | kubectl create -f -</strong></p><h5><strong>Alternatives</strong></h5><p>If you have a large number of job objects, you may find that:</p><ul><li>Even using labels, managing so many Job objects is cumbersome.</li><li>You exceed resource quota when creating all the Jobs at once, and do not want to wait to create them incrementally.</li><li>Very large numbers of jobs created at once overload the Kubernetes apiserver, controller, or scheduler.</li></ul><p>In this case, you can consider one of the other <a href="https://kubernetes.io/docs/concepts/jobs/run-to-completion-finite-workloads/#job-patterns">job patterns</a>.</p><h3>Coarse Parallel Processing Using a Work Queue</h3><ul><li><a href="https://kubernetes.io/docs/tasks/job/coarse-parallel-processing-work-queue/#example-job-with-work-queue-with-pod-per-work-item"><strong>Example: Job with Work Queue with Pod Per Work Item</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/job/coarse-parallel-processing-work-queue/#starting-a-message-queue-service"><strong>Starting a message queue service</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/job/coarse-parallel-processing-work-queue/#testing-the-message-queue-service"><strong>Testing the message queue service</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/job/coarse-parallel-processing-work-queue/#filling-the-queue-with-tasks"><strong>Filling the Queue with tasks</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/job/coarse-parallel-processing-work-queue/#create-an-image"><strong>Create an Image</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/job/coarse-parallel-processing-work-queue/#defining-a-job"><strong>Defining a Job</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/job/coarse-parallel-processing-work-queue/#running-the-job"><strong>Running the Job</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/job/coarse-parallel-processing-work-queue/#alternatives"><strong>Alternatives</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/job/coarse-parallel-processing-work-queue/#caveats"><strong>Caveats</strong></a></li></ul></li></ul><h4>Example: Job with Work Queue with Pod Per Work Item</h4><p>In this example, we will run a Kubernetes Job with multiple parallel worker processes. You may want to be familiar with the basic, non-parallel, use of <a href="https://kubernetes.io/docs/concepts/jobs/run-to-completion-finite-workloads/">Job</a> first.</p><p>In this example, as each pod is created, it picks up one unit of work from a task queue, completes it, deletes it from the queue, and exits.</p><p>Here is an overview of the steps in this example:</p><ol><li><strong>Start a message queue service.</strong> In this example, we use RabbitMQ, but you could use another one. In practice you would set up a message queue service once and reuse it for many jobs.</li><li><strong>Create a queue, and fill it with messages.</strong> Each message represents one task to be done. In this example, a message is just an integer that we will do a lengthy computation on.</li><li><strong>Start a Job that works on tasks from the queue</strong>. The Job starts several pods. Each pod takes one task from the message queue, processes it, and repeats until the end of the queue is reached.</li></ol><h5><strong>Starting a message queue service</strong></h5><p>This example uses RabbitMQ, but it should be easy to adapt to another AMQP-type message service.</p><p>In practice you could set up a message queue service once in a cluster and reuse it for many jobs, as well as for long-running services.</p><p>Start RabbitMQ as follows:</p><p><strong>$ kubectl create -f examples/celery-rabbitmq/rabbitmq-service.yaml</strong></p><p><strong>service &quot;rabbitmq-service&quot; created</strong></p><p><strong>$ kubectl create -f examples/celery-rabbitmq/rabbitmq-controller.yaml</strong></p><p><strong>replicationcontroller &quot;rabbitmq-controller&quot; created</strong></p><p>We will only use the rabbitmq part from the <a href="https://github.com/kubernetes/kubernetes/tree/release-1.3/examples/celery-rabbitmq">celery-rabbitmq example</a>.</p><h5><strong>Testing the message queue service</strong></h5><p>Now, we can experiment with accessing the message queue. We will create a temporary interactive pod, install some tools on it, and experiment with queues.</p><p>First create a temporary interactive Pod.</p><p><strong><em># Create a temporary interactive container</em></strong></p><p><strong>$ kubectl run -i --tty temp --image ubuntu:14.04</strong></p><p><strong>Waiting for pod default/temp-loe07 to be running, status is Pending, pod ready: false</strong></p><p><strong>.<!-- -->.. <!-- -->[ previous line repeats several times .. hit return when it stops ]<!-- --> <!-- -->.<!-- -->..</strong></p><p>Note that your pod name and command prompt will be different.</p><p>Next install the <strong>amqp-tools</strong> so we can work with message queues.</p><p><strong><em># Install some tools</em></strong></p><p><strong>root@temp-loe07:/# apt-get update</strong></p><p><strong>.<!-- -->... <!-- -->[ lots of output ]<!-- --> <!-- -->.<!-- -->...</strong></p><p><strong>root@temp-loe07:/# apt-get install -y curl ca-certificates amqp-tools python dnsutils</strong></p><p><strong>.<!-- -->... <!-- -->[ lots of output ]<!-- --> <!-- -->.<!-- -->...</strong></p><p>Later, we will make a docker image that includes these packages.</p><p>Next, we will check that we can discover the rabbitmq service:</p><p><strong># Note the rabbitmq-service has a DNS name, provided by Kubernetes:</strong></p><p><strong>root@temp-loe07:/# nslookup rabbitmq-service</strong></p><p><strong>Server: 10.0.0.10</strong></p><p><strong>Address: 10.0.0.10#53</strong></p><p><strong>Name: rabbitmq-service.default.svc.cluster.local</strong></p><p><strong>Address: 10.0.147.152</strong></p><p><strong># Your address will vary.</strong></p><p>If Kube-DNS is not setup correctly, the previous step may not work for you. You can also find the service IP in an env var:</p><p><strong># env | grep RABBIT | grep HOST</strong></p><p><strong>RABBITMQ_SERVICE_SERVICE_HOST=10.0.147.152</strong></p><p><strong># Your address will vary.</strong></p><p>Next we will verify we can create a queue, and publish and consume messages.</p><p><strong><em># In the next line, rabbitmq-service is the hostname where the rabbitmq-service</em></strong></p><p><strong><em># can be reached. 5672 is the standard port for rabbitmq.</em></strong></p><p><strong>root@temp-loe07:/# export BROKER_URL=amqp://guest:guest@rabbitmq-service:5672</strong></p><p><strong><em># If you could not resolve &quot;rabbitmq-service&quot; in the previous step,</em></strong></p><p><strong><em># then use this command instead:</em></strong></p><p><strong><em># root@temp-loe07:/# BROKER_URL=amqp://guest:guest@$RABBITMQ_SERVICE_SERVICE_HOST:5672</em></strong></p><p><strong><em># Now create a queue:</em></strong></p><p><strong>root@temp-loe07:/# /usr/bin/amqp-declare-queue --url=$BROKER_URL -q foo -d</strong></p><p><strong>foo</strong></p><p><strong><em># Publish one message to it:</em></strong></p><p><strong>root@temp-loe07:/# /usr/bin/amqp-publish --url=$BROKER_URL -r foo -p -b Hello</strong></p><p><strong><em># And get it back.</em></strong></p><p><strong>root@temp-loe07:/# /usr/bin/amqp-consume --url=$BROKER_URL -q foo -c 1 cat &amp;&amp; echo</strong></p><p><strong>Hello</strong></p><p><strong>root@temp-loe07:/#</strong></p><p>In the last command, the <strong>amqp-consume</strong> tool takes one message (<strong>-c 1</strong>) from the queue, and passes that message to the standard input of an arbitrary command. In this case, the program <strong>cat</strong>is just printing out what it gets on the standard input, and the echo is just to add a carriage return so the example is readable.</p><h5><strong>Filling the Queue with tasks</strong></h5><p>Now lets fill the queue with some &quot;tasks&quot;. In our example, our tasks are just strings to be printed.</p><p>In a practice, the content of the messages might be:</p><ul><li>names of files to that need to be processed</li><li>extra flags to the program</li><li>ranges of keys in a database table</li><li>configuration parameters to a simulation</li><li>frame numbers of a scene to be rendered</li></ul><p>In practice, if there is large data that is needed in a read-only mode by all pods of the Job, you will typically put that in a shared file system like NFS and mount that readonly on all the pods, or the program in the pod will natively read data from a cluster file system like HDFS.</p><p>For our example, we will create the queue and fill it using the amqp command line tools. In practice, you might write a program to fill the queue using an amqp client library.</p><p><strong>$ /usr/bin/amqp-declare-queue --url=$BROKER_URL -q job1 -d</strong></p><p><strong>job1</strong></p><p><strong>$ for f in apple banana cherry date fig grape lemon melon</strong></p><p><strong>do</strong></p><p><strong>/usr/bin/amqp-publish --url=$BROKER_URL -r job1 -p -b $f</strong></p><p><strong>done</strong></p><p>So, we filled the queue with 8 messages.</p><h5><strong>Create an Image</strong></h5><p>Now we are ready to create an image that we will run as a job.</p><p>We will use the <strong>amqp-consume</strong> utility to read the message from the queue and run our actual program. Here is a very simple example program:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<em>                                                                    |
| </em>worker.py** ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/website/m">https://raw.githubusercontent.com/kubernetes/website/m</a> |
| aster/docs/tasks/job/coarse-parallel-processing-work-queue/worker.py) |
+=======================================================================+
| <strong><em>#!/usr/bin/env python</em></strong>                                          |
|                                                                       |
| <strong><em># Just prints standard out and sleeps for 10 seconds.</em></strong>          |
|                                                                       |
| <strong>import sys</strong>                                                        |
|                                                                       |
| <strong>import time</strong>                                                       |
|                                                                       |
| <strong>print(&quot;Processing &quot; + sys.stdin.lines())</strong>                        |
|                                                                       |
| <strong>time.sleep(10)</strong>                                                    |
+-----------------------------------------------------------------------+</p><p>Now, build an image. If you are working in the source tree, then change directory to <strong>examples/job/work-queue-1</strong>. Otherwise, make a temporary directory, change to it, download the <a href="https://kubernetes.io/docs/tasks/job/coarse-parallel-processing-work-queue/Dockerfile?raw=true">Dockerfile</a>, and <a href="https://kubernetes.io/docs/tasks/job/coarse-parallel-processing-work-queue/worker.py?raw=true">worker.py</a>. In either case, build the image with this command:</p><p><strong>$ docker build -t job-wq-1 .</strong></p><p>For the <a href="https://hub.docker.com/">Docker Hub</a>, tag your app image with your username and push to the Hub with the below commands. Replace <strong><code>&lt;username&gt;</code></strong> with your Hub username.</p><p><strong>docker tag job-wq-1 <code>&lt;username&gt;</code>/job-wq-1</strong></p><p><strong>docker push <code>&lt;username&gt;</code>/job-wq-1</strong></p><p>If you are using <a href="https://cloud.google.com/tools/container-registry/">Google Container Registry</a>, tag your app image with your project ID, and push to GCR. Replace <strong><code>&lt;project&gt;</code></strong> with your project ID.</p><p><strong>docker tag job-wq-1 gcr.io/<code>&lt;project&gt;</code>/job-wq-1</strong></p><p><strong>gcloud docker -- push gcr.io/<code>&lt;project&gt;</code>/job-wq-1</strong></p><h5><strong>Defining a Job</strong></h5><p>Here is a job definition. You&#x27;ll need to make a copy of the Job and edit the image to match the name you used, and call it <strong>./job.yaml</strong>.</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>job.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/website/">https://raw.githubusercontent.com/kubernetes/website/</a> |
| master/docs/tasks/job/coarse-parallel-processing-work-queue/job.yaml) |
+=======================================================================+
| <strong>apiVersion: batch/v1</strong>                                              |
|                                                                       |
| <strong>kind: Job</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: job-wq-1</strong>                                                    |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>completions: 8</strong>                                                    |
|                                                                       |
| <strong>parallelism: 2</strong>                                                    |
|                                                                       |
| <strong>template:</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: job-wq-1</strong>                                                    |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: c</strong>                                                         |
|                                                                       |
| <strong>image: gcr.io/<code>&lt;project&gt;</code>/job-wq-1</strong>                                |
|                                                                       |
| <strong>env:</strong>                                                              |
|                                                                       |
| <strong>- name: BROKER_URL</strong>                                                |
|                                                                       |
| <strong>value: amqp://guest:guest@rabbitmq-service:5672</strong>                  |
|                                                                       |
| <strong>- name: QUEUE</strong>                                                     |
|                                                                       |
| <strong>value: job1</strong>                                                       |
|                                                                       |
| <strong>restartPolicy: OnFailure</strong>                                          |
+-----------------------------------------------------------------------+</p><p>In this example, each pod works on one item from the queue and then exits. So, the completion count of the Job corresponds to the number of work items done. So we set, <strong>.spec.completions: 8</strong> for the example, since we put 8 items in the queue.</p><h5><strong>Running the Job</strong></h5><p>So, now run the Job:</p><p><strong>kubectl create -f ./job.yaml</strong></p><p>Now wait a bit, then check on the job.</p><p><strong>$ kubectl describe jobs/job-wq-1</strong></p><p><strong>Name: job-wq-1</strong></p><p><strong>Namespace: default</strong></p><p><strong>Selector: controller-uid=41d75705-92df-11e7-b85e-fa163ee3c11f</strong></p><p><strong>Labels: controller-uid=41d75705-92df-11e7-b85e-fa163ee3c11f</strong></p><p><strong>job-name=job-wq-1</strong></p><p><strong>Annotations: <code>&lt;none&gt;</code></strong></p><p><strong>Parallelism: 2</strong></p><p><strong>Completions: 8</strong></p><p><strong>Start Time: Wed, 06 Sep 2017 16:42:02 +0800</strong></p><p><strong>Pods Statuses: 0 Running / 8 Succeeded / 0 Failed</strong></p><p><strong>Pod Template:</strong></p><p><strong>Labels: controller-uid=41d75705-92df-11e7-b85e-fa163ee3c11f</strong></p><p><strong>job-name=job-wq-1</strong></p><p><strong>Containers:</strong></p><p><strong>c:</strong></p><p><strong>Image: gcr.io/causal-jigsaw-637/job-wq-1</strong></p><p><strong>Port:</strong></p><p><strong>Environment:</strong></p><p><strong>BROKER_URL: amqp://guest:guest@rabbitmq-service:5672</strong></p><p><strong>QUEUE: job1</strong></p><p><strong>Mounts: <code>&lt;none&gt;</code></strong></p><p><strong>Volumes: <code>&lt;none&gt;</code></strong></p><p><strong>Events:</strong></p><p><strong>FirstSeen LastSeen Count From SubobjectPath Type Reason Message</strong></p><p><strong>───────── ──────── ───── ──── ───────────── ────── ────── ───────</strong></p><p><strong>27s 27s 1 {job } Normal SuccessfulCreate Created pod: job-wq-1-hcobb</strong></p><p><strong>27s 27s 1 {job } Normal SuccessfulCreate Created pod: job-wq-1-weytj</strong></p><p><strong>27s 27s 1 {job } Normal SuccessfulCreate Created pod: job-wq-1-qaam5</strong></p><p><strong>27s 27s 1 {job } Normal SuccessfulCreate Created pod: job-wq-1-b67sr</strong></p><p><strong>26s 26s 1 {job } Normal SuccessfulCreate Created pod: job-wq-1-xe5hj</strong></p><p><strong>15s 15s 1 {job } Normal SuccessfulCreate Created pod: job-wq-1-w2zqe</strong></p><p><strong>14s 14s 1 {job } Normal SuccessfulCreate Created pod: job-wq-1-d6ppa</strong></p><p><strong>14s 14s 1 {job } Normal SuccessfulCreate Created pod: job-wq-1-p17e0</strong></p><p>All our pods succeeded. Yay.</p><h5><strong>Alternatives</strong></h5><p>This approach has the advantage that you do not need to modify your &quot;worker&quot; program to be aware that there is a work queue.</p><p>It does require that you run a message queue service. If running a queue service is inconvenient, you may want to consider one of the other <a href="https://kubernetes.io/docs/concepts/jobs/run-to-completion-finite-workloads/#job-patterns">job patterns</a>.</p><p>This approach creates a pod for every work item. If your work items only take a few seconds, though, creating a Pod for every work item may add a lot of overhead. Consider another <a href="https://kubernetes.io/docs/tasks/job/fine-parallel-processing-work-queue/">example</a>, that executes multiple work items per Pod.</p><p>In this example, we used use the <strong>amqp-consume</strong> utility to read the message from the queue and run our actual program. This has the advantage that you do not need to modify your program to be aware of the queue. A <a href="https://kubernetes.io/docs/tasks/job/fine-parallel-processing-work-queue/">different example</a>, shows how to communicate with the work queue using a client library.</p><h5><strong>Caveats</strong></h5><p>If the number of completions is set to less than the number of items in the queue, then not all items will be processed.</p><p>If the number of completions is set to more than the number of items in the queue, then the Job will not appear to be completed, even though all items in the queue have been processed. It will start additional pods which will block waiting for a message.</p><p>There is an unlikely race with this pattern. If the container is killed in between the time that the message is acknowledged by the amqp-consume command and the time that the container exits with success, or if the node crashes before the kubelet is able to post the success of the pod back to the api-server, then the Job will not appear to be complete, even though all items in the queue have been processed.</p><h3>Fine Parallel Processing Using a Work Queue</h3><ul><li><a href="https://kubernetes.io/docs/tasks/job/fine-parallel-processing-work-queue/#example-job-with-work-queue-with-multiple-work-items-per-pod"><strong>Example: Job with Work Queue with Multiple Work Items Per Pod</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/job/fine-parallel-processing-work-queue/#starting-redis"><strong>Starting Redis</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/job/fine-parallel-processing-work-queue/#filling-the-queue-with-tasks"><strong>Filling the Queue with tasks</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/job/fine-parallel-processing-work-queue/#create-an-image"><strong>Create an Image</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/job/fine-parallel-processing-work-queue/#push-the-image"><strong>Push the image</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/job/fine-parallel-processing-work-queue/#defining-a-job"><strong>Defining a Job</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/job/fine-parallel-processing-work-queue/#running-the-job"><strong>Running the Job</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/job/fine-parallel-processing-work-queue/#alternatives"><strong>Alternatives</strong></a></li></ul></li></ul><h4>Example: Job with Work Queue with Multiple Work Items Per Pod</h4><p>In this example, we will run a Kubernetes Job with multiple parallel worker processes. You may want to be familiar with the basic, non-parallel, use of <a href="https://kubernetes.io/docs/concepts/jobs/run-to-completion-finite-workloads/">Job</a> first.</p><p>In this example, as each pod is created, it picks up one unit of work from a task queue, processes it, and repeats until the end of the queue is reached.</p><p>Here is an overview of the steps in this example:</p><ol><li><strong>Start a storage service to hold the work queue.</strong> In this example, we use Redis to store our work items. In the previous example, we used RabbitMQ. In this example, we use Redis and a custom work-queue client library because AMQP does not provide a good way for clients to detect when a finite-length work queue is empty. In practice you would set up a store such as Redis once and reuse it for the work queues of many jobs, and other things.</li><li><strong>Create a queue, and fill it with messages.</strong> Each message represents one task to be done. In this example, a message is just an integer that we will do a lengthy computation on.</li><li><strong>Start a Job that works on tasks from the queue</strong>. The Job starts several pods. Each pod takes one task from the message queue, processes it, and repeats until the end of the queue is reached.</li></ol><h5><strong>Starting Redis</strong></h5><p>For this example, for simplicity, we will start a single instance of Redis. See the <a href="https://github.com/kubernetes/examples/tree/master/guestbook">Redis Example</a> for an example of deploying Redis scalably and redundantly.</p><p>Start a temporary Pod running Redis and a service so we can find it.</p><p><strong>$ kubectl create -f docs/tasks/job/fine-parallel-processing-work-queue/redis-pod.yaml</strong></p><p><strong>pod &quot;redis-master&quot; created</strong></p><p><strong>$ kubectl create -f docs/tasks/job/fine-parallel-processing-work-queue/redis-service.yaml</strong></p><p><strong>service &quot;redis&quot; created</strong></p><p>If you&#x27;re not working from the source tree, you could also download <a href="https://kubernetes.io/docs/tasks/job/fine-parallel-processing-work-queue/redis-pod.yaml?raw=true"><strong>redis-pod.yaml</strong></a> and <a href="https://kubernetes.io/docs/tasks/job/fine-parallel-processing-work-queue/redis-service.yaml?raw=true"><strong>redis-service.yaml</strong></a> directly.</p><h5><strong>Filling the Queue with tasks</strong></h5><p>Now let&#x27;s fill the queue with some &quot;tasks&quot;. In our example, our tasks are just strings to be printed.</p><p>Start a temporary interactive pod for running the Redis CLI.</p><p><strong>$ kubectl run -i --tty temp --image redis --command &quot;/bin/sh&quot;</strong></p><p><strong>Waiting for pod default/redis2-c7h78 to be running, status is Pending, pod ready: false</strong></p><p><strong>Hit enter for command prompt</strong></p><p>Now hit enter, start the redis CLI, and create a list with some work items in it.</p><p><strong># redis-cli -h redis</strong></p><p><strong>redis:6379&gt; rpush job2 &quot;apple&quot;</strong></p><p><strong>(integer) 1</strong></p><p><strong>redis:6379&gt; rpush job2 &quot;banana&quot;</strong></p><p><strong>(integer) 2</strong></p><p><strong>redis:6379&gt; rpush job2 &quot;cherry&quot;</strong></p><p><strong>(integer) 3</strong></p><p><strong>redis:6379&gt; rpush job2 &quot;date&quot;</strong></p><p><strong>(integer) 4</strong></p><p><strong>redis:6379&gt; rpush job2 &quot;fig&quot;</strong></p><p><strong>(integer) 5</strong></p><p><strong>redis:6379&gt; rpush job2 &quot;grape&quot;</strong></p><p><strong>(integer) 6</strong></p><p><strong>redis:6379&gt; rpush job2 &quot;lemon&quot;</strong></p><p><strong>(integer) 7</strong></p><p><strong>redis:6379&gt; rpush job2 &quot;melon&quot;</strong></p><p><strong>(integer) 8</strong></p><p><strong>redis:6379&gt; rpush job2 &quot;orange&quot;</strong></p><p><strong>(integer) 9</strong></p><p><strong>redis:6379&gt; lrange job2 0 -1</strong></p><p><strong>1) &quot;apple&quot;</strong></p><p><strong>2) &quot;banana&quot;</strong></p><p><strong>3) &quot;cherry&quot;</strong></p><p><strong>4) &quot;date&quot;</strong></p><p><strong>5) &quot;fig&quot;</strong></p><p><strong>6) &quot;grape&quot;</strong></p><p><strong>7) &quot;lemon&quot;</strong></p><p><strong>8) &quot;melon&quot;</strong></p><p><strong>9) &quot;orange&quot;</strong></p><p>So, the list with key <strong>job2</strong> will be our work queue.</p><p>Note: if you do not have Kube DNS setup correctly, you may need to change the first step of the above block to <strong>redis-cli -h $REDIS_SERVICE_HOST</strong>.</p><h5><strong>Create an Image</strong></h5><p>Now we are ready to create an image that we will run.</p><p>We will use a python worker program with a redis client to read the messages from the message queue.</p><p>A simple Redis work queue client library is provided, called rediswq.py (<a href="https://kubernetes.io/docs/tasks/job/fine-parallel-processing-work-queue/rediswq.py?raw=true">Download</a>).</p><p>The &quot;worker&quot; program in each Pod of the Job uses the work queue client library to get work. Here it is:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>worker.py</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/website">https://raw.githubusercontent.com/kubernetes/website</a> |
| /master/docs/tasks/job/fine-parallel-processing-work-queue/worker.py) |
+=======================================================================+
| <strong><em>#!/usr/bin/env python</em></strong>                                          |
|                                                                       |
| <strong>import time</strong>                                                       |
|                                                                       |
| <strong>import rediswq</strong>                                                    |
|                                                                       |
| <strong>host=&quot;redis&quot;</strong>                                                    |
|                                                                       |
| <strong><em># Uncomment next two lines if you do not have Kube-DNS            |
| working.</em></strong>                                                           |
|                                                                       |
| <strong><em># import os</em></strong>                                                    |
|                                                                       |
| <strong><em># host = os.getenv(&quot;REDIS_SERVICE_HOST&quot;)</em></strong>                     |
|                                                                       |
| <strong>q = rediswq.RedisWQ(name=&quot;job2&quot;, host=&quot;redis&quot;)</strong>                |
|                                                                       |
| <strong>print(&quot;Worker with sessionID: &quot; + q.sessionID())</strong>                |
|                                                                       |
| <strong>print(&quot;Initial queue state: empty=&quot; + str(q.empty()))</strong>           |
|                                                                       |
| <strong>while not q.empty():</strong>                                              |
|                                                                       |
| <strong>item = q.lease(lease_secs=10, block=True, timeout=2)</strong>              |
|                                                                       |
| <strong>if item is not None:</strong>                                              |
|                                                                       |
| <strong>itemstr = item.decode(&quot;utf=8&quot;)</strong>                                  |
|                                                                       |
| <strong>print(&quot;Working on &quot; + itemstr)</strong>                                  |
|                                                                       |
| <strong>time.sleep(10) <em># Put your actual work here instead of sleep.</em></strong>   |
|                                                                       |
| <strong>q.complete(item)</strong>                                                  |
|                                                                       |
| <strong>else:</strong>                                                             |
|                                                                       |
| <strong>print(&quot;Waiting for work&quot;)</strong>                                       |
|                                                                       |
| <strong>print(&quot;Queue empty, exiting&quot;)</strong>                                   |
+-----------------------------------------------------------------------+</p><p>If you are working from the source tree, change directory to the <strong>docs/tasks/job/fine-parallel-processing-work-queue/</strong> directory. Otherwise, download <a href="https://kubernetes.io/docs/tasks/job/fine-parallel-processing-work-queue/worker.py?raw=true"><strong>worker.py</strong></a>, <a href="https://kubernetes.io/docs/tasks/job/fine-parallel-processing-work-queue/rediswq.py?raw=true"><strong>rediswq.py</strong></a>, and <a href="https://kubernetes.io/docs/tasks/job/fine-parallel-processing-work-queue/Dockerfile?raw=true"><strong>Dockerfile</strong></a> using above links. Then build the image:</p><p><strong>docker build -t job-wq-2 .</strong></p><h6><strong>Push the image</strong></h6><p>For the <a href="https://hub.docker.com/">Docker Hub</a>, tag your app image with your username and push to the Hub with the below commands. Replace <strong><code>&lt;username&gt;</code></strong> with your Hub username.</p><p><strong>docker tag job-wq-2 <code>&lt;username&gt;</code>/job-wq-2</strong></p><p><strong>docker push <code>&lt;username&gt;</code>/job-wq-2</strong></p><p>You need to push to a public repository or <a href="https://kubernetes.io/docs/concepts/containers/images/">configure your cluster to be able to access your private repository</a>.</p><p>If you are using <a href="https://cloud.google.com/tools/container-registry/">Google Container Registry</a>, tag your app image with your project ID, and push to GCR. Replace <strong><code>&lt;project&gt;</code></strong> with your project ID.</p><p><strong>docker tag job-wq-2 gcr.io/<code>&lt;project&gt;</code>/job-wq-2</strong></p><p><strong>gcloud docker -- push gcr.io/<code>&lt;project&gt;</code>/job-wq-2</strong></p><h5><strong>Defining a Job</strong></h5><p>Here is the job definition:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>job.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/websit">https://raw.githubusercontent.com/kubernetes/websit</a>   |
| e/master/docs/tasks/job/fine-parallel-processing-work-queue/job.yaml) |
+=======================================================================+
| <strong>apiVersion: batch/v1</strong>                                              |
|                                                                       |
| <strong>kind: Job</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: job-wq-2</strong>                                                    |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>parallelism: 2</strong>                                                    |
|                                                                       |
| <strong>template:</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: job-wq-2</strong>                                                    |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: c</strong>                                                         |
|                                                                       |
| <strong>image: gcr.io/myproject/job-wq-2</strong>                                  |
|                                                                       |
| <strong>restartPolicy: OnFailure</strong>                                          |
+-----------------------------------------------------------------------+</p><p>Be sure to edit the job template to change <strong>gcr.io/myproject</strong> to your own path.</p><p>In this example, each pod works on several items from the queue and then exits when there are no more items. Since the workers themselves detect when the workqueue is empty, and the Job controller does not know about the workqueue, it relies on the workers to signal when they are done working. The workers signal that the queue is empty by exiting with success. So, as soon as any worker exits with success, the controller knows the work is done, and the Pods will exit soon. So, we set the completion count of the Job to 1. The job controller will wait for the other pods to complete too.</p><h5><strong>Running the Job</strong></h5><p>So, now run the Job:</p><p><strong>kubectl create -f ./job.yaml</strong></p><p>Now wait a bit, then check on the job.</p><p><strong>$ kubectl describe jobs/job-wq-2</strong></p><p><strong>Name: job-wq-2</strong></p><p><strong>Namespace: default</strong></p><p><strong>Selector: controller-uid=b1c7e4e3-92e1-11e7-b85e-fa163ee3c11f</strong></p><p><strong>Labels: controller-uid=b1c7e4e3-92e1-11e7-b85e-fa163ee3c11f</strong></p><p><strong>job-name=job-wq-2</strong></p><p><strong>Annotations: <code>&lt;none&gt;</code></strong></p><p><strong>Parallelism: 2</strong></p><p><strong>Completions: <code>&lt;unset&gt;</code></strong></p><p><strong>Start Time: Mon, 11 Jan 2016 17:07:59 -0800</strong></p><p><strong>Pods Statuses: 1 Running / 0 Succeeded / 0 Failed</strong></p><p><strong>Pod Template:</strong></p><p><strong>Labels: controller-uid=b1c7e4e3-92e1-11e7-b85e-fa163ee3c11f</strong></p><p><strong>job-name=job-wq-2</strong></p><p><strong>Containers:</strong></p><p><strong>c:</strong></p><p><strong>Image: gcr.io/exampleproject/job-wq-2</strong></p><p><strong>Port:</strong></p><p><strong>Environment: <code>&lt;none&gt;</code></strong></p><p><strong>Mounts: <code>&lt;none&gt;</code></strong></p><p><strong>Volumes: <code>&lt;none&gt;</code></strong></p><p><strong>Events:</strong></p><p><strong>FirstSeen LastSeen Count From SubobjectPath Type Reason Message</strong></p><p><strong>--------- -------- ----- ---- ------------- -------- ------ -------</strong></p><p><strong>33s 33s 1 {job-controller } Normal SuccessfulCreate Created pod: job-wq-2-lglf8</strong></p><p><strong>$ kubectl logs pods/job-wq-2-7r7b2</strong></p><p><strong>Worker with sessionID: bbd72d0a-9e5c-4dd6-abf6-416cc267991f</strong></p><p><strong>Initial queue state: empty=False</strong></p><p><strong>Working on banana</strong></p><p><strong>Working on date</strong></p><p><strong>Working on lemon</strong></p><p>As you can see, one of our pods worked on several work units.</p><h5><strong>Alternatives</strong></h5><p>If running a queue service or modifying your containers to use a work queue is inconvenient, you may want to consider one of the other <a href="https://kubernetes.io/docs/concepts/jobs/run-to-completion-finite-workloads/#job-patterns">job patterns</a>.</p><p>If you have a continuous stream of background processing work to run, then consider running your background workers with a <strong>replicationController</strong> instead, and consider running a background processing library such as <code style="background-color:lightgray">&lt;https://github.com/resque/resque&gt;</code>.</p><h2>Access Applications in a Cluster</h2><h3>Web UI (Dashboard)</h3><p>Dashboard is a web-based Kubernetes user interface. You can use Dashboard to deploy containerized applications to a Kubernetes cluster, troubleshoot your containerized application, and manage the cluster itself along with its attendant resources. You can use Dashboard to get an overview of applications running on your cluster, as well as for creating or modifying individual Kubernetes resources (such as Deployments, Jobs, DaemonSets, etc). For example, you can scale a Deployment, initiate a rolling update, restart a pod or deploy new applications using a deploy wizard.</p><p>Dashboard also provides information on the state of Kubernetes resources in your cluster, and on any errors that may have occurred.</p><ul><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/#deploying-the-dashboard-ui"><strong>Deploying the Dashboard UI</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/#accessing-the-dashboard-ui"><strong>Accessing the Dashboard UI</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/#command-line-proxy"><strong>Command line proxy</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/#master-server"><strong>Master server</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/#welcome-view"><strong>Welcome view</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/#deploying-containerized-applications"><strong>Deploying containerized applications</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/#specifying-application-details"><strong>Specifying application details</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/#uploading-a-yaml-or-json-file"><strong>Uploading a YAML or JSON file</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/#using-dashboard"><strong>Using Dashboard</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/#navigation"><strong>Navigation</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/#admin"><strong>Admin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/#workloads"><strong>Workloads</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/#services-and-discovery"><strong>Services and discovery</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/#storage"><strong>Storage</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/#config"><strong>Config</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/#logs-viewer"><strong>Logs viewer</strong></a></li></ul></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/#more-information"><strong>More information</strong></a></li></ul><h4>Deploying the Dashboard UI</h4><p>The Dashboard UI is not deployed by default. To deploy it, run the following command:</p><p><strong>kubectl create -f <a href="https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml">https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml</a></strong></p><h4>Accessing the Dashboard UI</h4><p>There are multiple ways you can access the Dashboard UI; either by using the kubectl command-line interface, or by accessing the Kubernetes master apiserver using your web browser.</p><h5><strong>Command line proxy</strong></h5><p>You can access Dashboard using the kubectl command-line tool by running the following command:</p><p><strong>kubectl proxy</strong></p><p>Kubectl will handle authentication with apiserver and make Dashboard available at http://localhost:8001/ui.</p><p>The UI can only be accessed from the machine where the command is executed. See <strong>kubectl proxy --help</strong> for more options.</p><h5><strong>Master server</strong></h5><p>You may access the UI directly via the Kubernetes master apiserver. Open a browser and navigate to <strong>https://<code>&lt;kubernetes-master&gt;/ui, where &lt;kubernetes-master&gt;</code></strong> is IP address or domain name of the Kubernetes master.</p><p>Please note, this works only if the apiserver is set up to allow authentication with username and password. This is not currently the case with some setup tools (e.g., <strong>kubeadm</strong>). Refer to the<a href="https://kubernetes.io/docs/admin/authentication/">authentication admin documentation</a> for information on how to configure authentication manually.</p><p>If the username and password are configured but unknown to you, then use <strong>kubectl config view</strong>to find it.</p><h4>Welcome view</h4><p>When you access Dashboard on an empty cluster, you&#x27;ll see the welcome page. This page contains a link to this document as well as a button to deploy your first application. In addition, you can view which system applications are running by default in the <strong>kube-system</strong> <a href="https://kubernetes.io/docs/tasks/administer-cluster/namespaces/">namespace</a> of your cluster, for example the Dashboard itself.</p><h4>Deploying containerized applications</h4><p>Dashboard lets you create and deploy a containerized application as a Deployment and optional Service with a simple wizard. You can either manually specify application details, or upload a YAML or JSON file containing application configuration.</p><p>To access the deploy wizard from the Welcome page, click the respective button. To access the wizard at a later point in time, click the <strong>CREATE</strong> button in the upper right corner of any page.</p><h5><strong>Specifying application details</strong></h5><p>The deploy wizard expects that you provide the following information:</p><ul><li><strong>App name</strong> (mandatory): Name for your application. A <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/">label</a> with the name will be added to the Deployment and Service, if any, that will be deployed.</li></ul><p>The application name must be unique within the selected Kubernetes <a href="https://kubernetes.io/docs/tasks/administer-cluster/namespaces/">namespace</a>. It must start with a lowercase character, and end with a lowercase character or a number, and contain only lowercase letters, numbers and dashes (-). It is limited to 24 characters. Leading and trailing spaces are ignored.</p><ul><li><strong>Container image</strong> (mandatory): The URL of a public Docker <a href="https://kubernetes.io/docs/concepts/containers/images/">container image</a> on any registry, or a private image (commonly hosted on the Google Container Registry or Docker Hub). The container image specification must end with a colon.</li><li><strong>Number of pods</strong> (mandatory): The target number of Pods you want your application to be deployed in. The value must be a positive integer.</li></ul><p>A <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployment</a> will be created to maintain the desired number of Pods across your cluster.</p><ul><li><strong>Service</strong> (optional): For some parts of your application (e.g. frontends) you may want to expose a <a href="https://kubernetes.io/docs/concepts/services-networking/service/">Service</a> onto an external, maybe public IP address outside of your cluster (external Service). For external Services, you may need to open up one or more ports to do so. Find more details <a href="https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/">here</a>.</li></ul><p>Other Services that are only visible from inside the cluster are called internal Services.</p><p>Irrespective of the Service type, if you choose to create a Service and your container listens on a port (incoming), you need to specify two ports. The Service will be created mapping the port (incoming) to the target port seen by the container. This Service will route to your deployed Pods. Supported protocols are TCP and UDP. The internal DNS name for this Service will be the value you specified as application name above.</p><p>If needed, you can expand the <strong>Advanced options</strong> section where you can specify more settings:</p><ul><li><strong>Description</strong>: The text you enter here will be added as an <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/">annotation</a> to the Deployment and displayed in the application&#x27;s details.</li><li><strong>Labels</strong>: Default <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/">labels</a> to be used for your application are application name and version. You can specify additional labels to be applied to the Deployment, Service (if any), and Pods, such as release, environment, tier, partition, and release track.</li></ul><p>Example:</p><p><strong>release=1.0</strong></p><p><strong>tier=frontend</strong></p><p><strong>environment=pod</strong></p><p><strong>track=stable</strong></p><ul><li><strong>Namespace</strong>: Kubernetes supports multiple virtual clusters backed by the same physical cluster. These virtual clusters are called <a href="https://kubernetes.io/docs/tasks/administer-cluster/namespaces/">namespaces</a>. They let you partition resources into logically named groups.</li></ul><p>Dashboard offers all available namespaces in a dropdown list, and allows you to create a new namespace. The namespace name may contain a maximum of 63 alphanumeric characters and dashes (-) but can not contain capital letters. Namespace names should not consist of only numbers. If the name is set as a number, such as 10, the pod will be put in the default namespace.</p><p>In case the creation of the namespace is successful, it is selected by default. If the creation fails, the first namespace is selected.</p><ul><li><strong>Image Pull Secret</strong>: In case the specified Docker container image is private, it may require <a href="https://kubernetes.io/docs/concepts/configuration/secret/">pull secret</a> credentials.</li></ul><p>Dashboard offers all available secrets in a dropdown list, and allows you to create a new secret. The secret name must follow the DNS domain name syntax, e.g. <strong>new.image-pull.secret</strong>. The content of a secret must be base64-encoded and specified in a <a href="https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod"><strong>.dockercfg</strong></a> file. The secret name may consist of a maximum of 253 characters.</p><p>In case the creation of the image pull secret is successful, it is selected by default. If the creation fails, no secret is applied.</p><ul><li><strong>CPU requirement (cores)</strong> and <strong>Memory requirement (MiB)</strong>: You can specify the minimum <a href="https://kubernetes.io/docs/tasks/configure-pod-container/limit-range/">resource limits</a> for the container. By default, Pods run with unbounded CPU and memory limits.</li><li><strong>Run command</strong> and <strong>Run command arguments</strong>: By default, your containers run the specified Docker image&#x27;s default <a href="https://kubernetes.io/docs/user-guide/containers/#containers-and-commands">entrypoint command</a>. You can use the command options and arguments to override the default.</li><li><strong>Run as privileged</strong>: This setting determines whether processes in <a href="https://kubernetes.io/docs/user-guide/pods/#privileged-mode-for-pod-containers">privileged containers</a> are equivalent to processes running as root on the host. Privileged containers can make use of capabilities like manipulating the network stack and accessing devices.</li><li><strong>Environment variables</strong>: Kubernetes exposes Services through <a href="https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/">environment variables</a>. You can compose environment variable or pass arguments to your commands using the values of environment variables. They can be used in applications to find a Service. Values can reference other variables using the <strong>$(VAR_NAME)</strong> syntax.</li></ul><h5><strong>Uploading a YAML or JSON file</strong></h5><p>Kubernetes supports declarative configuration. In this style, all configuration is stored in YAML or JSON configuration files using the Kubernetes <a href="https://kubernetes.io/docs/concepts/overview/kubernetes-api/">API</a> resource schemas.</p><p>As an alternative to specifying application details in the deploy wizard, you can define your application in YAML or JSON files, and upload the files using Dashboard:</p><h4>Using Dashboard</h4><p>Following sections describe views of the Kubernetes Dashboard UI; what they provide and how can they be used.</p><h5><strong>Navigation</strong></h5><p>When there are Kubernetes objects defined in the cluster, Dashboard shows them in the initial view. By default only objects from the default namespace are shown and this can be changed using the namespace selector located in the navigation menu.</p><p>Dashboard shows most Kubernetes object kinds and groups them in a few menu categories.</p><h6><strong>Admin</strong></h6><p>View for cluster and namespace administrators. It lists Nodes, Namespaces and Persistent Volumes and has detail views for them. Node list view contains CPU and memory usage metrics aggregated across all Nodes. The details view shows the metrics for a Node, its specification, status, allocated resources, events and pods running on the node.</p><h6><strong>Workloads</strong></h6><p>Entry point view that shows all applications running in the selected namespace. The view lists applications by workload kind (e.g., Deployments, Replica Sets, Stateful Sets, etc.) and each workload kind can be viewed separately. The lists summarize actionable information about the workloads, such as the number of ready pods for a Replica Set or current memory usage for a Pod.</p><p>Detail views for workloads show status and specification information and surface relationships between objects. For example, Pods that Replica Set is controlling or New Replica Sets and Horizontal Pod Autoscalers for Deployments.</p><h6><strong>Services and discovery</strong></h6><p>Services and discovery view shows Kubernetes resources that allow for exposing services to external world and discovering them within a cluster. For that reason, Service and Ingress views show Pods targeted by them, internal endpoints for cluster connections and external endpoints for external users.</p><h6><strong>Storage</strong></h6><p>Storage view shows Persistent Volume Claim resources which are used by applications for storing data.</p><h6><strong>Config</strong></h6><p>Config view shows all Kubernetes resources that are used for live configuration of applications running in clusters. This is now Config Maps and Secrets. The view allows for editing and managing config objects and displays secrets hidden by default.</p><h6><strong>Logs viewer</strong></h6><p>Pod lists and detail pages link to logs viewer that is built into Dashboard. The viewer allows for drilling down logs from containers belonging to a single Pod.</p><h4>More information</h4><p>For more information, see the <a href="https://github.com/kubernetes/dashboard">Kubernetes Dashboard project page</a>.</p><h3>Accessing Clusters</h3><ul><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#accessing-the-cluster-api"><strong>Accessing the cluster API</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#accessing-for-the-first-time-with-kubectl"><strong>Accessing for the first time with kubectl</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#directly-accessing-the-rest-api"><strong>Directly accessing the REST API</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#using-kubectl-proxy"><strong>Using kubectl proxy</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#without-kubectl-proxy-before-v13x"><strong>Without kubectl proxy (before v1.3.x)</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#without-kubectl-proxy-post-v13x"><strong>Without kubectl proxy (post v1.3.x)</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#programmatic-access-to-the-api"><strong>Programmatic access to the API</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#go-client"><strong>Go client</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#python-client"><strong>Python client</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#other-languages"><strong>Other languages</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#accessing-the-api-from-a-pod"><strong>Accessing the API from a Pod</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#accessing-services-running-on-the-cluster"><strong>Accessing services running on the cluster</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#ways-to-connect"><strong>Ways to connect</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#discovering-builtin-services"><strong>Discovering builtin services</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#manually-constructing-apiserver-proxy-urls"><strong>Manually constructing apiserver proxy URLs</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#examples"><strong>Examples</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#using-web-browsers-to-access-services-running-on-the-cluster"><strong>Using web browsers to access services running on the cluster</strong></a></li></ul></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#requesting-redirects"><strong>Requesting redirects</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#so-many-proxies"><strong>So Many Proxies</strong></a></li></ul><h4>Accessing the cluster API</h4><h5><strong>Accessing for the first time with kubectl</strong></h5><p>When accessing the Kubernetes API for the first time, we suggest using the Kubernetes CLI, <strong>kubectl</strong>.</p><p>To access a cluster, you need to know the location of the cluster and have credentials to access it. Typically, this is automatically set-up when you work through a <a href="https://kubernetes.io/docs/setup/">Getting started guide</a>, or someone else setup the cluster and provided you with credentials and a location.</p><p>Check the location and credentials that kubectl knows about with this command:</p><p><strong>$ kubectl config view</strong></p><p>Many of the <a href="https://kubernetes.io/docs/user-guide/kubectl-cheatsheet">examples</a> provide an introduction to using kubectl and complete documentation is found in the <a href="https://kubernetes.io/docs/user-guide/kubectl-overview">kubectl manual</a>.</p><h5><strong>Directly accessing the REST API</strong></h5><p>Kubectl handles locating and authenticating to the apiserver. If you want to directly access the REST API with an http client like curl or wget, or a browser, there are several ways to locate and authenticate:</p><ul><li>Run kubectl in proxy mode.<ul><li>Recommended approach.</li><li>Uses stored apiserver location.</li><li>Verifies identity of apiserver using self-signed cert. No MITM possible.</li><li>Authenticates to apiserver.</li><li>In future, may do intelligent client-side load-balancing and failover.</li></ul></li><li>Provide the location and credentials directly to the http client.<ul><li>Alternate approach.</li><li>Works with some types of client code that are confused by using a proxy.</li><li>Need to import a root cert into your browser to protect against MITM.</li></ul></li></ul><h6><strong>Using kubectl proxy</strong></h6><p>The following command runs kubectl in a mode where it acts as a reverse proxy. It handles locating the apiserver and authenticating. Run it like this:</p><p><strong>$ kubectl proxy --port=8080 &amp;</strong></p><p>See <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#proxy">kubectl proxy</a> for more details.</p><p>Then you can explore the API with curl, wget, or a browser, replacing localhost with <!-- -->[::1]<!-- --> for IPv6, like so:</p><p><strong>$ curl http://localhost:8080/api/</strong></p><p><strong>{</strong></p><p><strong>&quot;versions&quot;: [</strong></p><p><strong>&quot;v1&quot;</strong></p><p><strong>]</strong></p><p><strong>}</strong></p><h6><strong>Without kubectl proxy (before v1.3.x)</strong></h6><p>It is possible to avoid using kubectl proxy by passing an authentication token directly to the apiserver, like this:</p><p><strong>$ APISERVER=$(kubectl config view | grep server | cut -f 2- -d &quot;:&quot; | tr -d &quot; &quot;)</strong></p><p><strong>$ TOKEN=$(kubectl config view | grep token | cut -f 2 -d &quot;:&quot; | tr -d &quot; &quot;)</strong></p><p><strong>$ curl $APISERVER/api --header &quot;Authorization: Bearer $TOKEN&quot; --insecure</strong></p><p><strong>{</strong></p><p><strong>&quot;versions&quot;: [</strong></p><p><strong>&quot;v1&quot;</strong></p><p><strong>]</strong></p><p><strong>}</strong></p><h6><strong>Without kubectl proxy (post v1.3.x)</strong></h6><p>In Kubernetes version 1.3 or later, <strong>kubectl config view</strong> no longer displays the token. Use <strong>kubectl describe secret<!-- -->.<!-- -->..</strong> to get the token for the default service account, like this:</p><p><strong>$ APISERVER=$(kubectl config view | grep server | cut -f 2- -d &quot;:&quot; | tr -d &quot; &quot;)</strong></p><p><strong>$ TOKEN=$(kubectl describe secret $(kubectl get secrets | grep default | cut -f1 -d \&#x27; \&#x27;) | grep -E \&#x27;\^token\&#x27; | cut -f2 -d\&#x27;:\&#x27; | tr -d \&#x27;<!-- -->\<!-- -->t\&#x27;)</strong></p><p><strong>$ curl $APISERVER/api --header &quot;Authorization: Bearer $TOKEN&quot; --insecure</strong></p><p><strong>{</strong></p><p><strong>&quot;kind&quot;: &quot;APIVersions&quot;,</strong></p><p><strong>&quot;versions&quot;: [</strong></p><p><strong>&quot;v1&quot;</strong></p><p><strong>],</strong></p><p><strong>&quot;serverAddressByClientCIDRs&quot;: [</strong></p><p><strong>{</strong></p><p><strong>&quot;clientCIDR&quot;: &quot;0.0.0.0/0&quot;,</strong></p><p><strong>&quot;serverAddress&quot;: &quot;10.0.1.149:443&quot;</strong></p><p><strong>}</strong></p><p><strong>]</strong></p><p><strong>}</strong></p><p>The above examples use the <strong>--insecure</strong> flag. This leaves it subject to MITM attacks. When kubectl accesses the cluster it uses a stored root certificate and client certificates to access the server. (These are installed in the <strong>~<!-- -->/.kube</strong> directory). Since cluster certificates are typically self-signed, it may take special configuration to get your http client to use root certificate.</p><p>On some clusters, the apiserver does not require authentication; it may serve on localhost, or be protected by a firewall. There is not a standard for this. <a href="https://kubernetes.io/docs/admin/accessing-the-api">Configuring Access to the API</a> describes how a cluster admin can configure this. Such approaches may conflict with future high-availability support.</p><h5><strong>Programmatic access to the API</strong></h5><p>Kubernetes officially supports <a href="https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#go-client">Go</a> and <a href="https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#python-client">Python</a> client libraries.</p><h6><strong>Go client</strong></h6><ul><li>To get the library, run the following command: go get k8s.io/client-go/<code>&lt;version number&gt;/kubernetes</code>. See <a href="https://github.com/kubernetes/client-go">https://github.com/kubernetes/client-go</a> to see which versions are supported.</li><li>Write an application atop of the client-go clients. Note that client-go defines its own API objects, so if needed, please import API definitions from client-go rather than from the main repository, e.g., <strong>import &quot;k8s.io/client-go/1.4/pkg/api/v1&quot;</strong> is correct.</li></ul><p>The Go client can use the same <a href="https://kubernetes.io/docs/concepts/cluster-administration/authenticate-across-clusters-kubeconfig/">kubeconfig file</a> as the kubectl CLI does to locate and authenticate to the apiserver. See this <a href="https://git.k8s.io/client-go/examples/out-of-cluster-client-configuration/main.go">example</a>.</p><p>If the application is deployed as a Pod in the cluster, please refer to the <a href="https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#accessing-the-api-from-a-pod">next section</a>.</p><h6><strong>Python client</strong></h6><p>To use <a href="https://github.com/kubernetes-client/python">Python client</a>, run the following command: <strong>pip install kubernetes</strong>. See <a href="https://github.com/kubernetes-client/python">Python Client Library page</a> for more installation options.</p><p>The Python client can use the same <a href="https://kubernetes.io/docs/concepts/cluster-administration/authenticate-across-clusters-kubeconfig/">kubeconfig file</a> as the kubectl CLI does to locate and authenticate to the apiserver. See this <a href="https://github.com/kubernetes-client/python/tree/master/examples/example1.py">example</a>.</p><h6><strong>Other languages</strong></h6><p>There are <a href="https://kubernetes.io/docs/reference/client-libraries/">client libraries</a> for accessing the API from other languages. See documentation for other libraries for how they authenticate.</p><h5><strong>Accessing the API from a Pod</strong></h5><p>When accessing the API from a pod, locating and authenticating to the apiserver are somewhat different.</p><p>The recommended way to locate the apiserver within the pod is with the <strong>kubernetes</strong> DNS name, which resolves to a Service IP which in turn will be routed to an apiserver.</p><p>The recommended way to authenticate to the apiserver is with a <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/">service account</a> credential. By kube-system, a pod is associated with a service account, and a credential (token) for that service account is placed into the filesystem tree of each container in that pod, at <strong>/var/run/secrets/kubernetes.io/serviceaccount/token</strong>.</p><p>If available, a certificate bundle is placed into the filesystem tree of each container at <strong>/var/run/secrets/kubernetes.io/serviceaccount/ca.crt</strong>, and should be used to verify the serving certificate of the apiserver.</p><p>Finally, the default namespace to be used for namespaced API operations is placed in a file at <strong>/var/run/secrets/kubernetes.io/serviceaccount/namespace</strong> in each container.</p><p>From within a pod the recommended ways to connect to API are:</p><ul><li>run <strong>kubectl proxy</strong> in a sidecar container in the pod, or as a background process within the container. This proxies the Kubernetes API to the localhost interface of the pod, so that other processes in any container of the pod can access it.</li><li>use the Go client library, and create a client using the <strong>rest.InClusterConfig()</strong> and <strong>kubernetes.NewForConfig()</strong> functions. They handle locating and authenticating to the apiserver. <a href="https://git.k8s.io/client-go/examples/in-cluster-client-configuration/main.go">example</a></li></ul><p>In each case, the credentials of the pod are used to communicate securely with the apiserver.</p><h4>Accessing services running on the cluster</h4><p>The previous section was about connecting the Kubernetes API server. This section is about connecting to other services running on Kubernetes cluster. In Kubernetes, the <a href="https://kubernetes.io/docs/admin/node">nodes</a>, <a href="https://kubernetes.io/docs/user-guide/pods">pods</a> and <a href="https://kubernetes.io/docs/user-guide/services">services</a> all have their own IPs. In many cases, the node IPs, pod IPs, and some service IPs on a cluster will not be routable, so they will not be reachable from a machine outside the cluster, such as your desktop machine.</p><h5><strong>Ways to connect</strong></h5><p>You have several options for connecting to nodes, pods and services from outside the cluster:</p><ul><li>Access services through public IPs.<ul><li>Use a service with type <strong>NodePort</strong> or <strong>LoadBalancer</strong> to make the service reachable outside the cluster. See the <a href="https://kubernetes.io/docs/user-guide/services">services</a> and <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#expose">kubectl expose</a> documentation.</li><li>Depending on your cluster environment, this may just expose the service to your corporate network, or it may expose it to the internet. Think about whether the service being exposed is secure. Does it do its own authentication?</li><li>Place pods behind services. To access one specific pod from a set of replicas, such as for debugging, place a unique label on the pod and create a new service which selects this label.</li><li>In most cases, it should not be necessary for application developer to directly access nodes via their nodeIPs.</li></ul></li><li>Access services, nodes, or pods using the Proxy Verb.<ul><li>Does apiserver authentication and authorization prior to accessing the remote service. Use this if the services are not secure enough to expose to the internet, or to gain access to ports on the node IP, or for debugging.</li><li>Proxies may cause problems for some web applications.</li><li>Only works for HTTP/HTTPS.</li><li>Described <a href="https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#manually-constructing-apiserver-proxy-urls">here</a>.</li></ul></li><li>Access from a node or pod in the cluster.<ul><li>Run a pod, and then connect to a shell in it using <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#exec">kubectl exec</a>. Connect to other nodes, pods, and services from that shell.</li><li>Some clusters may allow you to ssh to a node in the cluster. From there you may be able to access cluster services. This is a non-standard method, and will work on some clusters but not others. Browsers and other tools may or may not be installed. Cluster DNS may not work.</li></ul></li></ul><h5><strong>Discovering builtin services</strong></h5><p>Typically, there are several services which are started on a cluster by kube-system. Get a list of these with the <strong>kubectl cluster-info</strong> command:</p><p><strong>$ kubectl cluster-info</strong></p><p><strong>Kubernetes master is running at <a href="https://104.197.5.247">https://104.197.5.247</a></strong></p><p><strong>elasticsearch-logging is running at <a href="https://104.197.5.247/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy">https://104.197.5.247/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy</a></strong></p><p><strong>kibana-logging is running at <a href="https://104.197.5.247/api/v1/namespaces/kube-system/services/kibana-logging/proxy">https://104.197.5.247/api/v1/namespaces/kube-system/services/kibana-logging/proxy</a></strong></p><p><strong>kube-dns is running at <a href="https://104.197.5.247/api/v1/namespaces/kube-system/services/kube-dns/proxy">https://104.197.5.247/api/v1/namespaces/kube-system/services/kube-dns/proxy</a></strong></p><p><strong>grafana is running at <a href="https://104.197.5.247/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy">https://104.197.5.247/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy</a></strong></p><p><strong>heapster is running at <a href="https://104.197.5.247/api/v1/namespaces/kube-system/services/monitoring-heapster/proxy">https://104.197.5.247/api/v1/namespaces/kube-system/services/monitoring-heapster/proxy</a></strong></p><p>This shows the proxy-verb URL for accessing each service. For example, this cluster has cluster-level logging enabled (using Elasticsearch), which can be reached at <strong><a href="https://104.197.5.247/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/">https://104.197.5.247/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/</a></strong>if suitable credentials are passed. Logging can also be reached through a kubectl proxy, for example at: <strong>http://localhost:8080/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/</strong>. (See <a href="https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#accessing-the-cluster-api">above</a> for how to pass credentials or use kubectl proxy.)</p><h6><strong>Manually constructing apiserver proxy URLs</strong></h6><p>As mentioned above, you use the <strong>kubectl cluster-info</strong> command to retrieve the service&#x27;s proxy URL. To create proxy URLs that include service endpoints, suffixes, and parameters, you simply append to the service&#x27;s proxy URL: <strong>http://<em>kubernetes_master_address</em>/api/v1/namespaces/<em>namespace_name</em>/services/<em>service_name<!-- -->[:port_name]</em>/proxy</strong></p><p>If you haven&#x27;t specified a name for your port, you don&#x27;t have to specify port_name in the URL.</p><p>By default, the API server proxies to your service using http. To use https, prefix the service name with <strong>https:</strong>: <strong>http://<em>kubernetes_master_address</em>/api/v1/namespaces/<em>namespace_name</em>/services/<em>https:service_name:<!-- -->[port_name]</em>/proxy</strong></p><p>The supported formats for the name segment of the URL are:</p><ul><li><strong><code>&lt;service_name&gt;</code></strong> - proxies to the default or unnamed port using http</li><li><strong><code>&lt;service_name&gt;:&lt;port_name&gt;</code></strong> - proxies to the specified port using http</li><li><strong>https:<code>&lt;service_name&gt;</code>:</strong> - proxies to the default or unnamed port using https (note the trailing colon)</li><li><strong>https:<code>&lt;service_name&gt;:&lt;port_name&gt;</code></strong> - proxies to the specified port using https</li></ul><p><strong>Examples</strong></p><ul><li>To access the Elasticsearch service endpoint <strong>_<!-- -->search?q=user:kimchy</strong>, you would use:<strong><a href="http://104.197.5.247/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/%5C_search?q=user:kimchy">http://104.197.5.247/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/\_search?q=user:kimchy</a></strong></li><li>To access the Elasticsearch cluster health information <strong>_<!-- -->cluster/health?pretty=true</strong>, you would use:<strong><a href="https://104.197.5.247/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/%5C_cluster/health?pretty=true">https://104.197.5.247/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/\_cluster/health?pretty=true</a></strong></li></ul><p><strong>{</strong></p><p><strong>&quot;cluster_name&quot; : &quot;kubernetes_logging&quot;,</strong></p><p><strong>&quot;status&quot; : &quot;yellow&quot;,</strong></p><p><strong>&quot;timed_out&quot; : false,</strong></p><p><strong>&quot;number_of_nodes&quot; : 1,</strong></p><p><strong>&quot;number_of_data_nodes&quot; : 1,</strong></p><p><strong>&quot;active_primary_shards&quot; : 5,</strong></p><p><strong>&quot;active_shards&quot; : 5,</strong></p><p><strong>&quot;relocating_shards&quot; : 0,</strong></p><p><strong>&quot;initializing_shards&quot; : 0,</strong></p><p><strong>&quot;unassigned_shards&quot; : 5</strong></p><p><strong>}</strong></p><h6><strong>Using web browsers to access services running on the cluster</strong></h6><p>You may be able to put an apiserver proxy url into the address bar of a browser. However:</p><ul><li>Web browsers cannot usually pass tokens, so you may need to use basic (password) auth. Apiserver can be configured to accept basic auth, but your cluster may not be configured to accept basic auth.</li><li>Some web apps may not work, particularly those with client side javascript that construct urls in a way that is unaware of the proxy path prefix.</li></ul><h4>Requesting redirects</h4><p>The redirect capabilities have been deprecated and removed. Please use a proxy (see below) instead.</p><h4>So Many Proxies</h4><p>There are several different proxies you may encounter when using Kubernetes:</p><ol><li>The <a href="https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#directly-accessing-the-rest-api">kubectl proxy</a>:<ul><li>runs on a user&#x27;s desktop or in a pod</li><li>proxies from a localhost address to the Kubernetes apiserver</li><li>client to proxy uses HTTP</li><li>proxy to apiserver uses HTTPS</li><li>locates apiserver</li><li>adds authentication headers</li></ul></li><li>The <a href="https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#discovering-builtin-services">apiserver proxy</a>:<ul><li>is a bastion built into the apiserver</li><li>connects a user outside of the cluster to cluster IPs which otherwise might not be reachable</li><li>runs in the apiserver processes</li><li>client to proxy uses HTTPS (or http if apiserver so configured)</li><li>proxy to target may use HTTP or HTTPS as chosen by proxy using available information</li><li>can be used to reach a Node, Pod, or Service</li><li>does load balancing when used to reach a Service</li></ul></li><li>The <a href="https://kubernetes.io/docs/concepts/services-networking/service/#ips-and-vips">kube proxy</a>:<ul><li>runs on each node</li><li>proxies UDP and TCP</li><li>does not understand HTTP</li><li>provides load balancing</li><li>is just used to reach services</li></ul></li><li>A Proxy/Load-balancer in front of apiserver(s):<ul><li>existence and implementation varies from cluster to cluster (e.g. nginx)</li><li>sits between all clients and one or more apiservers</li><li>acts as load balancer if there are several apiservers.</li></ul></li><li>Cloud Load Balancers on external services:<ul><li>are provided by some cloud providers (e.g. AWS ELB, Google Cloud Load Balancer)</li><li>are created automatically when the Kubernetes service has type <strong>LoadBalancer</strong></li><li>use UDP/TCP only</li><li>implementation varies by cloud provider.</li></ul></li></ol><p>Kubernetes users will typically not need to worry about anything other than the first two types. The cluster admin will typically ensure that the latter types are setup correctly.</p><h3>Configure Access to Multiple Clusters</h3><p>This page shows how to configure access to multiple clusters by using configuration files. After your clusters, users, and contexts are defined in one or more configuration files, you can quickly switch between clusters by using the <strong>kubectl config use-context</strong> command.</p><p><strong>Note:</strong> A file that is used to configure access to a cluster is sometimes called a kubeconfig file. This is a generic way of referring to configuration files. It does not mean that there is a file named <strong>kubeconfig</strong>.</p><ul><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/#define-clusters-users-and-contexts"><strong>Define clusters, users, and contexts</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/#create-a-second-configuration-file"><strong>Create a second configuration file</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/#set-the-kubeconfig-environment-variable"><strong>Set the KUBECONFIG environment variable</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/#explore-the-homekube-directory"><strong>Explore the $HOME/.kube directory</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/#append-homekubeconfig-to-your-kubeconfig-environment-variable"><strong>Append $HOME/.kube/config to your KUBECONFIG environment variable</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/#clean-up"><strong>Clean up</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Before you begin</h4><p>You need to have the <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/"><strong>kubectl</strong></a> command-line tool installed.</p><h4>Define clusters, users, and contexts</h4><p>Suppose you have two clusters, one for development work and one for scratch work. In the <strong>development</strong> cluster, your frontend developers work in a namespace called <strong>frontend</strong>, and your storage developers work in a namespace called <strong>storage</strong>. In your <strong>scratch</strong> cluster, developers work in the default namespace, or they create auxiliary namespaces as they see fit. Access to the development cluster requires authentication by certificate. Access to the scratch cluster requires authentication by username and password.</p><p>Create a directory named <strong>config-exercise</strong>. In your <strong>config-exercise</strong> directory, create a file named <strong>config-demo</strong> with this content:</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Config</strong></p><p><strong>preferences: {}</strong></p><p><strong>clusters:</strong></p><p><strong>- cluster:</strong></p><p><strong>name: development</strong></p><p><strong>- cluster:</strong></p><p><strong>name: scratch</strong></p><p><strong>users:</strong></p><p><strong>- name: developer</strong></p><p><strong>- name: experimenter</strong></p><p><strong>contexts:</strong></p><p><strong>- context:</strong></p><p><strong>name: dev-frontend</strong></p><p><strong>- context:</strong></p><p><strong>name: dev-storage</strong></p><p><strong>- context:</strong></p><p><strong>name: exp-scratch</strong></p><p>A configuration file describes clusters, users, and contexts. Your <strong>config-demo</strong> file has the framework to describe two clusters, two users, and three contexts.</p><p>Go to your <strong>config-exercise</strong> directory. Enter these commands to add cluster details to your configuration file:</p><p><strong>kubectl config --kubeconfig=config-demo set-cluster development --server=<a href="https://1.2.3.4">https://1.2.3.4</a> --certificate-authority=fake-ca-file</strong></p><p><strong>kubectl config --kubeconfig=config-demo set-cluster scratch --server=<a href="https://5.6.7.8">https://5.6.7.8</a> --insecure-skip-tls-verify</strong></p><p>Add user details to your configuration file:</p><p><strong>kubectl config --kubeconfig=config-demo set-credentials developer --client-certificate=fake-cert-file --client-key=fake-key-seefile</strong></p><p><strong>kubectl config --kubeconfig=config-demo set-credentials experimenter --username=exp --password=some-password</strong></p><p>Add context details to your configuration file:</p><p><strong>kubectl config --kubeconfig=config-demo set-context dev-frontend --cluster=development --namespace=frontend --user=developer</strong></p><p><strong>kubectl config --kubeconfig=config-demo set-context dev-storage --cluster=development --namespace=storage --user=developer</strong></p><p><strong>kubectl config --kubeconfig=config-demo set-context exp-scratch --cluster=scratch --namespace=default --user=experimenter</strong></p><p>Open your <strong>config-demo</strong> file to see the added details. As an alternative to opening the <strong>config-demo</strong> file, you can use the <strong>config view</strong> command.</p><p><strong>kubectl config --kubeconfig=config-demo view</strong></p><p>The output shows the two clusters, two users, and three contexts:</p><p><strong>apiVersion: v1</strong></p><p><strong>clusters:</strong></p><p><strong>- cluster:</strong></p><p><strong>certificate-authority: fake-ca-file</strong></p><p><strong>server: <a href="https://1.2.3.4">https://1.2.3.4</a></strong></p><p><strong>name: development</strong></p><p><strong>- cluster:</strong></p><p><strong>insecure-skip-tls-verify: true</strong></p><p><strong>server: <a href="https://5.6.7.8">https://5.6.7.8</a></strong></p><p><strong>name: scratch</strong></p><p><strong>contexts:</strong></p><p><strong>- context:</strong></p><p><strong>cluster: development</strong></p><p><strong>namespace: frontend</strong></p><p><strong>user: developer</strong></p><p><strong>name: dev-frontend</strong></p><p><strong>- context:</strong></p><p><strong>cluster: development</strong></p><p><strong>namespace: storage</strong></p><p><strong>user: developer</strong></p><p><strong>name: dev-storage</strong></p><p><strong>- context:</strong></p><p><strong>cluster: scratch</strong></p><p><strong>namespace: default</strong></p><p><strong>user: experimenter</strong></p><p><strong>name: exp-scratch</strong></p><p><strong>current-context: &quot;&quot;</strong></p><p><strong>kind: Config</strong></p><p><strong>preferences: {}</strong></p><p><strong>users:</strong></p><p><strong>- name: developer</strong></p><p><strong>user:</strong></p><p><strong>client-certificate: fake-cert-file</strong></p><p><strong>client-key: fake-key-file</strong></p><p><strong>- name: experimenter</strong></p><p><strong>user:</strong></p><p><strong>password: some-password</strong></p><p><strong>username: exp</strong></p><p>Each context is a triple (cluster, user, namespace). For example, the <strong>dev-frontend</strong> context says, Use the credentials of the <strong>developer</strong> user to access the <strong>frontend</strong> namespace of the <strong>development</strong> cluster.</p><p>Set the current context:</p><p><strong>kubectl config --kubeconfig=config-demo use-context dev-frontend</strong></p><p>Now whenever you enter a <strong>kubectl</strong> command, the action will apply to the cluster, and namespace listed in the <strong>dev-frontend</strong> context. And the command will use the credentials of the user listed in the <strong>dev-frontend</strong> context.</p><p>To see only the configuration information associated with the current context, use the <strong>--minify</strong>flag.</p><p><strong>kubectl config --kubeconfig=config-demo view --minify</strong></p><p>The output shows configuration information associated with the <strong>dev-frontend</strong> context:</p><p><strong>apiVersion: v1</strong></p><p><strong>clusters:</strong></p><p><strong>- cluster:</strong></p><p><strong>certificate-authority: fake-ca-file</strong></p><p><strong>server: <a href="https://1.2.3.4">https://1.2.3.4</a></strong></p><p><strong>name: development</strong></p><p><strong>contexts:</strong></p><p><strong>- context:</strong></p><p><strong>cluster: development</strong></p><p><strong>namespace: frontend</strong></p><p><strong>user: developer</strong></p><p><strong>name: dev-frontend</strong></p><p><strong>current-context: dev-frontend</strong></p><p><strong>kind: Config</strong></p><p><strong>preferences: {}</strong></p><p><strong>users:</strong></p><p><strong>- name: developer</strong></p><p><strong>user:</strong></p><p><strong>client-certificate: fake-cert-file</strong></p><p><strong>client-key: fake-key-file</strong></p><p>Now suppose you want to work for a while in the scratch cluster.</p><p>Change the current context to <strong>exp-scratch</strong>:</p><p><strong>kubectl config --kubeconfig=config-demo use-context exp-scratch</strong></p><p>Now any <strong>kubectl</strong> command you give will apply to the default namespace of the <strong>scratch</strong> cluster. And the command will use the credentials of the user listed in the <strong>exp-scratch</strong> context.</p><p>View configuration associated with the new current context, <strong>exp-scratch</strong>.</p><p><strong>kubectl config --kubeconfig=config-demo view --minify</strong></p><p>Finally, suppose you want to work for a while in the <strong>storage</strong> namespace of the <strong>development</strong>cluster.</p><p>Change the current context to <strong>dev-storage</strong>:</p><p><strong>kubectl config --kubeconfig=config-demo use-context dev-storage</strong></p><p>View configuration associated with the new current context, <strong>dev-storage</strong>.</p><p><strong>kubectl config --kubeconfig=config-demo view --minify</strong></p><h4>Create a second configuration file</h4><p>In your <strong>config-exercise</strong> directory, create a file named <strong>config-demo-2</strong> with this content:</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Config</strong></p><p><strong>preferences: {}</strong></p><p><strong>contexts:</strong></p><p><strong>- context:</strong></p><p><strong>cluster: development</strong></p><p><strong>namespace: ramp</strong></p><p><strong>user: developer</strong></p><p><strong>name: dev-ramp-up</strong></p><p>The preceding configuration file defines a new context named <strong>dev-ramp-up</strong>.</p><h4>Set the KUBECONFIG environment variable</h4><p>See whether you have an environment variable named <strong>KUBECONFIG</strong>. If so, save the current value of your <strong>KUBECONFIG</strong> environment variable, so you can restore it later. For example, on Linux:</p><p><strong>export KUBECONFIG_SAVED=$KUBECONFIG</strong></p><p>The <strong>KUBECONFIG</strong> environment variable is a list of paths to configuration files. The list is colon-delimited for Linux and Mac, and semicolon-delimited for Windows. If you have a <strong>KUBECONFIG</strong>environment variable, familiarize yourself with the configuration files in the list.</p><p>Temporarily append two paths to your <strong>KUBECONFIG</strong> environment variable. For example, on Linux:</p><p><strong>export KUBECONFIG=$KUBECONFIG:config-demo:config-demo-2</strong></p><p>In your <strong>config-exercise</strong> directory, enter this command:</p><p><strong>kubectl config view</strong></p><p>The output shows merged information from all the files listed in your <strong>KUBECONFIG</strong> environment variable. In particular, notice that the merged information has the <strong>dev-ramp-up</strong> context from the <strong>config-demo-2</strong> file and the three contexts from the <strong>config-demo</strong> file:</p><p><strong>contexts:</strong></p><p><strong>- context:</strong></p><p><strong>cluster: development</strong></p><p><strong>namespace: frontend</strong></p><p><strong>user: developer</strong></p><p><strong>name: dev-frontend</strong></p><p><strong>- context:</strong></p><p><strong>cluster: development</strong></p><p><strong>namespace: ramp</strong></p><p><strong>user: developer</strong></p><p><strong>name: dev-ramp-up</strong></p><p><strong>- context:</strong></p><p><strong>cluster: development</strong></p><p><strong>namespace: storage</strong></p><p><strong>user: developer</strong></p><p><strong>name: dev-storage</strong></p><p><strong>- context:</strong></p><p><strong>cluster: scratch</strong></p><p><strong>namespace: default</strong></p><p><strong>user: experimenter</strong></p><p><strong>name: exp-scratch</strong></p><p>For more information about how kubeconfig files are merged, see <a href="https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/">Organizing Cluster Access Using kubeconfig Files</a></p><h4>Explore the $HOME/.kube directory</h4><p>If you already have a cluster, and you can use <strong>kubectl</strong> to interact with the cluster, then you probably have a file named <strong>config</strong> in the <strong>$HOME/.kube</strong> directory.</p><p>Go to <strong>$HOME/.kube</strong>, and see what files are there. Typically, there is a file named <strong>config</strong>. There might also be other configuration files in this directory. Briefly familiarize yourself with the contents of these files.</p><h4>Append $HOME/.kube/config to your KUBECONFIG environment variable</h4><p>If you have a <strong>$HOME/.kube/config</strong> file, and it&#x27;s not already listed in your <strong>KUBECONFIG</strong> environment variable, append it to your <strong>KUBECONFIG</strong> environment variable now. For example, on Linux:</p><p><strong>export KUBECONFIG=$KUBECONFIG:$HOME/.kube/config</strong></p><p>View configuration information merged from all the files that are now listed in your <strong>KUBECONFIG</strong>environment variable. In your config-exercise directory, enter:</p><p><strong>kubectl config view</strong></p><h4>Clean up</h4><p>Return your <strong>KUBECONFIG</strong> environment variable to its original value. For example, on Linux:</p><p><strong>export KUBECONFIG=$KUBECONFIG_SAVED</strong></p><h4>What&#x27;s next</h4><ul><li><a href="https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/">Organizing Cluster Access Using kubeconfig Files</a></li><li><a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/">kubectl config</a></li></ul><h3>Use Port Forwarding to Access Applications in a Cluster</h3><p>This page shows how to use <strong>kubectl port-forward</strong> to connect to a Redis server running in a Kubernetes cluster. This type of connection can be useful for database debugging.</p><ul><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/#creating-redis-deployment-and-service"><strong>Creating Redis deployment and service</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/#forward-a-local-port-to-a-port-on-the-pod"><strong>Forward a local port to a port on the pod</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/#discussion"><strong>Discussion</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Before you begin</h4><ul><li>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using <a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</li><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <strong>kubectl version</strong>.</p><ul><li>Install <a href="http://redis.io/topics/rediscli">redis-cli</a>.</li></ul><h4>Creating Redis deployment and service</h4><ol><li>Create a Redis deployment:</li><li><strong>kubectl create -f <a href="https://k8s.io/docs/tutorials/stateless-application/guestbook/redis-master-deployment.yaml">https://k8s.io/docs/tutorials/stateless-application/guestbook/redis-master-deployment.yaml</a></strong></li></ol><p>The output of a successful command verifies that the deployment was created:</p><p><strong>deployment &quot;redis-master&quot; created</strong></p><p>When the pod is ready, you can get:</p><p><strong>kubectl get pods</strong></p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>redis-master-765d459796-258hz 1/1 Running 0 50s</strong></p><p><strong>kubectl get deployment</strong></p><p><strong>NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE</strong></p><p><strong>redis-master 1 1 1 1 55s</strong></p><p><strong>kubectl get rs</strong></p><p><strong>NAME DESIRED CURRENT READY AGE</strong></p><p><strong>redis-master-765d459796 1 1 1 1m</strong></p><ol><li>Create a Redis service:</li><li><strong>kubectl create -f <a href="https://k8s.io/docs/tutorials/stateless-application/guestbook/redis-master-service.yaml">https://k8s.io/docs/tutorials/stateless-application/guestbook/redis-master-service.yaml</a></strong></li></ol><p>The output of a successful command verifies that the service was created:</p><p><strong>service &quot;redis-master&quot; created</strong></p><p>Check the service created:</p><p><strong>kubectl get svc | grep redis</strong></p><p><strong>NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE</strong></p><p><strong>redis-master ClusterIP 10.0.0.213 <code>&lt;none&gt;</code> 6379/TCP 27s</strong></p><ol><li>Verify that the Redis server is running in the pod and listening on port 6379:</li><li><strong>kubectl get pods redis-master-765d459796-258hz --template=\&#x27;{{(index (index .spec.containers 0).ports 0).containerPort}}{{&quot;<!-- -->\<!-- -->n&quot;}}\&#x27;</strong></li></ol><p>The output displays the port:</p><p><strong>6379</strong></p><h4>Forward a local port to a port on the pod</h4><ol><li><strong>kubectl port-forward</strong> allows using resource name, such as a service name, to select a matching pod to port forward to since Kubernetes v1.10.</li><li><strong>kubectl port-forward redis-master-765d459796-258hz 6379:6379</strong></li></ol><p>which is the same as</p><p><strong>kubectl port-forward pods/redis-master-765d459796-258hz 6379:6379</strong></p><p>or</p><p><strong>kubectl port-forward deployment/redis-master 6379:6379</strong></p><p>or</p><p><strong>kubectl port-forward rs/redis-master 6379:6379</strong></p><p>or</p><p><strong>kubectl port-forward svc/redis-master 6379:6379</strong></p><p>Any of the above commands works. The output is similar to this:</p><p><strong>I0710 14:43:38.274550 3655 portforward.go:225] Forwarding from 127.0.0.1:6379 -&gt; 6379</strong></p><p><strong>I0710 14:43:38.274797 3655 portforward.go:225] Forwarding from <!-- -->[::1]<!-- -->:6379 -&gt; 6379</strong></p><ol><li>Start the Redis command line interface:</li><li><strong>redis-cli</strong></li><li>At the Redis command line prompt, enter the <strong>ping</strong> command:</li><li><strong>127.0.0.1:6379&gt;ping</strong></li></ol><p>A successful ping request returns PONG.</p><h4>Discussion</h4><p>Connections made to local port 6379 are forwarded to port 6379 of the pod that is running the Redis server. With this connection in place you can use your local workstation to debug the database that is running in the pod.</p><p><strong>Warning</strong>: Due to known limitations, port forward today only works for TCP protocol. The support to UDP protocol is being tracked in <a href="https://github.com/kubernetes/kubernetes/issues/47862">issue 47862</a>.</p><h4>What&#x27;s next</h4><p>Learn more about <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#port-forward">kubectl port-forward</a>.</p><h3>Provide Load-Balanced Access to an Application in a Cluster</h3><p>This page shows how to create a Kubernetes Service object that provides load-balanced access to an application running in a cluster.</p><ul><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/load-balance-access-application-cluster/#objectives"><strong>Objectives</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/load-balance-access-application-cluster/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/load-balance-access-application-cluster/#creating-a-service-for-an-application-running-in-two-pods"><strong>Creating a Service for an application running in two pods</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/load-balance-access-application-cluster/#using-a-service-configuration-file"><strong>Using a service configuration file</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/load-balance-access-application-cluster/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Objectives</h4><ul><li>Run two instances of a Hello World application</li><li>Create a Service object</li><li>Use the Service object to access the running application</li></ul><h4>Before you begin</h4><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using<a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <strong>kubectl version</strong>.</p><h4>Creating a Service for an application running in two pods</h4><ol><li>Run a Hello World application in your cluster:</li><li><strong>kubectl run hello-world --replicas=2 --labels=&quot;run=load-balancer-example&quot; --image=gcr.io/google-samples/node-hello:1.0 --port=8080</strong></li><li>List the pods that are running the Hello World application:</li><li><strong>kubectl get pods --selector=&quot;run=load-balancer-example&quot;</strong></li></ol><p>The output is similar to this:</p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>hello-world-2189936611-8fyp0 1/1 Running 0 6m</strong></p><p><strong>hello-world-2189936611-9isq8 1/1 Running 0 6m</strong></p><ol><li>List the replica set for the two Hello World pods:</li><li><strong>kubectl get replicasets --selector=&quot;run=load-balancer-example&quot;</strong></li></ol><p>The output is similar to this:</p><p><strong>NAME DESIRED CURRENT AGE</strong></p><p><strong>hello-world-2189936611 2 2 12m</strong></p><ol><li>Create a Service object that exposes the replica set:</li><li><strong>kubectl expose rs <code>&lt;your-replica-set-name&gt;</code> --type=&quot;LoadBalancer&quot; --name=&quot;example-service&quot;</strong></li></ol><p>where <strong><code>&lt;your-replica-set-name&gt;</code></strong> is the name of your replica set.</p><ol><li>Display the IP addresses for your service:</li><li><strong>kubectl get services example-service</strong></li></ol><p>The output shows the internal IP address and the external IP address of your service. If the external IP address shows as <strong><code>&lt;pending&gt;</code></strong>, repeat the command.</p><p>Note: If you are using Minikube, you don&#x27;t get an external IP address. The external IP address remains in the pending state.</p><p><strong>NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE</strong></p><p><strong>example-service 10.0.0.160 <code>&lt;pending&gt;</code> 8080/TCP 40s</strong></p><ol><li>Use your Service object to access the Hello World application:</li><li><strong>curl <code>&lt;your-external-ip-address&gt;</code>:8080</strong></li></ol><p>where <strong><code>&lt;your-external-ip-address&gt;</code></strong> is the external IP address of your service.</p><p>The output is a hello message from the application:</p><p><strong>Hello Kubernetes!</strong></p><p>Note: If you are using Minikube, enter these commands:</p><p><strong>kubectl cluster-info</strong></p><p><strong>kubectl describe services example-service</strong></p><p>The output displays the IP address of your Minikube node and the NodePort value for your service. Then enter this command to access the Hello World application:</p><p><strong>curl <code>&lt;minikube-node-ip-address&gt;:&lt;service-node-port&gt;</code></strong></p><p>where <code style="background-color:lightgray">&lt;minikube-node-ip-address&gt;</code> us the IP address of your Minikube node, and <code style="background-color:lightgray">&lt;service-node-port&gt;</code> is the NodePort value for your service.</p><h4>Using a service configuration file</h4><p>As an alternative to using <strong>kubectl expose</strong>, you can use a <a href="https://kubernetes.io/docs/concepts/services-networking/service/">service configuration file</a> to create a Service.</p><h4>What&#x27;s next</h4><p>Learn more about <a href="https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/">connecting applications with services</a>.</p><h3>Use a Service to Access an Application in a Cluster</h3><p>This page shows how to create a Kubernetes Service object that external clients can use to access an application running in a cluster. The Service provides load balancing for an application that has two running instances.</p><ul><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/service-access-application-cluster/#objectives"><strong>Objectives</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/service-access-application-cluster/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/service-access-application-cluster/#creating-a-service-for-an-application-running-in-two-pods"><strong>Creating a service for an application running in two pods</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/service-access-application-cluster/#using-a-service-configuration-file"><strong>Using a service configuration file</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/service-access-application-cluster/#cleaning-up"><strong>Cleaning up</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/service-access-application-cluster/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Objectives</h4><ul><li>Run two instances of a Hello World application.</li><li>Create a Service object that exposes a node port.</li><li>Use the Service object to access the running application.</li></ul><h4>Before you begin</h4><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using<a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <strong>kubectl version</strong>.</p><h4>Creating a service for an application running in two pods</h4><ol><li>Run a Hello World application in your cluster:</li><li><strong>kubectl run hello-world --replicas=2 --labels=&quot;run=load-balancer-example&quot; --image=gcr.io/google-samples/node-hello:1.0 --port=8080</strong></li></ol><p>The preceding command creates a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployment</a> object and an associated <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/">ReplicaSet</a> object. The ReplicaSet has two <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/">Pods</a>, each of which runs the Hello World application.</p><ol><li>Display information about the Deployment:</li><li><strong>kubectl get deployments hello-world</strong></li><li><strong>kubectl describe deployments hello-world</strong></li><li>Display information about your ReplicaSet objects:</li><li><strong>kubectl get replicasets</strong></li><li><strong>kubectl describe replicasets</strong></li><li>Create a Service object that exposes the deployment:</li><li><strong>kubectl expose deployment hello-world --type=NodePort --name=example-service</strong></li><li>Display information about the Service:</li><li><strong>kubectl describe services example-service</strong></li></ol><p>The output is similar to this:</p><p><strong>Name: example-service</strong></p><p><strong>Namespace: default</strong></p><p><strong>Labels: run=load-balancer-example</strong></p><p><strong>Annotations: <code>&lt;none&gt;</code></strong></p><p><strong>Selector: run=load-balancer-example</strong></p><p><strong>Type: NodePort</strong></p><p><strong>IP: 10.32.0.16</strong></p><p><strong>Port: <code>&lt;unset&gt;</code> 8080/TCP</strong></p><p><strong>TargetPort: 8080/TCP</strong></p><p><strong>NodePort: <code>&lt;unset&gt;</code> 31496/TCP</strong></p><p><strong>Endpoints: 10.200.1.4:8080,10.200.2.5:8080</strong></p><p><strong>Session Affinity: None</strong></p><p><strong>Events: <code>&lt;none&gt;</code></strong></p><p>Make a note of the NodePort value for the service. For example, in the preceding output, the NodePort value is 31496.</p><ol><li>List the pods that are running the Hello World application:</li><li><strong>kubectl get pods --selector=&quot;run=load-balancer-example&quot; --output=wide</strong></li></ol><p>The output is similar to this:</p><p><strong>NAME READY STATUS <!-- -->.<!-- -->.. IP NODE</strong></p><p><strong>hello-world-2895499144-bsbk5 1/1 Running <!-- -->.<!-- -->.. 10.200.1.4 worker1</strong></p><p><strong>hello-world-2895499144-m1pwt 1/1 Running <!-- -->.<!-- -->.. 10.200.2.5 worker2</strong></p><ol><li>Get the public IP address of one of your nodes that is running a Hello World pod. How you get this address depends on how you set up your cluster. For example, if you are using Minikube, you can see the node address by running <strong>kubectl cluster-info</strong>. If you are using Google Compute Engine instances, you can use the <strong>gcloud compute instances list</strong> command to see the public addresses of your nodes. For more information about this command, see the <a href="https://cloud.google.com/sdk/gcloud/reference/compute/instances/list">GCE documentation</a>.</li><li>On your chosen node, create a firewall rule that allows TCP traffic on your node port. For example, if your Service has a NodePort value of 31568, create a firewall rule that allows TCP traffic on port 31568. Different cloud providers offer different ways of configuring firewall rules. See <a href="https://cloud.google.com/compute/docs/vpc/firewalls">the GCE documentation on firewall rules</a>, for example.</li><li>Use the node address and node port to access the Hello World application:</li><li><code>curl http://&lt;public-node-ip&gt;:&lt;node-port&gt;</code></li></ol><p>where <strong><code>&lt;public-node-ip&gt; is the public IP address of your node, and &lt;node-port&gt;</code></strong> is the NodePort value for your service.</p><p>The response to a successful request is a hello message:</p><p><strong>Hello Kubernetes!</strong></p><h4>Using a service configuration file</h4><p>As an alternative to using <strong>kubectl expose</strong>, you can use a <a href="https://kubernetes.io/docs/concepts/services-networking/service/">service configuration file</a> to create a Service.</p><h4>Cleaning up</h4><p>To delete the Service, enter this command:</p><p><strong>kubectl delete services example-service</strong></p><p>To delete the Deployment, the ReplicaSet, and the Pods that are running the Hello World application, enter this command:</p><p><strong>kubectl delete deployment hello-world</strong></p><h4>What&#x27;s next</h4><p>Learn more about <a href="https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/">connecting applications with services</a>.</p><h3>Connect a Front End to a Back End Using a Service</h3><p>This task shows how to create a frontend and a backend microservice. The backend microservice is a hello greeter. The frontend and backend are connected using a Kubernetes Service object.</p><ul><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/connecting-frontend-backend/#objectives"><strong>Objectives</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/connecting-frontend-backend/#before-you-begin"><strong>Before you begin</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/connecting-frontend-backend/#creating-the-backend-using-a-deployment"><strong>Creating the backend using a Deployment</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/connecting-frontend-backend/#creating-the-backend-service-object"><strong>Creating the backend Service object</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/connecting-frontend-backend/#creating-the-frontend"><strong>Creating the frontend</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/connecting-frontend-backend/#interact-with-the-frontend-service"><strong>Interact with the frontend Service</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/connecting-frontend-backend/#send-traffic-through-the-frontend"><strong>Send traffic through the frontend</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/connecting-frontend-backend/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Objectives</h4><ul><li>Create and run a microservice using a Deployment object.</li><li>Route traffic to the backend using a frontend.</li><li>Use a Service object to connect the frontend application to the backend application.</li></ul><h4>Before you begin</h4><ul><li>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using <a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</li><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <strong>kubectl version</strong>.</p><ul><li>This task uses <a href="https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/">Services with external load balancers</a>, which require a supported environment. If your environment does not support this, you can use a Service of type <a href="https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport">NodePort</a> instead.</li></ul><h5><strong>Creating the backend using a Deployment</strong></h5><p>The backend is a simple hello greeter microservice. Here is the configuration file for the backend Deployment:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>hello.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubern">https://raw.githubusercontent.com/kubern</a>            |
| etes/website/master/docs/tasks/access-application-cluster/hello.yaml) |
+=======================================================================+
| <strong>apiVersion: apps/v1</strong>                                               |
|                                                                       |
| <strong>kind: Deployment</strong>                                                  |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: hello</strong>                                                       |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>matchLabels:</strong>                                                      |
|                                                                       |
| <strong>app: hello</strong>                                                        |
|                                                                       |
| <strong>tier: backend</strong>                                                     |
|                                                                       |
| <strong>track: stable</strong>                                                     |
|                                                                       |
| <strong>replicas: 7</strong>                                                       |
|                                                                       |
| <strong>template:</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: hello</strong>                                                        |
|                                                                       |
| <strong>tier: backend</strong>                                                     |
|                                                                       |
| <strong>track: stable</strong>                                                     |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: hello</strong>                                                     |
|                                                                       |
| <strong>image: &quot;gcr.io/google-samples/hello-go-gke:1.0&quot;</strong>                 |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- name: http</strong>                                                      |
|                                                                       |
| <strong>containerPort: 80</strong>                                                 |
+-----------------------------------------------------------------------+</p><p>Create the backend Deployment:</p><p><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/access-application-cluster/hello.yaml">https://k8s.io/docs/tasks/access-application-cluster/hello.yaml</a></strong></p><p>View information about the backend Deployment:</p><p><strong>kubectl describe deployment hello</strong></p><p>The output is similar to this:</p><p><strong>Name: hello</strong></p><p><strong>Namespace: default</strong></p><p><strong>CreationTimestamp: Mon, 24 Oct 2016 14:21:02 -0700</strong></p><p><strong>Labels: app=hello</strong></p><p><strong>tier=backend</strong></p><p><strong>track=stable</strong></p><p><strong>Annotations: deployment.kubernetes.io/revision=1</strong></p><p><strong>Selector: app=hello,tier=backend,track=stable</strong></p><p><strong>Replicas: 7 desired | 7 updated | 7 total | 7 available | 0 unavailable</strong></p><p><strong>StrategyType: RollingUpdate</strong></p><p><strong>MinReadySeconds: 0</strong></p><p><strong>RollingUpdateStrategy: 1 max unavailable, 1 max surge</strong></p><p><strong>Pod Template:</strong></p><p><strong>Labels: app=hello</strong></p><p><strong>tier=backend</strong></p><p><strong>track=stable</strong></p><p><strong>Containers:</strong></p><p><strong>hello:</strong></p><p><strong>Image: &quot;gcr.io/google-samples/hello-go-gke:1.0&quot;</strong></p><p><strong>Port: 80/TCP</strong></p><p><strong>Environment: <code>&lt;none&gt;</code></strong></p><p><strong>Mounts: <code>&lt;none&gt;</code></strong></p><p><strong>Volumes: <code>&lt;none&gt;</code></strong></p><p><strong>Conditions:</strong></p><p><strong>Type Status Reason</strong></p><p><strong>---- ------ ------</strong></p><p><strong>Available True MinimumReplicasAvailable</strong></p><p><strong>Progressing True NewReplicaSetAvailable</strong></p><p><strong>OldReplicaSets: <code>&lt;none&gt;</code></strong></p><p><strong>NewReplicaSet: hello-3621623197 (7/7 replicas created)</strong></p><p><strong>Events:</strong></p><p><strong>.<!-- -->..</strong></p><h5><strong>Creating the backend Service object</strong></h5><p>The key to connecting a frontend to a backend is the backend Service. A Service creates a persistent IP address and DNS name entry so that the backend microservice can always be reached. A Service uses selector labels to find the Pods that it routes traffic to.</p><p>First, explore the Service configuration file:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>he                                                                 |
| llo-service.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/web">https://raw.githubusercontent.com/kubernetes/web</a> |
| site/master/docs/tasks/access-application-cluster/hello-service.yaml) |
+=======================================================================+
| <strong>kind: Service</strong>                                                     |
|                                                                       |
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: hello</strong>                                                       |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>app: hello</strong>                                                        |
|                                                                       |
| <strong>tier: backend</strong>                                                     |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- protocol: TCP</strong>                                                   |
|                                                                       |
| <strong>port: 80</strong>                                                          |
|                                                                       |
| <strong>targetPort: http</strong>                                                  |
+-----------------------------------------------------------------------+</p><p>In the configuration file, you can see that the Service routes traffic to Pods that have the labels <strong>app: hello</strong> and <strong>tier: backend</strong>.</p><p>Create the <strong>hello</strong> Service:</p><p><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/access-application-cluster/hello-service.yaml">https://k8s.io/docs/tasks/access-application-cluster/hello-service.yaml</a></strong></p><p>At this point, you have a backend Deployment running, and you have a Service that can route traffic to it.</p><h5><strong>Creating the frontend</strong></h5><p>Now that you have your backend, you can create a frontend that connects to the backend. The frontend connects to the backend worker Pods by using the DNS name given to the backend Service. The DNS name is &quot;hello&quot;, which is the value of the <strong>name</strong> field in the preceding Service configuration file.</p><p>The Pods in the frontend Deployment run an nginx image that is configured to find the hello backend Service. Here is the nginx configuration file:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>frontend/f                                                         |
| rontend.conf</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/website">https://raw.githubusercontent.com/kubernetes/website</a> |
| /master/docs/tasks/access-application-cluster/frontend/frontend.conf) |
+=======================================================================+
| <strong>upstream hello {</strong>                                                  |
|                                                                       |
| <strong>server hello;</strong>                                                     |
|                                                                       |
| <strong>}</strong>                                                                 |
|                                                                       |
| <strong>server {</strong>                                                          |
|                                                                       |
| <strong>listen 80;</strong>                                                        |
|                                                                       |
| <strong>location / {</strong>                                                      |
|                                                                       |
| <strong>proxy_pass http://hello;</strong>                                          |
|                                                                       |
| <strong>}</strong>                                                                 |
|                                                                       |
| <strong>}</strong>                                                                 |
+-----------------------------------------------------------------------+</p><p>Similar to the backend, the frontend has a Deployment and a Service. The configuration for the Service has <strong>type: LoadBalancer</strong>, which means that the Service uses the default load balancer of your cloud provider.</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>frontend.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernete">https://raw.githubusercontent.com/kubernete</a>      |
| s/website/master/docs/tasks/access-application-cluster/frontend.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Service</strong>                                                     |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: frontend</strong>                                                    |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>app: hello</strong>                                                        |
|                                                                       |
| <strong>tier: frontend</strong>                                                    |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- protocol: &quot;TCP&quot;</strong>                                               |
|                                                                       |
| <strong>port: 80</strong>                                                          |
|                                                                       |
| <strong>targetPort: 80</strong>                                                    |
|                                                                       |
| <strong>type: LoadBalancer</strong>                                                |
|                                                                       |
| <strong>---</strong>                                                             |
|                                                                       |
| <strong>apiVersion: apps/v1</strong>                                               |
|                                                                       |
| <strong>kind: Deployment</strong>                                                  |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: frontend</strong>                                                    |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>matchLabels:</strong>                                                      |
|                                                                       |
| <strong>app: hello</strong>                                                        |
|                                                                       |
| <strong>tier: frontend</strong>                                                    |
|                                                                       |
| <strong>track: stable</strong>                                                     |
|                                                                       |
| <strong>replicas: 1</strong>                                                       |
|                                                                       |
| <strong>template:</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: hello</strong>                                                        |
|                                                                       |
| <strong>tier: frontend</strong>                                                    |
|                                                                       |
| <strong>track: stable</strong>                                                     |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: nginx</strong>                                                     |
|                                                                       |
| <strong>image: &quot;gcr.io/google-samples/hello-frontend:1.0&quot;</strong>               |
|                                                                       |
| <strong>lifecycle:</strong>                                                        |
|                                                                       |
| <strong>preStop:</strong>                                                          |
|                                                                       |
| <strong>exec:</strong>                                                             |
|                                                                       |
| <strong>command: <!-- -->[&quot;/usr/sbin/nginx&quot;,&quot;-s&quot;,&quot;quit&quot;]</strong>                  |
+-----------------------------------------------------------------------+</p><p>Create the frontend Deployment and Service:</p><p><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/access-application-cluster/frontend.yaml">https://k8s.io/docs/tasks/access-application-cluster/frontend.yaml</a></strong></p><p>The output verifies that both resources were created:</p><p><strong>deployment &quot;frontend&quot; created</strong></p><p><strong>service &quot;frontend&quot; created</strong></p><p><strong>Note</strong>: The nginx configuration is baked into the <a href="https://kubernetes.io/docs/tasks/access-application-cluster/frontend/Dockerfile">container image</a>. A better way to do this would be to use a <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/">ConfigMap</a>, so that you can change the configuration more easily.</p><h5><strong>Interact with the frontend Service</strong></h5><p>Once you&#x27;ve created a Service of type LoadBalancer, you can use this command to find the external IP:</p><p><strong>kubectl get service frontend</strong></p><p>The external IP field may take some time to populate. If this is the case, the external IP is listed as <strong><code>&lt;pending&gt;</code></strong>.</p><p><strong>NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE</strong></p><p><strong>frontend 10.51.252.116 <code>&lt;pending&gt;</code> 80/TCP 10s</strong></p><p>Repeat the same command again until it shows an external IP address:</p><p><strong>NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE</strong></p><p><strong>frontend 10.51.252.116 XXX.XXX.XXX.XXX 80/TCP 1m</strong></p><h5><strong>Send traffic through the frontend</strong></h5><p>The frontend and backends are now connected. You can hit the endpoint by using the curl command on the external IP of your frontend Service.</p><p><strong>curl http://<code>&lt;EXTERNAL-IP&gt;</code></strong></p><p>The output shows the message generated by the backend:</p><p><strong>{&quot;message&quot;:&quot;Hello&quot;}</strong></p><h4>What&#x27;s next</h4><ul><li>Learn more about <a href="https://kubernetes.io/docs/concepts/services-networking/service/">Services</a></li><li>Learn more about <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/">ConfigMaps</a></li></ul><h3>Create an External Load Balancer</h3><p>This page shows how to create an External Load Balancer.</p><p>When creating a service, you have the option of automatically creating a cloud network load balancer. This provides an externally-accessible IP address that sends traffic to the correct port on your cluster nodes provided your cluster runs in a supported environment and is configured with the correct cloud load balancer provider package.</p><p>For information on provisioning and using an Ingress resource that can give services externally-reachable URLs, load balance the traffic, terminate SSL etc., please check the <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">Ingress</a>documentation.</p><ul><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#configuration-file"><strong>Configuration file</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#using-kubectl"><strong>Using kubectl</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#finding-your-ip-address"><strong>Finding your IP address</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip"><strong>Preserving the client source IP</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#feature-availability"><strong>Feature availability</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#external-load-balancer-providers"><strong>External Load Balancer Providers</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#caveats-and-limitations-when-preserving-source-ips"><strong>Caveats and Limitations when preserving source IPs</strong></a></li></ul><h4>Before you begin</h4><ul><li>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using <a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</li><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <strong>kubectl version</strong>.</p><h4>Configuration file</h4><p>To create an external load balancer, add the following line to your <a href="https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer">service configuration file</a>:</p><p><strong>&quot;type&quot;: &quot;LoadBalancer&quot;</strong></p><p>Your configuration file might look like:</p><p><strong>{</strong></p><p><strong>&quot;kind&quot;: &quot;Service&quot;,</strong></p><p><strong>&quot;apiVersion&quot;: &quot;v1&quot;,</strong></p><p><strong>&quot;metadata&quot;: {</strong></p><p><strong>&quot;name&quot;: &quot;example-service&quot;</strong></p><p><strong>},</strong></p><p><strong>&quot;spec&quot;: {</strong></p><p><strong>&quot;ports&quot;: [{</strong></p><p><strong>&quot;port&quot;: 8765,</strong></p><p><strong>&quot;targetPort&quot;: 9376</strong></p><p><strong>}],</strong></p><p><strong>&quot;selector&quot;: {</strong></p><p><strong>&quot;app&quot;: &quot;example&quot;</strong></p><p><strong>},</strong></p><p><strong>&quot;type&quot;: &quot;LoadBalancer&quot;</strong></p><p><strong>}</strong></p><p><strong>}</strong></p><h4>Using kubectl</h4><p>You can alternatively create the service with the <strong>kubectl expose</strong> command and its <strong>--type=LoadBalancer</strong> flag:</p><p><strong>kubectl expose rc example --port=8765 --target-port=9376 <!-- -->\</strong></p><p><strong>--name=example-service --type=LoadBalancer</strong></p><p>This command creates a new service using the same selectors as the referenced resource (in the case of the example above, a replication controller named <strong>example</strong>).</p><p>For more information, including optional flags, refer to the <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#expose"><strong>kubectl expose</strong> reference</a>.</p><h4>Finding your IP address</h4><p>You can find the IP address created for your service by getting the service information through <strong>kubectl</strong>:</p><p><strong>kubectl describe services example-service</strong></p><p>which should produce output like this:</p><p><strong>Name: example-service</strong></p><p><strong>Namespace: default</strong></p><p><strong>Labels: <code>&lt;none&gt;</code></strong></p><p><strong>Annotations: <code>&lt;none&gt;</code></strong></p><p><strong>Selector: app=example</strong></p><p><strong>Type: LoadBalancer</strong></p><p><strong>IP: 10.67.252.103</strong></p><p><strong>LoadBalancer Ingress: 123.45.678.9</strong></p><p><strong>Port: <code>&lt;unnamed&gt;</code> 80/TCP</strong></p><p><strong>NodePort: <code>&lt;unnamed&gt;</code> 32445/TCP</strong></p><p><strong>Endpoints: 10.64.0.4:80,10.64.1.5:80,10.64.2.4:80</strong></p><p><strong>Session Affinity: None</strong></p><p><strong>Events: <code>&lt;none&gt;</code></strong></p><p>The IP address is listed next to <strong>LoadBalancer Ingress</strong>.</p><p><strong>Note</strong>: If you are running your service on Minikube, you can find the assigned IP address and port with:</p><p><strong>minikube service example-service --url</strong></p><h4>Preserving the client source IP</h4><p>Due to the implementation of this feature, the source IP seen in the target container will not be the original source IP of the client. To enable preservation of the client IP, the following fields can be configured in the service spec (supported in GCE/Google Kubernetes Engine environments):</p><ul><li><strong>service.spec.externalTrafficPolicy</strong> - denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints. There are two available options: &quot;Cluster&quot; (default) and &quot;Local&quot;. &quot;Cluster&quot; obscures the client source IP and may cause a second hop to another node, but should have good overall load-spreading. &quot;Local&quot; preserves the client source IP and avoids a second hop for LoadBalancer and NodePort type services, but risks potentially imbalanced traffic spreading.</li><li><strong>service.spec.healthCheckNodePort</strong> - specifies the healthcheck nodePort (numeric port number) for the service. If not specified, healthCheckNodePort is created by the service API backend with the allocated nodePort. It will use the user-specified nodePort value if specified by the client. It only has an effect when type is set to &quot;LoadBalancer&quot; and externalTrafficPolicy is set to &quot;Local&quot;.</li></ul><p>This feature can be activated by setting <strong>externalTrafficPolicy</strong> to &quot;Local&quot; in the Service Configuration file.</p><p><strong>{</strong></p><p><strong>&quot;kind&quot;: &quot;Service&quot;,</strong></p><p><strong>&quot;apiVersion&quot;: &quot;v1&quot;,</strong></p><p><strong>&quot;metadata&quot;: {</strong></p><p><strong>&quot;name&quot;: &quot;example-service&quot;</strong></p><p><strong>},</strong></p><p><strong>&quot;spec&quot;: {</strong></p><p><strong>&quot;ports&quot;: [{</strong></p><p><strong>&quot;port&quot;: 8765,</strong></p><p><strong>&quot;targetPort&quot;: 9376</strong></p><p><strong>}],</strong></p><p><strong>&quot;selector&quot;: {</strong></p><p><strong>&quot;app&quot;: &quot;example&quot;</strong></p><p><strong>},</strong></p><p><strong>&quot;type&quot;: &quot;LoadBalancer&quot;,</strong></p><p><strong>&quot;externalTrafficPolicy&quot;: &quot;Local&quot;</strong></p><p><strong>}</strong></p><p><strong>}</strong></p><h5><strong>Feature availability</strong></h5><p>  k8s version   Feature support</p><hr/><p>  1.7+          Supports the full API fields
1.5 - 1.6     Supports Beta Annotations
&lt;1.5         Unsupported</p><p>Below you could find the deprecated Beta annotations used to enable this feature prior to its stable version. Newer Kubernetes versions may stop supporting these after v1.7. Please update existing applications to use the fields directly.</p><ul><li><strong>service.beta.kubernetes.io/external-traffic</strong> annotation <code>&lt;-&gt;</code> <strong>service.spec.externalTrafficPolicy</strong> field</li><li><strong>service.beta.kubernetes.io/healthcheck-nodeport</strong> annotation <code>&lt;-&gt;</code> <strong>service.spec.healthCheckNodePort</strong> field</li></ul><p><strong>service.beta.kubernetes.io/external-traffic</strong> annotation has a different set of values compared to the <strong>service.spec.externalTrafficPolicy</strong> field. The values match as follows:</p><ul><li>&quot;OnlyLocal&quot; for annotation <code>&lt;-&gt;</code> &quot;Local&quot; for field</li><li>&quot;Global&quot; for annotation <code>&lt;-&gt;</code> &quot;Cluster&quot; for field</li></ul><p><strong>Note that this feature is not currently implemented for all cloudproviders/environments.</strong></p><p>Known issues:</p><ul><li>AWS: <a href="https://github.com/kubernetes/kubernetes/issues/35758">kubernetes/kubernetes#35758</a></li><li>Weave-Net: <a href="https://github.com/weaveworks/weave/issues/2924">weaveworks/weave/#2924</a></li></ul><h4>External Load Balancer Providers</h4><p>It is important to note that the datapath for this functionality is provided by a load balancer external to the Kubernetes cluster.</p><p>When the service type is set to <strong>LoadBalancer</strong>, Kubernetes provides functionality equivalent to <strong>type=<code>&lt;ClusterIP&gt;</code></strong> to pods within the cluster and extends it by programming the (external to Kubernetes) load balancer with entries for the Kubernetes pods. The Kubernetes service controller automates the creation of the external load balancer, health checks (if needed), firewall rules (if needed) and retrieves the external IP allocated by the cloud provider and populates it in the service object.</p><h4>Caveats and Limitations when preserving source IPs</h4><p>GCE/AWS load balancers do not provide weights for their target pools. This was not an issue with the old LB kube-proxy rules which would correctly balance across all endpoints.</p><p>With the new functionality, the external traffic will not be equally load balanced across pods, but rather equally balanced at the node level (because GCE/AWS and other external LB implementations do not have the ability for specifying the weight per node, they balance equally across all target nodes, disregarding the number of pods on each node).</p><p>We can, however, state that for NumServicePods « NumNodes or NumServicePods » NumNodes, a fairly close-to-equal distribution will be seen, even without weights.</p><p>Once the external load balancers provide weights, this functionality can be added to the LB programming path. Future Work: No support for weights is provided for the 1.4 release, but may be added at a future date</p><p>Internal pod to pod traffic should behave similar to ClusterIP services, with equal probability across all pods.</p><h3>Configure Your Cloud Provider\&#x27;s Firewalls</h3><p>Many cloud providers (e.g. Google Compute Engine) define firewalls that help prevent inadvertent exposure to the internet. When exposing a service to the external world, you may need to open up one or more ports in these firewalls to serve traffic. This document describes this process, as well as any provider specific details that may be necessary.</p><h5><strong>Restrict Access For LoadBalancer Service</strong></h5><p>When using a Service with <strong>spec.type: LoadBalancer</strong>, you can specify the IP ranges that are allowed to access the load balancer by using <strong>spec.loadBalancerSourceRanges</strong>. This field takes a list of IP CIDR ranges, which Kubernetes will use to configure firewall exceptions. This feature is currently supported on Google Compute Engine, Google Kubernetes Engine and AWS. This field will be ignored if the cloud provider does not support the feature.</p><p>Assuming 10.0.0.0/8 is the internal subnet. In the following example, a load balancer will be created that is only accessible to cluster internal IPs. This will not allow clients from outside of your Kubernetes cluster to access the load balancer.</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Service</strong></p><p><strong>metadata:</strong></p><p><strong>name: myapp</strong></p><p><strong>spec:</strong></p><p><strong>ports:</strong></p><p><strong>- port: 8765</strong></p><p><strong>targetPort: 9376</strong></p><p><strong>selector:</strong></p><p><strong>app: example</strong></p><p><strong>type: LoadBalancer</strong></p><p><strong>loadBalancerSourceRanges:</strong></p><p><strong>- 10.0.0.0/8</strong></p><p>In the following example, a load balancer will be created that is only accessible to clients with IP addresses from 130.211.204.1 and 130.211.204.2.</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Service</strong></p><p><strong>metadata:</strong></p><p><strong>name: myapp</strong></p><p><strong>spec:</strong></p><p><strong>ports:</strong></p><p><strong>- port: 8765</strong></p><p><strong>targetPort: 9376</strong></p><p><strong>selector:</strong></p><p><strong>app: example</strong></p><p><strong>type: LoadBalancer</strong></p><p><strong>loadBalancerSourceRanges:</strong></p><p><strong>- 130.211.204.1/32</strong></p><p><strong>- 130.211.204.2/32</strong></p><h5><strong>Google Compute Engine</strong></h5><p>When using a Service with <strong>spec.type: LoadBalancer</strong>, the firewall will be opened automatically. When using <strong>spec.type: NodePort</strong>, however, the firewall is not opened by default.</p><p>Google Compute Engine firewalls are documented <a href="https://cloud.google.com/compute/docs/networking#firewalls_1">elsewhere</a>.</p><p>You can add a firewall with the <strong>gcloud</strong> command line tool:
<code style="background-color:lightgray">gcloud compute firewall-rules create my-rule --allow=tcp:&lt;port&gt;</code></p><p><strong>Note</strong> There is one important security note when using firewalls on Google Compute Engine:</p><p>as of Kubernetes v1.0.0, GCE firewalls are defined per-vm, rather than per-ip address. This means that when you open a firewall for a service&#x27;s ports, anything that serves on that port on that VM&#x27;s host IP address may potentially serve traffic. Note that this is not a problem for other Kubernetes services, as they listen on IP addresses that are different than the host node&#x27;s external IP address.</p><p>Consider:</p><ul><li>You create a Service with an external load balancer (IP Address 1.2.3.4) and port 80</li><li>You open the firewall for port 80 for all nodes in your cluster, so that the external Service actually can deliver packets to your Service</li><li>You start an nginx server, running on port 80 on the host virtual machine (IP Address 2.3.4.5). This nginx is also exposed to the internet on the VM&#x27;s external IP address.</li></ul><p>Consequently, please be careful when opening firewalls in Google Compute Engine or Google Kubernetes Engine. You may accidentally be exposing other services to the wilds of the internet.</p><p>This will be fixed in an upcoming release of Kubernetes.</p><h5><strong>Other cloud providers</strong></h5><p>Coming soon.</p><h3>List All Container Images Running in a Cluster</h3><p>This page shows how to use kubectl to list all of the Container images for Pods running in a cluster.</p><ul><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/list-all-running-container-images/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/list-all-running-container-images/#list-all-containers-in-all-namespaces"><strong>List all Containers in all namespaces</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/list-all-running-container-images/#list-containers-by-pod"><strong>List Containers by Pod</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/list-all-running-container-images/#list-containers-filtering-by-pod-label"><strong>List Containers filtering by Pod label</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/list-all-running-container-images/#list-containers-filtering-by-pod-namespace"><strong>List Containers filtering by Pod namespace</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/list-all-running-container-images/#list-containers-using-a-go-template-instead-of-jsonpath"><strong>List Containers using a go-template instead of jsonpath</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/list-all-running-container-images/#whats-next"><strong>What&#x27;s next</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/list-all-running-container-images/#reference"><strong>Reference</strong></a></li></ul></li></ul><h4>Before you begin</h4><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using<a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <strong>kubectl version</strong>.</p><p>In this exercise you will use kubectl to fetch all of the Pods running in a cluster, and format the output to pull out the list of Containers for each.</p><h4>List all Containers in all namespaces</h4><ul><li>Fetch all Pods in all namespaces using <strong>kubectl get pods --all-namespaces</strong></li><li>Format the output to include only the list of Container image names using <strong>-o jsonpath={..image}</strong>. This will recursively parse out the <strong>image</strong> field from the returned json.<ul><li>See the <a href="https://kubernetes.io/docs/user-guide/jsonpath/">jsonpath reference</a> for further information on how to use jsonpath.</li></ul></li><li>Format the output using standard tools: <strong>tr</strong>, <strong>sort</strong>, <strong>uniq</strong><ul><li>Use <strong>tr</strong> to replace spaces with newlines</li><li>Use <strong>sort</strong> to sort the results</li><li>Use <strong>uniq</strong> to aggregate image counts</li></ul></li></ul><p><strong>kubectl get pods --all-namespaces -o jsonpath=&quot;{..image}&quot; |<!-- -->\</strong></p><p><strong>tr -s \&#x27;[<!-- -->[:space:]<!-- -->]\&#x27; \&#x27;<!-- -->\<!-- -->n\&#x27; |<!-- -->\</strong></p><p><strong>sort |<!-- -->\</strong></p><p><strong>uniq -c</strong></p><p>The above command will recursively return all fields named <strong>image</strong> for all items returned.</p><p>As an alternative, it is possible to use the absolute path to the image field within the Pod. This ensures the correct field is retrieved even when the field name is repeated, e.g. many fields are called <strong>name</strong> within a given item:</p><p><strong>kubectl get pods --all-namespaces -o jsonpath=&quot;{.items<!-- -->[*]<!-- -->.spec.containers<!-- -->[*]<!-- -->.image}&quot;</strong></p><p>The jsonpath is interpreted as follows:</p><ul><li><strong>.items<!-- -->[*]</strong>: for each returned value</li><li><strong>.spec</strong>: get the spec</li><li><strong>.containers<!-- -->[*]</strong>: for each container</li><li><strong>.image</strong>: get the image</li></ul><p><strong>Note:</strong> When fetching a single Pod by name, e.g. <strong>kubectl get pod nginx</strong>, the <strong>.items<!-- -->[*]</strong> portion of the path should be omitted because a single Pod is returned instead of a list of items.</p><h4>List Containers by Pod</h4><p>The formatting can be controlled further by using the <strong>range</strong> operation to iterate over elements individually.</p><p><strong>kubectl get pods --all-namespaces -o=jsonpath=\&#x27;{range .items<!-- -->[*]<!-- -->}{&quot;<!-- -->\<!-- -->n&quot;}{.metadata.name}{&quot;:<!-- -->\<!-- -->t&quot;}{range .spec.containers<!-- -->[*]<!-- -->}{.image}{&quot;, &quot;}{end}{end}\&#x27; |<!-- -->\</strong></p><p><strong>sort</strong></p><h4>List Containers filtering by Pod label</h4><p>To target only Pods matching a specific label, use the -l flag. The following matches only Pods with labels matching <strong>app=nginx</strong>.</p><p><strong>kubectl get pods --all-namespaces -o=jsonpath=&quot;{..image}&quot; -l app=nginx</strong></p><h4>List Containers filtering by Pod namespace</h4><p>To target only pods in a specific namespace, use the namespace flag. The following matches only Pods in the <strong>kube-system</strong> namespace.</p><p><strong>kubectl get pods --namespace kube-system -o jsonpath=&quot;{..image}&quot;</strong></p><h4>List Containers using a go-template instead of jsonpath</h4><p>As an alternative to jsonpath, Kubectl supports using <a href="https://golang.org/pkg/text/template/">go-templates</a> for formatting the output:</p><p><strong>kubectl get pods --all-namespaces -o go-template --template=&quot;{{range .items}}{{range .spec.containers}}{{.image}} {{end}}{{end}}&quot;</strong></p><h4>What&#x27;s next</h4><h5><strong>Reference</strong></h5><ul><li><a href="https://kubernetes.io/docs/user-guide/jsonpath/">Jsonpath</a> reference guide</li><li><a href="https://golang.org/pkg/text/template/">Go template</a> reference guide</li></ul><h3>Communicate Between Containers in the Same Pod Using a Shared Volume</h3><p>This page shows how to use a Volume to communicate between two Containers running in the same Pod.</p><ul><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/communicate-containers-same-pod-shared-volume/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/communicate-containers-same-pod-shared-volume/#creating-a-pod-that-runs-two-containers"><strong>Creating a Pod that runs two Containers</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/communicate-containers-same-pod-shared-volume/#discussion"><strong>Discussion</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/communicate-containers-same-pod-shared-volume/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Before you begin</h4><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using<a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <strong>kubectl version</strong>.</p><h4>Creating a Pod that runs two Containers</h4><p>In this exercise, you create a Pod that runs two Containers. The two containers share a Volume that they can use to communicate. Here is the configuration file for the Pod:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>two-contai                                                         |
| ner-pod.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/website">https://raw.githubusercontent.com/kubernetes/website</a> |
| /master/docs/tasks/access-application-cluster/two-container-pod.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: two-containers</strong>                                              |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>restartPolicy: Never</strong>                                              |
|                                                                       |
| <strong>volumes:</strong>                                                          |
|                                                                       |
| <strong>- name: shared-data</strong>                                               |
|                                                                       |
| <strong>emptyDir: {}</strong>                                                      |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: nginx-container</strong>                                           |
|                                                                       |
| <strong>image: nginx</strong>                                                      |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- name: shared-data</strong>                                               |
|                                                                       |
| <strong>mountPath: /usr/share/nginx/html</strong>                                  |
|                                                                       |
| <strong>- name: debian-container</strong>                                          |
|                                                                       |
| <strong>image: debian</strong>                                                     |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- name: shared-data</strong>                                               |
|                                                                       |
| <strong>mountPath: /pod-data</strong>                                              |
|                                                                       |
| <strong>command: <!-- -->[&quot;/bin/sh&quot;]</strong>                                          |
|                                                                       |
| <strong>args: <!-- -->[&quot;-c&quot;, &quot;echo Hello from the debian container &gt;           |
| /pod-data/index.html&quot;]</strong>                                            |
+-----------------------------------------------------------------------+</p><p>In the configuration file, you can see that the Pod has a Volume named <strong>shared-data</strong>.</p><p>The first container listed in the configuration file runs an nginx server. The mount path for the shared Volume is <strong>/usr/share/nginx/html</strong>. The second container is based on the debian image, and has a mount path of <strong>/pod-data</strong>. The second container runs the following command and then terminates.</p><p><strong>echo Hello from the debian container &gt; /pod-data/index.html</strong></p><p>Notice that the second container writes the <strong>index.html</strong> file in the root directory of the nginx server.</p><p>Create the Pod and the two Containers:</p><p><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/access-application-cluster/two-container-pod.yaml">https://k8s.io/docs/tasks/access-application-cluster/two-container-pod.yaml</a></strong></p><p>View information about the Pod and the Containers:</p><p><strong>kubectl get pod two-containers --output=yaml</strong></p><p>Here is a portion of the output:</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>name: two-containers</strong></p><p><strong>namespace: default</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>spec:</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>containerStatuses:</strong></p><p><strong>- containerID: docker://c1d8abd1 <!-- -->.<!-- -->..</strong></p><p><strong>image: debian</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>lastState:</strong></p><p><strong>terminated:</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>name: debian-container</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>- containerID: docker://96c1ff2c5bb <!-- -->.<!-- -->..</strong></p><p><strong>image: nginx</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>name: nginx-container</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>state:</strong></p><p><strong>running:</strong></p><p><strong>.<!-- -->..</strong></p><p>You can see that the debian Container has terminated, and the nginx Container is still running.</p><p>Get a shell to nginx Container:</p><p><strong>kubectl exec -it two-containers -c nginx-container -- /bin/bash</strong></p><p>In your shell, verify that nginx is running:</p><p><strong>root@two-containers:/# apt-get update</strong></p><p><strong>root@two-containers:/# apt-get install curl procps</strong></p><p><strong>root@two-containers:/# ps aux</strong></p><p>The output is similar to this:</p><p><strong>USER PID <!-- -->.<!-- -->.. STAT START TIME COMMAND</strong></p><p><strong>root 1 <!-- -->.<!-- -->.. Ss 21:12 0:00 nginx: master process nginx -g daemon off;</strong></p><p>Recall that the debian Container created the <strong>index.html</strong> file in the nginx root directory. Use <strong>curl</strong>to send a GET request to the nginx server:</p><p><strong>root@two-containers:/# curl localhost</strong></p><p>The output shows that nginx serves a web page written by the debian container:</p><p><strong>Hello from the debian container</strong></p><h4>Discussion</h4><p>The primary reason that Pods can have multiple containers is to support helper applications that assist a primary application. Typical examples of helper applications are data pullers, data pushers, and proxies. Helper and primary applications often need to communicate with each other. Typically this is done through a shared filesystem, as shown in this exercise, or through the loopback network interface, localhost. An example of this pattern is a web server along with a helper program that polls a Git repository for new updates.</p><p>The Volume in this exercise provides a way for Containers to communicate during the life of the Pod. If the Pod is deleted and recreated, any data stored in the shared Volume is lost.</p><h4>What&#x27;s next</h4><ul><li>Learn more about <a href="http://blog.kubernetes.io/2015/06/the-distributed-system-toolkit-patterns.html">patterns for composite containers</a>.</li><li>Learn about <a href="http://www.slideshare.net/Docker/slideshare-burns">composite containers for modular architecture</a>.</li><li>See <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-volume-storage/">Configuring a Pod to Use a Volume for Storage</a>.</li><li>See <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#volume-v1-core">Volume</a>.</li><li>See <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#pod-v1-core">Pod</a>.</li></ul><h3>Kubernetes DNS example</h3><p>This is a toy example demonstrating how to use kubernetes DNS.</p><h4>Step Zero: Prerequisites</h4><p>This example assumes that you have forked the repository and <a href="https://github.com/kubernetes/kubernetes/blob/release-1.5/docs/getting-started-guides">turned up a Kubernetes cluster</a>. Make sure DNS is enabled in your setup, see <a href="https://github.com/kubernetes/kubernetes/blob/release-1.5/build/kube-dns">DNS doc</a>.</p><p>$ cd kubernetes</p><p>$ hack/dev-build-and-up.sh</p><h4>Step One: Create two namespaces</h4><p>We\&#x27;ll see how cluster DNS works across multiple <a href="https://github.com/kubernetes/kubernetes/blob/release-1.5/docs/user-guide/namespaces.md">namespaces</a>, first we need to create two namespaces:</p><p>$ kubectl create -f examples/cluster-dns/namespace-dev.yaml</p><p>$ kubectl create -f examples/cluster-dns/namespace-prod.yaml</p><p>Now list all namespaces:</p><p>$ kubectl get namespaces</p><p>NAME LABELS STATUS</p><p>default <code style="background-color:lightgray">&lt;none&gt;</code> Active</p><p>development name=development Active</p><p>production name=production Active</p><p>For kubectl client to work with each namespace, we define two contexts:</p><p>$ kubectl config set-context dev --namespace=development --cluster=${CLUSTER_NAME} --user=${USER_NAME}</p><p>$ kubectl config set-context prod --namespace=production --cluster=${CLUSTER_NAME} --user=${USER_NAME}</p><p>You can view your cluster name and user name in kubernetes config at <!-- -->~<!-- -->/.kube/config.</p><h4>Step Two: Create backend replication controller in each namespace</h4><p>Use the file <a href="https://github.com/kubernetes/kubernetes/blob/release-1.5/examples/cluster-dns/dns-backend-rc.yaml">examples/cluster-dns/dns-backend-rc.yaml</a> to create a backend server <a href="https://github.com/kubernetes/kubernetes/blob/release-1.5/docs/user-guide/replication-controller.md">replication controller</a> in each namespace.</p><p>$ kubectl config use-context dev</p><p>$ kubectl create -f examples/cluster-dns/dns-backend-rc.yaml</p><p>Once that\&#x27;s up you can list the pod in the cluster:</p><p>$ kubectl get rc</p><p>CONTROLLER CONTAINER(S) IMAGE(S) SELECTOR REPLICAS</p><p>dns-backend dns-backend ddysher/dns-backend name=dns-backend 1</p><p>Now repeat the above commands to create a replication controller in prod namespace:</p><p>$ kubectl config use-context prod</p><p>$ kubectl create -f examples/cluster-dns/dns-backend-rc.yaml</p><p>$ kubectl get rc</p><p>CONTROLLER CONTAINER(S) IMAGE(S) SELECTOR REPLICAS</p><p>dns-backend dns-backend ddysher/dns-backend name=dns-backend 1</p><h4>Step Three: Create backend service</h4><p>Use the file <a href="https://github.com/kubernetes/kubernetes/blob/release-1.5/examples/cluster-dns/dns-backend-service.yaml">examples/cluster-dns/dns-backend-service.yaml</a> to create a <a href="https://github.com/kubernetes/kubernetes/blob/release-1.5/docs/user-guide/services.md">service</a> for the backend server.</p><p>$ kubectl config use-context dev</p><p>$ kubectl create -f examples/cluster-dns/dns-backend-service.yaml</p><p>Once that\&#x27;s up you can list the service in the cluster:</p><p>$ kubectl get service dns-backend</p><p>NAME CLUSTER_IP EXTERNAL_IP PORT(S) SELECTOR AGE</p><p>dns-backend 10.0.2.3 <code style="background-color:lightgray">&lt;none&gt;</code> 8000/TCP name=dns-backend 1d</p><p>Again, repeat the same process for prod namespace:</p><p>$ kubectl config use-context prod</p><p>$ kubectl create -f examples/cluster-dns/dns-backend-service.yaml</p><p>$ kubectl get service dns-backend</p><p>NAME CLUSTER_IP EXTERNAL_IP PORT(S) SELECTOR AGE</p><p>dns-backend 10.0.2.4 <code style="background-color:lightgray">&lt;none&gt;</code> 8000/TCP name=dns-backend 1d</p><h4>Step Four: Create client pod in one namespace</h4><p>Use the file <a href="https://github.com/kubernetes/kubernetes/blob/release-1.5/examples/cluster-dns/dns-frontend-pod.yaml">examples/cluster-dns/dns-frontend-pod.yaml</a> to create a client <a href="https://github.com/kubernetes/kubernetes/blob/release-1.5/docs/user-guide/pods.md">pod</a> in dev namespace. The client pod will make a connection to backend and exit. Specifically, it tries to connect to address <a href="http://dns-backend.development.cluster.local:8000">http://dns-backend.development.cluster.local:8000</a>.</p><p>$ kubectl config use-context dev</p><p>$ kubectl create -f examples/cluster-dns/dns-frontend-pod.yaml</p><p>Once that\&#x27;s up you can list the pod in the cluster:</p><p>$ kubectl get pods dns-frontend</p><p>NAME READY STATUS RESTARTS AGE</p><p>dns-frontend 0/1 ExitCode:0 0 1m</p><p>Wait until the pod succeeds, then we can see the output from the client pod:</p><p>$ kubectl logs dns-frontend</p><p>2015-05-07T20:13:54.147664936Z 10.0.236.129</p><p>2015-05-07T20:13:54.147721290Z Send request to: <a href="http://dns-backend.development.cluster.local:8000">http://dns-backend.development.cluster.local:8000</a></p><p>2015-05-07T20:13:54.147733438Z <code style="background-color:lightgray">&lt;Response [200]&gt;</code></p><p>2015-05-07T20:13:54.147738295Z Hello World!</p><p>Please refer to the <a href="https://github.com/kubernetes/kubernetes/blob/release-1.5/examples/cluster-dns/images/frontend/client.py">source code</a> about the log. First line prints out the ip address associated with the service in dev namespace; remaining lines print out our request and server response.</p><p>If we switch to prod namespace with the same pod config, we\&#x27;ll see the same result, i.e. dns will resolve across namespace.</p><p>$ kubectl config use-context prod</p><p>$ kubectl create -f examples/cluster-dns/dns-frontend-pod.yaml</p><p>$ kubectl logs dns-frontend</p><p>2015-05-07T20:13:54.147664936Z 10.0.236.129</p><p>2015-05-07T20:13:54.147721290Z Send request to: <a href="http://dns-backend.development.cluster.local:8000">http://dns-backend.development.cluster.local:8000</a></p><p>2015-05-07T20:13:54.147733438Z <code style="background-color:lightgray">&lt;Response [200]&gt;</code></p><p>2015-05-07T20:13:54.147738295Z Hello World!</p><h5>Note about default namespace</h5><p>If you prefer not using namespace, then all your services can be addressed using default namespace, e.g. <a href="http://dns-backend.default.svc.cluster.local:8000">http://dns-backend.default.svc.cluster.local:8000</a>, or shorthand version http://dns-backend:8000</p><h4>tl; dr;</h4><p>For those of you who are impatient, here is the summary of the commands we ran in this tutorial. Remember to set first $CLUSTER_NAME and $USER_NAME to the values found in <!-- -->~<!-- -->/.kube/config.</p><h1>create dev and prod namespaces</h1><p>kubectl create -f examples/cluster-dns/namespace-dev.yaml</p><p>kubectl create -f examples/cluster-dns/namespace-prod.yaml</p><h1>create two contexts</h1><p>kubectl config set-context dev --namespace=development --cluster=${CLUSTER_NAME} --user=${USER_NAME}</p><p>kubectl config set-context prod --namespace=production --cluster=${CLUSTER_NAME} --user=${USER_NAME}</p><h1>create two backend replication controllers</h1><p>kubectl config use-context dev</p><p>kubectl create -f examples/cluster-dns/dns-backend-rc.yaml</p><p>kubectl config use-context prod</p><p>kubectl create -f examples/cluster-dns/dns-backend-rc.yaml</p><h1>create backend services</h1><p>kubectl config use-context dev</p><p>kubectl create -f examples/cluster-dns/dns-backend-service.yaml</p><p>kubectl config use-context prod</p><p>kubectl create -f examples/cluster-dns/dns-backend-service.yaml</p><h1>create a pod in each namespace and get its output</h1><p>kubectl config use-context dev</p><p>kubectl create -f examples/cluster-dns/dns-frontend-pod.yaml</p><p>kubectl logs dns-frontend</p><p>kubectl config use-context prod</p><p>kubectl create -f examples/cluster-dns/dns-frontend-pod.yaml</p><p>kubectl logs dns-frontend</p><h2>Monitor, Log, and Debug</h2><h3>Core metrics pipeline</h3><p>Starting from Kubernetes 1.8, resource usage metrics, such as container CPU and memory usage, are available in Kubernetes through the Metrics API. These metrics can be either accessed directly by user, for example by using <strong>kubectl top</strong> command, or used by a controller in the cluster, e.g. Horizontal Pod Autoscaler, to make decisions.</p><h4>The Metrics API</h4><p>Through the Metrics API you can get the amount of resource currently used by a given node or a given pod. This API doesn&#x27;t store the metric values, so it&#x27;s not possible for example to get the amount of resources used by a given node 10 minutes ago.</p><p>The API is no different from any other API:</p><ul><li>it is discoverable through the same endpoint as the other Kubernetes APIs under <strong>/apis/metrics.k8s.io/</strong> path</li><li>it offers the same security, scalability and reliability guarantees</li></ul><p>The API is defined in <a href="https://github.com/kubernetes/metrics/blob/master/pkg/apis/metrics/v1beta1/types.go">k8s.io/metrics</a> repository. You can find more information about the API there.</p><p><strong>Note:</strong> The API requires metrics server to be deployed in the cluster. Otherwise it will be not available.</p><h4>Metrics Server</h4><p><a href="https://github.com/kubernetes-incubator/metrics-server">Metrics Server</a> is a cluster-wide aggregator of resource usage data. Starting from Kubernetes 1.8 it&#x27;s deployed by default in clusters created by <strong>kube-up.sh</strong> script as a Deployment object. If you use a different Kubernetes setup mechanism you can deploy it using the provided <a href="https://github.com/kubernetes-incubator/metrics-server/tree/master/deploy">deployment yamls</a>. It&#x27;s supported in Kubernetes 1.7+ (see details below).</p><p>Metric server collects metrics from the Summary API, exposed by <a href="https://kubernetes.io/docs/admin/kubelet/">Kubelet</a> on each node.</p><p>Metrics Server registered in the main API server through <a href="https://kubernetes.io/docs/concepts/api-extension/apiserver-aggregation/">Kubernetes aggregator</a>, which was introduced in Kubernetes 1.7.</p><p>Learn more about the metrics server in <a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/metrics-server.md">the design doc</a>.</p><h3>Tools for Monitoring Compute, Storage, and Network Resources</h3><p>Understanding how an application behaves when deployed is crucial to scaling the application and providing a reliable service. In a Kubernetes cluster, application performance can be examined at many different levels: containers, <a href="https://kubernetes.io/docs/user-guide/pods">pods</a>, <a href="https://kubernetes.io/docs/user-guide/services">services</a>, and whole clusters. As part of Kubernetes we want to provide users with detailed resource usage information about their running applications at all these levels. This will give users deep insights into how their applications are performing and where possible application bottlenecks may be found. In comes <a href="https://github.com/kubernetes/heapster">Heapster</a>, a project meant to provide a base monitoring platform on Kubernetes.</p><h4>Overview</h4><p>Heapster is a cluster-wide aggregator of monitoring and event data. It currently supports Kubernetes natively and works on all Kubernetes setups. Heapster runs as a pod in the cluster, similar to how any Kubernetes application would run. The Heapster pod discovers all nodes in the cluster and queries usage information from the nodes&#x27; <a href="https://kubernetes.io/docs/admin/kubelet/">Kubelet</a>s, the on-machine Kubernetes agent. The Kubelet itself fetches the data from <a href="https://github.com/google/cadvisor">cAdvisor</a>. Heapster groups the information by pod along with the relevant labels. This data is then pushed to a configurable backend for storage and visualization. Currently supported backends include <a href="http://influxdb.com/">InfluxDB</a> (with <a href="http://grafana.org/">Grafana</a> for visualization), <a href="https://cloud.google.com/monitoring/">Google Cloud Monitoring</a> and many others described in more details <a href="https://git.k8s.io/heapster/docs/sink-configuration.md">here</a>. The overall architecture of the service can be seen below:</p><p>Let&#x27;s look at some of the other components in more detail.</p><h5><strong>cAdvisor</strong></h5><p>cAdvisor is an open source container resource usage and performance analysis agent. It is purpose-built for containers and supports Docker containers natively. In Kubernetes, cAdvisor is integrated into the Kubelet binary. cAdvisor auto-discovers all containers in the machine and collects CPU, memory, filesystem, and network usage statistics. cAdvisor also provides the overall machine usage by analyzing the &#x27;root&#x27; container on the machine.</p><p>On most Kubernetes clusters, cAdvisor exposes a simple UI for on-machine containers on port 4194. Here is a snapshot of part of cAdvisor&#x27;s UI that shows the overall machine usage:</p><h5><strong>Kubelet</strong></h5><p>The Kubelet acts as a bridge between the Kubernetes master and the nodes. It manages the pods and containers running on a machine. Kubelet translates each pod into its constituent containers and fetches individual container usage statistics from cAdvisor. It then exposes the aggregated pod resource usage statistics via a REST API.</p><h4>Storage Backends</h4><h5><strong>InfluxDB and Grafana</strong></h5><p>A Grafana setup with InfluxDB is a very popular combination for monitoring in the open source world. InfluxDB exposes an easy to use API to write and fetch time series data. Heapster is setup to use this storage backend by default on most Kubernetes clusters. A detailed setup guide can be found <a href="https://github.com/GoogleCloudPlatform/heapster/blob/master/docs/influxdb.md">here</a>. InfluxDB and Grafana run in Pods. The pod exposes itself as a Kubernetes service which is how Heapster discovers it.</p><p>The Grafana container serves Grafana&#x27;s UI which provides an easy to configure dashboard interface. The default dashboard for Kubernetes contains an example dashboard that monitors resource usage of the cluster and the pods inside of it. This dashboard can easily be customized and expanded. Take a look at the storage schema for InfluxDB <a href="https://github.com/GoogleCloudPlatform/heapster/blob/master/docs/storage-schema.md#metrics">here</a>.</p><p>Here is a video showing how to monitor a Kubernetes cluster using heapster, InfluxDB and Grafana:</p><p>Here is a snapshot of the default Kubernetes Grafana dashboard that shows the CPU and Memory usage of the entire cluster, individual pods and containers:</p><h5><strong>Google Cloud Monitoring</strong></h5><p>Google Cloud Monitoring is a hosted monitoring service that allows you to visualize and alert on important metrics in your application. Heapster can be setup to automatically push all collected metrics to Google Cloud Monitoring. These metrics are then available in the <a href="https://app.google.stackdriver.com/">Cloud Monitoring Console</a>. This storage backend is the easiest to setup and maintain. The monitoring console allows you to easily create and customize dashboards using the exported data.</p><p>Here is a video showing how to setup and run a Google Cloud Monitoring backed Heapster:</p><p>Here is a snapshot of the Google Cloud Monitoring dashboard showing cluster-wide resource usage.</p><h4>Try it out!</h4><p>Now that you&#x27;ve learned a bit about Heapster, feel free to try it out on your own clusters! The <a href="https://github.com/kubernetes/heapster">Heapster repository</a> is available on GitHub. It contains detailed instructions to setup Heapster and its storage backends. Heapster runs by default on most Kubernetes clusters, so you may already have it! Feedback is always welcome. Please let us know if you run into any issues via the troubleshooting <a href="https://kubernetes.io/docs/troubleshooting/">channels</a>.</p><p>Authors: Vishnu Kannan and Victor Marmol, Google Software Engineers. This article was originally posted in <a href="http://blog.kubernetes.io/2015/05/resource-usage-monitoring-kubernetes.html"><em>Kubernetes blog</em></a>.</p><h3>Get a Shell to a Running Container</h3><p>This page shows how to use <strong>kubectl exec</strong> to get a shell to a running Container.</p><ul><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/get-shell-running-container/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/get-shell-running-container/#getting-a-shell-to-a-container"><strong>Getting a shell to a Container</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/get-shell-running-container/#writing-the-root-page-for-nginx"><strong>Writing the root page for nginx</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/get-shell-running-container/#running-individual-commands-in-a-container"><strong>Running individual commands in a Container</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/get-shell-running-container/#opening-a-shell-when-a-pod-has-more-than-one-container"><strong>Opening a shell when a Pod has more than one Container</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/get-shell-running-container/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Before you begin</h4><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using<a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <strong>kubectl version</strong>.</p><h4>Getting a shell to a Container</h4><p>In this exercise, you create a Pod that has one Container. The Container runs the nginx image. Here is the configuration file for the Pod:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>shell-demo.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes">https://raw.githubusercontent.com/kubernetes</a>   |
| /website/master/docs/tasks/debug-application-cluster/shell-demo.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: shell-demo</strong>                                                  |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>volumes:</strong>                                                          |
|                                                                       |
| <strong>- name: shared-data</strong>                                               |
|                                                                       |
| <strong>emptyDir: {}</strong>                                                      |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: nginx</strong>                                                     |
|                                                                       |
| <strong>image: nginx</strong>                                                      |
|                                                                       |
| <strong>volumeMounts:</strong>                                                     |
|                                                                       |
| <strong>- name: shared-data</strong>                                               |
|                                                                       |
| <strong>mountPath: /usr/share/nginx/html</strong>                                  |
+-----------------------------------------------------------------------+</p><p>Create the Pod:</p><p><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/debug-application-cluster/shell-demo.yaml">https://k8s.io/docs/tasks/debug-application-cluster/shell-demo.yaml</a></strong></p><p>Verify that the Container is running:</p><p><strong>kubectl get pod shell-demo</strong></p><p>Get a shell to the running Container:</p><p><strong>kubectl exec -it shell-demo -- /bin/bash</strong></p><p>In your shell, list the root directory:</p><p><strong>root@shell-demo:/# ls /</strong></p><p>In your shell, experiment with other commands. Here are some examples:</p><p><strong>root@shell-demo:/# ls /</strong></p><p><strong>root@shell-demo:/# cat /proc/mounts</strong></p><p><strong>root@shell-demo:/# cat /proc/1/maps</strong></p><p><strong>root@shell-demo:/# apt-get update</strong></p><p><strong>root@shell-demo:/# apt-get install -y tcpdump</strong></p><p><strong>root@shell-demo:/# tcpdump</strong></p><p><strong>root@shell-demo:/# apt-get install -y lsof</strong></p><p><strong>root@shell-demo:/# lsof</strong></p><p><strong>root@shell-demo:/# apt-get install -y procps</strong></p><p><strong>root@shell-demo:/# ps aux</strong></p><p><strong>root@shell-demo:/# ps aux | grep nginx</strong></p><h4>Writing the root page for nginx</h4><p>Look again at the configuration file for your Pod. The Pod has an <strong>emptyDir</strong> volume, and the Container mounts the volume at <strong>/usr/share/nginx/html</strong>.</p><p>In your shell, create an <strong>index.html</strong> file in the <strong>/usr/share/nginx/html</strong> directory:</p><p><strong>root@shell-demo:/# echo Hello shell demo &gt; /usr/share/nginx/html/index.html</strong></p><p>In your shell, send a GET request to the nginx server:</p><p><strong>root@shell-demo:/# apt-get update</strong></p><p><strong>root@shell-demo:/# apt-get install curl</strong></p><p><strong>root@shell-demo:/# curl localhost</strong></p><p>The output shows the text that you wrote to the <strong>index.html</strong> file:</p><p><strong>Hello shell demo</strong></p><p>When you are finished with your shell, enter <strong>exit</strong>.</p><h4>Running individual commands in a Container</h4><p>In an ordinary command window, not your shell, list the environment variables in the running Container:</p><p><strong>kubectl exec shell-demo env</strong></p><p>Experiment running other commands. Here are some examples:</p><p><strong>kubectl exec shell-demo ps aux</strong></p><p><strong>kubectl exec shell-demo ls /</strong></p><p><strong>kubectl exec shell-demo cat /proc/1/mounts</strong></p><h4>Opening a shell when a Pod has more than one Container</h4><p>If a Pod has more than one Container, use <strong>--container</strong> or <strong>-c</strong> to specify a Container in the <strong>kubectl exec</strong> command. For example, suppose you have a Pod named my-pod, and the Pod has two containers named main-app and helper-app. The following command would open a shell to the main-app Container.</p><p><strong>kubectl exec -it my-pod --container main-app -- /bin/bash</strong></p><h4>What&#x27;s next</h4><ul><li><a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#exec">kubectl exec</a></li></ul><h3>Monitor Node Health</h3><ul><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/monitor-node-health/#node-problem-detector"><strong>Node Problem Detector</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/monitor-node-health/#limitations"><strong>Limitations</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/monitor-node-health/#enabledisable-in-gce-cluster"><strong>Enable/Disable in GCE cluster</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/monitor-node-health/#use-in-other-environment"><strong>Use in Other Environment</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/monitor-node-health/#kubectl"><strong>Kubectl</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/monitor-node-health/#addon-pod"><strong>Addon Pod</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/monitor-node-health/#overwrite-the-configuration"><strong>Overwrite the Configuration</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/monitor-node-health/#kernel-monitor"><strong>Kernel Monitor</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/monitor-node-health/#add-new-nodeconditions"><strong>Add New NodeConditions</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/monitor-node-health/#detect-new-problems"><strong>Detect New Problems</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/monitor-node-health/#change-log-path"><strong>Change Log Path</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/monitor-node-health/#support-other-log-format"><strong>Support Other Log Format</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/monitor-node-health/#caveats"><strong>Caveats</strong></a></li></ul><h4>Node Problem Detector</h4><p>Node problem detector is a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/">DaemonSet</a> monitoring the node health. It collects node problems from various daemons and reports them to the apiserver as <a href="https://kubernetes.io/docs/concepts/architecture/nodes/#condition">NodeCondition</a> and <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#event-v1-core">Event</a>.</p><p>It supports some known kernel issue detection now, and will detect more and more node problems over time.</p><p>Currently Kubernetes won&#x27;t take any action on the node conditions and events generated by node problem detector. In the future, a remedy system could be introduced to deal with node problems.</p><p>See more information <a href="https://github.com/kubernetes/node-problem-detector">here</a>.</p><h4>Limitations</h4><ul><li>The kernel issue detection of node problem detector only supports file based kernel log now. It doesn&#x27;t support log tools like journald.</li><li>The kernel issue detection of node problem detector has assumption on kernel log format, and now it only works on Ubuntu and Debian. However, it is easy to extend it to <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/monitor-node-health/#support-other-log-format">support other log format</a>.</li></ul><h4>Enable/Disable in GCE cluster</h4><p>Node problem detector is <a href="https://kubernetes.io/docs/admin/cluster-large/#addon-resources">running as a cluster addon</a> enabled by default in the gce cluster.</p><p>You can enable/disable it by setting the environment variable <strong>KUBE_ENABLE_NODE_PROBLEM_DETECTOR</strong> before <strong>kube-up.sh</strong>.</p><h4>Use in Other Environment</h4><p>To enable node problem detector in other environment outside of GCE, you can use either <strong>kubectl</strong>or addon pod.</p><h5><strong>Kubectl</strong></h5><p>This is the recommended way to start node problem detector outside of GCE. It provides more flexible management, such as overwriting the default configuration to fit it into your environment or detect customized node problems.</p><ul><li><strong>Step 1:</strong> Create <strong>node-problem-detector.yaml</strong>:</li></ul><p><strong>apiVersion: extensions/v1beta1</strong></p><p><strong>kind: DaemonSet</strong></p><p><strong>metadata:</strong></p><p><strong>name: node-problem-detector-v0.1</strong></p><p><strong>namespace: kube-system</strong></p><p><strong>labels:</strong></p><p><strong>k8s-app: node-problem-detector</strong></p><p><strong>version: v0.1</strong></p><p><strong>kubernetes.io/cluster-service: &quot;true&quot;</strong></p><p><strong>spec:</strong></p><p><strong>template:</strong></p><p><strong>metadata:</strong></p><p><strong>labels:</strong></p><p><strong>k8s-app: node-problem-detector</strong></p><p><strong>version: v0.1</strong></p><p><strong>kubernetes.io/cluster-service: &quot;true&quot;</strong></p><p><strong>spec:</strong></p><p><strong>hostNetwork: true</strong></p><p><strong>containers:</strong></p><p><strong>- name: node-problem-detector</strong></p><p><strong>image: k8s.gcr.io/node-problem-detector:v0.1</strong></p><p><strong>securityContext:</strong></p><p><strong>privileged: true</strong></p><p><strong>resources:</strong></p><p><strong>limits:</strong></p><p><strong>cpu: &quot;200m&quot;</strong></p><p><strong>memory: &quot;100Mi&quot;</strong></p><p><strong>requests:</strong></p><p><strong>cpu: &quot;20m&quot;</strong></p><p><strong>memory: &quot;20Mi&quot;</strong></p><p><strong>volumeMounts:</strong></p><p><strong>- name: log</strong></p><p><strong>mountPath: /log</strong></p><p><strong>readOnly: true</strong></p><p><strong>volumes:</strong></p><p><strong>- name: log</strong></p><p><strong>hostPath:</strong></p><p><strong>path: /var/log/</strong></p><p><strong>Notice that you should make sure the system log directory is right for your OS distro.</strong></p><ul><li><strong>Step 2:</strong> Start node problem detector with <strong>kubectl</strong>:</li></ul><p><strong>kubectl create -f node-problem-detector.yaml</strong></p><h5><strong>Addon Pod</strong></h5><p>This is for those who have their own cluster bootstrap solution, and don&#x27;t need to overwrite the default configuration. They could leverage the addon pod to further automate the deployment.</p><p>Just create <strong>node-problem-detector.yaml</strong>, and put it under the addon pods directory <strong>/etc/kubernetes/addons/node-problem-detector</strong> on master node.</p><h4>Overwrite the Configuration</h4><p>The <a href="https://github.com/kubernetes/node-problem-detector/tree/v0.1/config">default configuration</a> is embedded when building the docker image of node problem detector.</p><p>However, you can use <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/">ConfigMap</a> to overwrite it following the steps:</p><ul><li><strong>Step 1:</strong> Change the config files in <strong>config/</strong>.</li><li><strong>Step 2:</strong> Create the ConfigMap <strong>node-problem-detector-config</strong> with <strong>kubectl create configmap node-problem-detector-config --from-file=config/</strong>.</li><li><strong>Step 3:</strong> Change the <strong>node-problem-detector.yaml</strong> to use the ConfigMap:</li></ul><p><strong>apiVersion: extensions/v1beta1</strong></p><p><strong>kind: DaemonSet</strong></p><p><strong>metadata:</strong></p><p><strong>name: node-problem-detector-v0.1</strong></p><p><strong>namespace: kube-system</strong></p><p><strong>labels:</strong></p><p><strong>k8s-app: node-problem-detector</strong></p><p><strong>version: v0.1</strong></p><p><strong>kubernetes.io/cluster-service: &quot;true&quot;</strong></p><p><strong>spec:</strong></p><p><strong>template:</strong></p><p><strong>metadata:</strong></p><p><strong>labels:</strong></p><p><strong>k8s-app: node-problem-detector</strong></p><p><strong>version: v0.1</strong></p><p><strong>kubernetes.io/cluster-service: &quot;true&quot;</strong></p><p><strong>spec:</strong></p><p><strong>hostNetwork: true</strong></p><p><strong>containers:</strong></p><p><strong>- name: node-problem-detector</strong></p><p><strong>image: k8s.gcr.io/node-problem-detector:v0.1</strong></p><p><strong>securityContext:</strong></p><p><strong>privileged: true</strong></p><p><strong>resources:</strong></p><p><strong>limits:</strong></p><p><strong>cpu: &quot;200m&quot;</strong></p><p><strong>memory: &quot;100Mi&quot;</strong></p><p><strong>requests:</strong></p><p><strong>cpu: &quot;20m&quot;</strong></p><p><strong>memory: &quot;20Mi&quot;</strong></p><p><strong>volumeMounts:</strong></p><p><strong>- name: log</strong></p><p><strong>mountPath: /log</strong></p><p><strong>readOnly: true</strong></p><p><strong>- name: config <em># Overwrite the config/ directory with ConfigMap volume</em></strong></p><p><strong>mountPath: /config</strong></p><p><strong>readOnly: true</strong></p><p><strong>volumes:</strong></p><p><strong>- name: log</strong></p><p><strong>hostPath:</strong></p><p><strong>path: /var/log/</strong></p><p><strong>- name: config <em># Define ConfigMap volume</em></strong></p><p><strong>configMap:</strong></p><p><strong>name: node-problem-detector-config</strong></p><ul><li><strong>Step 4:</strong> Re-create the node problem detector with the new yaml file:</li></ul><p><strong>kubectl delete -f node-problem-detector.yaml <em># If you have a node-problem-detector running</em></strong></p><p><strong>kubectl create -f node-problem-detector.yaml</strong></p><p><strong>Notice that this approach only applies to node problem detector started with <em>kubectl</em>.</strong></p><p>For node problem detector running as cluster addon, because addon manager doesn&#x27;t support ConfigMap, configuration overwriting is not supported now.</p><h4>Kernel Monitor</h4><p>Kernel Monitor is a problem daemon in node problem detector. It monitors kernel log and detects known kernel issues following predefined rules.</p><p>The Kernel Monitor matches kernel issues according to a set of predefined rule list in <a href="https://github.com/kubernetes/node-problem-detector/blob/v0.1/config/kernel-monitor.json"><strong>config/kernel-monitor.json</strong></a>. The rule list is extensible, and you can always extend it by overwriting the configuration.</p><h5><strong>Add New NodeConditions</strong></h5><p>To support new node conditions, you can extend the <strong>conditions</strong> field in <strong>config/kernel-monitor.json</strong> with new condition definition:</p><p><strong>{</strong></p><p><strong>&quot;type&quot;: &quot;NodeConditionType&quot;,</strong></p><p><strong>&quot;reason&quot;: &quot;CamelCaseDefaultNodeConditionReason&quot;,</strong></p><p><strong>&quot;message&quot;: &quot;arbitrary default node condition message&quot;</strong></p><p><strong>}</strong></p><h5><strong>Detect New Problems</strong></h5><p>To detect new problems, you can extend the <strong>rules</strong> field in <strong>config/kernel-monitor.json</strong> with new rule definition:</p><p><strong>{</strong></p><p><strong>&quot;type&quot;: &quot;temporary/permanent&quot;,</strong></p><p><strong>&quot;condition&quot;: &quot;NodeConditionOfPermanentIssue&quot;,</strong></p><p><strong>&quot;reason&quot;: &quot;CamelCaseShortReason&quot;,</strong></p><p><strong>&quot;message&quot;: &quot;regexp matching the issue in the kernel log&quot;</strong></p><p><strong>}</strong></p><h5><strong>Change Log Path</strong></h5><p>Kernel log in different OS distros may locate in different path. The <strong>log</strong> field in <strong>config/kernel-monitor.json</strong> is the log path inside the container. You can always configure it to match your OS distro.</p><h5><strong>Support Other Log Format</strong></h5><p>Kernel monitor uses <a href="https://github.com/kubernetes/node-problem-detector/blob/v0.1/pkg/kernelmonitor/translator/translator.go"><strong>Translator</strong></a> plugin to translate kernel log the internal data structure. It is easy to implement a new translator for a new log format.</p><h4>Caveats</h4><p>It is recommended to run the node problem detector in your cluster to monitor the node health. However, you should be aware that this will introduce extra resource overhead on each node. Usually this is fine, because:</p><ul><li>The kernel log is generated relatively slowly.</li><li>Resource limit is set for node problem detector.</li><li>Even under high load, the resource usage is acceptable. (see <a href="https://github.com/kubernetes/node-problem-detector/issues/2#issuecomment-220255629">benchmark result</a>)</li></ul><h3>Logging Using Stackdriver</h3><p>Before reading this page, it&#x27;s highly recommended to familiarize yourself with the <a href="https://kubernetes.io/docs/concepts/cluster-administration/logging">overview of logging in Kubernetes</a>.</p><p><strong>Note:</strong> By default, Stackdriver logging collects only your container&#x27;s standard output and standard error streams. To collect any logs your application writes to a file (for example), see the <a href="https://kubernetes.io/docs/concepts/cluster-administration/logging#sidecar-container-with-a-logging-agent">sidecar approach</a> in the Kubernetes logging overview.</p><h4>Deploying</h4><p>To ingest logs, you must deploy the Stackdriver Logging agent to each node in your cluster. The agent is a configured <strong>fluentd</strong> instance, where the configuration is stored in a <strong>ConfigMap</strong> and the instances are managed using a Kubernetes <strong>DaemonSet</strong>. The actual deployment of the <strong>ConfigMap</strong>and <strong>DaemonSet</strong> for your cluster depends on your individual cluster setup.</p><h5><strong>Deploying to a new cluster</strong></h5><h6><strong>Google Kubernetes Engine</strong></h6><p>Stackdriver is the default logging solution for clusters deployed on Google Kubernetes Engine. Stackdriver Logging is deployed to a new cluster by default unless you explicitly opt-out.</p><h6><strong>Other platforms</strong></h6><p>To deploy Stackdriver Logging on a new cluster that you&#x27;re creating using <strong>kube-up.sh</strong>, do the following:</p><ol><li>Set the <strong>KUBE_LOGGING_DESTINATION</strong> environment variable to <strong>gcp</strong>.</li><li><strong>If not running on GCE</strong>, include the <strong>beta.kubernetes.io/fluentd-ds-ready=true</strong> in the <strong>KUBE_NODE_LABELS</strong> variable.</li></ol><p>Once your cluster has started, each node should be running the Stackdriver Logging agent. The <strong>DaemonSet</strong> and <strong>ConfigMap</strong> are configured as addons. If you&#x27;re not using <strong>kube-up.sh</strong>, consider starting a cluster without a pre-configured logging solution and then deploying Stackdriver Logging agents to the running cluster.</p><h5><strong>Deploying to an existing cluster</strong></h5><ol><li>Apply a label on each node, if not already present.</li></ol><p>The Stackdriver Logging agent deployment uses node labels to determine to which nodes it should be allocated. These labels were introduced to distinguish nodes with the Kubernetes version 1.6 or higher. If the cluster was created with Stackdriver Logging configured and node has version 1.5.X or lower, it will have fluentd as static pod. Node cannot have more than one instance of fluentd, therefore only apply labels to the nodes that don&#x27;t have fluentd pod allocated already. You can ensure that your node is labelled properly by running <strong>kubectl describe</strong> as follows:</p><p><strong>kubectl describe node $NODE_NAME</strong></p><p>The output should be similar to this:</p><p><strong>Name: NODE_NAME</strong></p><p><strong>Role:</strong></p><p><strong>Labels: beta.kubernetes.io/fluentd-ds-ready=true</strong></p><p><strong>.<!-- -->..</strong></p><p>Ensure that the output contains the label <strong>beta.kubernetes.io/fluentd-ds-ready=true</strong>. If it is not present, you can add it using the <strong>kubectl label</strong> command as follows:</p><p><strong>kubectl label node $NODE_NAME beta.kubernetes.io/fluentd-ds-ready=true</strong></p><p><strong>Note:</strong> If a node fails and has to be recreated, you must re-apply the label to the recreated node. To make this easier, you can use Kubelet&#x27;s command-line parameter for applying node labels in your node startup script.</p><ol><li>Deploy a <strong>ConfigMap</strong> with the logging agent configuration by running the following command:</li><li><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/debug-application-cluster/fluentd-gcp-configmap.yaml">https://k8s.io/docs/tasks/debug-application-cluster/fluentd-gcp-configmap.yaml</a></strong></li></ol><p>The command creates the <strong>ConfigMap</strong> in the <strong>default</strong> namespace. You can download the file manually and change it before creating the <strong>ConfigMap</strong> object.</p><ol><li>Deploy the logging agent <strong>DaemonSet</strong> by running the following command:</li><li><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/debug-application-cluster/fluentd-gcp-ds.yaml">https://k8s.io/docs/tasks/debug-application-cluster/fluentd-gcp-ds.yaml</a></strong></li></ol><p>You can download and edit this file before using it as well.</p><h4>Verifying your Logging Agent Deployment</h4><p>After Stackdriver <strong>DaemonSet</strong> is deployed, you can discover logging agent deployment status by running the following command:</p><p><strong>kubectl get ds --all-namespaces</strong></p><p>If you have 3 nodes in the cluster, the output should looks similar to this:</p><p><strong>NAMESPACE NAME DESIRED CURRENT READY NODE-SELECTOR AGE</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>kube-system fluentd-gcp-v2.0 3 3 3 beta.kubernetes.io/fluentd-ds-ready=true 6d</strong></p><p><strong>.<!-- -->..</strong></p><p>To understand how logging with Stackdriver works, consider the following synthetic log generator pod specification <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/counter-pod.yaml">counter-pod.yaml</a>:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>counter-pod.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/">https://raw.githubusercontent.com/kubernetes/</a> |
| website/master/docs/tasks/debug-application-cluster/counter-pod.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: counter</strong>                                                     |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: count</strong>                                                     |
|                                                                       |
| <strong>image: busybox</strong>                                                    |
|                                                                       |
| <strong>args: [/bin/sh, -c,</strong>                                              |
|                                                                       |
| <strong>\&#x27;i=0; while true; do echo &quot;$i: $(date)&quot;; i=$((i+1)); sleep 1; |
| done\&#x27;]</strong>                                                            |
+-----------------------------------------------------------------------+</p><p>This pod specification has one container that runs a bash script that writes out the value of a counter and the date once per second, and runs indefinitely. Let&#x27;s create this pod in the default namespace.</p><p><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/debug-application-cluster/counter-pod.yaml">https://k8s.io/docs/tasks/debug-application-cluster/counter-pod.yaml</a></strong></p><p>You can observe the running pod:</p><p><strong>$ kubectl get pods</strong></p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>counter 1/1 Running 0 5m</strong></p><p>For a short period of time you can observe the &#x27;Pending&#x27; pod status, because the kubelet has to download the container image first. When the pod status changes to <strong>Running</strong> you can use the <strong>kubectl logs</strong> command to view the output of this counter pod.</p><p><strong>$ kubectl logs counter</strong></p><p><strong>0: Mon Jan 1 00:00:00 UTC 2001</strong></p><p><strong>1: Mon Jan 1 00:00:01 UTC 2001</strong></p><p><strong>2: Mon Jan 1 00:00:02 UTC 2001</strong></p><p><strong>.<!-- -->..</strong></p><p>As described in the logging overview, this command fetches log entries from the container log file. If the container is killed and then restarted by Kubernetes, you can still access logs from the previous container. However, if the pod is evicted from the node, log files are lost. Let&#x27;s demonstrate this by deleting the currently running counter container:</p><p><strong>$ kubectl delete pod counter</strong></p><p><strong>pod &quot;counter&quot; deleted</strong></p><p>and then recreating it:</p><p><strong>$ kubectl create -f <a href="https://k8s.io/docs/tasks/debug-application-cluster/counter-pod.yaml">https://k8s.io/docs/tasks/debug-application-cluster/counter-pod.yaml</a></strong></p><p><strong>pod &quot;counter&quot; created</strong></p><p>After some time, you can access logs from the counter pod again:</p><p><strong>$ kubectl logs counter</strong></p><p><strong>0: Mon Jan 1 00:01:00 UTC 2001</strong></p><p><strong>1: Mon Jan 1 00:01:01 UTC 2001</strong></p><p><strong>2: Mon Jan 1 00:01:02 UTC 2001</strong></p><p><strong>.<!-- -->..</strong></p><p>As expected, only recent log lines are present. However, for a real-world application you will likely want to be able to access logs from all containers, especially for the debug purposes. This is exactly when the previously enabled Stackdriver Logging can help.</p><h4>Viewing logs</h4><p>Stackdriver Logging agent attaches metadata to each log entry, for you to use later in queries to select only the messages you&#x27;re interested in: for example, the messages from a particular pod.</p><p>The most important pieces of metadata are the resource type and log name. The resource type of a container log is <strong>container</strong>, which is named <strong>GKE Containers</strong> in the UI (even if the Kubernetes cluster is not on Google Kubernetes Engine). The log name is the name of the container, so that if you have a pod with two containers, named <strong>container_1</strong> and <strong>container_2</strong> in the spec, their logs will have log names <strong>container_1</strong> and <strong>container_2</strong> respectively.</p><p>System components have resource type <strong>compute</strong>, which is named <strong>GCE VM Instance</strong> in the interface. Log names for system components are fixed. For a Google Kubernetes Engine node, every log entry from a system component has one of the following log names:</p><ul><li>docker</li><li>kubelet</li><li>kube-proxy</li></ul><p>You can learn more about viewing logs on <a href="https://cloud.google.com/logging/docs/view/logs_viewer">the dedicated Stackdriver page</a>.</p><p>One of the possible ways to view logs is using the <a href="https://cloud.google.com/logging/docs/api/gcloud-logging"><strong>gcloud logging</strong></a> command line interface from the <a href="https://cloud.google.com/sdk/">Google Cloud SDK</a>. It uses Stackdriver Logging <a href="https://cloud.google.com/logging/docs/view/advanced_filters">filtering syntax</a> to query specific logs. For example, you can run the following command:</p><p><strong>$ gcloud beta logging read \&#x27;logName=&quot;projects/$YOUR_PROJECT_ID/logs/count&quot;\&#x27; --format json | jq \&#x27;.[].textPayload\&#x27;</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>&quot;2: Mon Jan 1 00:01:02 UTC 2001<!-- -->\<!-- -->n&quot;</strong></p><p><strong>&quot;1: Mon Jan 1 00:01:01 UTC 2001<!-- -->\<!-- -->n&quot;</strong></p><p><strong>&quot;0: Mon Jan 1 00:01:00 UTC 2001<!-- -->\<!-- -->n&quot;</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>&quot;2: Mon Jan 1 00:00:02 UTC 2001<!-- -->\<!-- -->n&quot;</strong></p><p><strong>&quot;1: Mon Jan 1 00:00:01 UTC 2001<!-- -->\<!-- -->n&quot;</strong></p><p><strong>&quot;0: Mon Jan 1 00:00:00 UTC 2001<!-- -->\<!-- -->n&quot;</strong></p><p>As you can see, it outputs messages for the count container from both the first and second runs, despite the fact that the kubelet already deleted the logs for the first container.</p><h5><strong>Exporting logs</strong></h5><p>You can export logs to <a href="https://cloud.google.com/storage/">Google Cloud Storage</a> or to <a href="https://cloud.google.com/bigquery/">BigQuery</a> to run further analysis. Stackdriver Logging offers the concept of sinks, where you can specify the destination of log entries. More information is available on the Stackdriver <a href="https://cloud.google.com/logging/docs/export/configure_export_v2">Exporting Logs page</a>.</p><h4>Configuring Stackdriver Logging Agents</h4><p>Sometimes the default installation of Stackdriver Logging may not suit your needs, for example:</p><ul><li>You may want to add more resources because default performance doesn&#x27;t suit your needs.</li><li>You may want to introduce additional parsing to extract more metadata from your log messages, like severity or source code reference.</li><li>You may want to send logs not only to Stackdriver or send it to Stackdriver only partially.</li></ul><p>In this case you need to be able to change the parameters of <strong>DaemonSet</strong> and <strong>ConfigMap</strong>.</p><h5><strong>Prerequisites</strong></h5><p>If you&#x27;re using GKE and Stackdriver Logging is enabled in your cluster, you cannot change its configuration, because it&#x27;s managed and supported by GKE. However, you can disable the default integration and deploy your own. Note, that you will have to support and maintain a newly deployed configuration yourself: update the image and configuration, adjust the resources and so on. To disable the default logging integration, use the following command:</p><p><strong>gcloud beta container clusters update --logging-service=none CLUSTER</strong></p><p>You can find notes on how to then install Stackdriver Logging agents into a running cluster in the <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/logging-stackdriver/#deploying">Deploying section</a>.</p><h5><strong>Changing </strong>DaemonSet<strong> parameters</strong></h5><p>When you have the Stackdriver Logging <strong>DaemonSet</strong> in your cluster, you can just modify the <strong>template</strong> field in its spec, daemonset controller will update the pods for you. For example, let&#x27;s assume you&#x27;ve just installed the Stackdriver Logging as described above. Now you want to change the memory limit to give fluentd more memory to safely process more logs.</p><p>Get the spec of <strong>DaemonSet</strong> running in your cluster:</p><p><strong>kubectl get ds fluentd-gcp-v2.0 --namespace kube-system -o yaml &gt; fluentd-gcp-ds.yaml</strong></p><p>Then edit resource requirements in the spec file and update the <strong>DaemonSet</strong> object in the apiserver using the following command:</p><p><strong>kubectl replace -f fluentd-gcp-ds.yaml</strong></p><p>After some time, Stackdriver Logging agent pods will be restarted with the new configuration.</p><h5><strong>Changing fluentd parameters</strong></h5><p>Fluentd configuration is stored in the <strong>ConfigMap</strong> object. It is effectively a set of configuration files that are merged together. You can learn about fluentd configuration on the <a href="http://docs.fluentd.org/">official site</a>.</p><p>Imagine you want to add a new parsing logic to the configuration, so that fluentd can understand default Python logging format. An appropriate fluentd filter looks similar to this:</p><p><strong><code>&lt;filter reform.\*\*&gt;</code></strong></p><p><strong>type parser</strong></p><p><strong>format /\^(?<code>&lt;severity&gt;\\w):(?&lt;logger_name&gt;\\w):(?&lt;log&gt;</code>.<!-- -->*<!-- -->)/</strong></p><p><strong>reserve_data true</strong></p><p><strong>suppress_parse_error_log true</strong></p><p><strong>key_name log</strong></p><p><strong><code>&lt;/filter&gt;</code></strong></p><p>Now you have to put it in the configuration and make Stackdriver Logging agents pick it up. Get the current version of the Stackdriver Logging <strong>ConfigMap</strong> in your cluster by running the following command:</p><p><strong>kubectl get cm fluentd-gcp-config --namespace kube-system -o yaml &gt; fluentd-gcp-configmap.yaml</strong></p><p>Then in the value for the key <strong>containers.input.conf</strong> insert a new filter right after the <strong>source</strong>section. <strong>Note:</strong> order is important.</p><p>Updating <strong>ConfigMap</strong> in the apiserver is more complicated than updating <strong>DaemonSet</strong>. It&#x27;s better to consider <strong>ConfigMap</strong> to be immutable. Then, in order to update the configuration, you should create <strong>ConfigMap</strong> with a new name and then change <strong>DaemonSet</strong> to point to it using <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/logging-stackdriver/#changing-daemonset-parameters">guide above</a>.</p><h5><strong>Adding fluentd plugins</strong></h5><p>Fluentd is written in Ruby and allows to extend its capabilities using <a href="http://www.fluentd.org/plugins">plugins</a>. If you want to use a plugin, which is not included in the default Stackdriver Logging container image, you have to build a custom image. Imagine you want to add Kafka sink for messages from a particular container for additional processing. You can re-use the default <a href="https://git.k8s.io/contrib/fluentd/fluentd-gcp-image">container image sources</a> with minor changes:</p><ul><li>Change Makefile to point to your container repository, e.g. <strong>PREFIX=gcr.io/<code>&lt;your-project-id&gt;</code></strong>.</li><li>Add your dependency to the Gemfile, for example <strong>gem \&#x27;fluent-plugin-kafka\&#x27;</strong>.</li></ul><p>Then run <strong>make build push</strong> from this directory. After updating <strong>DaemonSet</strong> to pick up the new image, you can use the plugin you installed in the fluentd configuration.</p><h3>Events in Stackdriver</h3><p>Kubernetes events are objects that provide insight into what is happening inside a cluster, such as what decisions were made by scheduler or why some pods were evicted from the node. You can read more about using events for debugging your application in the <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application-introspection/">Application Introspection and Debugging </a>section.</p><p>Since events are API objects, they are stored in the apiserver on master. To avoid filling up master&#x27;s disk, a retention policy is enforced: events are removed one hour after the last occurrence. To provide longer history and aggregation capabilities, a third party solution should be installed to capture events.</p><p>This article describes a solution that exports Kubernetes events to Stackdriver Logging, where they can be processed and analyzed.</p><p><strong>Note:</strong> it is not guaranteed that all events happening in a cluster will be exported to Stackdriver. One possible scenario when events will not be exported is when event exporter is not running (e.g. during restart or upgrade). In most cases it&#x27;s fine to use events for purposes like setting up <a href="https://cloud.google.com/logging/docs/view/logs_based_metrics">metrics</a> and <a href="https://cloud.google.com/logging/docs/view/logs_based_metrics#creating_an_alerting_policy">alerts</a>, but you should be aware of the potential inaccuracy.</p><ul><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/events-stackdriver/#deployment"><strong>Deployment</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/events-stackdriver/#google-kubernetes-engine"><strong>Google Kubernetes Engine</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/events-stackdriver/#deploying-to-the-existing-cluster"><strong>Deploying to the Existing Cluster</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/events-stackdriver/#user-guide"><strong>User Guide</strong></a></li></ul><h4>Deployment</h4><h5><strong>Google Kubernetes Engine</strong></h5><p>In Google Kubernetes Engine, if cloud logging is enabled, event exporter is deployed by default to the clusters with master running version 1.7 and higher. To prevent disturbing your workloads, event exporter does not have resources set and is in the best effort QOS class, which means that it will be the first to be killed in the case of resource starvation. If you want your events to be exported, make sure you have enough resources to facilitate the event exporter pod. This may vary depending on the workload, but on average, approximately 100Mb RAM and 100m CPU is needed.</p><h5><strong>Deploying to the Existing Cluster</strong></h5><p>Deploy event exporter to your cluster using the following command:</p><p><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/debug-application-cluster/event-exporter-deploy.yaml">https://k8s.io/docs/tasks/debug-application-cluster/event-exporter-deploy.yaml</a></strong></p><p>Since event exporter accesses the Kubernetes API, it requires permissions to do so. The following deployment is configured to work with RBAC authorization. It sets up a service account and a cluster role binding to allow event exporter to read events. To make sure that event exporter pod will not be evicted from the node, you can additionally set up resource requests. As mentioned earlier, 100Mb RAM and 100m CPU should be enough.</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>event-exporter-de                                                  |
| ploy.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/website/ma">https://raw.githubusercontent.com/kubernetes/website/ma</a> |
| ster/docs/tasks/debug-application-cluster/event-exporter-deploy.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: ServiceAccount</strong>                                              |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: event-exporter-sa</strong>                                           |
|                                                                       |
| <strong>namespace: default</strong>                                                |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: event-exporter</strong>                                               |
|                                                                       |
| <strong>---</strong>                                                             |
|                                                                       |
| <strong>apiVersion: rbac.authorization.k8s.io/v1</strong>                          |
|                                                                       |
| <strong>kind: ClusterRoleBinding</strong>                                          |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: event-exporter-rb</strong>                                           |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: event-exporter</strong>                                               |
|                                                                       |
| <strong>roleRef:</strong>                                                          |
|                                                                       |
| <strong>apiGroup: rbac.authorization.k8s.io</strong>                               |
|                                                                       |
| <strong>kind: ClusterRole</strong>                                                 |
|                                                                       |
| <strong>name: view</strong>                                                        |
|                                                                       |
| <strong>subjects:</strong>                                                         |
|                                                                       |
| <strong>- kind: ServiceAccount</strong>                                            |
|                                                                       |
| <strong>name: event-exporter-sa</strong>                                           |
|                                                                       |
| <strong>namespace: default</strong>                                                |
|                                                                       |
| <strong>---</strong>                                                             |
|                                                                       |
| <strong>apiVersion: apps/v1</strong>                                               |
|                                                                       |
| <strong>kind: Deployment</strong>                                                  |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: event-exporter-v0.1.0</strong>                                       |
|                                                                       |
| <strong>namespace: default</strong>                                                |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: event-exporter</strong>                                               |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>matchLabels:</strong>                                                      |
|                                                                       |
| <strong>app: event-exporter</strong>                                               |
|                                                                       |
| <strong>replicas: 1</strong>                                                       |
|                                                                       |
| <strong>template:</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: event-exporter</strong>                                               |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>serviceAccountName: event-exporter-sa</strong>                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: event-exporter</strong>                                            |
|                                                                       |
| <strong>image: gcr.io/google-containers/event-exporter:v0.1.0</strong>             |
|                                                                       |
| <strong>command:</strong>                                                          |
|                                                                       |
| <strong>- \&#x27;/event-exporter\&#x27;</strong>                                             |
|                                                                       |
| <strong>terminationGracePeriodSeconds: 30</strong>                                 |
+-----------------------------------------------------------------------+</p><h4>User Guide</h4><p>Events are exported to the <strong>GKE Cluster</strong> resource in Stackdriver Logging. You can find them by selecting an appropriate option from a drop-down menu of available resources:</p><p>You can filter based on the event object fields using Stackdriver Logging <a href="https://cloud.google.com/logging/docs/view/advanced_filters">filtering mechanism</a>. For example, the following query will show events from the scheduler about pods from deployment <strong>nginx-deployment</strong>:</p><p><strong>resource.type=&quot;gke_cluster&quot;</strong></p><p><strong>jsonPayload.kind=&quot;Event&quot;</strong></p><p><strong>jsonPayload.source.component=&quot;default-scheduler&quot;</strong></p><p><strong>jsonPayload.involvedObject.name:&quot;nginx-deployment&quot;</strong></p><h3>Logging Using Elasticsearch and Kibana</h3><p>On the Google Compute Engine (GCE) platform, the default logging support targets <a href="https://cloud.google.com/logging/">Stackdriver Logging</a>, which is described in detail in the <a href="https://kubernetes.io/docs/user-guide/logging/stackdriver">Logging With Stackdriver Logging</a>.</p><p>This article describes how to set up a cluster to ingest logs into <a href="https://www.elastic.co/products/elasticsearch">Elasticsearch</a> and view them using <a href="https://www.elastic.co/products/kibana">Kibana</a>, as an alternative to Stackdriver Logging when running on GCE. Note that Elasticsearch and Kibana cannot be setup automatically in the Kubernetes cluster hosted on Google Kubernetes Engine, you have to deploy it manually.</p><p>To use Elasticsearch and Kibana for cluster logging, you should set the following environment variable as shown below when creating your cluster with kube-up.sh:</p><p><strong>KUBE_LOGGING_DESTINATION=elasticsearch</strong></p><p>You should also ensure that <strong>KUBE_ENABLE_NODE_LOGGING=true</strong> (which is the default for the GCE platform).</p><p>Now, when you create a cluster, a message will indicate that the Fluentd log collection daemons that run on each node will target Elasticsearch:</p><p><strong>$ cluster/kube-up.sh</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>Project: kubernetes-satnam</strong></p><p><strong>Zone: us-central1-b</strong></p><p><strong>.<!-- -->.. calling kube-up</strong></p><p><strong>Project: kubernetes-satnam</strong></p><p><strong>Zone: us-central1-b</strong></p><p><strong>+++ Staging server tars to Google Storage: gs://kubernetes-staging-e6d0e81793/devel</strong></p><p><strong>+++ kubernetes-server-linux-amd64.tar.gz uploaded (sha1 = 6987c098277871b6d69623141276924ab687f89d)</strong></p><p><strong>+++ kubernetes-salt.tar.gz uploaded (sha1 = bdfc83ed6b60fa9e3bff9004b542cfc643464cd0)</strong></p><p><strong>Looking for already existing resources</strong></p><p><strong>Starting master and configuring firewalls</strong></p><p><strong>Created <!-- -->[https://www.googleapis.com/compute/v1/projects/kubernetes-satnam/zones/us-central1-b/disks/kubernetes-master-pd]<!-- -->.</strong></p><p><strong>NAME ZONE SIZE_GB TYPE STATUS</strong></p><p><strong>kubernetes-master-pd us-central1-b 20 pd-ssd READY</strong></p><p><strong>Created <!-- -->[https://www.googleapis.com/compute/v1/projects/kubernetes-satnam/regions/us-central1/addresses/kubernetes-master-ip]<!-- -->.</strong></p><p><strong>+++ Logging using Fluentd to elasticsearch</strong></p><p>The per-node Fluentd pods, the Elasticsearch pods, and the Kibana pods should all be running in the kube-system namespace soon after the cluster comes to life.</p><p><strong>$ kubectl get pods --namespace=kube-system</strong></p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>elasticsearch-logging-v1-78nog 1/1 Running 0 2h</strong></p><p><strong>elasticsearch-logging-v1-nj2nb 1/1 Running 0 2h</strong></p><p><strong>fluentd-elasticsearch-kubernetes-node-5oq0 1/1 Running 0 2h</strong></p><p><strong>fluentd-elasticsearch-kubernetes-node-6896 1/1 Running 0 2h</strong></p><p><strong>fluentd-elasticsearch-kubernetes-node-l1ds 1/1 Running 0 2h</strong></p><p><strong>fluentd-elasticsearch-kubernetes-node-lz9j 1/1 Running 0 2h</strong></p><p><strong>kibana-logging-v1-bhpo8 1/1 Running 0 2h</strong></p><p><strong>kube-dns-v3-7r1l9 3/3 Running 0 2h</strong></p><p><strong>monitoring-heapster-v4-yl332 1/1 Running 1 2h</strong></p><p><strong>monitoring-influx-grafana-v1-o79xf 2/2 Running 0 2h</strong></p><p>The <strong>fluentd-elasticsearch</strong> pods gather logs from each node and send them to the <strong>elasticsearch-logging</strong> pods, which are part of a <a href="https://kubernetes.io/docs/concepts/services-networking/service/">service</a> named <strong>elasticsearch-logging</strong>. These Elasticsearch pods store the logs and expose them via a REST API. The <strong>kibana-logging</strong>pod provides a web UI for reading the logs stored in Elasticsearch, and is part of a service named <strong>kibana-logging</strong>.</p><p>The Elasticsearch and Kibana services are both in the <strong>kube-system</strong> namespace and are not directly exposed via a publicly reachable IP address. To reach them, follow the instructions for <a href="https://kubernetes.io/docs/concepts/cluster-administration/access-cluster/#accessing-services-running-on-the-cluster">Accessing services running in a cluster</a>.</p><p>If you try accessing the <strong>elasticsearch-logging</strong> service in your browser, you&#x27;ll see a status page that looks something like this:</p><p>You can now type Elasticsearch queries directly into the browser, if you&#x27;d like. See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-uri-request.html">Elasticsearch&#x27;s documentation</a> for more details on how to do so.</p><p>Alternatively, you can view your cluster&#x27;s logs using Kibana (again using the <a href="https://kubernetes.io/docs/user-guide/accessing-the-cluster/#accessing-services-running-on-the-cluster">instructions for accessing a service running in the cluster</a>). The first time you visit the Kibana URL you will be presented with a page that asks you to configure your view of the ingested logs. Select the option for timeseries values and select <strong>@timestamp</strong>. On the following page select the <strong>Discover</strong> tab and then you should be able to see the ingested logs. You can set the refresh interval to 5 seconds to have the logs regularly refreshed.</p><p>Here is a typical view of ingested logs from the Kibana viewer:</p><p>Kibana opens up all sorts of powerful options for exploring your logs! For some ideas on how to dig into it, check out <a href="https://www.elastic.co/guide/en/kibana/current/discover.html">Kibana&#x27;s documentation</a>.</p><h3>Determine the Reason for Pod Failure</h3><p>This page shows how to write and read a Container termination message.</p><p>Termination messages provide a way for containers to write information about fatal events to a location where it can be easily retrieved and surfaced by tools like dashboards and monitoring software. In most cases, information that you put in a termination message should also be written to the general <a href="https://kubernetes.io/docs/concepts/cluster-administration/logging/">Kubernetes logs</a>.</p><ul><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/determine-reason-pod-failure/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/determine-reason-pod-failure/#writing-and-reading-a-termination-message"><strong>Writing and reading a termination message</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/determine-reason-pod-failure/#customizing-the-termination-message"><strong>Customizing the termination message</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/determine-reason-pod-failure/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Before you begin</h4><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using<a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <strong>kubectl version</strong>.</p><h4>Writing and reading a termination message</h4><p>In this exercise, you create a Pod that runs one container. The configuration file specifies a command that runs when the container starts.</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>termination.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/">https://raw.githubusercontent.com/kubernetes/</a> |
| website/master/docs/tasks/debug-application-cluster/termination.yaml) |
+=======================================================================+
| <strong>apiVersion: v1</strong>                                                    |
|                                                                       |
| <strong>kind: Pod</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: termination-demo</strong>                                            |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: termination-demo-container</strong>                                |
|                                                                       |
| <strong>image: debian</strong>                                                     |
|                                                                       |
| <strong>command: <!-- -->[&quot;/bin/sh&quot;]</strong>                                          |
|                                                                       |
| <strong>args: <!-- -->[&quot;-c&quot;, &quot;sleep 10 &amp;&amp; echo Sleep expired &gt;                 |
| /dev/termination-log&quot;]</strong>                                            |
+-----------------------------------------------------------------------+</p><ol><li>Create a Pod based on the YAML configuration file:</li><li><strong>kubectl create -f <a href="https://k8s.io/docs/tasks/debug-application-cluster/termination.yaml">https://k8s.io/docs/tasks/debug-application-cluster/termination.yaml</a></strong></li></ol><p>In the YAML file, in the <strong>cmd</strong> and <strong>args</strong> fields, you can see that the container sleeps for 10 seconds and then writes &quot;Sleep expired&quot; to the <strong>/dev/termination-log</strong> file. After the container writes the &quot;Sleep expired&quot; message, it terminates.</p><ol><li>Display information about the Pod:</li><li><strong>kubectl get pod termination-demo</strong></li></ol><p>Repeat the preceding command until the Pod is no longer running.</p><ol><li>Display detailed information about the Pod:</li><li><strong>kubectl get pod --output=yaml</strong></li></ol><p>The output includes the &quot;Sleep expired&quot; message:</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>lastState:</strong></p><p><strong>terminated:</strong></p><p><strong>containerID: <!-- -->.<!-- -->..</strong></p><p><strong>exitCode: 0</strong></p><p><strong>finishedAt: <!-- -->.<!-- -->..</strong></p><p><strong>message: |</strong></p><p><strong>Sleep expired</strong></p><p><strong>.<!-- -->..</strong></p><ol><li>Use a Go template to filter the output so that it includes only the termination message:</li></ol><p><strong>kubectl get pod termination-demo -o go-template=&quot;{{range .status.containerStatuses}}{{.lastState.terminated.message}}{{end}}&quot;</strong></p><h4>Customizing the termination message</h4><p>Kubernetes retrieves termination messages from the termination message file specified in the <strong>terminationMessagePath</strong> field of a Container, which as a default value of <strong>/dev/termination-log</strong>. By customizing this field, you can tell Kubernetes to use a different file. Kubernetes use the contents from the specified file to populate the Container&#x27;s status message on both success and failure.</p><p>In the following example, the container writes termination messages to <strong>/tmp/my-log</strong> for Kubernetes to retrieve:</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: msg-path-demo</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: msg-path-demo-container</strong></p><p><strong>image: debian</strong></p><p><strong>terminationMessagePath: &quot;/tmp/my-log&quot;</strong></p><p>Moreover, users can set the <strong>terminationMessagePolicy</strong> field of a Container for further customization. This field defaults to &quot;<strong>File</strong>&quot; which means the termination messages are retrieved only from the termination message file. By setting the <strong>terminationMessagePolicy</strong> to &quot;<strong>FallbackToLogsOnError</strong>&quot;, you can tell Kubernetes to use the last chunk of container log output if the termination message file is empty and the container exited with an error. The log output is limited to 2048 bytes or 80 lines, whichever is smaller.</p><h4>What&#x27;s next</h4><ul><li>See the <strong>terminationMessagePath</strong> field in <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#container-v1-core">Container</a>.</li><li>Learn about <a href="https://kubernetes.io/docs/concepts/cluster-administration/logging/">retrieving logs</a>.</li><li>Learn about <a href="https://golang.org/pkg/text/template/">Go templates</a>.</li></ul><h3>Debug Init Containers</h3><p>This page shows how to investigate problems related to the execution of Init Containers. The example command lines below refer to the Pod as <code style="background-color:lightgray">&lt;pod-name&gt;</code> and the Init Containers  as <code style="background-color:lightgray">&lt;init-container-1&gt;</code> and <code style="background-color:lightgray">&lt;init-container-2&gt;</code>.</p><ul><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-init-containers/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-init-containers/#checking-the-status-of-init-containers"><strong>Checking the status of Init Containers</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-init-containers/#getting-details-about-init-containers"><strong>Getting details about Init Containers</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-init-containers/#accessing-logs-from-init-containers"><strong>Accessing logs from Init Containers</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-init-containers/#understanding-pod-status"><strong>Understanding Pod status</strong></a></li></ul><h4>Before you begin</h4><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using<a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <strong>kubectl version</strong>.</p><ul><li>You should be familiar with the basics of <a href="https://kubernetes.io/docs/concepts/abstractions/init-containers/">Init Containers</a>.</li><li>You should have <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-initialization/#creating-a-pod-that-has-an-init-container/">Configured an Init Container</a>.</li></ul><h4>Checking the status of Init Containers</h4><p>Display the status of your pod:</p><p><code style="background-color:lightgray">kubectl get pod &lt;pod-name&gt;</code></p><p>For example, a status of <strong>Init:1/2</strong> indicates that one of two Init Containers has completed successfully:</p><div class="MuiContainer-root MuiContainer-maxWidthLg"><pre class="Code__Pre-gy960v-0 UDybk prism-code language-bash" style="color:#9CDCFE;background-color:#1E1E1E"><div class="MuiGrid-root MuiGrid-container MuiGrid-justify-xs-flex-end"><button class="Code__CopyCode-gy960v-1 llUIua">Copy</button></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">NAME READY STATUS RESTARTS AGE</span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain"></span><span class="token operator" style="color:rgb(212, 212, 212)">&lt;</span><span class="token plain">pod-name</span><span class="token operator" style="color:rgb(212, 212, 212)">&gt;</span><span class="token plain"> </span><span class="token number" style="color:rgb(181, 206, 168)">0</span><span class="token plain">/1 Init:1/2 </span><span class="token number" style="color:rgb(181, 206, 168)">0</span><span class="token plain"> 7s</span></div></pre></div><p>See <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-init-containers/#understanding-pod-status">Understanding Pod status</a> for more examples of status values and their meanings.</p><h4>Getting details about Init Containers</h4><p>View more detailed information about Init Container execution:</p><p><code style="background-color:lightgray">kubectl describe pod &lt;pod-name&gt;</code></p><p>For example, a Pod with two Init Containers might show the following:</p><p><strong>Init Containers:</strong></p><p><strong><code>&lt;init-container-1&gt;</code>:</strong></p><p><strong>Container ID: <!-- -->.<!-- -->..</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>State: Terminated</strong></p><p><strong>Reason: Completed</strong></p><p><strong>Exit Code: 0</strong></p><p><strong>Started: <!-- -->.<!-- -->..</strong></p><p><strong>Finished: <!-- -->.<!-- -->..</strong></p><p><strong>Ready: True</strong></p><p><strong>Restart Count: 0</strong></p><p><strong>.<!-- -->..</strong></p><p><strong><code>&lt;init-container-2&gt;</code>:</strong></p><p><strong>Container ID: <!-- -->.<!-- -->..</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>State: Waiting</strong></p><p><strong>Reason: CrashLoopBackOff</strong></p><p><strong>Last State: Terminated</strong></p><p><strong>Reason: Error</strong></p><p><strong>Exit Code: 1</strong></p><p><strong>Started: <!-- -->.<!-- -->..</strong></p><p><strong>Finished: <!-- -->.<!-- -->..</strong></p><p><strong>Ready: False</strong></p><p><strong>Restart Count: 3</strong></p><p><strong>.<!-- -->..</strong></p><p>You can also access the Init Container statuses programmatically by reading the <strong>status.initContainerStatuses</strong> field on the Pod Spec:</p><p><strong>kubectl get pod nginx --template \&#x27;{{.status.initContainerStatuses}}\&#x27;</strong></p><p>This command will return the same information as above in raw JSON.</p><h4>Accessing logs from Init Containers</h4><p>Pass the Init Container name along with the Pod name to access its logs.</p><p><code style="background-color:lightgray">kubectl logs &lt;pod-name&gt; -c &lt;init-container-2&gt;</code></p><p>Init Containers that run a shell script print commands as they&#x27;re executed. For example, you can do this in Bash by running <strong>set -x</strong> at the beginning of the script.</p><h4>Understanding Pod status</h4><p>A Pod status beginning with <strong>Init:</strong> summarizes the status of Init Container execution. The table below describes some example status values that you might see while debugging Init Containers.</p><p>  Status                               Meaning</p><hr/><p>  <strong>Init:N/M</strong>                         The Pod has <strong>M</strong> Init Containers, and <strong>N</strong> have completed so far.
<strong>Init:Error</strong>                       An Init Container has failed to execute.
<strong>Init:CrashLoopBackOff</strong>            An Init Container has failed repeatedly.
<strong>Pending</strong>                          The Pod has not yet begun executing Init Containers.
<strong>PodInitializing</strong> or <strong>Running</strong>   The Pod has already finished executing Init Containers.</p><h3>Debug Pods and Replication Controllers</h3><ul><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-pod-replication-controller/#debugging-pods"><strong>Debugging pods</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-pod-replication-controller/#my-pod-stays-pending"><strong>My pod stays pending</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-pod-replication-controller/#insufficient-resources"><strong>Insufficient resources</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-pod-replication-controller/#using-hostport"><strong>Using hostPort</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-pod-replication-controller/#my-pod-stays-waiting"><strong>My pod stays waiting</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-pod-replication-controller/#my-pod-is-crashing-or-otherwise-unhealthy"><strong>My pod is crashing or otherwise unhealthy</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-pod-replication-controller/#debugging-replication-controllers"><strong>Debugging Replication Controllers</strong></a></li></ul><h4>Debugging pods</h4><p>The first step in debugging a pod is taking a look at it. Check the current state of the pod and recent events with the following command:</p><p><strong>$ kubectl describe pods ${POD_NAME}</strong></p><p>Look at the state of the containers in the pod. Are they all <strong>Running</strong>? Have there been recent restarts?</p><p>Continue debugging depending on the state of the pods.</p><h5><strong>My pod stays pending</strong></h5><p>If a pod is stuck in <strong>Pending</strong> it means that it can not be scheduled onto a node. Generally this is because there are insufficient resources of one type or another that prevent scheduling. Look at the output of the <strong>kubectl describe <!-- -->.<!-- -->..</strong> command above. There should be messages from the scheduler about why it can not schedule your pod. Reasons include:</p><h6><strong>Insufficient resources</strong></h6><p>You may have exhausted the supply of CPU or Memory in your cluster. In this case you can try several things:</p><ul><li><a href="https://kubernetes.io/docs/admin/cluster-management/#resizing-a-cluster">Add more nodes</a> to the cluster.</li><li><a href="https://kubernetes.io/docs/user-guide/pods/single-container/#deleting_a_pod">Terminate unneeded pods</a> to make room for pending pods.</li><li>Check that the pod is not larger than your nodes. For example, if all nodes have a capacity of <strong>cpu:1</strong>, then a pod with a request of <strong>cpu: 1.1</strong> will never be scheduled.</li></ul><p>You can check node capacities with the <strong>kubectl get nodes -o <code>&lt;format&gt;</code></strong> command. Here are some example command lines that extract just the necessary information:</p><p><strong>kubectl get nodes -o yaml | grep \&#x27;<!-- -->\<!-- -->sname<!-- -->\<!-- -->|cpu<!-- -->\<!-- -->|memory\&#x27;</strong></p><p><strong>kubectl get nodes -o json | jq \&#x27;.items[] | {name: .metadata.name, cap: .status.capacity}\&#x27;</strong></p><p>The <a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/">resource quota</a> feature can be configured to limit the total amount of resources that can be consumed. If used in conjunction with namespaces, it can prevent one team from hogging all the resources.</p><h6><strong>Using hostPort</strong></h6><p>When you bind a pod to a <strong>hostPort</strong> there are a limited number of places that the pod can be scheduled. In most cases, <strong>hostPort</strong> is unnecessary; try using a service object to expose your pod. If you do require <strong>hostPort</strong> then you can only schedule as many pods as there are nodes in your container cluster.</p><h5><strong>My pod stays waiting</strong></h5><p>If a pod is stuck in the <strong>Waiting</strong> state, then it has been scheduled to a worker node, but it can&#x27;t run on that machine. Again, the information from <strong>kubectl describe <!-- -->.<!-- -->..</strong> should be informative. The most common cause of <strong>Waiting</strong> pods is a failure to pull the image. There are three things to check:</p><ul><li>Make sure that you have the name of the image correct.</li><li>Have you pushed the image to the repository?</li><li>Run a manual <strong>docker pull <code>&lt;image&gt;</code></strong> on your machine to see if the image can be pulled.</li></ul><h5><strong>My pod is crashing or otherwise unhealthy</strong></h5><p>First, take a look at the logs of the current container:</p><p><strong>$ kubectl logs ${POD_NAME} ${CONTAINER_NAME}</strong></p><p>If your container has previously crashed, you can access the previous container&#x27;s crash log with:</p><p><strong>$ kubectl logs --previous ${POD_NAME} ${CONTAINER_NAME}</strong></p><p>Alternately, you can run commands inside that container with <strong>exec</strong>:</p><p><strong>$ kubectl exec ${POD_NAME} -c ${CONTAINER_NAME} -- ${CMD} ${ARG1} ${ARG2} <!-- -->.<!-- -->.. ${ARGN}</strong></p><p>Note that <strong>-c ${CONTAINER_NAME}</strong> is optional and can be omitted for pods that only contain a single container.</p><p>As an example, to look at the logs from a running Cassandra pod, you might run:</p><p><strong>$ kubectl exec cassandra -- cat /var/log/cassandra/system.log</strong></p><p>If none of these approaches work, you can find the host machine that the pod is running on and SSH into that host.</p><h4>Debugging Replication Controllers</h4><p>Replication controllers are fairly straightforward. They can either create pods or they can&#x27;t. If they can&#x27;t create pods, then please refer to the <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-pod-replication-controller/#debugging_pods">instructions above</a> to debug your pods.</p><p>You can also use <strong>kubectl describe rc ${CONTROLLER_NAME}</strong> to inspect events related to the replication controller.</p><h3>Debug Services</h3><p>An issue that comes up rather frequently for new installations of Kubernetes is that a <strong>Service</strong> is not working properly. You&#x27;ve run your <strong>Deployment</strong> and created a <strong>Service</strong>, but you get no response when you try to access it. This document will hopefully help you to figure out what&#x27;s going wrong.</p><ul><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/#conventions"><strong>Conventions</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/#running-commands-in-a-pod"><strong>Running commands in a Pod</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/#setup"><strong>Setup</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/#does-the-service-exist"><strong>Does the Service exist?</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/#does-the-service-work-by-dns"><strong>Does the Service work by DNS?</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/#does-any-service-exist-in-dns"><strong>Does any Service exist in DNS?</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/#does-the-service-work-by-ip"><strong>Does the Service work by IP?</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/#is-the-service-correct"><strong>Is the Service correct?</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/#does-the-service-have-any-endpoints"><strong>Does the Service have any Endpoints?</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/#are-the-pods-working"><strong>Are the Pods working?</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/#is-the-kube-proxy-working"><strong>Is the kube-proxy working?</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/#is-kube-proxy-running"><strong>Is kube-proxy running?</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/#is-kube-proxy-writing-iptables-rules"><strong>Is kube-proxy writing iptables rules?</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/#userspace"><strong>Userspace</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/#iptables"><strong>Iptables</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/#is-kube-proxy-proxying"><strong>Is kube-proxy proxying?</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/#a-pod-cannot-reach-itself-via-service-ip"><strong>A Pod cannot reach itself via Service IP</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/#seek-help"><strong>Seek help</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/#more-information"><strong>More information</strong></a></li></ul><h4>Conventions</h4><p>Throughout this doc you will see various commands that you can run. Some commands need to be run within a <strong>Pod</strong>, others on a Kubernetes <strong>Node</strong>, and others can run anywhere you have <strong>kubectl</strong>and credentials for the cluster. To make it clear what is expected, this document will use the following conventions.</p><p>If the command &quot;COMMAND&quot; is expected to run in a <strong>Pod</strong> and produce &quot;OUTPUT&quot;:</p><p><strong>u@pod$ COMMAND</strong></p><p><strong>OUTPUT</strong></p><p>If the command &quot;COMMAND&quot; is expected to run on a <strong>Node</strong> and produce &quot;OUTPUT&quot;:</p><p><strong>u@node$ COMMAND</strong></p><p><strong>OUTPUT</strong></p><p>If the command is &quot;kubectl ARGS&quot;:</p><p><strong>$ kubectl ARGS</strong></p><p><strong>OUTPUT</strong></p><h4>Running commands in a Pod</h4><p>For many steps here you will want to see what a <strong>Pod</strong> running in the cluster sees. The simplest way to do this is to run an interactive busybox <strong>Pod</strong>:</p><p><strong>$ kubectl run -it --rm --restart=Never busybox --image=busybox sh</strong></p><p><strong>If you don\&#x27;t see a command prompt, try pressing enter.</strong></p><p><strong>/ #</strong></p><p>If you already have a running <strong>Pod</strong> that you prefer to use, you can run a command in it using:</p><p><strong>$ kubectl exec <code>&lt;POD-NAME&gt; -c &lt;CONTAINER-NAME&gt; -- &lt;COMMAND&gt;</code></strong></p><h4>Setup</h4><p>For the purposes of this walk-through, let&#x27;s run some <strong>Pods</strong>. Since you&#x27;re probably debugging your own <strong>Service</strong> you can substitute your own details, or you can follow along and get a second data point.</p><p><strong>$ kubectl run hostnames --image=k8s.gcr.io/serve_hostname <!-- -->\</strong></p><p><strong>--labels=app=hostnames <!-- -->\</strong></p><p><strong>--port=9376 <!-- -->\</strong></p><p><strong>--replicas=3</strong></p><p><strong>deployment &quot;hostnames&quot; created</strong></p><p><strong>kubectl</strong> commands will print the type and name of the resource created or mutated, which can then be used in subsequent commands. Note that this is the same as if you had started the <strong>Deployment</strong> with the following YAML:</p><p><strong>apiVersion: extensions/v1beta1</strong></p><p><strong>kind: Deployment</strong></p><p><strong>metadata:</strong></p><p><strong>name: hostnames</strong></p><p><strong>spec:</strong></p><p><strong>selector:</strong></p><p><strong>app: hostnames</strong></p><p><strong>replicas: 3</strong></p><p><strong>template:</strong></p><p><strong>metadata:</strong></p><p><strong>labels:</strong></p><p><strong>app: hostnames</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- name: hostnames</strong></p><p><strong>image: k8s.gcr.io/serve_hostname</strong></p><p><strong>ports:</strong></p><p><strong>- containerPort: 9376</strong></p><p><strong>protocol: TCP</strong></p><p>Confirm your <strong>Pods</strong> are running:</p><p><strong>$ kubectl get pods -l app=hostnames</strong></p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>hostnames-632524106-bbpiw 1/1 Running 0 2m</strong></p><p><strong>hostnames-632524106-ly40y 1/1 Running 0 2m</strong></p><p><strong>hostnames-632524106-tlaok 1/1 Running 0 2m</strong></p><h4>Does the Service exist?</h4><p>The astute reader will have noticed that we did not actually create a <strong>Service</strong> yet - that is intentional. This is a step that sometimes gets forgotten, and is the first thing to check.</p><p>So what would happen if I tried to access a non-existent <strong>Service</strong>? Assuming you have another <strong>Pod</strong> that consumes this <strong>Service</strong> by name you would get something like:</p><p><strong>u@pod$ wget -qO- hostnames</strong></p><p><strong>wget: bad address \&#x27;hostname\&#x27;</strong></p><p>So the first thing to check is whether that <strong>Service</strong> actually exists:</p><p><strong>$ kubectl get svc hostnames</strong></p><p><strong>Error from server (NotFound): services &quot;hostnames&quot; not found</strong></p><p>So we have a culprit, let&#x27;s create the <strong>Service</strong>. As before, this is for the walk-through - you can use your own <strong>Service</strong>&#x27;s details here.</p><p><strong>$ kubectl expose deployment hostnames --port=80 --target-port=9376</strong></p><p><strong>service &quot;hostnames&quot; exposed</strong></p><p>And read it back, just to be sure:</p><p><strong>$ kubectl get svc hostnames</strong></p><p><strong>NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE</strong></p><p><strong>hostnames 10.0.1.175 <code>&lt;none&gt;</code> 80/TCP 5s</strong></p><p>As before, this is the same as if you had started the <strong>Service</strong> with YAML:</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Service</strong></p><p><strong>metadata:</strong></p><p><strong>name: hostnames</strong></p><p><strong>spec:</strong></p><p><strong>selector:</strong></p><p><strong>app: hostnames</strong></p><p><strong>ports:</strong></p><p><strong>- name: default</strong></p><p><strong>protocol: TCP</strong></p><p><strong>port: 80</strong></p><p><strong>targetPort: 9376</strong></p><p>Now you can confirm that the <strong>Service</strong> exists.</p><h4>Does the Service work by DNS?</h4><p>From a <strong>Pod</strong> in the same <strong>Namespace</strong>:</p><p><strong>u@pod$ nslookup hostnames</strong></p><p><strong>Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local</strong></p><p><strong>Name: hostnames</strong></p><p><strong>Address 1: 10.0.1.175 hostnames.default.svc.cluster.local</strong></p><p>If this fails, perhaps your <strong>Pod</strong> and <strong>Service</strong> are in different <strong>Namespaces</strong>, try a namespace-qualified name:</p><p><strong>u@pod$ nslookup hostnames.default</strong></p><p><strong>Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local</strong></p><p><strong>Name: hostnames.default</strong></p><p><strong>Address 1: 10.0.1.175 hostnames.default.svc.cluster.local</strong></p><p>If this works, you&#x27;ll need to adjust your app to use a cross-namespace name, or run your app and <strong>Service</strong> in the same <strong>Namespace</strong>. If this still fails, try a fully-qualified name:</p><p><strong>u@pod$ nslookup hostnames.default.svc.cluster.local</strong></p><p><strong>Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local</strong></p><p><strong>Name: hostnames.default.svc.cluster.local</strong></p><p><strong>Address 1: 10.0.1.175 hostnames.default.svc.cluster.local</strong></p><p>Note the suffix here: &quot;default.svc.cluster.local&quot;. The &quot;default&quot; is the <strong>Namespace</strong> we&#x27;re operating in. The &quot;svc&quot; denotes that this is a <strong>Service</strong>. The &quot;cluster.local&quot; is your cluster domain, which COULD be different in your own cluster.</p><p>You can also try this from a <strong>Node</strong> in the cluster (note: 10.0.0.10 is my DNS <strong>Service</strong>, yours might be different):</p><p><strong>u@node$ nslookup hostnames.default.svc.cluster.local 10.0.0.10</strong></p><p><strong>Server: 10.0.0.10</strong></p><p><strong>Address: 10.0.0.10#53</strong></p><p><strong>Name: hostnames.default.svc.cluster.local</strong></p><p><strong>Address: 10.0.1.175</strong></p><p>If you are able to do a fully-qualified name lookup but not a relative one, you need to check that your <strong>/etc/resolv.conf</strong> file is correct.</p><p><strong>u@pod$ cat /etc/resolv.conf</strong></p><p><strong>nameserver 10.0.0.10</strong></p><p><strong>search default.svc.cluster.local svc.cluster.local cluster.local example.com</strong></p><p><strong>options ndots:5</strong></p><p>The <strong>nameserver</strong> line must indicate your cluster&#x27;s DNS <strong>Service</strong>. This is passed into <strong>kubelet</strong> with the <strong>--cluster-dns</strong> flag.</p><p>The <strong>search</strong> line must include an appropriate suffix for you to find the <strong>Service</strong> name. In this case it is looking for <strong>Services</strong> in the local <strong>Namespace</strong> (<strong>default.svc.cluster.local</strong>), <strong>Services</strong> in all <strong>Namespaces</strong> (<strong>svc.cluster.local</strong>), and the cluster (<strong>cluster.local</strong>). Depending on your own install you might have additional records after that (up to 6 total). The cluster suffix is passed into <strong>kubelet</strong> with the <strong>--cluster-domain</strong> flag. We assume that is &quot;cluster.local&quot; in this document, but yours might be different, in which case you should change that in all of the commands above.</p><p>The <strong>options</strong> line must set <strong>ndots</strong> high enough that your DNS client library considers search paths at all. Kubernetes sets this to 5 by default, which is high enough to cover all of the DNS names it generates.</p><h5><strong>Does any Service exist in DNS?</strong></h5><p>If the above still fails - DNS lookups are not working for your <strong>Service</strong> - we can take a step back and see what else is not working. The Kubernetes master <strong>Service</strong> should always work:</p><p><strong>u@pod$ nslookup kubernetes.default</strong></p><p><strong>Server: 10.0.0.10</strong></p><p><strong>Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local</strong></p><p><strong>Name: kubernetes.default</strong></p><p><strong>Address 1: 10.0.0.1 kubernetes.default.svc.cluster.local</strong></p><p>If this fails, you might need to go to the kube-proxy section of this doc, or even go back to the top of this document and start over, but instead of debugging your own <strong>Service</strong>, debug DNS.</p><h4>Does the Service work by IP?</h4><p>Assuming we can confirm that DNS works, the next thing to test is whether your <strong>Service</strong> works at all. From a node in your cluster, access the <strong>Service</strong>&#x27;s IP (from <strong>kubectl get</strong> above).</p><p><strong>u@node$ curl 10.0.1.175:80</strong></p><p><strong>hostnames-0uton</strong></p><p><strong>u@node$ curl 10.0.1.175:80</strong></p><p><strong>hostnames-yp2kp</strong></p><p><strong>u@node$ curl 10.0.1.175:80</strong></p><p><strong>hostnames-bvc05</strong></p><p>If your <strong>Service</strong> is working, you should get correct responses. If not, there are a number of things that could be going wrong. Read on.</p><h4>Is the Service correct?</h4><p>It might sound silly, but you should really double and triple check that your <strong>Service</strong> is correct and matches your <strong>Pod</strong>&#x27;s port. Read back your <strong>Service</strong> and verify it:</p><p><strong>$ kubectl get service hostnames -o json</strong></p><p><strong>{</strong></p><p><strong>&quot;kind&quot;: &quot;Service&quot;,</strong></p><p><strong>&quot;apiVersion&quot;: &quot;v1&quot;,</strong></p><p><strong>&quot;metadata&quot;: {</strong></p><p><strong>&quot;name&quot;: &quot;hostnames&quot;,</strong></p><p><strong>&quot;namespace&quot;: &quot;default&quot;,</strong></p><p><strong>&quot;selfLink&quot;: &quot;/api/v1/namespaces/default/services/hostnames&quot;,</strong></p><p><strong>&quot;uid&quot;: &quot;428c8b6c-24bc-11e5-936d-42010af0a9bc&quot;,</strong></p><p><strong>&quot;resourceVersion&quot;: &quot;347189&quot;,</strong></p><p><strong>&quot;creationTimestamp&quot;: &quot;2015-07-07T15:24:29Z&quot;,</strong></p><p><strong>&quot;labels&quot;: {</strong></p><p><strong>&quot;app&quot;: &quot;hostnames&quot;</strong></p><p><strong>}</strong></p><p><strong>},</strong></p><p><strong>&quot;spec&quot;: {</strong></p><p><strong>&quot;ports&quot;: [</strong></p><p><strong>{</strong></p><p><strong>&quot;name&quot;: &quot;default&quot;,</strong></p><p><strong>&quot;protocol&quot;: &quot;TCP&quot;,</strong></p><p><strong>&quot;port&quot;: 80,</strong></p><p><strong>&quot;targetPort&quot;: 9376,</strong></p><p><strong>&quot;nodePort&quot;: 0</strong></p><p><strong>}</strong></p><p><strong>],</strong></p><p><strong>&quot;selector&quot;: {</strong></p><p><strong>&quot;app&quot;: &quot;hostnames&quot;</strong></p><p><strong>},</strong></p><p><strong>&quot;clusterIP&quot;: &quot;10.0.1.175&quot;,</strong></p><p><strong>&quot;type&quot;: &quot;ClusterIP&quot;,</strong></p><p><strong>&quot;sessionAffinity&quot;: &quot;None&quot;</strong></p><p><strong>},</strong></p><p><strong>&quot;status&quot;: {</strong></p><p><strong>&quot;loadBalancer&quot;: {}</strong></p><p><strong>}</strong></p><p><strong>}</strong></p><p>Is the port you are trying to access in <strong>spec.ports[]</strong>? Is the <strong>targetPort</strong> correct for your <strong>Pods</strong>(many <strong>Pods</strong> choose to use a different port than the <strong>Service</strong>)? If you meant it to be a numeric port, is it a number (9376) or a string &quot;9376&quot;? If you meant it to be a named port, do your <strong>Pods</strong> expose a port with the same name? Is the port&#x27;s <strong>protocol</strong> the same as the <strong>Pod</strong>&#x27;s?</p><h4>Does the Service have any Endpoints?</h4><p>If you got this far, we assume that you have confirmed that your <strong>Service</strong> exists and is resolved by DNS. Now let&#x27;s check that the <strong>Pods</strong> you ran are actually being selected by the <strong>Service</strong>.</p><p>Earlier we saw that the <strong>Pods</strong> were running. We can re-check that:</p><p><strong>$ kubectl get pods -l app=hostnames</strong></p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>hostnames-0uton 1/1 Running 0 1h</strong></p><p><strong>hostnames-bvc05 1/1 Running 0 1h</strong></p><p><strong>hostnames-yp2kp 1/1 Running 0 1h</strong></p><p>The &quot;AGE&quot; column says that these <strong>Pods</strong> are about an hour old, which implies that they are running fine and not crashing.</p><p>The <strong>-l app=hostnames</strong> argument is a label selector - just like our <strong>Service</strong> has. Inside the Kubernetes system is a control loop which evaluates the selector of every <strong>Service</strong> and saves the results into an <strong>Endpoints</strong> object.</p><p><strong>$ kubectl get endpoints hostnames</strong></p><p><strong>NAME ENDPOINTS</strong></p><p><strong>hostnames 10.244.0.5:9376,10.244.0.6:9376,10.244.0.7:9376</strong></p><p>This confirms that the endpoints controller has found the correct <strong>Pods</strong> for your <strong>Service</strong>. If the <strong>hostnames</strong> row is blank, you should check that the <strong>spec.selector</strong> field of your <strong>Service</strong> actually selects for <strong>metadata.labels</strong> values on your <strong>Pods</strong>. A common mistake is to have a typo or other error, such as the <strong>Service</strong> selecting for <strong>run=hostnames</strong>, but the <strong>Deployment</strong> specifying <strong>app=hostnames</strong>.</p><h4>Are the Pods working?</h4><p>At this point, we know that your <strong>Service</strong> exists and has selected your <strong>Pods</strong>. Let&#x27;s check that the <strong>Pods</strong> are actually working - we can bypass the <strong>Service</strong> mechanism and go straight to the <strong>Pods</strong>. Note that these commands use the <strong>Pod</strong> port (9376), rather than the <strong>Service</strong> port (80).</p><p><strong>u@pod$ wget -qO- 10.244.0.5:9376</strong></p><p><strong>hostnames-0uton</strong></p><p><strong>pod $ wget -qO- 10.244.0.6:9376</strong></p><p><strong>hostnames-bvc05</strong></p><p><strong>u@pod$ wget -qO- 10.244.0.7:9376</strong></p><p><strong>hostnames-yp2kp</strong></p><p>We expect each <strong>Pod</strong> in the <strong>Endpoints</strong> list to return its own hostname. If this is not what happens (or whatever the correct behavior is for your own <strong>Pods</strong>), you should investigate what&#x27;s happening there. You might find <strong>kubectl logs</strong> to be useful or <strong>kubectl exec</strong> directly to your <strong>Pods</strong> and check service from there.</p><p>Another thing to check is that your <strong>Pods</strong> are not crashing or being restarted. Frequent restarts could lead to intermittent connectivity issues.</p><p><strong>$ kubectl get pods -l app=hostnames</strong></p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>hostnames-632524106-bbpiw 1/1 Running 0 2m</strong></p><p><strong>hostnames-632524106-ly40y 1/1 Running 0 2m</strong></p><p><strong>hostnames-632524106-tlaok 1/1 Running 0 2m</strong></p><p>If the restart count is high, read more about how to <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-pod-replication-controller/#debugging-pods">debug pods</a>.</p><h4>Is the kube-proxy working?</h4><p>If you get here, your <strong>Service</strong> is running, has <strong>Endpoints</strong>, and your <strong>Pods</strong> are actually serving. At this point, the whole <strong>Service</strong> proxy mechanism is suspect. Let&#x27;s confirm it, piece by piece.</p><h5><strong>Is kube-proxy running?</strong></h5><p>Confirm that <strong>kube-proxy</strong> is running on your <strong>Nodes</strong>. You should get something like the below:</p><p><strong>u@node$ ps auxw | grep kube-proxy</strong></p><p><strong>root 4194 0.4 0.1 101864 17696 ? Sl Jul04 25:43 /usr/local/bin/kube-proxy --master=https://kubernetes-master --kubeconfig=/var/lib/kube-proxy/kubeconfig --v=2</strong></p><p>Next, confirm that it is not failing something obvious, like contacting the master. To do this, you&#x27;ll have to look at the logs. Accessing the logs depends on your <strong>Node</strong> OS. On some OSes it is a file, such as /var/log/kube-proxy.log, while other OSes use <strong>journalctl</strong> to access logs. You should see something like:</p><p><strong>I1027 22:14:53.995134 5063 server.go:200] Running in resource-only container &quot;/kube-proxy&quot;</strong></p><p><strong>I1027 22:14:53.998163 5063 server.go:247] Using iptables Proxier.</strong></p><p><strong>I1027 22:14:53.999055 5063 server.go:255] Tearing down userspace rules. Errors here are acceptable.</strong></p><p><strong>I1027 22:14:54.038140 5063 proxier.go:352] Setting endpoints for &quot;kube-system/kube-dns:dns-tcp&quot; to <!-- -->[10.244.1.3:53]</strong></p><p><strong>I1027 22:14:54.038164 5063 proxier.go:352] Setting endpoints for &quot;kube-system/kube-dns:dns&quot; to <!-- -->[10.244.1.3:53]</strong></p><p><strong>I1027 22:14:54.038209 5063 proxier.go:352] Setting endpoints for &quot;default/kubernetes:https&quot; to <!-- -->[10.240.0.2:443]</strong></p><p><strong>I1027 22:14:54.038238 5063 proxier.go:429] Not syncing iptables until Services and Endpoints have been received from master</strong></p><p><strong>I1027 22:14:54.040048 5063 proxier.go:294] Adding new service &quot;default/kubernetes:https&quot; at 10.0.0.1:443/TCP</strong></p><p><strong>I1027 22:14:54.040154 5063 proxier.go:294] Adding new service &quot;kube-system/kube-dns:dns&quot; at 10.0.0.10:53/UDP</strong></p><p><strong>I1027 22:14:54.040223 5063 proxier.go:294] Adding new service &quot;kube-system/kube-dns:dns-tcp&quot; at 10.0.0.10:53/TCP</strong></p><p>If you see error messages about not being able to contact the master, you should double-check your <strong>Node</strong> configuration and installation steps.</p><p>One of the possible reasons that <strong>kube-proxy</strong> cannot run correctly is that the required <strong>conntrack</strong>binary cannot be found. This may happen on some Linux systems, depending on how you are installing the cluster, for example, you are installing Kubernetes from scratch. If this is the case, you need to manually install the <strong>conntrack</strong> package (e.g. <strong>sudo apt install conntrack</strong> on Ubuntu) and then retry.</p><h5><strong>Is kube-proxy writing iptables rules?</strong></h5><p>One of the main responsibilities of <strong>kube-proxy</strong> is to write the <strong>iptables</strong> rules which implement <strong>Services</strong>. Let&#x27;s check that those rules are getting written.</p><p>The kube-proxy can run in either &quot;userspace&quot; mode or &quot;iptables&quot; mode. Hopefully you are using the newer, faster, more stable &quot;iptables&quot; mode. You should see one of the following cases.</p><h6><strong>Userspace</strong></h6><p><strong>u@node$ iptables-save | grep hostnames</strong></p><p><strong>-A KUBE-PORTALS-CONTAINER -d 10.0.1.175/32 -p tcp -m comment --comment &quot;default/hostnames:default&quot; -m tcp --dport 80 -j REDIRECT --to-ports 48577</strong></p><p><strong>-A KUBE-PORTALS-HOST -d 10.0.1.175/32 -p tcp -m comment --comment &quot;default/hostnames:default&quot; -m tcp --dport 80 -j DNAT --to-destination 10.240.115.247:48577</strong></p><p>There should be 2 rules for each port on your <strong>Service</strong> (just one in this example) - a &quot;KUBE-PORTALS-CONTAINER&quot; and a &quot;KUBE-PORTALS-HOST&quot;. If you do not see these, try restarting <strong>kube-proxy</strong> with the <strong>-V</strong> flag set to 4, and then look at the logs again.</p><p>Almost nobody should be using the &quot;userspace&quot; mode any more, so we won&#x27;t spend more time on it here.</p><h6><strong>Iptables</strong></h6><p><strong>u@node$ iptables-save | grep hostnames</strong></p><p><strong>-A KUBE-SEP-57KPRZ3JQVENLNBR -s 10.244.3.6/32 -m comment --comment &quot;default/hostnames:&quot; -j MARK --set-xmark 0x00004000/0x00004000</strong></p><p><strong>-A KUBE-SEP-57KPRZ3JQVENLNBR -p tcp -m comment --comment &quot;default/hostnames:&quot; -m tcp -j DNAT --to-destination 10.244.3.6:9376</strong></p><p><strong>-A KUBE-SEP-WNBA2IHDGP2BOBGZ -s 10.244.1.7/32 -m comment --comment &quot;default/hostnames:&quot; -j MARK --set-xmark 0x00004000/0x00004000</strong></p><p><strong>-A KUBE-SEP-WNBA2IHDGP2BOBGZ -p tcp -m comment --comment &quot;default/hostnames:&quot; -m tcp -j DNAT --to-destination 10.244.1.7:9376</strong></p><p><strong>-A KUBE-SEP-X3P2623AGDH6CDF3 -s 10.244.2.3/32 -m comment --comment &quot;default/hostnames:&quot; -j MARK --set-xmark 0x00004000/0x00004000</strong></p><p><strong>-A KUBE-SEP-X3P2623AGDH6CDF3 -p tcp -m comment --comment &quot;default/hostnames:&quot; -m tcp -j DNAT --to-destination 10.244.2.3:9376</strong></p><p><strong>-A KUBE-SERVICES -d 10.0.1.175/32 -p tcp -m comment --comment &quot;default/hostnames: cluster IP&quot; -m tcp --dport 80 -j KUBE-SVC-NWV5X2332I4OT4T3</strong></p><p><strong>-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment &quot;default/hostnames:&quot; -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-WNBA2IHDGP2BOBGZ</strong></p><p><strong>-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment &quot;default/hostnames:&quot; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-X3P2623AGDH6CDF3</strong></p><p><strong>-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment &quot;default/hostnames:&quot; -j KUBE-SEP-57KPRZ3JQVENLNBR</strong></p><p>There should be 1 rule in <strong>KUBE-SERVICES</strong>, 1 or 2 rules per endpoint in <strong>KUBE-SVC-(hash)</strong>(depending on <strong>SessionAffinity</strong>), one <strong>KUBE-SEP-(hash)</strong> chain per endpoint, and a few rules in each <strong>KUBE-SEP-(hash)</strong> chain. The exact rules will vary based on your exact config (including node-ports and load-balancers).</p><h5><strong>Is kube-proxy proxying?</strong></h5><p>Assuming you do see the above rules, try again to access your <strong>Service</strong> by IP:</p><p><strong>u@node$ curl 10.0.1.175:80</strong></p><p><strong>hostnames-0uton</strong></p><p>If this fails and you are using the userspace proxy, you can try accessing the proxy directly. If you are using the iptables proxy, skip this section.</p><p>Look back at the <strong>iptables-save</strong> output above, and extract the port number that <strong>kube-proxy</strong> is using for your <strong>Service</strong>. In the above examples it is &quot;48577&quot;. Now connect to that:</p><p><strong>u@node$ curl localhost:48577</strong></p><p><strong>hostnames-yp2kp</strong></p><p>If this still fails, look at the <strong>kube-proxy</strong> logs for specific lines like:</p><p><strong>Setting endpoints for default/hostnames:default to <!-- -->[10.244.0.5:9376 10.244.0.6:9376 10.244.0.7:9376]</strong></p><p>If you don&#x27;t see those, try restarting <strong>kube-proxy</strong> with the <strong>-V</strong> flag set to 4, and then look at the logs again.</p><h5><strong>A Pod cannot reach itself via Service IP</strong></h5><p>This can happen when the network is not properly configured for &quot;hairpin&quot; traffic, usually when <strong>kube-proxy</strong> is running in <strong>iptables</strong> mode and Pods are connected with bridge network. The <strong>Kubelet</strong> exposes a <strong>hairpin-mode</strong> <a href="https://kubernetes.io/docs/admin/kubelet/">flag</a> that allows endpoints of a Service to loadbalance back to themselves if they try to access their own Service VIP. The <strong>hairpin-mode</strong> flag must either be set to <strong>hairpin-veth</strong> or <strong>promiscuous-bridge</strong>.</p><p>The common steps to trouble shoot this are as follows:</p><ul><li>Confirm <strong>hairpin-mode</strong> is set to <strong>hairpin-veth</strong> or <strong>promiscuous-bridge</strong>. You should see something like the below. <strong>hairpin-mode</strong> is set to <strong>promiscuous-bridge</strong> in the following example.</li></ul><p><strong>u@node$ ps auxw|grep kubelet</strong></p><p><strong>root 3392 1.1 0.8 186804 65208 ? Sl 00:51 11:11 /usr/local/bin/kubelet --enable-debugging-handlers=true --config=/etc/kubernetes/manifests --allow-privileged=True --v=4 --cluster-dns=10.0.0.10 --cluster-domain=cluster.local --configure-cbr0=true --cgroup-root=/ --system-cgroups=/system --hairpin-mode=promiscuous-bridge --runtime-cgroups=/docker-daemon --kubelet-cgroups=/kubelet --babysit-daemons=true --max-pods=110 --serialize-image-pulls=false --outofdisk-transition-frequency=0</strong></p><ul><li>Confirm the effective <strong>hairpin-mode</strong>. To do this, you&#x27;ll have to look at kubelet log. Accessing the logs depends on your Node OS. On some OSes it is a file, such as /var/log/kubelet.log, while other OSes use <strong>journalctl</strong> to access logs. Please be noted that the effective hairpin mode may not match <strong>--hairpin-mode</strong> flag due to compatibility. Check if there is any log lines with key word <strong>hairpin</strong> in kubelet.log. There should be log lines indicating the effective hairpin mode, like something below.</li></ul><p><strong>I0629 00:51:43.648698 3252 kubelet.go:380] Hairpin mode set to &quot;promiscuous-bridge&quot;</strong></p><ul><li>If the effective hairpin mode is <strong>hairpin-veth</strong>, ensure the <strong>Kubelet</strong> has the permission to operate in <strong>/sys</strong> on node. If everything works properly, you should see something like:</li></ul><p><strong>u@node$ for intf in /sys/devices/virtual/net/cbr0/brif/<!-- -->*<!-- -->; do cat $intf/hairpin_mode; done</strong></p><p><strong>1</strong></p><p><strong>1</strong></p><p><strong>1</strong></p><p><strong>1</strong></p><ul><li>If the effective hairpin mode is <strong>promiscuous-bridge</strong>, ensure <strong>Kubelet</strong> has the permission to manipulate linux bridge on node. If cbr0` bridge is used and configured properly, you should see:</li></ul><p><strong>u@node$ ifconfig cbr0 |grep PROMISC</strong></p><p><strong>UP BROADCAST RUNNING PROMISC MULTICAST MTU:1460 Metric:1</strong></p><ul><li>Seek help if none of above works out.</li></ul><h4>Seek help</h4><p>If you get this far, something very strange is happening. Your <strong>Service</strong> is running, has <strong>Endpoints</strong>, and your <strong>Pods</strong> are actually serving. You have DNS working, <strong>iptables</strong> rules installed, and <strong>kube-proxy</strong> does not seem to be misbehaving. And yet your <strong>Service</strong> is not working. You should probably let us know, so we can help investigate!</p><p>Contact us on <a href="https://kubernetes.io/docs/troubleshooting/#slack">Slack</a> or <a href="https://groups.google.com/forum/#!forum/kubernetes-users">email</a> or <a href="https://github.com/kubernetes/kubernetes">GitHub</a>.</p><h4>More information</h4><p>Visit <a href="https://kubernetes.io/docs/troubleshooting/">troubleshooting document</a> for more information.</p><h3>Troubleshoot Clusters</h3><p>This doc is about cluster troubleshooting; we assume you have already ruled out your application as the root cause of the problem you are experiencing. See the <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application">application troubleshooting guide</a> for tips on application debugging. You may also visit <a href="https://kubernetes.io/docs/troubleshooting/">troubleshooting document</a> for more information.</p><h4>Listing your cluster</h4><p>The first thing to debug in your cluster is if your nodes are all registered correctly.</p><p>Run</p><p><strong>kubectl get nodes</strong></p><p>And verify that all of the nodes you expect to see are present and that they are all in the <strong>Ready</strong> state.</p><h4>Looking at logs</h4><p>For now, digging deeper into the cluster requires logging into the relevant machines. Here are the locations of the relevant log files. (note that on systemd-based systems, you may need to use <strong>journalctl</strong> instead)</p><h5><strong>Master</strong></h5><ul><li>/var/log/kube-apiserver.log - API Server, responsible for serving the API</li><li>/var/log/kube-scheduler.log - Scheduler, responsible for making scheduling decisions</li><li>/var/log/kube-controller-manager.log - Controller that manages replication controllers</li></ul><h5><strong>Worker Nodes</strong></h5><ul><li>/var/log/kubelet.log - Kubelet, responsible for running containers on the node</li><li>/var/log/kube-proxy.log - Kube Proxy, responsible for service load balancing</li></ul><h4>A general overview of cluster failure modes</h4><p>This is an incomplete list of things that could go wrong, and how to adjust your cluster setup to mitigate the problems.</p><p>Root causes:</p><ul><li>VM(s) shutdown</li><li>Network partition within cluster, or between cluster and users</li><li>Crashes in Kubernetes software</li><li>Data loss or unavailability of persistent storage (e.g. GCE PD or AWS EBS volume)</li><li>Operator error, e.g. misconfigured Kubernetes software or application software</li></ul><p>Specific scenarios:</p><ul><li>Apiserver VM shutdown or apiserver crashing<ul><li>Results<ul><li>unable to stop, update, or start new pods, services, replication controller</li><li>existing pods and services should continue to work normally, unless they depend on the Kubernetes API</li></ul></li></ul></li><li>Apiserver backing storage lost<ul><li>Results<ul><li>apiserver should fail to come up</li><li>kubelets will not be able to reach it but will continue to run the same pods and provide the same service proxying</li><li>manual recovery or recreation of apiserver state necessary before apiserver is restarted</li></ul></li></ul></li><li>Supporting services (node controller, replication controller manager, scheduler, etc) VM shutdown or crashes<ul><li>currently those are colocated with the apiserver, and their unavailability has similar consequences as apiserver</li><li>in future, these will be replicated as well and may not be co-located</li><li>they do not have their own persistent state</li></ul></li><li>Individual node (VM or physical machine) shuts down<ul><li>Results<ul><li>pods on that Node stop running</li></ul></li></ul></li><li>Network partition<ul><li>Results<ul><li>partition A thinks the nodes in partition B are down; partition B thinks the apiserver is down. (Assuming the master VM ends up in partition A.)</li></ul></li></ul></li><li>Kubelet software fault<ul><li>Results<ul><li>crashing kubelet cannot start new pods on the node</li><li>kubelet might delete the pods or not</li><li>node marked unhealthy</li><li>replication controllers start new pods elsewhere</li></ul></li></ul></li><li>Cluster operator error<ul><li>Results<ul><li>loss of pods, services, etc</li><li>lost of apiserver backing store</li><li>users unable to read API</li><li>etc.</li></ul></li></ul></li></ul><p>Mitigations:</p><ul><li>Action: Use IaaS provider&#x27;s automatic VM restarting feature for IaaS VMs<ul><li>Mitigates: Apiserver VM shutdown or apiserver crashing</li><li>Mitigates: Supporting services VM shutdown or crashes</li></ul></li><li>Action: Use IaaS providers reliable storage (e.g. GCE PD or AWS EBS volume) for VMs with apiserver+etcd<ul><li>Mitigates: Apiserver backing storage lost</li></ul></li><li>Action: Use (experimental) <a href="https://kubernetes.io/docs/admin/high-availability">high-availability</a> configuration<ul><li>Mitigates: Master VM shutdown or master components (scheduler, API server, controller-managing) crashing<ul><li>Will tolerate one or more simultaneous node or component failures</li></ul></li><li>Mitigates: Apiserver backing storage (i.e., etcd&#x27;s data directory) lost<ul><li>Assuming you used clustered etcd.</li></ul></li></ul></li><li>Action: Snapshot apiserver PDs/EBS-volumes periodically<ul><li>Mitigates: Apiserver backing storage lost</li><li>Mitigates: Some cases of operator error</li><li>Mitigates: Some cases of Kubernetes software fault</li></ul></li><li>Action: use replication controller and services in front of pods<ul><li>Mitigates: Node shutdown</li><li>Mitigates: Kubelet software fault</li></ul></li><li>Action: applications (containers) designed to tolerate unexpected restarts<ul><li>Mitigates: Node shutdown</li><li>Mitigates: Kubelet software fault</li></ul></li><li>Action: <a href="https://kubernetes.io/docs/concepts/cluster-administration/federation/">Multiple independent clusters</a> (and avoid making risky changes to all clusters at once)<ul><li>Mitigates: Everything listed above.</li></ul></li></ul><h3>Troubleshoot Applications</h3><p>This guide is to help users debug applications that are deployed into Kubernetes and not behaving correctly. This is not a guide for people who want to debug their cluster. For that you should check out <a href="https://kubernetes.io/docs/admin/cluster-troubleshooting">this guide</a>.</p><ul><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/#diagnosing-the-problem"><strong>Diagnosing the problem</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/#debugging-pods"><strong>Debugging Pods</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/#my-pod-stays-pending"><strong>My pod stays pending</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/#my-pod-stays-waiting"><strong>My pod stays waiting</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/#my-pod-is-crashing-or-otherwise-unhealthy"><strong>My pod is crashing or otherwise unhealthy</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/#my-pod-is-running-but-not-doing-what-i-told-it-to-do"><strong>My pod is running but not doing what I told it to do</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/#debugging-replication-controllers"><strong>Debugging Replication Controllers</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/#debugging-services"><strong>Debugging Services</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/#my-service-is-missing-endpoints"><strong>My service is missing endpoints</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/#network-traffic-is-not-forwarded"><strong>Network traffic is not forwarded</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/#more-information"><strong>More information</strong></a></li></ul></li></ul></li></ul><h4>Diagnosing the problem</h4><p>The first step in troubleshooting is triage. What is the problem? Is it your Pods, your Replication Controller or your Service?</p><ul><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/#debugging-pods">Debugging Pods</a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/#debugging-replication-controllers">Debugging Replication Controllers</a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/#debugging-services">Debugging Services</a></li></ul><h5><strong>Debugging Pods</strong></h5><p>The first step in debugging a Pod is taking a look at it. Check the current state of the Pod and recent events with the following command:</p><p><strong>$ kubectl describe pods ${POD_NAME}</strong></p><p>Look at the state of the containers in the pod. Are they all <strong>Running</strong>? Have there been recent restarts?</p><p>Continue debugging depending on the state of the pods.</p><h6><strong>My pod stays pending</strong></h6><p>If a Pod is stuck in <strong>Pending</strong> it means that it can not be scheduled onto a node. Generally this is because there are insufficient resources of one type or another that prevent scheduling. Look at the output of the <strong>kubectl describe <!-- -->.<!-- -->..</strong> command above. There should be messages from the scheduler about why it can not schedule your pod. Reasons include:</p><ul><li><strong>You don&#x27;t have enough resources</strong>: You may have exhausted the supply of CPU or Memory in your cluster, in this case you need to delete Pods, adjust resource requests, or add new nodes to your cluster. See <a href="https://kubernetes.io/docs/user-guide/compute-resources/#my-pods-are-pending-with-event-message-failedscheduling">Compute Resources document</a> for more information.</li><li><strong>You are using hostPort</strong>: When you bind a Pod to a <strong>hostPort</strong> there are a limited number of places that pod can be scheduled. In most cases, <strong>hostPort</strong> is unnecessary, try using a Service object to expose your Pod. If you do require <strong>hostPort</strong> then you can only schedule as many Pods as there are nodes in your Kubernetes cluster.</li></ul><h6><strong>My pod stays waiting</strong></h6><p>If a Pod is stuck in the <strong>Waiting</strong> state, then it has been scheduled to a worker node, but it can&#x27;t run on that machine. Again, the information from <strong>kubectl describe <!-- -->.<!-- -->..</strong> should be informative. The most common cause of <strong>Waiting</strong> pods is a failure to pull the image. There are three things to check:</p><ul><li>Make sure that you have the name of the image correct.</li><li>Have you pushed the image to the repository?</li><li>Run a manual <strong>docker pull <code>&lt;image&gt;</code></strong> on your machine to see if the image can be pulled.</li></ul><h6><strong>My pod is crashing or otherwise unhealthy</strong></h6><p>First, take a look at the logs of the current container:</p><p><strong>$ kubectl logs ${POD_NAME} ${CONTAINER_NAME}</strong></p><p>If your container has previously crashed, you can access the previous container&#x27;s crash log with:</p><p><strong>$ kubectl logs --previous ${POD_NAME} ${CONTAINER_NAME}</strong></p><p>Alternately, you can run commands inside that container with <strong>exec</strong>:</p><p><strong>$ kubectl exec ${POD_NAME} -c ${CONTAINER_NAME} -- ${CMD} ${ARG1} ${ARG2} <!-- -->.<!-- -->.. ${ARGN}</strong></p><p>Note that <strong>-c ${CONTAINER_NAME}</strong> is optional and can be omitted for Pods that only contain a single container.</p><p>As an example, to look at the logs from a running Cassandra pod, you might run</p><p><strong>$ kubectl exec cassandra -- cat /var/log/cassandra/system.log</strong></p><p>If none of these approaches work, you can find the host machine that the pod is running on and SSH into that host, but this should generally not be necessary given tools in the Kubernetes API. Therefore, if you find yourself needing to ssh into a machine, please file a feature request on GitHub describing your use case and why these tools are insufficient.</p><h6><strong>My pod is running but not doing what I told it to do</strong></h6><p>If your pod is not behaving as you expected, it may be that there was an error in your pod description (e.g. <strong>mypod.yaml</strong> file on your local machine), and that the error was silently ignored when you created the pod. Often a section of the pod description is nested incorrectly, or a key name is typed incorrectly, and so the key is ignored. For example, if you misspelled <strong>command</strong> as <strong>commnd</strong> then the pod will be created but will not use the command line you intended it to use.</p><p>The first thing to do is to delete your pod and try creating it again with the <strong>--validate</strong> option. For example, run <strong>kubectl create --validate -f mypod.yaml</strong>. If you misspelled <strong>command</strong> as <strong>commnd</strong> then will give an error like this:</p><p><strong>I0805 10:43:25.129850 46757 schema.go:126] unknown field: commnd</strong></p><p><strong>I0805 10:43:25.129973 46757 schema.go:129] this may be a false alarm, see <a href="https://github.com/kubernetes/kubernetes/issues/6842">https://github.com/kubernetes/kubernetes/issues/6842</a></strong></p><p><strong>pods/mypod</strong></p><p>The next thing to check is whether the pod on the apiserver matches the pod you meant to create (e.g. in a yaml file on your local machine). For example, run <strong>kubectl get pods/mypod -o yaml &gt; mypod-on-apiserver.yaml</strong> and then manually compare the original pod description, <strong>mypod.yaml</strong> with the one you got back from apiserver, <strong>mypod-on-apiserver.yaml</strong>. There will typically be some lines on the &quot;apiserver&quot; version that are not on the original version. This is expected. However, if there are lines on the original that are not on the apiserver version, then this may indicate a problem with your pod spec.</p><h5><strong>Debugging Replication Controllers</strong></h5><p>Replication controllers are fairly straightforward. They can either create Pods or they can&#x27;t. If they can&#x27;t create pods, then please refer to the <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/#debugging-pods">instructions above</a> to debug your pods.</p><p>You can also use <strong>kubectl describe rc ${CONTROLLER_NAME}</strong> to introspect events related to the replication controller.</p><h5><strong>Debugging Services</strong></h5><p>Services provide load balancing across a set of pods. There are several common problems that can make Services not work properly. The following instructions should help debug Service problems.</p><p>First, verify that there are endpoints for the service. For every Service object, the apiserver makes an <strong>endpoints</strong> resource available.</p><p>You can view this resource with:</p><p><strong>$ kubectl get endpoints ${SERVICE_NAME}</strong></p><p>Make sure that the endpoints match up with the number of containers that you expect to be a member of your service. For example, if your Service is for an nginx container with 3 replicas, you would expect to see three different IP addresses in the Service&#x27;s endpoints.</p><h6><strong>My service is missing endpoints</strong></h6><p>If you are missing endpoints, try listing pods using the labels that Service uses. Imagine that you have a Service where the labels are:</p><p><strong>.<!-- -->..</strong></p><p><strong>spec:</strong></p><p><strong>- selector:</strong></p><p><strong>name: nginx</strong></p><p><strong>type: frontend</strong></p><p>You can use:</p><p><strong>$ kubectl get pods --selector=name=nginx,type=frontend</strong></p><p>to list pods that match this selector. Verify that the list matches the Pods that you expect to provide your Service.</p><p>If the list of pods matches expectations, but your endpoints are still empty, it&#x27;s possible that you don&#x27;t have the right ports exposed. If your service has a <strong>containerPort</strong> specified, but the Pods that are selected don&#x27;t have that port listed, then they won&#x27;t be added to the endpoints list.</p><p>Verify that the pod&#x27;s <strong>containerPort</strong> matches up with the Service&#x27;s <strong>containerPort</strong></p><h6><strong>Network traffic is not forwarded</strong></h6><p>If you can connect to the service, but the connection is immediately dropped, and there are endpoints in the endpoints list, it&#x27;s likely that the proxy can&#x27;t contact your pods.</p><p>There are three things to check:</p><ul><li>Are your pods working correctly? Look for restart count, and <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/#debugging-pods">debug pods</a>.</li><li>Can you connect to your pods directly? Get the IP address for the Pod, and try to connect directly to that IP.</li><li>Is your application serving on the port that you configured? Kubernetes doesn&#x27;t do port remapping, so if your application serves on 8080, the <strong>containerPort</strong> field needs to be 8080.</li></ul><h6><strong>More information</strong></h6><p>If none of the above solves your problem, follow the instructions in <a href="https://kubernetes.io/docs/user-guide/debugging-services">Debugging Service document</a> to make sure that your <strong>Service</strong> is running, has <strong>Endpoints</strong>, and your <strong>Pods</strong> are actually serving; you have DNS working, iptables rules installed, and kube-proxy does not seem to be misbehaving.</p><p>You may also visit <a href="https://kubernetes.io/docs/troubleshooting/">troubleshooting document</a> for more information.</p><h3>Debug a StatefulSet</h3><p>This task shows you how to debug a StatefulSet.</p><ul><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-stateful-set/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-stateful-set/#debugging-a-statefulset"><strong>Debugging a StatefulSet</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-stateful-set/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Before you begin</h4><ul><li>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster.</li><li>You should have a StatefulSet running that you want to investigate.</li></ul><h4>Debugging a StatefulSet</h4><p>In order to list all the pods which belong to a StatefulSet, which have a label <strong>app=myapp</strong> set on them, you can use the following:</p><p><strong>kubectl get pods -l app=myapp</strong></p><p>If you find that any Pods listed are in <strong>Unknown</strong> or <strong>Terminating</strong> state for an extended period of time, refer to the <a href="https://kubernetes.io/docs/tasks/manage-stateful-set/delete-pods/">Deleting StatefulSet Pods</a> task for instructions on how to deal with them. You can debug individual Pods in a StatefulSet using the <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-pod-replication-controller/">Debugging Pods</a> guide.</p><h4>What&#x27;s next</h4><p>Learn more about <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-init-containers/">debugging an init-container</a>.</p><h3>Application Introspection and Debugging</h3><p>Once your application is running, you&#x27;ll inevitably need to debug problems with it. Earlier we described how you can use <strong>kubectl get pods</strong> to retrieve simple status information about your pods. But there are a number of ways to get even more information about your application.</p><ul><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application-introspection/#using-kubectl-describe-pod-to-fetch-details-about-pods"><strong>Using kubectl describe pod to fetch details about pods</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application-introspection/#example-debugging-pending-pods"><strong>Example: debugging Pending Pods</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application-introspection/#example-debugging-a-downunreachable-node"><strong>Example: debugging a down/unreachable node</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application-introspection/#whats-next"><strong>What&#x27;s next?</strong></a></li></ul><h4>Using kubectl describe pod to fetch details about pods</h4><p>For this example we&#x27;ll use a Deployment to create two pods, similar to the earlier example.</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<strong>nginx-dep.yaml</strong> ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernete">https://raw.githubusercontent.com/kubernete</a>     |
| s/website/master/docs/tasks/debug-application-cluster/nginx-dep.yaml) |
+=======================================================================+
| <strong>apiVersion: apps/v1</strong>                                               |
|                                                                       |
| <strong>kind: Deployment</strong>                                                  |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>name: nginx-deployment</strong>                                            |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>selector:</strong>                                                         |
|                                                                       |
| <strong>matchLabels:</strong>                                                      |
|                                                                       |
| <strong>app: nginx</strong>                                                        |
|                                                                       |
| <strong>replicas: 2</strong>                                                       |
|                                                                       |
| <strong>template:</strong>                                                         |
|                                                                       |
| <strong>metadata:</strong>                                                         |
|                                                                       |
| <strong>labels:</strong>                                                           |
|                                                                       |
| <strong>app: nginx</strong>                                                        |
|                                                                       |
| <strong>spec:</strong>                                                             |
|                                                                       |
| <strong>containers:</strong>                                                       |
|                                                                       |
| <strong>- name: nginx</strong>                                                     |
|                                                                       |
| <strong>image: nginx</strong>                                                      |
|                                                                       |
| <strong>resources:</strong>                                                        |
|                                                                       |
| <strong>limits:</strong>                                                           |
|                                                                       |
| <strong>memory: &quot;128Mi&quot;</strong>                                                 |
|                                                                       |
| <strong>cpu: &quot;500m&quot;</strong>                                                     |
|                                                                       |
| <strong>ports:</strong>                                                            |
|                                                                       |
| <strong>- containerPort: 80</strong>                                               |
+-----------------------------------------------------------------------+</p><p>Create deployment by running following command:</p><p><strong>$ kubectl create -f <a href="https://k8s.io/docs/tasks/debug-application-cluster/nginx-dep.yaml">https://k8s.io/docs/tasks/debug-application-cluster/nginx-dep.yaml</a></strong></p><p><strong>deployment &quot;nginx-deployment&quot; created</strong></p><p><strong>$ kubectl get pods</strong></p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>nginx-deployment-1006230814-6winp 1/1 Running 0 11s</strong></p><p><strong>nginx-deployment-1006230814-fmgu3 1/1 Running 0 11s</strong></p><p>We can retrieve a lot more information about each of these pods using <strong>kubectl describe pod</strong>. For example:</p><p><strong>$ kubectl describe pod nginx-deployment-1006230814-6winp</strong></p><p><strong>Name: nginx-deployment-1006230814-6winp</strong></p><p><strong>Namespace: default</strong></p><p><strong>Node: kubernetes-node-wul5/10.240.0.9</strong></p><p><strong>Start Time: Thu, 24 Mar 2016 01:39:49 +0000</strong></p><p><strong>Labels: app=nginx,pod-template-hash=1006230814</strong></p><p><strong>Annotations: kubernetes.io/created-by={&quot;kind&quot;:&quot;SerializedReference&quot;,&quot;apiVersion&quot;:&quot;v1&quot;,&quot;reference&quot;:{&quot;kind&quot; :&quot;ReplicaSet&quot;,&quot;namespace&quot;:&quot;default&quot;,&quot;name&quot;:&quot;nginx-deployment-1956810328&quot;,&quot;uid&quot;:&quot;14e607e7-8ba1-11e7-b5cb-fa16&quot; <!-- -->.<!-- -->..</strong></p><p><strong>Status: Running</strong></p><p><strong>IP: 10.244.0.6</strong></p><p><strong>Controllers: ReplicaSet/nginx-deployment-1006230814</strong></p><p><strong>Containers:</strong></p><p><strong>nginx:</strong></p><p><strong>Container ID: docker://90315cc9f513c724e9957a4788d3e625a078de84750f244a40f97ae355eb1149</strong></p><p><strong>Image: nginx</strong></p><p><strong>Image ID: docker://6f62f48c4e55d700cf3eb1b5e33fa051802986b77b874cc351cce539e5163707</strong></p><p><strong>Port: 80/TCP</strong></p><p><strong>QoS Tier:</strong></p><p><strong>cpu: Guaranteed</strong></p><p><strong>memory: Guaranteed</strong></p><p><strong>Limits:</strong></p><p><strong>cpu: 500m</strong></p><p><strong>memory: 128Mi</strong></p><p><strong>Requests:</strong></p><p><strong>memory: 128Mi</strong></p><p><strong>cpu: 500m</strong></p><p><strong>State: Running</strong></p><p><strong>Started: Thu, 24 Mar 2016 01:39:51 +0000</strong></p><p><strong>Ready: True</strong></p><p><strong>Restart Count: 0</strong></p><p><strong>Environment: <code>&lt;none&gt;</code></strong></p><p><strong>Mounts:</strong></p><p><strong>/var/run/secrets/kubernetes.io/serviceaccount from default-token-5kdvl (ro)</strong></p><p><strong>Conditions:</strong></p><p><strong>Type Status</strong></p><p><strong>Initialized True</strong></p><p><strong>Ready True</strong></p><p><strong>PodScheduled True</strong></p><p><strong>Volumes:</strong></p><p><strong>default-token-4bcbi:</strong></p><p><strong>Type: Secret (a volume populated by a Secret)</strong></p><p><strong>SecretName: default-token-4bcbi</strong></p><p><strong>Optional: false</strong></p><p><strong>QoS Class: Guaranteed</strong></p><p><strong>Node-Selectors: <code>&lt;none&gt;</code></strong></p><p><strong>Tolerations: <code>&lt;none&gt;</code></strong></p><p><strong>Events:</strong></p><p><strong>FirstSeen LastSeen Count From SubobjectPath Type Reason Message</strong></p><p><strong>--------- -------- ----- ---- ------------- -------- ------ -------</strong></p><p><strong>54s 54s 1 {default-scheduler } Normal Scheduled Successfully assigned nginx-deployment-1006230814-6winp to kubernetes-node-wul5</strong></p><p><strong>54s 54s 1 {kubelet kubernetes-node-wul5} spec.containers{nginx} Normal Pulling pulling image &quot;nginx&quot;</strong></p><p><strong>53s 53s 1 {kubelet kubernetes-node-wul5} spec.containers{nginx} Normal Pulled Successfully pulled image &quot;nginx&quot;</strong></p><p><strong>53s 53s 1 {kubelet kubernetes-node-wul5} spec.containers{nginx} Normal Created Created container with docker id 90315cc9f513</strong></p><p><strong>53s 53s 1 {kubelet kubernetes-node-wul5} spec.containers{nginx} Normal Started Started container with docker id 90315cc9f513</strong></p><p>Here you can see configuration information about the container(s) and Pod (labels, resource requirements, etc.), as well as status information about the container(s) and Pod (state, readiness, restart count, events, etc.).</p><p>The container state is one of Waiting, Running, or Terminated. Depending on the state, additional information will be provided -- here you can see that for a container in Running state, the system tells you when the container started.</p><p>Ready tells you whether the container passed its last readiness probe. (In this case, the container does not have a readiness probe configured; the container is assumed to be ready if no readiness probe is configured.)</p><p>Restart Count tells you how many times the container has been restarted; this information can be useful for detecting crash loops in containers that are configured with a restart policy of &#x27;always.&#x27;</p><p>Currently the only Condition associated with a Pod is the binary Ready condition, which indicates that the pod is able to service requests and should be added to the load balancing pools of all matching services.</p><p>Lastly, you see a log of recent events related to your Pod. The system compresses multiple identical events by indicating the first and last time it was seen and the number of times it was seen. &quot;From&quot; indicates the component that is logging the event, &quot;SubobjectPath&quot; tells you which object (e.g. container within the pod) is being referred to, and &quot;Reason&quot; and &quot;Message&quot; tell you what happened.</p><h4>Example: debugging Pending Pods</h4><p>A common scenario that you can detect using events is when you&#x27;ve created a Pod that won&#x27;t fit on any node. For example, the Pod might request more resources than are free on any node, or it might specify a label selector that doesn&#x27;t match any nodes. Let&#x27;s say we created the previous Deployment with 5 replicas (instead of 2) and requesting 600 millicores instead of 500, on a four-node cluster where each (virtual) machine has 1 CPU. In that case one of the Pods will not be able to schedule. (Note that because of the cluster addon pods such as fluentd, skydns, etc., that run on each node, if we requested 1000 millicores then none of the Pods would be able to schedule.)</p><p><strong>$ kubectl get pods</strong></p><p><strong>NAME READY STATUS RESTARTS AGE</strong></p><p><strong>nginx-deployment-1006230814-6winp 1/1 Running 0 7m</strong></p><p><strong>nginx-deployment-1006230814-fmgu3 1/1 Running 0 7m</strong></p><p><strong>nginx-deployment-1370807587-6ekbw 1/1 Running 0 1m</strong></p><p><strong>nginx-deployment-1370807587-fg172 0/1 Pending 0 1m</strong></p><p><strong>nginx-deployment-1370807587-fz9sd 0/1 Pending 0 1m</strong></p><p>To find out why the nginx-deployment-1370807587-fz9sd pod is not running, we can use <strong>kubectl describe pod</strong> on the pending Pod and look at its events:</p><p><strong>$ kubectl describe pod nginx-deployment-1370807587-fz9sd</strong></p><p><strong>Name: nginx-deployment-1370807587-fz9sd</strong></p><p><strong>Namespace: default</strong></p><p><strong>Node: /</strong></p><p><strong>Labels: app=nginx,pod-template-hash=1370807587</strong></p><p><strong>Status: Pending</strong></p><p><strong>IP:</strong></p><p><strong>Controllers: ReplicaSet/nginx-deployment-1370807587</strong></p><p><strong>Containers:</strong></p><p><strong>nginx:</strong></p><p><strong>Image: nginx</strong></p><p><strong>Port: 80/TCP</strong></p><p><strong>QoS Tier:</strong></p><p><strong>memory: Guaranteed</strong></p><p><strong>cpu: Guaranteed</strong></p><p><strong>Limits:</strong></p><p><strong>cpu: 1</strong></p><p><strong>memory: 128Mi</strong></p><p><strong>Requests:</strong></p><p><strong>cpu: 1</strong></p><p><strong>memory: 128Mi</strong></p><p><strong>Environment Variables:</strong></p><p><strong>Volumes:</strong></p><p><strong>default-token-4bcbi:</strong></p><p><strong>Type: Secret (a volume populated by a Secret)</strong></p><p><strong>SecretName: default-token-4bcbi</strong></p><p><strong>Events:</strong></p><p><strong>FirstSeen LastSeen Count From SubobjectPath Type Reason Message</strong></p><p><strong>--------- -------- ----- ---- ------------- -------- ------ -------</strong></p><p><strong>1m 48s 7 {default-scheduler } Warning FailedScheduling pod (nginx-deployment-1370807587-fz9sd) failed to fit in any node</strong></p><p><strong>fit failure on node (kubernetes-node-6ta5): Node didn\&#x27;t have enough resource: CPU, requested: 1000, used: 1420, capacity: 2000</strong></p><p><strong>fit failure on node (kubernetes-node-wul5): Node didn\&#x27;t have enough resource: CPU, requested: 1000, used: 1100, capacity: 2000</strong></p><p>Here you can see the event generated by the scheduler saying that the Pod failed to schedule for reason <strong>FailedScheduling</strong> (and possibly others). The message tells us that there were not enough resources for the Pod on any of the nodes.</p><p>To correct this situation, you can use <strong>kubectl scale</strong> to update your Deployment to specify four or fewer replicas. (Or you could just leave the one Pod pending, which is harmless.)</p><p>Events such as the ones you saw at the end of <strong>kubectl describe pod</strong> are persisted in etcd and provide high-level information on what is happening in the cluster. To list all events you can use</p><p><strong>kubectl get events</strong></p><p>but you have to remember that events are namespaced. This means that if you&#x27;re interested in events for some namespaced object (e.g. what happened with Pods in namespace <strong>my-namespace</strong>) you need to explicitly provide a namespace to the command:</p><p><strong>kubectl get events --namespace=my-namespace</strong></p><p>To see events from all namespaces, you can use the <strong>--all-namespaces</strong> argument.</p><p>In addition to <strong>kubectl describe pod</strong>, another way to get extra information about a pod (beyond what is provided by <strong>kubectl get pod</strong>) is to pass the <strong>-o yaml</strong> output format flag to <strong>kubectl get pod</strong>. This will give you, in YAML format, even more information than <strong>kubectl describe pod</strong>--essentially all of the information the system has about the Pod. Here you will see things like annotations (which are key-value metadata without the label restrictions, that is used internally by Kubernetes system components), restart policy, ports, and volumes.</p><p><strong>$ kubectl get pod nginx-deployment-1006230814-6winp -o yaml</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>annotations:</strong></p><p><strong>kubernetes.io/created-by: |</strong></p><p><strong>{&quot;kind&quot;:&quot;SerializedReference&quot;,&quot;apiVersion&quot;:&quot;v1&quot;,&quot;reference&quot;:{&quot;kind&quot;:&quot;ReplicaSet&quot;,&quot;namespace&quot;:&quot;default&quot;,&quot;name&quot;:&quot;nginx-deployment-1006230814&quot;,&quot;uid&quot;:&quot;4c84c175-f161-11e5-9a78-42010af00005&quot;,&quot;apiVersion&quot;:&quot;extensions&quot;,&quot;resourceVersion&quot;:&quot;133434&quot;}}</strong></p><p><strong>creationTimestamp: 2016-03-24T01:39:50Z</strong></p><p><strong>generateName: nginx-deployment-1006230814-</strong></p><p><strong>labels:</strong></p><p><strong>app: nginx</strong></p><p><strong>pod-template-hash: &quot;1006230814&quot;</strong></p><p><strong>name: nginx-deployment-1006230814-6winp</strong></p><p><strong>namespace: default</strong></p><p><strong>resourceVersion: &quot;133447&quot;</strong></p><p><strong>selfLink: /api/v1/namespaces/default/pods/nginx-deployment-1006230814-6winp</strong></p><p><strong>uid: 4c879808-f161-11e5-9a78-42010af00005</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- image: nginx</strong></p><p><strong>imagePullPolicy: Always</strong></p><p><strong>name: nginx</strong></p><p><strong>ports:</strong></p><p><strong>- containerPort: 80</strong></p><p><strong>protocol: TCP</strong></p><p><strong>resources:</strong></p><p><strong>limits:</strong></p><p><strong>cpu: 500m</strong></p><p><strong>memory: 128Mi</strong></p><p><strong>requests:</strong></p><p><strong>cpu: 500m</strong></p><p><strong>memory: 128Mi</strong></p><p><strong>terminationMessagePath: /dev/termination-log</strong></p><p><strong>volumeMounts:</strong></p><p><strong>- mountPath: /var/run/secrets/kubernetes.io/serviceaccount</strong></p><p><strong>name: default-token-4bcbi</strong></p><p><strong>readOnly: true</strong></p><p><strong>dnsPolicy: ClusterFirst</strong></p><p><strong>nodeName: kubernetes-node-wul5</strong></p><p><strong>restartPolicy: Always</strong></p><p><strong>securityContext: {}</strong></p><p><strong>serviceAccount: default</strong></p><p><strong>serviceAccountName: default</strong></p><p><strong>terminationGracePeriodSeconds: 30</strong></p><p><strong>volumes:</strong></p><p><strong>- name: default-token-4bcbi</strong></p><p><strong>secret:</strong></p><p><strong>secretName: default-token-4bcbi</strong></p><p><strong>status:</strong></p><p><strong>conditions:</strong></p><p><strong>- lastProbeTime: null</strong></p><p><strong>lastTransitionTime: 2016-03-24T01:39:51Z</strong></p><p><strong>status: &quot;True&quot;</strong></p><p><strong>type: Ready</strong></p><p><strong>containerStatuses:</strong></p><p><strong>- containerID: docker://90315cc9f513c724e9957a4788d3e625a078de84750f244a40f97ae355eb1149</strong></p><p><strong>image: nginx</strong></p><p><strong>imageID: docker://6f62f48c4e55d700cf3eb1b5e33fa051802986b77b874cc351cce539e5163707</strong></p><p><strong>lastState: {}</strong></p><p><strong>name: nginx</strong></p><p><strong>ready: true</strong></p><p><strong>restartCount: 0</strong></p><p><strong>state:</strong></p><p><strong>running:</strong></p><p><strong>startedAt: 2016-03-24T01:39:51Z</strong></p><p><strong>hostIP: 10.240.0.9</strong></p><p><strong>phase: Running</strong></p><p><strong>podIP: 10.244.0.6</strong></p><p><strong>startTime: 2016-03-24T01:39:49Z</strong></p><h4>Example: debugging a down/unreachable node</h4><p>Sometimes when debugging it can be useful to look at the status of a node -- for example, because you&#x27;ve noticed strange behavior of a Pod that&#x27;s running on the node, or to find out why a Pod won&#x27;t schedule onto the node. As with Pods, you can use <strong>kubectl describe node</strong> and <strong>kubectl get node -o yaml</strong> to retrieve detailed information about nodes. For example, here&#x27;s what you&#x27;ll see if a node is down (disconnected from the network, or kubelet dies and won&#x27;t restart, etc.). Notice the events that show the node is NotReady, and also notice that the pods are no longer running (they are evicted after five minutes of NotReady status).</p><p><strong>$ kubectl get nodes</strong></p><p><strong>NAME STATUS AGE VERSION</strong></p><p><strong>kubernetes-node-861h NotReady 1h v1.6.0+fff5156</strong></p><p><strong>kubernetes-node-bols Ready 1h v1.6.0+fff5156</strong></p><p><strong>kubernetes-node-st6x Ready 1h v1.6.0+fff5156</strong></p><p><strong>kubernetes-node-unaj Ready 1h v1.6.0+fff5156</strong></p><p><strong>$ kubectl describe node kubernetes-node-861h</strong></p><p><strong>Name: kubernetes-node-861h</strong></p><p><strong>Role</strong></p><p><strong>Labels: beta.kubernetes.io/arch=amd64</strong></p><p><strong>beta.kubernetes.io/os=linux</strong></p><p><strong>kubernetes.io/hostname=kubernetes-node-861h</strong></p><p><strong>Annotations: node.alpha.kubernetes.io/ttl=0</strong></p><p><strong>volumes.kubernetes.io/controller-managed-attach-detach=true</strong></p><p><strong>Taints: <code>&lt;none&gt;</code></strong></p><p><strong>CreationTimestamp: Mon, 04 Sep 2017 17:13:23 +0800</strong></p><p><strong>Phase:</strong></p><p><strong>Conditions:</strong></p><p><strong>Type Status LastHeartbeatTime LastTransitionTime Reason Message</strong></p><p><strong>---- ------ ----------------- ------------------ ------ -------</strong></p><p><strong>OutOfDisk Unknown Fri, 08 Sep 2017 16:04:28 +0800 Fri, 08 Sep 2017 16:20:58 +0800 NodeStatusUnknown Kubelet stopped posting node status.</strong></p><p><strong>MemoryPressure Unknown Fri, 08 Sep 2017 16:04:28 +0800 Fri, 08 Sep 2017 16:20:58 +0800 NodeStatusUnknown Kubelet stopped posting node status.</strong></p><p><strong>DiskPressure Unknown Fri, 08 Sep 2017 16:04:28 +0800 Fri, 08 Sep 2017 16:20:58 +0800 NodeStatusUnknown Kubelet stopped posting node status.</strong></p><p><strong>Ready Unknown Fri, 08 Sep 2017 16:04:28 +0800 Fri, 08 Sep 2017 16:20:58 +0800 NodeStatusUnknown Kubelet stopped posting node status.</strong></p><p><strong>Addresses: 10.240.115.55,104.197.0.26</strong></p><p><strong>Capacity:</strong></p><p><strong>cpu: 2</strong></p><p><strong>hugePages: 0</strong></p><p><strong>memory: 4046788Ki</strong></p><p><strong>pods: 110</strong></p><p><strong>Allocatable:</strong></p><p><strong>cpu: 1500m</strong></p><p><strong>hugePages: 0</strong></p><p><strong>memory: 1479263Ki</strong></p><p><strong>pods: 110</strong></p><p><strong>System Info:</strong></p><p><strong>Machine ID: 8e025a21a4254e11b028584d9d8b12c4</strong></p><p><strong>System UUID: 349075D1-D169-4F25-9F2A-E886850C47E3</strong></p><p><strong>Boot ID: 5cd18b37-c5bd-4658-94e0-e436d3f110e0</strong></p><p><strong>Kernel Version: 4.4.0-31-generic</strong></p><p><strong>OS Image: Debian GNU/Linux 8 (jessie)</strong></p><p><strong>Operating System: linux</strong></p><p><strong>Architecture: amd64</strong></p><p><strong>Container Runtime Version: docker://1.12.5</strong></p><p><strong>Kubelet Version: v1.6.9+a3d1dfa6f4335</strong></p><p><strong>Kube-Proxy Version: v1.6.9+a3d1dfa6f4335</strong></p><p><strong>ExternalID: 15233045891481496305</strong></p><p><strong>Non-terminated Pods: (9 in total)</strong></p><p><strong>Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits</strong></p><p><strong>--------- ---- ------------ ---------- --------------- -------------</strong></p><p><strong>.<!-- -->..<!-- -->.<!-- -->..</strong></p><p><strong>Allocated resources:</strong></p><p><strong>(Total limits may be over 100 percent, i.e., overcommitted.)</strong></p><p><strong>CPU Requests CPU Limits Memory Requests Memory Limits</strong></p><p><strong>------------ ---------- --------------- -------------</strong></p><p><strong>900m (60%) 2200m (146%) 1009286400 (66%) 5681286400 (375%)</strong></p><p><strong>Events: <code>&lt;none&gt;</code></strong></p><p><strong>$ kubectl get node kubernetes-node-861h -o yaml</strong></p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Node</strong></p><p><strong>metadata:</strong></p><p><strong>creationTimestamp: 2015-07-10T21:32:29Z</strong></p><p><strong>labels:</strong></p><p><strong>kubernetes.io/hostname: kubernetes-node-861h</strong></p><p><strong>name: kubernetes-node-861h</strong></p><p><strong>resourceVersion: &quot;757&quot;</strong></p><p><strong>selfLink: /api/v1/nodes/kubernetes-node-861h</strong></p><p><strong>uid: 2a69374e-274b-11e5-a234-42010af0d969</strong></p><p><strong>spec:</strong></p><p><strong>externalID: &quot;15233045891481496305&quot;</strong></p><p><strong>podCIDR: 10.244.0.0/24</strong></p><p><strong>providerID: gce://striped-torus-760/us-central1-b/kubernetes-node-861h</strong></p><p><strong>status:</strong></p><p><strong>addresses:</strong></p><p><strong>- address: 10.240.115.55</strong></p><p><strong>type: InternalIP</strong></p><p><strong>- address: 104.197.0.26</strong></p><p><strong>type: ExternalIP</strong></p><p><strong>capacity:</strong></p><p><strong>cpu: &quot;1&quot;</strong></p><p><strong>memory: 3800808Ki</strong></p><p><strong>pods: &quot;100&quot;</strong></p><p><strong>conditions:</strong></p><p><strong>- lastHeartbeatTime: 2015-07-10T21:34:32Z</strong></p><p><strong>lastTransitionTime: 2015-07-10T21:35:15Z</strong></p><p><strong>reason: Kubelet stopped posting node status.</strong></p><p><strong>status: Unknown</strong></p><p><strong>type: Ready</strong></p><p><strong>nodeInfo:</strong></p><p><strong>bootID: 4e316776-b40d-4f78-a4ea-ab0d73390897</strong></p><p><strong>containerRuntimeVersion: docker://Unknown</strong></p><p><strong>kernelVersion: 3.16.0-0.bpo.4-amd64</strong></p><p><strong>kubeProxyVersion: v0.21.1-185-gffc5a86098dc01</strong></p><p><strong>kubeletVersion: v0.21.1-185-gffc5a86098dc01</strong></p><p><strong>machineID: &quot;&quot;</strong></p><p><strong>osImage: Debian GNU/Linux 7 (wheezy)</strong></p><p><strong>systemUUID: ABE5F6B4-D44B-108B-C46A-24CCE16C8B6E</strong></p><h4>What&#x27;s next?</h4><p>Learn about additional debugging tools, including:</p><ul><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/logging/">Logging</a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/resource-usage-monitoring/">Monitoring</a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/get-shell-running-container/">Getting into containers via <strong>exec</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/http-proxy-access-api/">Connecting to containers via proxies</a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/">Connecting to containers via port forwarding</a></li></ul><h3>Auditing</h3><ul><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#audit-policy"><strong>Audit Policy</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#audit-backends"><strong>Audit backends</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#log-backend"><strong>Log backend</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#webhook-backend"><strong>Webhook backend</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#batching"><strong>Batching</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#parameter-tuning"><strong>Parameter tuning</strong></a></li></ul></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#multi-cluster-setup"><strong>Multi-cluster setup</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#log-collector-examples"><strong>Log Collector Examples</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#use-fluentd-to-collect-and-distribute-audit-events-from-log-file"><strong>Use fluentd to collect and distribute audit events from log file</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#use-logstash-to-collect-and-distribute-audit-events-from-webhook-backend"><strong>Use logstash to collect and distribute audit events from webhook backend</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#legacy-audit"><strong>Legacy Audit</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#configuration"><strong>Configuration</strong></a></li></ul></li></ul><p><strong>FEATURE STATE:</strong> <strong>Kubernetes v1.10</strong> <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/">beta</a></p><p>Kubernetes auditing provides a security-relevant chronological set of records documenting the sequence of activities that have affected system by individual users, administrators or other components of the system. It allows cluster administrator to answer the following questions:</p><ul><li>what happened?</li><li>when did it happen?</li><li>who initiated it?</li><li>on what did it happen?</li><li>where was it observed?</li><li>from where was it initiated?</li><li>to where was it going?</li></ul><p><a href="https://kubernetes.io/docs/admin/kube-apiserver">Kube-apiserver</a> performs auditing. Each request on each stage of its execution generates an event, which is then pre-processed according to a certain policy and written to a backend. You can find more details about the pipeline in the <a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/auditing.md">design proposal</a>.</p><p><strong>Note,</strong> that audit logging feature increases apiserver memory consumption, since some context required for auditing is stored for each request. Additionally, memory consumption depends on the audit logging configuration.</p><h4>Audit Policy</h4><p>Audit policy defines rules about what events should be recorded and what data they should include. When an event is processed, it&#x27;s compared against the list of rules in order. The first matching rule sets the <a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/auditing.md#levels">audit level</a> of the event. The audit policy object structure is defined in the <a href="https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apiserver/pkg/apis/audit/v1beta1/types.go"><strong>audit.k8s.io</strong>API group</a>.</p><p>You can pass a file with the policy to <a href="https://kubernetes.io/docs/admin/kube-apiserver">kube-apiserver</a> using the <strong>--audit-policy-file</strong> flag. If the flag is omitted, no events are logged. <strong>Note:</strong> <strong>kind</strong> and <strong>apiVersion</strong> fields along with <strong>rules</strong> <strong>must</strong>be provided in the audit policy file. A policy with no (0) rules, or a policy that doesn&#x27;t provide valid <strong>apiVersion</strong> and <strong>kind</strong> values is treated as illegal.</p><p>Some example audit policy files:</p><p>+-----------------------------------------------------------------------+
| <!-- -->[<em>                                                                    |
| </em>audit-policy.yaml** ]<!-- -->(<a href="https://raw.githubusercontent.com/kubernetes/w">https://raw.githubusercontent.com/kubernetes/w</a> |
| ebsite/master/docs/tasks/debug-application-cluster/audit-policy.yaml) |
+=======================================================================+
| <strong>apiVersion: audit.k8s.io/v1beta1 <em># This is required.</em></strong>           |
|                                                                       |
| <strong>kind: Policy</strong>                                                      |
|                                                                       |
| <strong><em># Don\&#x27;t generate audit events for all requests in                |
| RequestReceived stage.</em></strong>                                             |
|                                                                       |
| <strong>omitStages:</strong>                                                       |
|                                                                       |
| <strong>- &quot;RequestReceived&quot;</strong>                                             |
|                                                                       |
| <strong>rules:</strong>                                                            |
|                                                                       |
| <strong><em># Log pod changes at RequestResponse level</em></strong>                     |
|                                                                       |
| <strong>- level: RequestResponse</strong>                                          |
|                                                                       |
| <strong>resources:</strong>                                                        |
|                                                                       |
| <strong>- group: &quot;&quot;</strong>                                                     |
|                                                                       |
| <strong><em># Resource &quot;pods&quot; doesn\&#x27;t match requests to any subresource of |
| pods,</em></strong>                                                              |
|                                                                       |
| <strong><em># which is consistent with the RBAC policy.</em></strong>                    |
|                                                                       |
| <strong>resources: <!-- -->[&quot;pods&quot;]</strong>                                           |
|                                                                       |
| <strong><em># Log &quot;pods/log&quot;, &quot;pods/status&quot; at Metadata level</em></strong>          |
|                                                                       |
| <strong>- level: Metadata</strong>                                                 |
|                                                                       |
| <strong>resources:</strong>                                                        |
|                                                                       |
| <strong>- group: &quot;&quot;</strong>                                                     |
|                                                                       |
| <strong>resources: <!-- -->[&quot;pods/log&quot;, &quot;pods/status&quot;]</strong>                      |
|                                                                       |
| <strong><em># Don\&#x27;t log requests to a configmap called                       |
| &quot;controller-leader&quot;</em></strong>                                              |
|                                                                       |
| <strong>- level: None</strong>                                                     |
|                                                                       |
| <strong>resources:</strong>                                                        |
|                                                                       |
| <strong>- group: &quot;&quot;</strong>                                                     |
|                                                                       |
| <strong>resources: <!-- -->[&quot;configmaps&quot;]</strong>                                     |
|                                                                       |
| <strong>resourceNames: <!-- -->[&quot;controller-leader&quot;]</strong>                          |
|                                                                       |
| <strong><em># Don\&#x27;t log watch requests by the &quot;system:kube-proxy&quot; on       |
| endpoints or services</em></strong>                                              |
|                                                                       |
| <strong>- level: None</strong>                                                     |
|                                                                       |
| <strong>users: <!-- -->[&quot;system:kube-proxy&quot;]</strong>                                  |
|                                                                       |
| <strong>verbs: <!-- -->[&quot;watch&quot;]</strong>                                              |
|                                                                       |
| <strong>resources:</strong>                                                        |
|                                                                       |
| <strong>- group: &quot;&quot; <em># core API group</em></strong>                                 |
|                                                                       |
| <strong>resources: <!-- -->[&quot;endpoints&quot;, &quot;services&quot;]</strong>                        |
|                                                                       |
| <strong><em># Don\&#x27;t log authenticated requests to certain non-resource URL   |
| paths.</em></strong>                                                             |
|                                                                       |
| <strong>- level: None</strong>                                                     |
|                                                                       |
| <strong>userGroups: <!-- -->[&quot;system:authenticated&quot;]</strong>                          |
|                                                                       |
| <strong>nonResourceURLs:</strong>                                                  |
|                                                                       |
| <strong>- &quot;/api<!-- -->*<!-- -->&quot; <em># Wildcard matching.</em></strong>                              |
|                                                                       |
| <strong>- &quot;/version&quot;</strong>                                                    |
|                                                                       |
| <strong><em># Log the request body of configmap changes in kube-system.</em></strong>    |
|                                                                       |
| <strong>- level: Request</strong>                                                  |
|                                                                       |
| <strong>resources:</strong>                                                        |
|                                                                       |
| <strong>- group: &quot;&quot; <em># core API group</em></strong>                                 |
|                                                                       |
| <strong>resources: <!-- -->[&quot;configmaps&quot;]</strong>                                     |
|                                                                       |
| <strong><em># This rule only applies to resources in the &quot;kube-system&quot;      |
| namespace.</em></strong>                                                         |
|                                                                       |
| <strong><em># The empty string &quot;&quot; can be used to select non-namespaced      |
| resources.</em></strong>                                                         |
|                                                                       |
| <strong>namespaces: <!-- -->[&quot;kube-system&quot;]</strong>                                   |
|                                                                       |
| <strong><em># Log configmap and secret changes in all other namespaces at the |
| Metadata level.</em></strong>                                                    |
|                                                                       |
| <strong>- level: Metadata</strong>                                                 |
|                                                                       |
| <strong>resources:</strong>                                                        |
|                                                                       |
| <strong>- group: &quot;&quot; <em># core API group</em></strong>                                 |
|                                                                       |
| <strong>resources: <!-- -->[&quot;secrets&quot;, &quot;configmaps&quot;]</strong>                        |
|                                                                       |
| <strong><em># Log all other resources in core and extensions at the Request   |
| level.</em></strong>                                                             |
|                                                                       |
| <strong>- level: Request</strong>                                                  |
|                                                                       |
| <strong>resources:</strong>                                                        |
|                                                                       |
| <strong>- group: &quot;&quot; <em># core API group</em></strong>                                 |
|                                                                       |
| <strong>- group: &quot;extensions&quot; <em># Version of group should NOT be          |
| included.</em></strong>                                                          |
|                                                                       |
| <strong><em># A catch-all rule to log all other requests at the Metadata      |
| level.</em></strong>                                                             |
|                                                                       |
| <strong>- level: Metadata</strong>                                                 |
|                                                                       |
| <strong><em># Long-running requests like watches that fall under this rule    |
| will not</em></strong>                                                           |
|                                                                       |
| <strong><em># generate an audit event in RequestReceived.</em></strong>                  |
|                                                                       |
| <strong>omitStages:</strong>                                                       |
|                                                                       |
| <strong>- &quot;RequestReceived&quot;</strong>                                             |
+-----------------------------------------------------------------------+</p><p>You can use a minimal audit policy file to log all requests at the <strong>Metadata</strong> level:</p><p><strong><em># Log all requests at the Metadata level.</em></strong></p><p><strong>apiVersion: audit.k8s.io/v1beta1</strong></p><p><strong>kind: Policy</strong></p><p><strong>rules:</strong></p><p><strong>- level: Metadata</strong></p><p>The <a href="https://github.com/kubernetes/kubernetes/blob/master/cluster/gce/gci/configure-helper.sh#L735">audit profile used by GCE</a> should be used as reference by admins constructing their own audit profiles.</p><h4>Audit backends</h4><p>Audit backends implement exporting audit events to an external storage. <a href="https://kubernetes.io/docs/admin/kube-apiserver">Kube-apiserver</a> out of the box provides two backends:</p><ul><li>Log backend, which writes events to a disk</li><li>Webhook backend, which sends events to an external API</li></ul><p>In both cases, audit events structure is defined by the API in the <strong>audit.k8s.io</strong> API group. The current version of the API is <a href="https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apiserver/pkg/apis/audit/v1beta1/types.go"><strong>v1beta1</strong></a>.</p><p><strong>Note:</strong> In case of patches, request body is a JSON array with patch operations, not a JSON object with an appropriate Kubernetes API object. For example, the following request body is a valid patch request to <strong>/apis/batch/v1/namespaces/some-namespace/jobs/some-job-name</strong>.</p><p><strong>[</strong></p><p><strong>{</strong></p><p><strong>&quot;op&quot;: &quot;replace&quot;,</strong></p><p><strong>&quot;path&quot;: &quot;/spec/parallelism&quot;,</strong></p><p><strong>&quot;value&quot;: 0</strong></p><p><strong>},</strong></p><p><strong>{</strong></p><p><strong>&quot;op&quot;: &quot;remove&quot;,</strong></p><p><strong>&quot;path&quot;: &quot;/spec/template/spec/containers/0/terminationMessagePolicy&quot;</strong></p><p><strong>}</strong></p><p><strong>]</strong></p><h5><strong>Log backend</strong></h5><p>Log backend writes audit events to a file in JSON format. You can configure log audit backend using the following <a href="https://kubernetes.io/docs/admin/kube-apiserver">kube-apiserver</a> flags:</p><ul><li><strong>--audit-log-path</strong> specifies the log file path that log backend uses to write audit events. Not specifying this flag disables log backend. <strong>-</strong> means standard out</li><li><strong>--audit-log-maxage</strong> defined the maximum number of days to retain old audit log files</li><li><strong>--audit-log-maxbackup</strong> defines the maximum number of audit log files to retain</li><li><strong>--audit-log-maxsize</strong> defines the maximum size in megabytes of the audit log file before it gets rotated</li></ul><h5><strong>Webhook backend</strong></h5><p>Webhook backend sends audit events to a remote API, which is assumed to be the same API as <a href="https://kubernetes.io/docs/admin/kube-apiserver">kube-apiserver</a> exposes. You can configure webhook audit backend using the following kube-apiserver flags:</p><ul><li><strong>--audit-webhook-config-file</strong> specifies the path to a file with a webhook configuration. Webhook configuration is effectively a <a href="https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/">kubeconfig</a>.</li><li><strong>--audit-webhook-initial-backoff</strong> specifies the amount of time to wait after the first failed request before retrying. Subsequent requests are retried with exponential backoff.</li></ul><p>The webhook config file uses the kubeconfig format to specify the remote address of the service and credentials used to connect to it.</p><h5><strong>Batching</strong></h5><p>Both log and webhook backends support batching. Using webhook as an example, here&#x27;s the list of available flags. To get the same flag for log backend, replace <strong>webhook</strong> with <strong>log</strong> in the flag name. By default, batching is enabled in <strong>webhook</strong> and disabled in <strong>log</strong>. Similarly, by default throttling is enabled in <strong>webhook</strong> and disabled in <strong>log</strong>.</p><ul><li><strong>--audit-webhook-mode</strong> defines the buffering strategy. One of the following:<ul><li><strong>batch</strong> - buffer events and asynchronously process them in batches. This is the default.</li><li><strong>blocking</strong> - block API server responses on processing each individual event.</li></ul></li></ul><p>The following flags are used only in the <strong>batch</strong> mode.</p><ul><li><strong>--audit-webhook-batch-buffer-size</strong> defines the number of events to buffer before batching. If the rate of incoming events overflows the buffer, events are dropped.</li><li><strong>--audit-webhook-batch-max-size</strong> defines the maximum number of events in one batch.</li><li><strong>--audit-webhook-batch-max-wait</strong> defines the maximum amount of time to wait before unconditionally batching events in the queue.</li><li><strong>--audit-webhook-batch-throttle-qps</strong> defines the maximum average number of batches generated per second.</li><li><strong>--audit-webhook-batch-throttle-burst</strong> defines the maximum number of batches generated at the same moment if the allowed QPS was underutilized previously.</li></ul><h6><strong>Parameter tuning</strong></h6><p>Parameters should be set to accommodate the load on the apiserver.</p><p>For example, if kube-apiserver receives 100 requests each second, and each request is audited only on <strong>ResponseStarted</strong> and <strong>ResponseComplete</strong> stages, you should account for <!-- -->~<!-- -->200 audit events being generated each second. Assuming that there are up to 100 events in a batch, you should set throttling level at at least 2 QPS. Assuming that the backend can take up to 5 seconds to write events, you should set the buffer size to hold up to 5 seconds of events, i.e. 10 batches, i.e. 1000 events.</p><p>In most cases however, the default parameters should be sufficient and you don&#x27;t have to worry about setting them manually. You can look at the following Prometheus metrics exposed by kube-apiserver and in the logs to monitor the state of the auditing subsystem.</p><ul><li><strong>apiserver_audit_event_total</strong> metric contains the total number of audit events exported.</li><li><strong>apiserver_audit_error_total</strong> metric contains the total number of events dropped due to an error during exporting.</li></ul><h4>Multi-cluster setup</h4><p>If you&#x27;re extending the Kubernetes API with the <a href="https://kubernetes.io/docs/concepts/api-extension/apiserver-aggregation">aggregation layer</a>, you can also set up audit logging for the aggregated apiserver. To do this, pass the configuration options in the same format as described above to the aggregated apiserver and set up the log ingesting pipeline to pick up audit logs. Different apiservers can have different audit configurations and different audit policies.</p><h4>Log Collector Examples</h4><h5><strong>Use fluentd to collect and distribute audit events from log file</strong></h5><p><a href="http://www.fluentd.org/">Fluentd</a> is an open source data collector for unified logging layer. In this example, we will use fluentd to split audit events by different namespaces.</p><ol><li>install <a href="http://docs.fluentd.org/v0.12/articles/quickstart#step1-installing-fluentd">fluentd, fluent-plugin-forest and fluent-plugin-rewrite-tag-filter</a> in the kube-apiserver node</li><li>create a config file for fluentd</li><li><strong>$ cat <code>&lt;&lt;EOF &gt;</code> /etc/fluentd/config</strong></li><li><strong># fluentd conf runs in the same host with kube-apiserver</strong></li><li><strong><code>&lt;source&gt;</code></strong></li><li><strong>@type tail</strong></li><li><strong># audit log path of kube-apiserver</strong></li><li><strong>path /var/log/audit</strong></li><li><strong>pos_file /var/log/audit.pos</strong></li><li><strong>format json</strong></li><li><strong>time_key time</strong></li><li><strong>time_format %Y-%m-%dT%H:%M:%S.%N%z</strong></li><li><strong>tag audit</strong></li><li><strong><code>&lt;/source&gt;</code></strong></li><li><strong><code>&lt;filter audit&gt;</code></strong></li><li><strong>#<a href="https://github.com/fluent/fluent-plugin-rewrite-tag-filter/issues/13">https://github.com/fluent/fluent-plugin-rewrite-tag-filter/issues/13</a></strong></li><li><strong>type record_transformer</strong></li><li><strong>enable_ruby</strong></li><li><strong><code>&lt;record&gt;</code></strong></li><li><strong>namespace ${record<!-- -->[&quot;objectRef&quot;]<!-- -->.nil? ? &quot;none&quot;:(record<!-- -->[&quot;objectRef&quot;][&quot;namespace&quot;]<!-- -->.nil? ? &quot;none&quot;:record<!-- -->[&quot;objectRef&quot;][&quot;namespace&quot;]<!-- -->)}</strong></li><li><strong><code>&lt;/record&gt;</code></strong></li><li><strong><code>&lt;/filter&gt;</code></strong></li><li><strong><code>&lt;match audit&gt;</code></strong></li><li><strong># route audit according to namespace element in context</strong></li><li><strong>@type rewrite_tag_filter</strong></li><li><strong>rewriterule1 namespace \^(.+) ${tag}.$1</strong></li><li><strong><code>&lt;/match&gt;</code></strong></li><li><strong><code>&lt;filter audit.\*\*&gt;</code></strong></li><li><strong>@type record_transformer</strong></li><li><strong>remove_keys namespace</strong></li><li><strong><code>&lt;/filter&gt;</code></strong></li><li><strong><code>&lt;match audit.\*\*&gt;</code></strong></li><li><strong>@type forest</strong></li><li><strong>subtype file</strong></li><li><strong>remove_prefix audit</strong></li><li><strong><code>&lt;template&gt;</code></strong></li><li><strong>time_slice_format %Y%m%d%H</strong></li><li><strong>compress gz</strong></li><li><strong>path /var/log/audit-${tag}.<!-- -->*<!-- -->.log</strong></li><li><strong>format json</strong></li><li><strong>include_time_key true</strong></li><li><strong><code>&lt;/template&gt;</code></strong></li><li><strong><code>&lt;/match&gt;</code></strong></li><li>start fluentd</li><li><strong>$ fluentd -c /etc/fluentd/config -vv</strong></li><li>start kube-apiserver with the following options:</li><li><strong>--audit-policy-file=/etc/kubernetes/audit-policy.yaml --audit-log-path=/var/log/kube-audit --audit-log-format=json</strong></li><li>check audits for different namespaces in /var/log/audit-<!-- -->*<!-- -->.log</li></ol><h5><strong>Use logstash to collect and distribute audit events from webhook backend</strong></h5><p><a href="https://www.elastic.co/products/logstash">Logstash</a> is an open source, server-side data processing tool. In this example, we will use logstash to collect audit events from webhook backend, and save events of different users into different files.</p><ol><li>install <a href="https://www.elastic.co/guide/en/logstash/current/installing-logstash.html">logstash</a></li><li>create config file for logstash</li><li><strong>$ cat <code>&lt;&lt;EOF &gt;</code> /etc/logstash/config</strong></li><li><strong>input{</strong></li><li><strong>http{</strong></li><li><strong>#TODO, figure out a way to use kubeconfig file to authenticate to logstash</strong></li><li><strong>#<a href="https://www.elastic.co/guide/en/logstash/current/plugins-inputs-http.html#plugins-inputs-http-ssl">https://www.elastic.co/guide/en/logstash/current/plugins-inputs-http.html#plugins-inputs-http-ssl</a></strong></li><li><strong>port=&gt;8888</strong></li><li><strong>}</strong></li><li><strong>}</strong></li><li><strong>filter{</strong></li><li><strong>split{</strong></li><li><strong># Webhook audit backend sends several events together with EventList</strong></li><li><strong># split each event here.</strong></li><li><strong>field=&gt;<!-- -->[items]</strong></li><li><strong># We only need event subelement, remove others.</strong></li><li><strong>remove_field=&gt;<!-- -->[headers, metadata, apiVersion, &quot;@timestamp&quot;, kind, &quot;@version&quot;, host]</strong></li><li><strong>}</strong></li><li><strong>mutate{</strong></li><li><strong>rename =&gt; {items=&gt;event}</strong></li><li><strong>}</strong></li><li><strong>}</strong></li><li><strong>output{</strong></li><li><strong>file{</strong></li><li><strong># Audit events from different users will be saved into different files.</strong></li><li><strong>path=&gt;&quot;/var/log/kube-audit-%{<!-- -->[event][user]<!-- -->[username]<!-- -->}/audit&quot;</strong></li><li><strong>}</strong></li><li><strong>}</strong></li><li>start logstash</li><li><strong>$ bin/logstash -f /etc/logstash/config --path.settings /etc/logstash/</strong></li><li>create a <a href="https://kubernetes.io/docs/tasks/access-application-cluster/authenticate-across-clusters-kubeconfig/">kubeconfig file</a> for kube-apiserver webhook audit backend</li><li><strong>$ cat <code>&lt;&lt;EOF &gt;</code> /etc/kubernetes/audit-webhook-kubeconfig</strong></li><li><strong>apiVersion: v1</strong></li><li><strong>clusters:</strong></li><li><strong>- cluster:</strong></li><li><strong>server: http://<code>&lt;ip_of_logstash&gt;</code>:8888</strong></li><li><strong>name: logstash</strong></li><li><strong>contexts:</strong></li><li><strong>- context:</strong></li><li><strong>cluster: logstash</strong></li><li><strong>user: &quot;&quot;</strong></li><li><strong>name: default-context</strong></li><li><strong>current-context: default-context</strong></li><li><strong>kind: Config</strong></li><li><strong>preferences: {}</strong></li><li><strong>users: []</strong></li><li><strong>EOF</strong></li><li>start kube-apiserver with the following options:</li><li><strong>--audit-policy-file=/etc/kubernetes/audit-policy.yaml --audit-webhook-config-file=/etc/kubernetes/audit-webhook-kubeconfig</strong></li><li>check audits in logstash node&#x27;s directories /var/log/kube-audit-<!-- -->*<!-- -->/audit</li></ol><p>Note that in addition to file output plugin, logstash has a variety of outputs that let users route data where they want. For example, users can emit audit events to elasticsearch plugin which supports full-text search and analytics.</p><h4>Legacy Audit</h4><p><strong>Note:</strong> Legacy Audit is deprecated and is disabled by default since Kubernetes 1.8. Legacy Audit will be removed in 1.12. To fallback to this legacy audit, disable the advanced auditing feature using the <strong>AdvancedAuditing</strong> feature gate in <a href="https://kubernetes.io/docs/admin/kube-apiserver">kube-apiserver</a>:</p><p><strong>--feature-gates=AdvancedAuditing=false</strong></p><p>In legacy format, each audit log entry contains two lines:</p><ol><li>The request line containing a unique ID to match the response and request metadata, such as the source IP, requesting user, impersonation information, resource being requested, etc.</li><li>The response line containing a unique ID matching the request line and the response code.</li></ol><p>Example output for <strong>admin</strong> user listing pods in the <strong>default</strong> namespace:</p><p><strong>2017-03-21T03:57:09.106841886-04:00 AUDIT: id=&quot;c939d2a7-1c37-4ef1-b2f7-4ba9b1e43b53&quot; ip=&quot;127.0.0.1&quot; method=&quot;GET&quot; user=&quot;admin&quot; groups=&quot;<!-- -->\<!-- -->&quot;system:masters<!-- -->\<!-- -->&quot;,<!-- -->\<!-- -->&quot;system:authenticated<!-- -->\<!-- -->&quot;&quot; as=&quot;<code>&lt;self&gt;&quot; asgroups=&quot;&lt;lookup&gt;</code>&quot; namespace=&quot;default&quot; uri=&quot;/api/v1/namespaces/default/pods&quot;</strong></p><p><strong>2017-03-21T03:57:09.108403639-04:00 AUDIT: id=&quot;c939d2a7-1c37-4ef1-b2f7-4ba9b1e43b53&quot; response=&quot;200&quot;</strong></p><h5><strong>Configuration</strong></h5><p><a href="https://kubernetes.io/docs/admin/kube-apiserver">Kube-apiserver</a> provides the following options which are responsible for configuring where and how audit logs are handled:</p><ul><li><strong>audit-log-path</strong> - enables the audit log pointing to a file where the requests are being logged to, &#x27;-&#x27; means standard out.</li><li><strong>audit-log-maxage</strong> - specifies maximum number of days to retain old audit log files based on the timestamp encoded in their filename.</li><li><strong>audit-log-maxbackup</strong> - specifies maximum number of old audit log files to retain.</li><li><strong>audit-log-maxsize</strong> - specifies maximum size in megabytes of the audit log file before it gets rotated. Defaults to 100MB.</li></ul><p>If an audit log file already exists, Kubernetes appends new audit logs to that file. Otherwise, Kubernetes creates an audit log file at the location you specified in <strong>audit-log-path</strong>. If the audit log file exceeds the size you specify in <strong>audit-log-maxsize</strong>, Kubernetes will rename the current log file by appending the current timestamp on the file name (before the file extension) and create a new audit log file. Kubernetes may delete old log files when creating a new log file; you can configure how many files are retained and how old they can be by specifying the <strong>audit-log-maxbackup</strong> and <strong>audit-log-maxage</strong> options.</p><h3>Developing and debugging services locally</h3><p>Kubernetes applications usually consist of multiple, separate services, each running in its own container. Developing and debugging these services on a remote Kubernetes cluster can be cumbersome, requiring you to <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/get-shell-running-container/">get a shell on a running container</a> and running your tools inside the remote shell.</p><p><strong>telepresence</strong> is a tool to ease the process of developing and debugging services locally, while proxying the service to a remote Kubernetes cluster. Using <strong>telepresence</strong> allows you to use custom tools, such as a debugger and IDE, for a local service and provides the service full access to ConfigMap, secrets, and the services running on the remote cluster.</p><p>This document describes using <strong>telepresence</strong> to develop and debug services running on a remote cluster locally.</p><ul><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/local-debugging/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/local-debugging/#getting-a-shell-on-a-remote-cluster"><strong>Getting a shell on a remote cluster</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/local-debugging/#developing-or-debugging-an-existing-service"><strong>Developing or debugging an existing service</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/local-debugging/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Before you begin</h4><ul><li>Kubernetes cluster is installed</li><li><strong>kubectl</strong> is configured to communicate with the cluster</li><li><a href="https://www.telepresence.io/reference/install">Telepresence</a> is installed</li></ul><h4>Getting a shell on a remote cluster</h4><p>Open a terminal and run <strong>telepresence</strong> with no arguments to get a <strong>telepresence</strong> shell. This shell runs locally, giving you full access to your local filesystem.</p><p>The <strong>telepresence</strong> shell can be used in a variety of ways. For example, write a shell script on your laptop, and run it directly from the shell in real time. You can do this on a remote shell as well, but you might not be able to use your preferred code editor, and the script is deleted when the container is terminated.</p><p>Enter <strong>exit</strong> to quit and close the shell.</p><h4>Developing or debugging an existing service</h4><p>When developing an application on Kubernetes, you typically program or debug a single service. The service might require access to other services for testing and debugging. One option is to use the continuous deployment pipeline, but even the fastest deployment pipeline introduces a delay in the program or debug cycle.</p><p>Use the <strong>--swap-deployment</strong> option to swap an existing deployment with the Telepresence proxy. Swapping allows you to run a service locally and connect to the remote Kubernetes cluster. The services in the remote cluster can now access the locally running instance.</p><p>To run telepresence with <strong>--swap-deployment</strong>, enter:</p><p><strong>telepresence --swap-deployment $DEPLOYMENT_NAME</strong></p><p>where $DEPLOYMENT_NAME is the name of your existing deployment.</p><p>Running this command spawns a shell. In the shell, start your service. You can then make edits to the source code locally, save, and see the changes take effect immediately. You can also run your service in a debugger, or any other local development tool.</p><h4>What&#x27;s next</h4><p>If you&#x27;re interested in a hands-on tutorial, check out <a href="https://cloud.google.com/community/tutorials/developing-services-with-k8s">this tutorial</a> that walks through locally developing the Guestbook application on Google Kubernetes Engine.</p><p>Telepresence has <a href="https://www.telepresence.io/reference/methods">numerous proxying options</a>, depending on your situation.</p><p>For further reading, visit the <a href="https://www.telepresence.io/">Telepresence website</a>.</p><h3>Explorer</h3><p>Explorer is a little container for examining the runtime environment Kubernetes produces for your pods.</p><p>The intended use is to substitute gcr.io/google_containers/explorer for your intended container, and then visit it via the proxy.</p><p>Currently, you can look at:</p><ul><li>The environment variables to make sure Kubernetes is doing what you expect.</li><li>The filesystem to make sure the mounted volumes and files are also what you expect.</li><li>Perform DNS lookups, to see how DNS works.</li></ul><p>pod.yaml is supplied as an example. You can control the port it serves on with the -port flag.</p><p>Example from command line (the DNS lookup looks better from a web browser):</p><p>$ kubectl create -f examples/explorer/pod.yaml</p><p>$ kubectl proxy &amp;</p><p>Starting to serve on localhost:8001</p><p>$ curl localhost:8001/api/v1/proxy/namespaces/default/pods/explorer:8080/vars/</p><p>PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin</p><p>HOSTNAME=explorer</p><p>KIBANA_LOGGING_PORT_5601_TCP_PORT=5601</p><p>KUBERNETES_SERVICE_HOST=10.0.0.2</p><p>MONITORING_GRAFANA_PORT_80_TCP_PROTO=tcp</p><p>MONITORING_INFLUXDB_UI_PORT_80_TCP_PROTO=tcp</p><p>KIBANA_LOGGING_SERVICE_PORT=5601</p><p>MONITORING_HEAPSTER_PORT_80_TCP_PORT=80</p><p>MONITORING_INFLUXDB_UI_PORT_80_TCP_PORT=80</p><p>KIBANA_LOGGING_SERVICE_HOST=10.0.204.206</p><p>KIBANA_LOGGING_PORT_5601_TCP=tcp://10.0.204.206:5601</p><p>KUBERNETES_PORT=tcp://10.0.0.2:443</p><p>MONITORING_INFLUXDB_PORT=tcp://10.0.2.30:80</p><p>MONITORING_INFLUXDB_PORT_80_TCP_PROTO=tcp</p><p>MONITORING_INFLUXDB_UI_PORT=tcp://10.0.36.78:80</p><p>KUBE_DNS_PORT_53_UDP=udp://10.0.0.10:53</p><p>MONITORING_INFLUXDB_SERVICE_HOST=10.0.2.30</p><p>ELASTICSEARCH_LOGGING_PORT=tcp://10.0.48.200:9200</p><p>ELASTICSEARCH_LOGGING_PORT_9200_TCP_PORT=9200</p><p>KUBERNETES_PORT_443_TCP=tcp://10.0.0.2:443</p><p>ELASTICSEARCH_LOGGING_PORT_9200_TCP_PROTO=tcp</p><p>KIBANA_LOGGING_PORT_5601_TCP_ADDR=10.0.204.206</p><p>KUBE_DNS_PORT_53_UDP_ADDR=10.0.0.10</p><p>MONITORING_HEAPSTER_PORT_80_TCP_PROTO=tcp</p><p>MONITORING_INFLUXDB_PORT_80_TCP_ADDR=10.0.2.30</p><p>KIBANA_LOGGING_PORT=tcp://10.0.204.206:5601</p><p>MONITORING_GRAFANA_SERVICE_PORT=80</p><p>MONITORING_HEAPSTER_SERVICE_PORT=80</p><p>MONITORING_HEAPSTER_PORT_80_TCP=tcp://10.0.150.238:80</p><p>ELASTICSEARCH_LOGGING_PORT_9200_TCP=tcp://10.0.48.200:9200</p><p>ELASTICSEARCH_LOGGING_PORT_9200_TCP_ADDR=10.0.48.200</p><p>MONITORING_GRAFANA_PORT_80_TCP_PORT=80</p><p>MONITORING_HEAPSTER_PORT=tcp://10.0.150.238:80</p><p>MONITORING_INFLUXDB_PORT_80_TCP=tcp://10.0.2.30:80</p><p>KUBE_DNS_SERVICE_PORT=53</p><p>KUBE_DNS_PORT_53_UDP_PORT=53</p><p>MONITORING_GRAFANA_PORT_80_TCP_ADDR=10.0.100.174</p><p>MONITORING_INFLUXDB_UI_SERVICE_HOST=10.0.36.78</p><p>KIBANA_LOGGING_PORT_5601_TCP_PROTO=tcp</p><p>MONITORING_GRAFANA_PORT=tcp://10.0.100.174:80</p><p>MONITORING_INFLUXDB_UI_PORT_80_TCP_ADDR=10.0.36.78</p><p>KUBE_DNS_SERVICE_HOST=10.0.0.10</p><p>KUBERNETES_PORT_443_TCP_PORT=443</p><p>MONITORING_HEAPSTER_PORT_80_TCP_ADDR=10.0.150.238</p><p>MONITORING_INFLUXDB_UI_SERVICE_PORT=80</p><p>KUBE_DNS_PORT=udp://10.0.0.10:53</p><p>ELASTICSEARCH_LOGGING_SERVICE_HOST=10.0.48.200</p><p>KUBERNETES_SERVICE_PORT=443</p><p>MONITORING_HEAPSTER_SERVICE_HOST=10.0.150.238</p><p>MONITORING_INFLUXDB_SERVICE_PORT=80</p><p>MONITORING_INFLUXDB_PORT_80_TCP_PORT=80</p><p>KUBE_DNS_PORT_53_UDP_PROTO=udp</p><p>MONITORING_GRAFANA_PORT_80_TCP=tcp://10.0.100.174:80</p><p>ELASTICSEARCH_LOGGING_SERVICE_PORT=9200</p><p>MONITORING_GRAFANA_SERVICE_HOST=10.0.100.174</p><p>MONITORING_INFLUXDB_UI_PORT_80_TCP=tcp://10.0.36.78:80</p><p>KUBERNETES_PORT_443_TCP_PROTO=tcp</p><p>KUBERNETES_PORT_443_TCP_ADDR=10.0.0.2</p><p>HOME=/</p><p>$ curl localhost:8001/api/v1/proxy/namespaces/default/pods/explorer:8080/fs/</p><p>mount/</p><p>var/</p><p>.dockerenv</p><p>etc/</p><p>dev/</p><p>proc/</p><p>.dockerinit</p><p>sys/</p><p>README.md</p><p>explorer</p><p>$ curl localhost:8001/api/v1/proxy/namespaces/default/pods/explorer:8080/dns?q=elasticsearch-logging</p><p><code style="background-color:lightgray">&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;</code></p><p><code style="background-color:lightgray">&lt;form action=&quot;/api/v1/proxy/namespaces/default/pods/explorer:8080/dns&quot;&gt;</code></p><p><code style="background-color:lightgray">&lt;input name=&quot;q&quot; type=&quot;text&quot; value=&quot;elasticsearch-logging&quot;/&gt;</code></p><p><code style="background-color:lightgray">&lt;button type=&quot;submit&quot;&gt;Lookup&lt;/button&gt;</code></p><p><code style="background-color:lightgray">&lt;/form&gt;</code></p><p><code style="background-color:lightgray">&lt;br/&gt;&lt;br/&gt;&lt;pre&gt;</code>LookupNS(elasticsearch-logging):</p><p>Result: ([]<!-- -->*<!-- -->net.NS)<code style="background-color:lightgray">&lt;nil&gt;</code></p><p>Error: <!-- -->&lt;<!-- -->*<!-- -->&gt;<!-- -->lookup elasticsearch-logging: no such host</p><p>LookupTXT(elasticsearch-logging):</p><p>Result: ([]string)<code style="background-color:lightgray">&lt;nil&gt;</code></p><p>Error: <!-- -->&lt;<!-- -->*<!-- -->&gt;<!-- -->lookup elasticsearch-logging: no such host</p><p>LookupSRV(<!-- -->&quot;<!-- -->&quot;<!-- -->, <!-- -->&quot;<!-- -->&quot;<!-- -->, elasticsearch-logging):</p><p>cname: elasticsearch-logging.default.svc.cluster.local.</p><p>Result: ([]<!-- -->*<!-- -->net.SRV)<!-- -->[&lt;<!-- -->*<!-- -->&gt;<!-- -->{Target:(string)elasticsearch-logging.default.svc.cluster.local. Port:(uint16)9200 Priority:(uint16)10 Weight:(uint16)100}]</p><p>Error: <code style="background-color:lightgray">&lt;nil&gt;</code></p><p>LookupHost(elasticsearch-logging):</p><p>Result: ([]string)<!-- -->[10.0.60.245]</p><p>Error: <code style="background-color:lightgray">&lt;nil&gt;</code></p><p>LookupIP(elasticsearch-logging):</p><p>Result: ([]net.IP)<!-- -->[10.0.60.245]</p><p>Error: <code style="background-color:lightgray">&lt;nil&gt;</code></p><p>LookupMX(elasticsearch-logging):</p><p>Result: ([]<!-- -->*<!-- -->net.MX)<code style="background-color:lightgray">&lt;nil&gt;</code></p><p>Error: <!-- -->&lt;<!-- -->*<!-- -->&gt;<!-- -->lookup elasticsearch-logging: no such host</p><p><code style="background-color:lightgray">&lt;/nil&gt;&lt;/nil&gt;&lt;/nil&gt;&lt;/nil&gt;&lt;/nil&gt;&lt;/nil&gt;&lt;/pre&gt;</code></p><p><code style="background-color:lightgray">&lt;/body&gt;&lt;/html&gt;</code></p><h2>Extend Kubernetes</h2><h3>Use an HTTP Proxy to Access the Kubernetes API</h3><p>This page shows how to use an HTTP proxy to access the Kubernetes API.</p><ul><li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/http-proxy-access-api/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/http-proxy-access-api/#using-kubectl-to-start-a-proxy-server"><strong>Using kubectl to start a proxy server</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/http-proxy-access-api/#exploring-the-kubernetes-api"><strong>Exploring the Kubernetes API</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/http-proxy-access-api/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Before you begin</h4><ul><li>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using <a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</li><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <strong>kubectl version</strong>.</p><ul><li>If you do not already have an application running in your cluster, start a Hello world application by entering this command:</li><li><strong>kubectl run node-hello --image=gcr.io/google-samples/node-hello:1.0 --port=8080</strong></li></ul><h4>Using kubectl to start a proxy server</h4><p>This command starts a proxy to the Kubernetes API server:</p><p><strong>kubectl proxy --port=8080</strong></p><h4>Exploring the Kubernetes API</h4><p>When the proxy server is running, you can explore the API using <strong>curl</strong>, <strong>wget</strong>, or a browser.</p><p>Get the API versions:</p><p><strong>curl http://localhost:8080/api/</strong></p><p><strong>{</strong></p><p><strong>&quot;kind&quot;: &quot;APIVersions&quot;,</strong></p><p><strong>&quot;versions&quot;: [</strong></p><p><strong>&quot;v1&quot;</strong></p><p><strong>],</strong></p><p><strong>&quot;serverAddressByClientCIDRs&quot;: [</strong></p><p><strong>{</strong></p><p><strong>&quot;clientCIDR&quot;: &quot;0.0.0.0/0&quot;,</strong></p><p><strong>&quot;serverAddress&quot;: &quot;10.0.2.15:8443&quot;</strong></p><p><strong>}</strong></p><p><strong>]</strong></p><p><strong>}</strong></p><p>Get a list of pods:</p><p><strong>curl http://localhost:8080/api/v1/namespaces/default/pods</strong></p><p><strong>{</strong></p><p><strong>&quot;kind&quot;: &quot;PodList&quot;,</strong></p><p><strong>&quot;apiVersion&quot;: &quot;v1&quot;,</strong></p><p><strong>&quot;metadata&quot;: {</strong></p><p><strong>&quot;selfLink&quot;: &quot;/api/v1/namespaces/default/pods&quot;,</strong></p><p><strong>&quot;resourceVersion&quot;: &quot;33074&quot;</strong></p><p><strong>},</strong></p><p><strong>&quot;items&quot;: [</strong></p><p><strong>{</strong></p><p><strong>&quot;metadata&quot;: {</strong></p><p><strong>&quot;name&quot;: &quot;kubernetes-bootcamp-2321272333-ix8pt&quot;,</strong></p><p><strong>&quot;generateName&quot;: &quot;kubernetes-bootcamp-2321272333-&quot;,</strong></p><p><strong>&quot;namespace&quot;: &quot;default&quot;,</strong></p><p><strong>&quot;selfLink&quot;: &quot;/api/v1/namespaces/default/pods/kubernetes-bootcamp-2321272333-ix8pt&quot;,</strong></p><p><strong>&quot;uid&quot;: &quot;ba21457c-6b1d-11e6-85f7-1ef9f1dab92b&quot;,</strong></p><p><strong>&quot;resourceVersion&quot;: &quot;33003&quot;,</strong></p><p><strong>&quot;creationTimestamp&quot;: &quot;2016-08-25T23:43:30Z&quot;,</strong></p><p><strong>&quot;labels&quot;: {</strong></p><p><strong>&quot;pod-template-hash&quot;: &quot;2321272333&quot;,</strong></p><p><strong>&quot;run&quot;: &quot;kubernetes-bootcamp&quot;</strong></p><p><strong>},</strong></p><p><strong>.<!-- -->..</strong></p><p><strong>}</strong></p><h4>What&#x27;s next</h4><p>Learn more about <a href="https://kubernetes.io/docs/user-guide/kubectl/v1.10/#proxy">kubectl proxy</a>.</p><h3>Extend the Kubernetes API with CustomResourceDefinitions</h3><p>This page shows how to install a <a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/">custom resource</a> into the Kubernetes API by creating a<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#customresourcedefinition-v1beta1-apiextensions">CustomResourceDefinition</a>.</p><ul><li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/#create-a-customresourcedefinition"><strong>Create a CustomResourceDefinition</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/#create-custom-objects"><strong>Create custom objects</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/#delete-a-customresourcedefinition"><strong>Delete a CustomResourceDefinition</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/#advanced-topics"><strong>Advanced topics</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/#finalizers"><strong>Finalizers</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/#validation"><strong>Validation</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/#subresources"><strong>Subresources</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/#status-subresource"><strong>Status subresource</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/#scale-subresource"><strong>Scale subresource</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/#categories"><strong>Categories</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Before you begin</h4><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using<a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <strong>kubectl version</strong>.</p><ul><li>Make sure your Kubernetes cluster has a master version of 1.7.0 or higher.</li><li>Read about <a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/">custom resources</a>.</li></ul><h4>Create a CustomResourceDefinition</h4><p>When you create a new CustomResourceDefinition (CRD), the Kubernetes API Server reacts by creating a new RESTful resource path, either namespaced or cluster-scoped, as specified in the CRD&#x27;s <strong>scope</strong> field. As with existing built-in objects, deleting a namespace deletes all custom objects in that namespace. CustomResourceDefinitions themselves are non-namespaced and are available to all namespaces.</p><p>For example, if you save the following CustomResourceDefinition to <strong>resourcedefinition.yaml</strong>:</p><div class="MuiContainer-root MuiContainer-maxWidthLg"><pre class="Code__Pre-gy960v-0 UDybk prism-code language-yaml" style="color:#9CDCFE;background-color:#1E1E1E"><div class="MuiGrid-root MuiGrid-container MuiGrid-justify-xs-flex-end"><button class="Code__CopyCode-gy960v-1 llUIua">Copy</button></div><div class="token-line" style="color:#9CDCFE"><span class="token key atrule">**apiVersion</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> apiextensions.k8s.io/v1beta1**</span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain"></span><span class="token key atrule">**kind</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> CustomResourceDefinition**</span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">*</span><span class="token important">*metadata</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain">**</span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">***</span><span class="token comment" style="color:rgb(106, 153, 85)"># name must match the spec fields below, and be in the form: `&lt;plural&gt;.&lt;group&gt;`***</span><span class="token plain"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain"></span><span class="token key atrule">**name</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> crontabs.stable.example.com**</span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">*</span><span class="token important">*spec</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain">**</span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">***</span><span class="token comment" style="color:rgb(106, 153, 85)"># group name to use for REST API: /apis/&lt;group&gt;/&lt;version&gt;***</span><span class="token plain"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain"></span><span class="token key atrule">**group</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> stable.example.com**</span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">***</span><span class="token comment" style="color:rgb(106, 153, 85)"># version name to use for REST API: /apis/`&lt;group&gt;/&lt;version&gt;***</span><span class="token plain"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain"></span><span class="token key atrule">**version</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> v1**</span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">***</span><span class="token comment" style="color:rgb(106, 153, 85)"># either Namespaced or Cluster***</span><span class="token plain"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain"></span><span class="token key atrule">**scope</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> Namespaced**</span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">*</span><span class="token important">*names</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain">**</span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">***</span><span class="token comment" style="color:rgb(106, 153, 85)"># plural name to be used in the URL: /apis/&lt;group&gt;/&lt;version&gt;/&lt;plural&gt;***</span><span class="token plain"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain"></span><span class="token key atrule">**plural</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> crontabs**</span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">***</span><span class="token comment" style="color:rgb(106, 153, 85)"># singular name to be used as an alias on the CLI and for display***</span><span class="token plain"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain"></span><span class="token key atrule">**singular</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> crontab**</span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">***</span><span class="token comment" style="color:rgb(106, 153, 85)"># kind is normally the CamelCased singular type. Your resource manifests use this.***</span><span class="token plain"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain"></span><span class="token key atrule">**kind</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> CronTab**</span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">***</span><span class="token comment" style="color:rgb(106, 153, 85)"># shortNames allow shorter string to match your resource on the CLI***</span><span class="token plain"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">*</span><span class="token important">*shortNames</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain">**</span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span></div><div class="token-line" style="color:#9CDCFE"><span class="token plain">**</span><span class="token punctuation" style="color:rgb(212, 212, 212)">-</span><span class="token plain"> ct**</span></div></pre></div><p>And create it:</p><p><strong>kubectl create -f resourcedefinition.yaml</strong></p><p>Then a new namespaced RESTful API endpoint is created at:</p><p><strong>/apis/stable.example.com/v1/namespaces/<!-- -->*<!-- -->/crontabs/<!-- -->.<!-- -->..</strong></p><p>This endpoint URL can then be used to create and manage custom objects. The <strong>kind</strong> of these objects will be <strong>CronTab</strong> from the spec of the CustomResourceDefinition object you created above.</p><p>Please note that it might take a few seconds for the endpoint to be created. You can watch the <strong>Established</strong> condition of your CustomResourceDefinition to be true or watch the discovery information of the API server for your resource to show up.</p><h4>Create custom objects</h4><p>After the CustomResourceDefinition object has been created, you can create custom objects. Custom objects can contain custom fields. These fields can contain arbitrary JSON. In the following example, the <strong>cronSpec</strong> and <strong>image</strong> custom fields are set in a custom object of kind <strong>CronTab</strong>. The kind <strong>CronTab</strong> comes from the spec of the CustomResourceDefinition object you created above.</p><p>If you save the following YAML to <strong>my-crontab.yaml</strong>:</p><p><strong>apiVersion: &quot;stable.example.com/v1&quot;</strong></p><p><strong>kind: CronTab</strong></p><p><strong>metadata:</strong></p><p><strong>name: my-new-cron-object</strong></p><p><strong>spec:</strong></p><p><strong>cronSpec: &quot;<!-- -->*<!-- --> <!-- -->*<!-- --> <!-- -->*<!-- --> <!-- -->*<!-- --> <!-- -->*<!-- -->/5&quot;</strong></p><p><strong>image: my-awesome-cron-image</strong></p><p>and create it:</p><p><strong>kubectl create -f my-crontab.yaml</strong></p><p>You can then manage your CronTab objects using kubectl. For example:</p><p><strong>kubectl get crontab</strong></p><p>Should print a list like this:</p><p><strong>NAME KIND</strong></p><p><strong>my-new-cron-object CronTab.v1.stable.example.com</strong></p><p>Note that resource names are not case-sensitive when using kubectl, and you can use either the singular or plural forms defined in the CRD, as well as any short names.</p><p>You can also view the raw YAML data:</p><p><strong>kubectl get ct -o yaml</strong></p><p>You should see that it contains the custom <strong>cronSpec</strong> and <strong>image</strong> fields from the yaml you used to create it:</p><p><strong>apiVersion: v1</strong></p><p><strong>items:</strong></p><p><strong>- apiVersion: stable.example.com/v1</strong></p><p><strong>kind: CronTab</strong></p><p><strong>metadata:</strong></p><p><strong>clusterName: &quot;&quot;</strong></p><p><strong>creationTimestamp: 2017-05-31T12:56:35Z</strong></p><p><strong>deletionGracePeriodSeconds: null</strong></p><p><strong>deletionTimestamp: null</strong></p><p><strong>name: my-new-cron-object</strong></p><p><strong>namespace: default</strong></p><p><strong>resourceVersion: &quot;285&quot;</strong></p><p><strong>selfLink: /apis/stable.example.com/v1/namespaces/default/crontabs/my-new-cron-object</strong></p><p><strong>uid: 9423255b-4600-11e7-af6a-28d2447dc82b</strong></p><p><strong>spec:</strong></p><p><strong>cronSpec: \&#x27;<!-- -->*<!-- --> <!-- -->*<!-- --> <!-- -->*<!-- --> <!-- -->*<!-- --> <!-- -->*<!-- -->/5\&#x27;</strong></p><p><strong>image: my-awesome-cron-image</strong></p><p><strong>kind: List</strong></p><p><strong>metadata:</strong></p><p><strong>resourceVersion: &quot;&quot;</strong></p><p><strong>selfLink: &quot;&quot;</strong></p><h4>Delete a CustomResourceDefinition</h4><p>When you delete a CustomResourceDefinition, the server will uninstall the RESTful API endpoint and <strong>delete all custom objects stored in it</strong>.</p><p><strong>kubectl delete -f resourcedefinition.yaml</strong></p><p><strong>kubectl get crontabs</strong></p><p><strong>Error from server (NotFound): Unable to list &quot;crontabs&quot;: the server could not find the requested resource (get crontabs.stable.example.com)</strong></p><p>If you later recreate the same CustomResourceDefinition, it will start out empty.</p><h4>Advanced topics</h4><h5><strong>Finalizers</strong></h5><p>Finalizers allow controllers to implement asynchronous pre-delete hooks. Custom objects support finalizers just like built-in objects.</p><p>You can add a finalizer to a custom object like this:</p><p><strong>apiVersion: &quot;stable.example.com/v1&quot;</strong></p><p><strong>kind: CronTab</strong></p><p><strong>metadata:</strong></p><p><strong>finalizers:</strong></p><p><strong>- finalizer.stable.example.com</strong></p><p>Finalizers are arbitrary string values, that when present ensure that a hard delete of a resource is not possible while they exist.</p><p>The first delete request on an object with finalizers merely sets a value for the <strong>metadata.deletionTimestamp</strong> field instead of deleting it. Once this value is set, entries in the <strong>finalizer</strong> list can only be removed.</p><p>This triggers controllers watching the object to execute any finalizers they handle. This will be represented via polling update requests for that object, until all finalizers have been removed and the resource is deleted.</p><p>The time period of polling update can be controlled by <strong>metadata.deletionGracePeriodSeconds</strong>.</p><p>It is the responsibility of each controller to removes its finalizer from the list.</p><p>Kubernetes will only finally delete the object if the list of finalizers is empty, meaning all finalizers are done.</p><h5><strong>Validation</strong></h5><p>Validation of custom objects is possible via <a href="https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.0.md#schemaObject">OpenAPI v3 schema</a>. Additionally, the following restrictions are applied to the schema:</p><ul><li>The fields <strong>default</strong>, <strong>nullable</strong>, <strong>discriminator</strong>, <strong>readOnly</strong>, <strong>writeOnly</strong>, <strong>xml</strong>, <strong>deprecated</strong>and <strong>$ref</strong> cannot be set.</li><li>The field <strong>uniqueItems</strong> cannot be set to true.</li><li>The field <strong>additionalProperties</strong> cannot be set to false.</li></ul><p>This feature is <strong>beta</strong> in v1.9. You can disable this feature using the <strong>CustomResourceValidation</strong>feature gate on the <a href="https://kubernetes.io/docs/admin/kube-apiserver">kube-apiserver</a>:</p><p><strong>--feature-gates=CustomResourceValidation=false</strong></p><p>The schema is defined in the CustomResourceDefinition. In the following example, the CustomResourceDefinition applies the following validations on the custom object:</p><ul><li><strong>spec.cronSpec</strong> must be a string and must be of the form described by the regular expression.</li><li><strong>spec.replicas</strong> must be an integer and must have a minimum value of 1 and a maximum value of 10.</li></ul><p>Save the CustomResourceDefinition to <strong>resourcedefinition.yaml</strong>:</p><p><strong>apiVersion: apiextensions.k8s.io/v1beta1</strong></p><p><strong>kind: CustomResourceDefinition</strong></p><p><strong>metadata:</strong></p><p><strong>name: crontabs.stable.example.com</strong></p><p><strong>spec:</strong></p><p><strong>group: stable.example.com</strong></p><p><strong>version: v1</strong></p><p><strong>scope: Namespaced</strong></p><p><strong>names:</strong></p><p><strong>plural: crontabs</strong></p><p><strong>singular: crontab</strong></p><p><strong>kind: CronTab</strong></p><p><strong>shortNames:</strong></p><p><strong>- ct</strong></p><p><strong>validation:</strong></p><p><strong><em># openAPIV3Schema is the schema for validating custom objects.</em></strong></p><p><strong>openAPIV3Schema:</strong></p><p><strong>properties:</strong></p><p><strong>spec:</strong></p><p><strong>properties:</strong></p><p><strong>cronSpec:</strong></p><p><strong>type: string</strong></p><p><strong>pattern: \&#x27;\^(<!-- -->\<!-- -->d+|<!-- -->\<!-- -->*<!-- -->)(/<!-- -->\<!-- -->d+)?(<!-- -->\<!-- -->s+(<!-- -->\<!-- -->d+|<!-- -->\<!-- -->*<!-- -->)(/<!-- -->\<!-- -->d+)?){4}$\&#x27;</strong></p><p><strong>replicas:</strong></p><p><strong>type: integer</strong></p><p><strong>minimum: 1</strong></p><p><strong>maximum: 10</strong></p><p>And create it:</p><p><strong>kubectl create -f resourcedefinition.yaml</strong></p><p>A request to create a custom object of kind <strong>CronTab</strong> will be rejected if there are invalid values in its fields. In the following example, the custom object contains fields with invalid values:</p><ul><li><strong>spec.cronSpec</strong> does not match the regular expression.</li><li><strong>spec.replicas</strong> is greater than 10.</li></ul><p>If you save the following YAML to <strong>my-crontab.yaml</strong>:</p><p><strong>apiVersion: &quot;stable.example.com/v1&quot;</strong></p><p><strong>kind: CronTab</strong></p><p><strong>metadata:</strong></p><p><strong>name: my-new-cron-object</strong></p><p><strong>spec:</strong></p><p><strong>cronSpec: &quot;<!-- -->*<!-- --> <!-- -->*<!-- --> <!-- -->*<!-- --> <!-- -->*<!-- -->&quot;</strong></p><p><strong>image: my-awesome-cron-image</strong></p><p><strong>replicas: 15</strong></p><p>and create it:</p><p><strong>kubectl create -f my-crontab.yaml</strong></p><p>you will get an error:</p><p><strong>The CronTab &quot;my-new-cron-object&quot; is invalid: []: Invalid value: map<!-- -->[string]<!-- -->interface {}{&quot;apiVersion&quot;:&quot;stable.example.com/v1&quot;, &quot;kind&quot;:&quot;CronTab&quot;, &quot;metadata&quot;:map<!-- -->[string]<!-- -->interface {}{&quot;name&quot;:&quot;my-new-cron-object&quot;, &quot;namespace&quot;:&quot;default&quot;, &quot;deletionTimestamp&quot;:interface {}(nil), &quot;deletionGracePeriodSeconds&quot;:(<!-- -->*<!-- -->int64)(nil), &quot;creationTimestamp&quot;:&quot;2017-09-05T05:20:07Z&quot;, &quot;uid&quot;:&quot;e14d79e7-91f9-11e7-a598-f0761cb232d1&quot;, &quot;selfLink&quot;:&quot;&quot;, &quot;clusterName&quot;:&quot;&quot;}, &quot;spec&quot;:map<!-- -->[string]<!-- -->interface {}{&quot;cronSpec&quot;:&quot;<!-- -->*<!-- --> <!-- -->*<!-- --> <!-- -->*<!-- --> <!-- -->*<!-- -->&quot;, &quot;image&quot;:&quot;my-awesome-cron-image&quot;, &quot;replicas&quot;:15}}:</strong></p><p><strong>validation failure list:</strong></p><p><strong>spec.cronSpec in body should match \&#x27;\^(<!-- -->\<!-- -->d+|<!-- -->\<!-- -->*<!-- -->)(/<!-- -->\<!-- -->d+)?(<!-- -->\<!-- -->s+(<!-- -->\<!-- -->d+|<!-- -->\<!-- -->*<!-- -->)(/<!-- -->\<!-- -->d+)?){4}$\&#x27;</strong></p><p><strong>spec.replicas in body should be less than or equal to 10</strong></p><p>If the fields contain valid values, the object creation request is accepted.</p><p>Save the following YAML to <strong>my-crontab.yaml</strong>:</p><p><strong>apiVersion: &quot;stable.example.com/v1&quot;</strong></p><p><strong>kind: CronTab</strong></p><p><strong>metadata:</strong></p><p><strong>name: my-new-cron-object</strong></p><p><strong>spec:</strong></p><p><strong>cronSpec: &quot;<!-- -->*<!-- --> <!-- -->*<!-- --> <!-- -->*<!-- --> <!-- -->*<!-- --> <!-- -->*<!-- -->/5&quot;</strong></p><p><strong>image: my-awesome-cron-image</strong></p><p><strong>replicas: 5</strong></p><p>And create it:</p><p><strong>kubectl create -f my-crontab.yaml</strong></p><p><strong>crontab &quot;my-new-cron-object&quot; created</strong></p><h5><strong>Subresources</strong></h5><p>Custom resources support <strong>/status</strong> and <strong>/scale</strong> subresources. This feature is <strong>alpha</strong> in v1.10 and may change in backward incompatible ways.</p><p>Enable this feature using the <strong>CustomResourceSubresources</strong> feature gate on the <a href="https://kubernetes.io/docs/admin/kube-apiserver">kube-apiserver</a>:</p><p><strong>--feature-gates=CustomResourceSubresources=true</strong></p><p>When the <strong>CustomResourceSubresources</strong> feature gate is enabled, only the <strong>properties</strong> construct is allowed in the root schema for custom resource validation.</p><p>The status and scale subresources can be optionally enabled by defining them in the CustomResourceDefinition.</p><h6><strong>Status subresource</strong></h6><p>When the status subresource is enabled, the <strong>/status</strong> subresource for the custom resource is exposed.</p><ul><li>The status and the spec stanzas are represented by the <strong>.status</strong> and <strong>.spec</strong> JSONPaths respectively inside of a custom resource.</li><li><strong>PUT</strong> requests to the <strong>/status</strong> subresource take a custom resource object and ignore changes to anything except the status stanza.</li><li><strong>PUT</strong> requests to the <strong>/status</strong> subresource only validate the status stanza of the custom resource.</li><li><strong>PUT</strong>/<strong>POST</strong>/<strong>PATCH</strong> requests to the custom resource ignore changes to the status stanza.</li><li>Any changes to the spec stanza increments the value at <strong>.metadata.generation</strong>.</li></ul><h6><strong>Scale subresource</strong></h6><p>When the scale subresource is enabled, the <strong>/scale</strong> subresource for the custom resource is exposed. The <strong>autoscaling/v1.Scale</strong> object is sent as the payload for <strong>/scale</strong>.</p><p>To enable the scale subresource, the following values are defined in the CustomResourceDefinition.</p><ul><li><strong>SpecReplicasPath</strong> defines the JSONPath inside of a custom resource that corresponds to <strong>Scale.Spec.Replicas</strong>.<ul><li>It is a required value.</li><li>Only JSONPaths under <strong>.spec</strong> and with the dot notation are allowed.</li><li>If there is no value under the <strong>SpecReplicasPath</strong> in the custom resource, the <strong>/scale</strong>subresource will return an error on GET.</li></ul></li><li><strong>StatusReplicasPath</strong> defines the JSONPath inside of a custom resource that corresponds to <strong>Scale.Status.Replicas</strong>.<ul><li>It is a required value.</li><li>Only JSONPaths under <strong>.status</strong> and with the dotation are allowed.</li><li>If there is no value under the <strong>StatusReplicasPath</strong> in the custom resource, the status replica value in the <strong>/scale</strong> subresource will default to 0.</li></ul></li><li><strong>LabelSelectorPath</strong> defines the JSONPath inside of a custom resource that corresponds to <strong>Scale.Status.Selector</strong>.<ul><li>It is an optional value.</li><li>It must be set to work with HPA.</li><li>Only JSONPaths under <strong>.status</strong> and with the dotation are allowed.</li><li>If there is no value under the <strong>LabelSelectorPath</strong> in the custom resource, the status selector value in the <strong>/scale</strong> subresource will default to the empty string.</li></ul></li></ul><p>In the following example, both status and scale subresources are enabled.</p><p>Save the CustomResourceDefinition to <strong>resourcedefinition.yaml</strong>:</p><p><strong>apiVersion: apiextensions.k8s.io/v1beta1</strong></p><p><strong>kind: CustomResourceDefinition</strong></p><p><strong>metadata:</strong></p><p><strong>name: crontabs.stable.example.com</strong></p><p><strong>spec:</strong></p><p><strong>group: stable.example.com</strong></p><p><strong>version: v1</strong></p><p><strong>scope: Namespaced</strong></p><p><strong>names:</strong></p><p><strong>plural: crontabs</strong></p><p><strong>singular: crontab</strong></p><p><strong>kind: CronTab</strong></p><p><strong>shortNames:</strong></p><p><strong>- ct</strong></p><p><strong><em># subresources describes the subresources for custom resources.</em></strong></p><p><strong>subresources:</strong></p><p><strong><em># status enables the status subresource.</em></strong></p><p><strong>status: {}</strong></p><p><strong><em># scale enables the scale subresource.</em></strong></p><p><strong>scale:</strong></p><p><strong><em># specReplicasPath defines the JSONPath inside of a custom resource that corresponds to Scale.Spec.Replicas.</em></strong></p><p><strong>specReplicasPath: .spec.replicas</strong></p><p><strong><em># statusReplicasPath defines the JSONPath inside of a custom resource that corresponds to Scale.Status.Replicas.</em></strong></p><p><strong>statusReplicasPath: .status.replicas</strong></p><p><strong><em># labelSelectorPath defines the JSONPath inside of a custom resource that corresponds to Scale.Status.Selector.</em></strong></p><p><strong>labelSelectorPath: .status.labelSelector</strong></p><p>And create it:</p><p><strong>kubectl create -f resourcedefinition.yaml</strong></p><p>After the CustomResourceDefinition object has been created, you can create custom objects.</p><p>If you save the following YAML to <strong>my-crontab.yaml</strong>:</p><p><strong>apiVersion: &quot;stable.example.com/v1&quot;</strong></p><p><strong>kind: CronTab</strong></p><p><strong>metadata:</strong></p><p><strong>name: my-new-cron-object</strong></p><p><strong>spec:</strong></p><p><strong>cronSpec: &quot;<!-- -->*<!-- --> <!-- -->*<!-- --> <!-- -->*<!-- --> <!-- -->*<!-- --> <!-- -->*<!-- -->/5&quot;</strong></p><p><strong>image: my-awesome-cron-image</strong></p><p><strong>replicas: 3</strong></p><p>and create it:</p><p><strong>kubectl create -f my-crontab.yaml</strong></p><p>Then new namespaced RESTful API endpoints are created at:</p><p><strong>/apis/stable.example.com/v1/namespaces/<!-- -->*<!-- -->/crontabs/status</strong></p><p>and</p><p><strong>/apis/stable.example.com/v1/namespaces/<!-- -->*<!-- -->/crontabs/scale</strong></p><p>A custom resource can be scaled using the <strong>kubectl scale</strong> command. For example, the following command sets <strong>.spec.replicas</strong> of the custom resource created above to 5:</p><p><strong>kubectl scale --replicas=5 crontabs/my-new-cron-object</strong></p><p><strong>crontabs &quot;my-new-cron-object&quot; scaled</strong></p><p><strong>kubectl get crontabs my-new-cron-object -o jsonpath=\&#x27;{.spec.replicas}\&#x27;</strong></p><p><strong>5</strong></p><h5><strong>Categories</strong></h5><p>Categories is a list of grouped resources the custom resource belongs to (eg. <strong>all</strong>). You can use <strong>kubectl get <code>&lt;category-name&gt;</code></strong> to list the resources belonging to the category. This feature is <strong>beta</strong>and available for custom resources from v1.10.</p><p>The following example adds <strong>all</strong> in the list of categories in the CustomResourceDefinition and illustrates how to output the custom resource using <strong>kubectl get all</strong>.</p><p>Save the following CustomResourceDefinition to <strong>resourcedefinition.yaml</strong>:</p><p><strong>apiVersion: apiextensions.k8s.io/v1beta1</strong></p><p><strong>kind: CustomResourceDefinition</strong></p><p><strong>metadata:</strong></p><p><strong>name: crontabs.stable.example.com</strong></p><p><strong>spec:</strong></p><p><strong>group: stable.example.com</strong></p><p><strong>version: v1</strong></p><p><strong>scope: Namespaced</strong></p><p><strong>names:</strong></p><p><strong>plural: crontabs</strong></p><p><strong>singular: crontab</strong></p><p><strong>kind: CronTab</strong></p><p><strong>shortNames:</strong></p><p><strong>- ct</strong></p><p><strong><em># categories is a list of grouped resources the custom resource belongs to.</em></strong></p><p><strong>categories:</strong></p><p><strong>- all</strong></p><p>And create it:</p><p><strong>kubectl create -f resourcedefinition.yaml</strong></p><p>After the CustomResourceDefinition object has been created, you can create custom objects.</p><p>Save the following YAML to <strong>my-crontab.yaml</strong>:</p><p><strong>apiVersion: &quot;stable.example.com/v1&quot;</strong></p><p><strong>kind: CronTab</strong></p><p><strong>metadata:</strong></p><p><strong>name: my-new-cron-object</strong></p><p><strong>spec:</strong></p><p><strong>cronSpec: &quot;<!-- -->*<!-- --> <!-- -->*<!-- --> <!-- -->*<!-- --> <!-- -->*<!-- --> <!-- -->*<!-- -->/5&quot;</strong></p><p><strong>image: my-awesome-cron-image</strong></p><p>and create it:</p><p><strong>kubectl create -f my-crontab.yaml</strong></p><p>You can specify the category using <strong>kubectl get</strong>:</p><p><strong>kubectl get all</strong></p><p>and it will include the custom resources of kind <strong>CronTab</strong>:</p><p><strong>NAME AGE</strong></p><p><strong>crontabs/my-new-cron-object 3s</strong></p><h4>What&#x27;s next</h4><ul><li>Learn how to <a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/migrate-third-party-resource/">Migrate a ThirdPartyResource to CustomResourceDefinition</a>.</li><li>See <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#customresourcedefinition-v1beta1-apiextensions">CustomResourceDefinition</a>.</li></ul><h3>Extend the Kubernetes API with ThirdPartyResources</h3><p><strong>DEPRECATION NOTICE:</strong> As of <strong>Kubernetes 1.7</strong>, this has been <a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-third-party-resource/">deprecated</a></p><ul><li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-third-party-resource/#what-is-thirdpartyresource"><strong>What is ThirdPartyResource?</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-third-party-resource/#structure-of-a-thirdpartyresource"><strong>Structure of a ThirdPartyResource</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-third-party-resource/#creating-a-thirdpartyresource"><strong>Creating a ThirdPartyResource</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-third-party-resource/#creating-custom-objects"><strong>Creating Custom Objects</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-third-party-resource/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>What is ThirdPartyResource?</h4><p><strong>ThirdPartyResource is deprecated as of Kubernetes 1.7 and has been removed in version 1.8 in accordance with the </strong><a href="https://kubernetes.io/docs/reference/deprecation-policy"><strong>deprecation policy</strong></a><strong> for beta features.</strong></p><p><strong>To avoid losing data stored in ThirdPartyResources, you must </strong><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/migrate-third-party-resource/"><strong>migrate to CustomResourceDefinition</strong></a><strong> before upgrading to Kubernetes 1.8 or higher.</strong></p><p>Kubernetes comes with many built-in API objects. However, there are often times when you might need to extend Kubernetes with your own API objects in order to do custom automation.</p><p><strong>ThirdPartyResource</strong> objects are a way to extend the Kubernetes API with a new API object type. The new API object type will be given an API endpoint URL and support CRUD operations, and watch API. You can then create custom objects using this API endpoint. You can think of <strong>ThirdPartyResources</strong> as being much like the schema for a database table. Once you have created the table, you can then start storing rows in the table. Once created, <strong>ThirdPartyResources</strong> can act as the data model behind custom controllers or automation programs.</p><h4>Structure of a ThirdPartyResource</h4><p>Each <strong>ThirdPartyResource</strong> has the following:</p><ul><li><strong>metadata</strong> - Standard Kubernetes object metadata.</li><li><strong>kind</strong> - The kind of the resources described by this third party resource.</li><li><strong>description</strong> - A free text description of the resource.</li><li><strong>versions</strong> - A list of the versions of the resource.</li></ul><p>The <strong>kind</strong> for a <strong>ThirdPartyResource</strong> takes the form <strong><code>&lt;kind name&gt;.&lt;domain&gt;</code></strong>. You are expected to provide a unique kind and domain name in order to avoid conflicts with other <strong>ThirdPartyResource</strong> objects. Kind names will be converted to CamelCase when creating instances of the <strong>ThirdPartyResource</strong>. Hyphens in the <strong>kind</strong> are assumed to be word breaks. For instance the kind <strong>camel-case</strong> would be converted to <strong>CamelCase</strong> but <strong>camelcase</strong> would be converted to <strong>Camelcase</strong>.</p><p>Other fields on the <strong>ThirdPartyResource</strong> are treated as custom data fields. These fields can hold arbitrary JSON data and have any structure.</p><p>You can view the full documentation about <strong>ThirdPartyResources</strong> using the <strong>explain</strong> command in kubectl.</p><p><strong>$ kubectl explain thirdpartyresource</strong></p><h4>Creating a ThirdPartyResource</h4><p>When you create a new <strong>ThirdPartyResource</strong>, the Kubernetes API Server reacts by creating a new, namespaced RESTful resource path. For now, non-namespaced objects are not supported. As with existing built-in objects, deleting a namespace deletes all custom objects in that namespace. <strong>ThirdPartyResources</strong> themselves are non-namespaced and are available to all namespaces.</p><p>For example, if you save the following <strong>ThirdPartyResource</strong> to <strong>resource.yaml</strong>:</p><p><strong>apiVersion: extensions/v1beta1</strong></p><p><strong>kind: ThirdPartyResource</strong></p><p><strong>metadata:</strong></p><p><strong>name: cron-tab.stable.example.com</strong></p><p><strong>description: &quot;A specification of a Pod to run on a cron style schedule&quot;</strong></p><p><strong>versions:</strong></p><p><strong>- name: v1</strong></p><p>And create it:</p><p><strong>$ kubectl create -f resource.yaml</strong></p><p><strong>thirdpartyresource &quot;cron-tab.stable.example.com&quot; created</strong></p><p>Then a new RESTful API endpoint is created at:</p><p><strong>/apis/stable.example.com/v1/namespaces/<code>&lt;namespace&gt;</code>/crontabs/<!-- -->.<!-- -->..</strong></p><p>This endpoint URL can then be used to create and manage custom objects. The <strong>kind</strong> of these objects will be <strong>CronTab</strong> following the camel case rules applied to the <strong>metadata.name</strong> of this <strong>ThirdPartyResource</strong> (<strong>cron-tab.stable.example.com</strong>)</p><h4>Creating Custom Objects</h4><p>After the <strong>ThirdPartyResource</strong> object has been created you can create custom objects. Custom objects can contain custom fields. These fields can contain arbitrary JSON. In the following example, a <strong>cronSpec</strong> and <strong>image</strong> custom fields are set to the custom object of kind <strong>CronTab</strong>. The kind <strong>CronTab</strong> is derived from the <strong>metadata.name</strong> of the <strong>ThirdPartyResource</strong> object we created above.</p><p>If you save the following YAML to <strong>my-crontab.yaml</strong>:</p><p><strong>apiVersion: &quot;stable.example.com/v1&quot;</strong></p><p><strong>kind: CronTab</strong></p><p><strong>metadata:</strong></p><p><strong>name: my-new-cron-object</strong></p><p><strong>cronSpec: &quot;<!-- -->*<!-- --> <!-- -->*<!-- --> <!-- -->*<!-- --> <!-- -->*<!-- --> /5&quot;</strong></p><p><strong>image: my-awesome-cron-image</strong></p><p>and create it:</p><p><strong>$ kubectl create -f my-crontab.yaml</strong></p><p><strong>crontab &quot;my-new-cron-object&quot; created</strong></p><p>You can then manage our <strong>CronTab</strong> objects using kubectl. Note that resource names are not case-sensitive when using kubectl:</p><p><strong>$ kubectl get crontab</strong></p><p><strong>NAME KIND</strong></p><p><strong>my-new-cron-object CronTab.v1.stable.example.com</strong></p><p>You can also view the raw JSON data. Here you can see that it contains the custom <strong>cronSpec</strong> and <strong>image</strong> fields from the yaml you used to create it:</p><p><strong>$ kubectl get crontab -o json</strong></p><p><strong>{</strong></p><p><strong>&quot;apiVersion&quot;: &quot;v1&quot;,</strong></p><p><strong>&quot;items&quot;: [</strong></p><p><strong>{</strong></p><p><strong>&quot;apiVersion&quot;: &quot;stable.example.com/v1&quot;,</strong></p><p><strong>&quot;cronSpec&quot;: &quot;<!-- -->*<!-- --> <!-- -->*<!-- --> <!-- -->*<!-- --> <!-- -->*<!-- --> /5&quot;,</strong></p><p><strong>&quot;image&quot;: &quot;my-awesome-cron-image&quot;,</strong></p><p><strong>&quot;kind&quot;: &quot;CronTab&quot;,</strong></p><p><strong>&quot;metadata&quot;: {</strong></p><p><strong>&quot;creationTimestamp&quot;: &quot;2016-09-29T04:59:00Z&quot;,</strong></p><p><strong>&quot;name&quot;: &quot;my-new-cron-object&quot;,</strong></p><p><strong>&quot;namespace&quot;: &quot;default&quot;,</strong></p><p><strong>&quot;resourceVersion&quot;: &quot;12601503&quot;,</strong></p><p><strong>&quot;selfLink&quot;: &quot;/apis/stable.example.com/v1/namespaces/default/crontabs/my-new-cron-object&quot;,</strong></p><p><strong>&quot;uid&quot;: &quot;6f65e7a3-8601-11e6-a23e-42010af0000c&quot;</strong></p><p><strong>}</strong></p><p><strong>}</strong></p><p><strong>],</strong></p><p><strong>&quot;kind&quot;: &quot;List&quot;,</strong></p><p><strong>&quot;metadata&quot;: {},</strong></p><p><strong>&quot;resourceVersion&quot;: &quot;&quot;,</strong></p><p><strong>&quot;selfLink&quot;: &quot;&quot;</strong></p><p><strong>}</strong></p><h4>What&#x27;s next</h4><ul><li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/migrate-third-party-resource/">Migrate a ThirdPartyResource to a CustomResourceDefinition</a></li><li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/">Extend the Kubernetes API with CustomResourceDefinitions</a></li><li><a href="https://v1-7.docs.kubernetes.io/docs/reference/v1.7/#thirdpartyresource-v1beta1-extensions">ThirdPartyResource</a></li></ul><h3>Migrate a ThirdPartyResource to CustomResourceDefinition</h3><p>This page shows how to migrate data stored in a ThirdPartyResource (TPR) to a<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#customresourcedefinition-v1beta1-apiextensions">CustomResourceDefinition</a> (CRD).</p><p>Kubernetes does not automatically migrate existing TPRs. This is due to API changes introduced as part of <a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/thirdpartyresources.md">graduating to beta</a> under a new name and API group. Instead, both TPR and CRD are available and operate independently in Kubernetes 1.7. Users must migrate each TPR one by one to preserve their data before upgrading to Kubernetes 1.8.</p><p>The simplest way to migrate is to stop all clients that use a given TPR, then delete the TPR and start from scratch with a CRD. This page describes an optional process that eases the transition by migrating existing TPR data for you <strong>on a best-effort basis</strong>.</p><ul><li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/migrate-third-party-resource/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/migrate-third-party-resource/#migrate-tpr-data"><strong>Migrate TPR data</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/migrate-third-party-resource/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Before you begin</h4><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using<a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <strong>kubectl version</strong>.</p><ul><li>Make sure your Kubernetes cluster has a <strong>master version of exactly 1.7.x</strong> (any patch release), as this is the only version that supports both TPR and CRD.</li><li>If you use a TPR-based custom controller, check with the author of the controller first. Some or all of these steps may be unnecessary if the custom controller handles the migration for you.</li><li>Be familiar with the concept of <a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/">custom resources</a>, which were known as third-party resources until Kubernetes 1.7.</li><li>Be familiar with <a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/#customresourcedefinitions">CustomResourceDefinitions</a>, which are a simple way to implement custom resources.</li><li><strong>Before performing a migration on real data, conduct a dry run by going through these steps in a test cluster.</strong></li></ul><h4>Migrate TPR data</h4><ol><li><strong>Rewrite the TPR definition</strong></li></ol><p>Clients that access the REST API for your custom resource should not need any changes. However, you will need to rewrite your TPR definition as a CRD.</p><p>Make sure you specify values for the CRD fields that match what the server used to fill in for you with TPR.</p><p>For example, if your ThirdPartyResource looks like this:</p><p><strong>apiVersion: extensions/v1beta1</strong></p><p><strong>kind: ThirdPartyResource</strong></p><p><strong>metadata:</strong></p><p><strong>name: cron-tab.stable.example.com</strong></p><p><strong>description: &quot;A specification of a Pod to run on a cron style schedule&quot;</strong></p><p><strong>versions:</strong></p><p><strong>- name: v1</strong></p><p>A matching CustomResourceDefinition could look like this:</p><p><strong>apiVersion: apiextensions.k8s.io/v1beta1</strong></p><p><strong>kind: CustomResourceDefinition</strong></p><p><strong>metadata:</strong></p><p><strong>name: crontabs.stable.example.com</strong></p><p><strong>spec:</strong></p><p><strong>scope: Namespaced</strong></p><p><strong>group: stable.example.com</strong></p><p><strong>version: v1</strong></p><p><strong>names:</strong></p><p><strong>kind: CronTab</strong></p><p><strong>plural: crontabs</strong></p><p><strong>singular: crontab</strong></p><ol><li><strong>Install the CustomResourceDefinition</strong></li></ol><p>While the source TPR is still active, install the matching CRD with <strong>kubectl create</strong>. Existing TPR data remains accessible because TPRs take precedence over CRDs when both try to serve the same resource.</p><p>After you create the CRD, make sure the Established condition goes to True. You can check it with a command like this:</p><p><strong>kubectl get crd -o \&#x27;custom-columns=NAME:{.metadata.name},ESTABLISHED:{.status.conditions<!-- -->[?(@.type==&quot;Established&quot;)]<!-- -->.status}\&#x27;</strong></p><p>The output should look like this:</p><p><strong>NAME ESTABLISHED</strong></p><p><strong>crontabs.stable.example.com True</strong></p><ol><li><strong>Stop all clients that use the TPR</strong></li></ol><p>The API server attempts to prevent TPR data for the resource from changing while it copies objects to the CRD, but it can&#x27;t guarantee consistency in all cases, such as with <a href="https://kubernetes.io/docs/admin/high-availability/">multiple masters</a>. Stopping clients, such as TPR-based custom controllers, helps to avoid inconsistencies in the copied data.</p><p>In addition, clients that watch TPR data do not receive any more events once the migration begins. You must restart them after the migration completes so they start watching CRD data instead.</p><ol><li><strong>Back up TPR data</strong></li></ol><p>In case the data migration fails, save a copy of existing data for the resource:</p><p><strong>kubectl get crontabs --all-namespaces -o yaml &gt; crontabs.yaml</strong></p><p>You should also save a copy of the TPR definition if you don&#x27;t have one already:</p><p><strong>kubectl get thirdpartyresource cron-tab.stable.example.com -o yaml --export &gt; tpr.yaml</strong></p><ol><li><strong>Delete the TPR definition</strong></li></ol><p>Normally, when you delete a TPR definition, the API server tries to clean up any objects stored in that resource. Because a matching CRD exists, the server copies objects to the CRD instead of deleting them.</p><p><strong>kubectl delete thirdpartyresource cron-tab.stable.example.com</strong></p><ol><li><strong>Verify the new CRD data</strong></li></ol><p>It can take up to 10 seconds for the TPR controller to notice when you delete the TPR definition and to initiate the migration. The TPR data remains accessible during this time.</p><p>Once the migration completes, the resource begins serving through the CRD. Check that all your objects were correctly copied:</p><p><strong>kubectl get crontabs --all-namespaces -o yaml</strong></p><p>If the copy failed, you can quickly revert to the set of objects that existed just before the migration by recreating the TPR definition:</p><p><strong>kubectl create -f tpr.yaml</strong></p><ol><li><strong>Restart clients</strong></li></ol><p>After verifying the CRD data, restart any clients you stopped before the migration, such as custom controllers and other watchers. These clients now access CRD data when they make requests on the same API endpoints that the TPR previously served.</p><h4>What&#x27;s next</h4><ul><li>Learn more about <a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/">custom resources</a>.</li><li>Learn more about <a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/">using CustomResourceDefinitions</a>.</li><li>See <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#customresourcedefinition-v1beta1-apiextensions">CustomResourceDefinition</a>.</li></ul><h3>Configure the aggregation layer</h3><p>Configuring the <a href="https://kubernetes.io/docs/concepts/api-extension/apiserver-aggregation/">aggregation layer</a> allows the Kubernetes apiserver to be extended with additional APIs, which are not part of the core Kubernetes APIs.</p><ul><li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/configure-aggregation-layer/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/configure-aggregation-layer/#enable-apiserver-flags"><strong>Enable apiserver flags</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/configure-aggregation-layer/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Before you begin</h4><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using<a href="https://kubernetes.io/docs/getting-started-guides/minikube">Minikube</a>, or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li><li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <strong>kubectl version</strong>.</p><p><strong>Note:</strong> There are a few setup requirements for getting the aggregation layer working in your environment to support mutual TLS auth between the proxy and extension apiservers. Kubernetes and the kube-apiserver have multiple CAs, so make sure that the proxy is signed by the aggregation layer CA and not by something else, like the master CA.</p><h4>Enable apiserver flags</h4><p>Enable the aggregation layer via the following kube-apiserver flags. They may have already been taken care of by your provider.</p><p><strong>--requestheader-client-ca-file=<code>&lt;path to aggregator CA cert&gt;</code></strong></p><p><strong>--requestheader-allowed-names=aggregator</strong></p><p><strong>--requestheader-extra-headers-prefix=X-Remote-Extra-</strong></p><p><strong>--requestheader-group-headers=X-Remote-Group</strong></p><p><strong>--requestheader-username-headers=X-Remote-User</strong></p><p><strong>--proxy-client-cert-file=<code>&lt;path to aggregator proxy cert&gt;</code></strong></p><p><strong>--proxy-client-key-file=<code>&lt;path to aggregator proxy key&gt;</code></strong></p><p>If you are not running kube-proxy on a host running the API server, then you must make sure that the system is enabled with the following apiserver flag:</p><p><strong>--enable-aggregator-routing=true</strong></p><h4>What&#x27;s next</h4><ul><li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/setup-extension-api-server/">Setup an extension api-server</a> to work with the aggregation layer.</li><li>For a high level overview, see <a href="https://kubernetes.io/docs/concepts/api-extension/apiserver-aggregation/">Extending the Kubernetes API with the aggregation layer</a>.</li><li>Learn how to <a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/">Extend the Kubernetes API Using Custom Resource Definitions</a>.</li></ul><h3>Setup an extension API server</h3><p>Setting up an extension API server to work the aggregation layer allows the Kubernetes apiserver to be extended with additional APIs, which are not part of the core Kubernetes APIs.</p><ul><li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/setup-extension-api-server/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/setup-extension-api-server/#setup-an-extension-api-server-to-work-with-the-aggregation-layer"><strong>Setup an extension api-server to work with the aggregation layer</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/setup-extension-api-server/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Before you begin</h4><ul><li>You need to have a Kubernetes cluster running.</li><li>You must <a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/configure-aggregation-layer/">configure the aggregation layer</a> and enable the apiserver flags.</li></ul><h4>Setup an extension api-server to work with the aggregation layer</h4><p>The following steps describe how to set up an extension-apiserver at a high level. These steps apply regardless if you&#x27;re using YAML configs or using APIs. An attempt is made to specifically identify any differences between the two. For a concrete example of how they can be implemented using YAML configs, you can look at the <a href="https://github.com/kubernetes/sample-apiserver/blob/master/README.md">sample-apiserver</a> in the Kubernetes repo.</p><p>Alternatively, you can use an existing 3rd party solution, such as <a href="https://github.com/Kubernetes-incubator/apiserver-builder/blob/master/README.md">apiserver-builder</a>, which should generate a skeleton and automate all of the following steps for you.</p><ol><li>Make sure the APIService API is enabled (check <strong>--runtime-config</strong>). It should be on by default, unless it&#x27;s been deliberately turned off in your cluster.</li><li>You may need to make an RBAC rule allowing you to add APIService objects, or get your cluster administrator to make one. (Since API extensions affect the entire cluster, it is not recommended to do testing/development/debug of an API extension in a live cluster.)</li><li>Create the Kubernetes namespace you want to run your extension api-service in.</li><li>Create/get a CA cert to be used to sign the server cert the extension api-server uses for HTTPS.</li><li>Create a server cert/key for the api-server to use for HTTPS. This cert should be signed by the above CA. It should also have a CN of the Kube DNS name. This is derived from the Kubernetes service and be of the form ..svc</li><li>Create a Kubernetes secret with the server cert/key in your namespace.</li><li>Create a Kubernetes deployment for the extension api-server and make sure you are loading the secret as a volume. It should contain a reference to a working image of your extension api-server. The deployment should also be in your namespace.</li><li>Make sure that your extension-apiserver loads those certs from that volume and that they are used in the HTTPS handshake.</li><li>Create a Kubernetes service account in your namespace.</li><li>Create a Kubernetes cluster role for the operations you want to allow on your resources.</li><li>Create a Kubernetes cluster role binding from the service account in your namespace to the cluster role you just created.</li><li>Create a Kubernetes cluster role binding from the service account in your namespace to the <strong>system:auth-delegator</strong> cluster role to delegate auth decisions to the Kubernetes core API server.</li><li>Create a Kubernetes role binding from the service account in your namespace to the <strong>extension-apiserver-authentication-reader</strong> role. This allows your extension api-server to access the <strong>extension-apiserver-authentication</strong> configmap.</li><li>Create a Kubernetes apiservice. The CA cert above should be base64 encoded, stripped of new lines and used as the spec.caBundle in the apiservice. This should not be namespaced. If using the <a href="https://github.com/kubernetes/kube-aggregator/">kube-aggregator API</a>, only pass in the PEM encoded CA bundle because the base 64 encoding is done for you.</li><li>Use kubectl to get your resource. It should return &quot;No resources found.&quot; Which means that everything worked but you currently have no objects of that resource type created yet.</li></ol><h4>What&#x27;s next</h4><ul><li>If you haven&#x27;t already, <a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/configure-aggregation-layer/">configure the aggregation layer</a> and enable the apiserver flags.</li><li>For a high level overview, see <a href="https://kubernetes.io/docs/concepts/api-extension/apiserver-aggregation">Extending the Kubernetes API with the aggregation layer</a>.</li><li>Learn how to <a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/">Extend the Kubernetes API Using Custom Resource Definitions</a>.</li></ul><h3>Install Service Catalog using Helm</h3><p>Service Catalog is an extension API that enables applications running in Kubernetes clusters to easily use external managed software offerings, such as a datastore service offered by a cloud provider.</p><p>It provides a way to list, provision, and bind with external <a href="https://kubernetes.io/docs/reference/glossary/?all=true#term-managed-service">Managed Services</a> from <a href="https://kubernetes.io/docs/reference/glossary/?all=true#term-service-broker">Service Brokers</a>without needing detailed knowledge about how those services are created or managed.</p><p>Use <a href="https://helm.sh/">Helm</a> to install Service Catalog on your Kubernetes cluster. Up to date information on this process can be found at the <a href="https://github.com/kubernetes-incubator/service-catalog/blob/master/docs/install.md">kubernetes-incubator/service-catalog</a> repo.</p><ul><li><a href="https://kubernetes.io/docs/tasks/service-catalog/install-service-catalog-using-helm/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/service-catalog/install-service-catalog-using-helm/#add-the-service-catalog-helm-repository"><strong>Add the service-catalog Helm repository</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/service-catalog/install-service-catalog-using-helm/#enable-rbac"><strong>Enable RBAC</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/service-catalog/install-service-catalog-using-helm/#install-service-catalog-in-your-kubernetes-cluster"><strong>Install Service Catalog in your Kubernetes cluster</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/service-catalog/install-service-catalog-using-helm/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Before you begin</h4><ul><li>Understand the key concepts of <a href="https://kubernetes.io/docs/concepts/service-catalog/">Service Catalog</a>.</li><li>Service Catalog requires a Kubernetes cluster running version 1.7 or higher.</li><li>You must have a Kubernetes cluster with cluster DNS enabled.<ul><li>If you are using a cloud-based Kubernetes cluster or <a href="https://kubernetes.io/docs/getting-started-guides/minikube/">Minikube</a>, you may already have cluster DNS enabled.</li><li>If you are using <strong>hack/local-up-cluster.sh</strong>, ensure that the <strong>KUBE_ENABLE_CLUSTER_DNS</strong>environment variable is set, then run the install script.</li></ul></li><li><a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">Install and setup kubectl</a> v1.7 or higher. Make sure it is configured to connect to the Kubernetes cluster.</li><li>Install <a href="http://helm.sh/">Helm</a> v2.7.0 or newer.<ul><li>Follow the <a href="https://github.com/kubernetes/helm/blob/master/docs/install.md">Helm install instructions</a>.</li><li>If you already have an appropriate version of Helm installed, execute <strong>helm init</strong> to install Tiller, the server-side component of Helm.</li></ul></li></ul><h4>Add the service-catalog Helm repository</h4><p>Once Helm is installed, add the service-catalog Helm repository to your local machine by executing the following command:</p><p><strong>helm repo add svc-cat <a href="https://svc-catalog-charts.storage.googleapis.com">https://svc-catalog-charts.storage.googleapis.com</a></strong></p><p>Check to make sure that it installed successfully by executing the following command:</p><p><strong>helm search service-catalog</strong></p><p>If the installation was successful, the command should output the following:</p><p><strong>NAME VERSION DESCRIPTION</strong></p><p><strong>svc-cat/catalog 0.0.1 service-catalog API server and controller-manag<!-- -->.<!-- -->..</strong></p><h4>Enable RBAC</h4><p>Your Kubernetes cluster must have RBAC enabled, which requires your Tiller Pod(s) to have <strong>cluster-admin</strong> access.</p><p>If you are using Minikube, run the <strong>minikube start</strong> command with the following flag:</p><p><strong>minikube start --extra-config=apiserver.Authorization.Mode=RBAC</strong></p><p>If you are using <strong>hack/local-up-cluster.sh</strong>, set the <strong>AUTHORIZATION_MODE</strong> environment variable with the following values:</p><p><strong>AUTHORIZATION_MODE=Node,RBAC hack/local-up-cluster.sh -O</strong></p><p>By default, <strong>helm init</strong> installs the Tiller Pod into the <strong>kube-system</strong> namespace, with Tiller configured to use the <strong>default</strong> service account.</p><p><strong>NOTE:</strong> If you used the <strong>--tiller-namespace</strong> or <strong>--service-account</strong> flags when running <strong>helm init</strong>, the <strong>--serviceaccount</strong> flag in the following command needs to be adjusted to reference the appropriate namespace and ServiceAccount name.</p><p>Configure Tiller to have <strong>cluster-admin</strong> access:</p><p><strong>kubectl create clusterrolebinding tiller-cluster-admin <!-- -->\</strong></p><p><strong>--clusterrole=cluster-admin <!-- -->\</strong></p><p><strong>--serviceaccount=kube-system:default</strong></p><h4>Install Service Catalog in your Kubernetes cluster</h4><p>Install Service Catalog from the root of the Helm repository using the following command:</p><p><strong>helm install svc-cat/catalog <!-- -->\</strong></p><p><strong>--name catalog --namespace catalog</strong></p><h4>What&#x27;s next</h4><ul><li>View <a href="https://github.com/openservicebrokerapi/servicebroker/blob/master/gettingStarted.md#sample-service-brokers">sample service brokers</a>.</li><li>Explore the <a href="https://github.com/kubernetes-incubator/service-catalog">kubernetes-incubator/service-catalog</a> project.</li></ul><h3>Install Service Catalog using SC</h3><p>Service Catalog is an extension API that enables applications running in Kubernetes clusters to easily use external managed software offerings, such as a datastore service offered by a cloud provider.</p><p>It provides a way to list, provision, and bind with external <a href="https://kubernetes.io/docs/reference/glossary/?all=true#term-managed-service">Managed Services</a> from <a href="https://kubernetes.io/docs/reference/glossary/?all=true#term-service-broker">Service Brokers</a>without needing detailed knowledge about how those services are created or managed.</p><p>Use the <a href="https://github.com/GoogleCloudPlatform/k8s-service-catalog#installation">Service Catalog Installer</a> tool to easily install or uninstall Service Catalog on your Kubernetes cluster. This CLI tool is installed as <strong>sc</strong> in your local environment.</p><ul><li><a href="https://kubernetes.io/docs/tasks/service-catalog/install-service-catalog-using-sc/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/service-catalog/install-service-catalog-using-sc/#install-sc-in-your-local-environment"><strong>Install sc in your local environment</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/service-catalog/install-service-catalog-using-sc/#install-service-catalog-in-your-kubernetes-cluster"><strong>Install Service Catalog in your Kubernetes cluster</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/service-catalog/install-service-catalog-using-sc/#uninstall-service-catalog"><strong>Uninstall Service Catalog</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/service-catalog/install-service-catalog-using-sc/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Before you begin</h4><ul><li>Understand the key concepts of <a href="https://kubernetes.io/docs/concepts/service-catalog/">Service Catalog</a>.</li><li>Install <a href="https://golang.org/dl/">Go 1.6+</a> and set the <strong>GOPATH</strong>.</li><li>Install the <a href="https://github.com/cloudflare/cfssl">cfssl</a> tool needed for generating SSL artifacts.</li><li>Service Catalog requires Kubernetes version 1.7+.</li><li><a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">Install and setup kubectl</a> so that it is configured to connect to a Kubernetes v1.7+ cluster.</li><li>The kubectl user must be bound to the cluster-admin role for it to install Service Catalog. To ensure that this is true, run the following command:</li><li><strong>kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=<code>&lt;user-name&gt;</code></strong></li></ul><h4>Install sc in your local environment</h4><p>Install the <strong>sc</strong> CLI tool using the <strong>go get</strong> command:</p><p><strong>go get github.com/GoogleCloudPlatform/k8s-service-catalog/installer/cmd/sc</strong></p><p>After running the above command, <strong>sc</strong> should be installed in your <strong>GOPATH/bin</strong> directory.</p><h4>Install Service Catalog in your Kubernetes cluster</h4><p>First, verify that all dependencies have been installed. Run:</p><p><strong>sc check</strong></p><p>If the check is successful, it should return:</p><p><strong>Dependency check passed. You are good to go.</strong></p><p>Next, run the install command and specify the <strong>storageclass</strong> that you want to use for the backup:</p><p><strong>sc install --etcd-backup-storageclass &quot;standard&quot;</strong></p><h4>Uninstall Service Catalog</h4><p>If you would like to uninstall Service Catalog from your Kubernetes cluster using the <strong>sc</strong> tool, run:</p><p><strong>sc uninstall</strong></p><h4>What&#x27;s next</h4><ul><li>View <a href="https://github.com/openservicebrokerapi/servicebroker/blob/master/gettingStarted.md#sample-service-brokers">sample service brokers</a>.</li><li>Explore the <a href="https://github.com/kubernetes-incubator/service-catalog">kubernetes-incubator/service-catalog</a> project.</li></ul><h2>TLS</h2><h3>Manage TLS Certificates in a Cluster</h3><ul><li><a href="https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/#overview"><strong>Overview</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/#trusting-tls-in-a-cluster"><strong>Trusting TLS in a Cluster</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/#requesting-a-certificate"><strong>Requesting a Certificate</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/#step-0-download-and-install-cfssl"><strong>Step 0. Download and install CFSSL</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/#step-1-create-a-certificate-signing-request"><strong>Step 1. Create a Certificate Signing Request</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/#step-2-create-a-certificate-signing-request-object-to-send-to-the-kubernetes-api"><strong>Step 2. Create a Certificate Signing Request object to send to the Kubernetes API</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/#step-3-get-the-certificate-signing-request-approved"><strong>Step 3. Get the Certificate Signing Request Approved</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/#step-4-download-the-certificate-and-use-it"><strong>Step 4. Download the Certificate and Use It</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/#approving-certificate-signing-requests"><strong>Approving Certificate Signing Requests</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/#a-word-of-warning-on-the-approval-permission"><strong>A Word of Warning on the Approval Permission</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/#a-note-to-cluster-administrators"><strong>A Note to Cluster Administrators</strong></a></li></ul><h4>Overview</h4><p>Every Kubernetes cluster has a cluster root Certificate Authority (CA). The CA is generally used by cluster components to validate the API server&#x27;s certificate, by the API server to validate kubelet client certificates, etc. To support this, the CA certificate bundle is distributed to every node in the cluster and is distributed as a secret attached to default service accounts. Optionally, your workloads can use this CA to establish trust. Your application can request a certificate signing using the <strong>certificates.k8s.io</strong> API using a protocol that is similar to the <a href="https://github.com/ietf-wg-acme/acme/">ACME draft</a>.</p><h4>Trusting TLS in a Cluster</h4><p>Trusting the cluster root CA from an application running as a pod usually requires some extra application configuration. You will need to add the CA certificate bundle to the list of CA certificates that the TLS client or server trusts. For example, you would do this with a golang TLS config by parsing the certificate chain and adding the parsed certificates to the <strong>Certificates</strong> field in the <a href="https://godoc.org/crypto/tls#Config"><strong>tls.Config</strong></a> struct.</p><p>The CA certificate bundle is automatically mounted into pods using the default service account at the path <strong>/var/run/secrets/kubernetes.io/serviceaccount/ca.crt</strong>. If you are not using the default service account, ask a cluster administrator to build a configmap containing the certificate bundle that you have access to use.</p><h4>Requesting a Certificate</h4><p>The following section demonstrates how to create a TLS certificate for a Kubernetes service accessed through DNS.</p><p><strong>Note:</strong> This tutorial uses CFSSL: Cloudflare&#x27;s PKI and TLS toolkit <a href="https://blog.cloudflare.com/introducing-cfssl/">click here</a> to know more.</p><h5><strong>Step 0. Download and install CFSSL</strong></h5><p>The cfssl tools used in this example can be downloaded at <code style="background-color:lightgray">&lt;https://pkg.cfssl.org/&gt;</code>.</p><h5><strong>Step 1. Create a Certificate Signing Request</strong></h5><p>Generate a private key and certificate signing request (or CSR) by running the following command:</p><p><strong>$ cat &lt;&lt;EOF | cfssl genkey - | cfssljson -bare server</strong></p><p><strong>{</strong></p><p><strong>&quot;hosts&quot;: [</strong></p><p><strong>&quot;my-svc.my-namespace.svc.cluster.local&quot;,</strong></p><p><strong>&quot;my-pod.my-namespace.pod.cluster.local&quot;,</strong></p><p><strong>&quot;172.168.0.24&quot;,</strong></p><p><strong>&quot;10.0.34.2&quot;</strong></p><p><strong>],</strong></p><p><strong>&quot;CN&quot;: &quot;my-pod.my-namespace.pod.cluster.local&quot;,</strong></p><p><strong>&quot;key&quot;: {</strong></p><p><strong>&quot;algo&quot;: &quot;ecdsa&quot;,</strong></p><p><strong>&quot;size&quot;: 256</strong></p><p><strong>}</strong></p><p><strong>}</strong></p><p><strong>EOF</strong></p><p>Where <strong>172.168.0.24</strong> is the service&#x27;s cluster IP, <strong>my-svc.my-namespace.svc.cluster.local</strong> is the service&#x27;s DNS name, <strong>10.0.34.2</strong> is the pod&#x27;s IP and <strong>my-pod.my-namespace.pod.cluster.local</strong> is the pod&#x27;s DNS name. You should see the following output:</p><p><strong>2017/03/21 06:48:17 <!-- -->[INFO]<!-- --> generate received request</strong></p><p><strong>2017/03/21 06:48:17 <!-- -->[INFO]<!-- --> received CSR</strong></p><p><strong>2017/03/21 06:48:17 <!-- -->[INFO]<!-- --> generating key: ecdsa-256</strong></p><p><strong>2017/03/21 06:48:17 <!-- -->[INFO]<!-- --> encoded CSR</strong></p><p>This command generates two files; it generates <strong>server.csr</strong> containing the PEM encoded <a href="https://tools.ietf.org/html/rfc2986">pkcs#10</a>certification request, and <strong>server-key.pem</strong> containing the PEM encoded key to the certificate that is still to be created.</p><h5><strong>Step 2. Create a Certificate Signing Request object to send to the Kubernetes API</strong></h5><p>Generate a CSR yaml blob and send it to the apiserver by running the following command:</p><p><strong>$ cat &lt;&lt;EOF | kubectl create -f -</strong></p><p><strong>apiVersion: certificates.k8s.io/v1beta1</strong></p><p><strong>kind: CertificateSigningRequest</strong></p><p><strong>metadata:</strong></p><p><strong>name: my-svc.my-namespace</strong></p><p><strong>spec:</strong></p><p><strong>groups:</strong></p><p><strong>- system:authenticated</strong></p><p><strong>request: $(cat server.csr | base64 | tr -d \&#x27;<!-- -->\<!-- -->n\&#x27;)</strong></p><p><strong>usages:</strong></p><p><strong>- digital signature</strong></p><p><strong>- key encipherment</strong></p><p><strong>- server auth</strong></p><p><strong>EOF</strong></p><p>Notice that the <strong>server.csr</strong> file created in step 1 is base64 encoded and stashed in the <strong>.spec.request</strong> field. We are also requesting a certificate with the &quot;digital signature&quot;, &quot;key encipherment&quot;, and &quot;server auth&quot; key usages. We support all key usages and extended key usages listed <a href="https://godoc.org/k8s.io/api/certificates/v1beta1#KeyUsage">here</a> so you can request client certificates and other certificates using this same API.</p><p>The CSR should now be visible from the API in a Pending state. You can see it by running:</p><p><strong>$ kubectl describe csr my-svc.my-namespace</strong></p><p><strong>Name: my-svc.my-namespace</strong></p><p><strong>Labels: <code>&lt;none&gt;</code></strong></p><p><strong>Annotations: <code>&lt;none&gt;</code></strong></p><p><strong>CreationTimestamp: Tue, 21 Mar 2017 07:03:51 -0700</strong></p><p><strong>Requesting User: <a href="mailto:yourname@example.com">yourname@example.com</a></strong></p><p><strong>Status: Pending</strong></p><p><strong>Subject:</strong></p><p><strong>Common Name: my-svc.my-namespace.svc.cluster.local</strong></p><p><strong>Serial Number:</strong></p><p><strong>Subject Alternative Names:</strong></p><p><strong>DNS Names: my-svc.my-namespace.svc.cluster.local</strong></p><p><strong>IP Addresses: 172.168.0.24</strong></p><p><strong>10.0.34.2</strong></p><p><strong>Events: <code>&lt;none&gt;</code></strong></p><h5><strong>Step 3. Get the Certificate Signing Request Approved</strong></h5><p>Approving the certificate signing request is either done by an automated approval process or on a one off basis by a cluster administrator. More information on what this involves is covered below.</p><h5><strong>Step 4. Download the Certificate and Use It</strong></h5><p>Once the CSR is signed and approved you should see the following:</p><p><strong>$ kubectl get csr</strong></p><p><strong>NAME AGE REQUESTOR CONDITION</strong></p><p><strong>my-svc.my-namespace 10m <a href="mailto:yourname@example.com">yourname@example.com</a> Approved,Issued</strong></p><p>You can download the issued certificate and save it to a <strong>server.crt</strong> file by running the following:</p><p><strong>$ kubectl get csr my-svc.my-namespace -o jsonpath=\&#x27;{.status.certificate}\&#x27; <!-- -->\</strong></p><p><strong>| base64 -d &gt; server.crt</strong></p><p>Now you can use <strong>server.crt</strong> and <strong>server-key.pem</strong> as the keypair to start your HTTPS server.</p><h4>Approving Certificate Signing Requests</h4><p>A Kubernetes administrator (with appropriate permissions) can manually approve (or deny) Certificate Signing Requests by using the <strong>kubectl certificate approve</strong> and <strong>kubectl certificate deny</strong> commands. However if you intend to make heavy usage of this API, you might consider writing an automated certificates controller.</p><p>Whether a machine or a human using kubectl as above, the role of the approver is to verify that the CSR satisfies two requirements:</p><ol><li>The subject of the CSR controls the private key used to sign the CSR. This addresses the threat of a third party masquerading as an authorized subject. In the above example, this step would be to verify that the pod controls the private key used to generate the CSR.</li><li>The subject of the CSR is authorized to act in the requested context. This addresses the threat of an undesired subject joining the cluster. In the above example, this step would be to verify that the pod is allowed to participate in the requested service.</li></ol><p>If and only if these two requirements are met, the approver should approve the CSR and otherwise should deny the CSR.</p><h4>A Word of *<strong>*Warning**</strong> on the Approval Permission</h4><p>The ability to approve CSRs decides who trusts who within the cluster. This includes who the Kubernetes API trusts. The ability to approve CSRs should not be granted broadly or lightly. The requirements of the challenge noted in the previous section and the repercussions of issuing a specific certificate should be fully understood before granting this permission. See <a href="https://kubernetes.io/docs/admin/authentication#x509-client-certs">here</a> for information on how certificates interact with authentication.</p><h4>A Note to Cluster Administrators</h4><p>This tutorial assumes that a signer is setup to serve the certificates API. The Kubernetes controller manager provides a default implementation of a signer. To enable it, pass the <strong>--cluster-signing-cert-file</strong> and <strong>--cluster-signing-key-file</strong> parameters to the controller manager with paths to your Certificate Authority&#x27;s keypair.</p><h3>Certificate Rotation</h3><p>This page shows how to enable and configure certificate rotation for the kubelet.</p><ul><li><a href="https://kubernetes.io/docs/tasks/tls/certificate-rotation/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/tls/certificate-rotation/#overview"><strong>Overview</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/tls/certificate-rotation/#enabling-client-certificate-rotation"><strong>Enabling client certificate rotation</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/tls/certificate-rotation/#understanding-the-certificate-rotation-configuration"><strong>Understanding the certificate rotation configuration</strong></a></li></ul><h4>Before you begin</h4><ul><li>Kubernetes version 1.8.0 or later is required</li><li>Kubelet certificate rotation is beta in 1.8.0 which means it may change without notice.</li></ul><h3>Overview</h3><p>The kubelet uses certificates for authenticating to the Kubernetes API. By default, these certificates are issued with one year expiration so that they do not need to be renewed too frequently.</p><p>Kubernetes 1.8 contains <a href="https://kubernetes.io/docs/tasks/administer-cluster/certificate-rotation/">kubelet certificate rotation</a>, a beta feature that will automatically generate a new key and request a new certificate from the Kubernetes API as the current certificate approaches expiration. Once the new certificate is available, it will be used for authenticating connections to the Kubernetes API.</p><h3>Enabling client certificate rotation</h3><p>The <strong>kubelet</strong> process accepts an argument <strong>--rotate-certificates</strong> that controls if the kubelet will automatically request a new certificate as the expiration of the certificate currently in use approaches. Since certificate rotation is a beta feature, the feature flag must also be enabled with <strong>--feature-gates=RotateKubeletClientCertificate=true</strong>.</p><p>The <strong>kube-controller-manager</strong> process accepts an argument <strong>--experimental-cluster-signing-duration</strong> that controls how long certificates will be issued for.</p><h3>Understanding the certificate rotation configuration</h3><p>When a kubelet starts up, if it is configured to bootstrap (using the <strong>--bootstrap-kubeconfig</strong> flag), it will use its initial certificate to connect to the Kubernetes API and issue a certificate signing request. You can view the status of certificate signing requests using:</p><p><strong>kubectl get csr</strong></p><p>Initially a certificate signing request from the kubelet on a node will have a status of <strong>Pending</strong>. If the certificate signing requests meets specific criteria, it will be auto approved by the controller manager, then it will have a status of <strong>Approved</strong>. Next, the controller manager will sign a certificate, issued for the duration specified by the <strong>--experimental-cluster-signing-duration</strong> parameter, and the signed certificate will be attached to the certificate signing requests.</p><p>The kubelet will retrieve the signed certificate from the Kubernetes API and write that to disk, in the location specified by <strong>--cert-dir</strong>. Then the kubelet will use the new certificate to connect to the Kubernetes API.</p><p>As the expiration of the signed certificate approaches, the kubelet will automatically issue a new certificate signing request, using the Kubernetes API. Again, the controller manager will automatically approve the certificate request and attach a signed certificate to the certificate signing request. The kubelet will retrieve the new signed certificate from the Kubernetes API and write that to disk. Then it will update the connections it has to the Kubernetes API to reconnect using the new certificate.</p><h2>Administer a Cluster</h2><h2>Federation - Run an App on Multiple Clusters</h2><h2>Manage Cluster Daemons</h2><h3>Perform a Rolling Update on a DaemonSet</h3><p>This page shows how to perform a rolling update on a DaemonSet.</p><ul><li><a href="https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/#daemonset-update-strategy"><strong>DaemonSet Update Strategy</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/#caveat-updating-daemonset-created-from-kubernetes-version-15-or-before"><strong>Caveat: Updating DaemonSet created from Kubernetes version 1.5 or before</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/#performing-a-rolling-update"><strong>Performing a Rolling Update</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/#step-1-checking-daemonset-rollingupdate-update-strategy"><strong>Step 1: Checking DaemonSet RollingUpdate update strategy</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/#step-2-creating-a-daemonset-with-rollingupdate-update-strategy"><strong>Step 2: Creating a DaemonSet with RollingUpdate update strategy</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/#step-3-updating-a-daemonset-template"><strong>Step 3: Updating a DaemonSet template</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/#declarative-commands"><strong>Declarative commands</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/#imperative-commands"><strong>Imperative commands</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/#updating-only-the-container-image"><strong>Updating only the container image</strong></a></li></ul></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/#step-4-watching-the-rolling-update-status"><strong>Step 4: Watching the rolling update status</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/#troubleshooting"><strong>Troubleshooting</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/#daemonset-rolling-update-is-stuck"><strong>DaemonSet rolling update is stuck</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/#some-nodes-run-out-of-resources"><strong>Some nodes run out of resources</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/#broken-rollout"><strong>Broken rollout</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/#clock-skew"><strong>Clock skew</strong></a></li></ul></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h4>Before you begin</h4><ul><li>The DaemonSet rolling update feature is only supported in Kubernetes version 1.6 or later.</li></ul><h4>DaemonSet Update Strategy</h4><p>DaemonSet has two update strategy types:</p><ul><li>OnDelete: This is the default update strategy for backward-compatibility. With <strong>OnDelete</strong> update strategy, after you update a DaemonSet template, new DaemonSet pods will only be created when you manually delete old DaemonSet pods. This is the same behavior of DaemonSet in Kubernetes version 1.5 or before.</li><li>RollingUpdate: With <strong>RollingUpdate</strong> update strategy, after you update a DaemonSet template, old DaemonSet pods will be killed, and new DaemonSet pods will be created automatically, in a controlled fashion.</li></ul><h4>Caveat: Updating DaemonSet created from Kubernetes version 1.5 or before</h4><p>If you try a rolling update on a DaemonSet that was created from Kubernetes version 1.5 or before, a rollout will be triggered when you first change the DaemonSet update strategy to <strong>RollingUpdate</strong>, no matter if DaemonSet template is modified or not. If the DaemonSet template is not changed, all existing DaemonSet pods will be restarted (deleted and created).</p><p>Therefore, make sure you want to trigger a rollout before you first switch the strategy to <strong>RollingUpdate</strong>.</p><h4>Performing a Rolling Update</h4><p>To enable the rolling update feature of a DaemonSet, you must set its <strong>.spec.updateStrategy.type</strong> to <strong>RollingUpdate</strong>.</p><p>You may want to set <strong>.spec.updateStrategy.rollingUpdate.maxUnavailable</strong> (default to 1) and <strong>.spec.minReadySeconds</strong> (default to 0) as well.</p><h5><strong>Step 1: Checking DaemonSet </strong>RollingUpdate<strong> update strategy</strong></h5><p>First, check the update strategy of your DaemonSet, and make sure it&#x27;s set to <strong>RollingUpdate</strong>:</p><p><strong>kubectl get ds/<code>&lt;daemonset-name&gt;</code> -o go-template=\&#x27;{{.spec.updateStrategy.type}}{{&quot;<!-- -->\<!-- -->n&quot;}}\&#x27;</strong></p><p>If you haven&#x27;t created the DaemonSet in the system, check your DaemonSet manifest with the following command instead:</p><p><strong>kubectl create -f ds.yaml --dry-run -o go-template=\&#x27;{{.spec.updateStrategy.type}}{{&quot;<!-- -->\<!-- -->n&quot;}}\&#x27;</strong></p><p>The output from both commands should be:</p><p><strong>RollingUpdate</strong></p><p>If the output isn&#x27;t <strong>RollingUpdate</strong>, go back and modify the DaemonSet object or manifest accordingly.</p><h5><strong>Step 2: Creating a DaemonSet with </strong>RollingUpdate<strong> update strategy</strong></h5><p>If you have already created the DaemonSet, you may skip this step and jump to step 3.</p><p>After verifying the update strategy of the DaemonSet manifest, create the DaemonSet:</p><p><strong>kubectl create -f ds.yaml</strong></p><p>Alternatively, use <strong>kubectl apply</strong> to create the same DaemonSet if you plan to update the DaemonSet with <strong>kubectl apply</strong>.</p><p><strong>kubectl apply -f ds.yaml</strong></p><h5><strong>Step 3: Updating a DaemonSet template</strong></h5><p>Any updates to a <strong>RollingUpdate</strong> DaemonSet <strong>.spec.template</strong> will trigger a rolling update. This can be done with several different <strong>kubectl</strong> commands.</p><h6><strong>Declarative commands</strong></h6><p>If you update DaemonSets using <a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/">configuration files</a>, use <strong>kubectl apply</strong>:</p><p><strong>kubectl apply -f ds-v2.yaml</strong></p><h6><strong>Imperative commands</strong></h6><p>If you update DaemonSets using <a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/imperative-command/">imperative commands</a>, use <strong>kubectl edit</strong> or <strong>kubectl patch</strong>:</p><p><strong>kubectl edit ds/<code>&lt;daemonset-name&gt;</code></strong></p><p><strong>kubectl patch ds/<code>&lt;daemonset-name&gt; -p=&lt;strategic-merge-patch&gt;</code></strong></p><p><strong>Updating only the container image</strong></p><p>If you just need to update the container image in the DaemonSet template, i.e. <strong>.spec.template.spec.containers<!-- -->[*]<!-- -->.image</strong>, use <strong>kubectl set image</strong>:</p><p><strong>kubectl set image ds/<code>&lt;daemonset-name&gt; &lt;container-name&gt;=&lt;container-new-image&gt;</code></strong></p><h5><strong>Step 4: Watching the rolling update status</strong></h5><p>Finally, watch the rollout status of the latest DaemonSet rolling update:</p><p><strong>kubectl rollout status ds/<code>&lt;daemonset-name&gt;</code></strong></p><p>When the rollout is complete, the output is similar to this:</p><p><strong>daemonset &quot;<code>&lt;daemonset-name&gt;</code>&quot; successfully rolled out</strong></p><h4>Troubleshooting</h4><h5><strong>DaemonSet rolling update is stuck</strong></h5><p>Sometimes, a DaemonSet rolling update may be stuck. Here are some possible causes:</p><h6><strong>Some nodes run out of resources</strong></h6><p>The rollout is stuck because new DaemonSet pods can&#x27;t be scheduled on at least one node. This is possible when the node is <a href="https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/">running out of resources</a>.</p><p>When this happens, find the nodes that don&#x27;t have the DaemonSet pods scheduled on by comparing the output of <strong>kubectl get nodes</strong> and the output of:</p><p><strong>kubectl get pods -l <code>&lt;daemonset-selector-key&gt;=&lt;daemonset-selector-value&gt;</code> -o wide</strong></p><p>Once you&#x27;ve found those nodes, delete some non-DaemonSet pods from the node to make room for new DaemonSet pods. Note that this will cause service disruption if the deleted pods are not controlled by any controllers, or if the pods aren&#x27;t replicated. This doesn&#x27;t respect<a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-disruption-budget/">PodDisruptionBudget</a> either.</p><h6><strong>Broken rollout</strong></h6><p>If the recent DaemonSet template update is broken, for example, the container is crash looping, or the container image doesn&#x27;t exist (often due to a typo), DaemonSet rollout won&#x27;t progress.</p><p>To fix this, just update the DaemonSet template again. New rollout won&#x27;t be blocked by previous unhealthy rollouts.</p><h6><strong>Clock skew</strong></h6><p>If <strong>.spec.minReadySeconds</strong> is specified in the DaemonSet, clock skew between master and nodes will make DaemonSet unable to detect the right rollout progress.</p><h4>What&#x27;s next</h4><ul><li>See <a href="https://kubernetes.io/docs/tasks/manage-daemon/rollback-daemon-set/">Task: Performing a rollback on a DaemonSet</a></li><li>See <a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/">Concepts: Creating a DaemonSet to adopt existing DaemonSet pods</a></li></ul><h3>Performing a Rollback on a DaemonSet</h3><p>This page shows how to perform a rollback on a DaemonSet.</p><ul><li><a href="https://kubernetes.io/docs/tasks/manage-daemon/rollback-daemon-set/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/manage-daemon/rollback-daemon-set/#performing-a-rollback-on-a-daemonset"><strong>Performing a Rollback on a DaemonSet</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/manage-daemon/rollback-daemon-set/#step-1-find-the-daemonset-revision-you-want-to-roll-back-to"><strong>Step 1: Find the DaemonSet revision you want to roll back to</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/manage-daemon/rollback-daemon-set/#step-2-roll-back-to-a-specific-revision"><strong>Step 2: Roll back to a specific revision</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/manage-daemon/rollback-daemon-set/#step-3-watch-the-progress-of-the-daemonset-rollback"><strong>Step 3: Watch the progress of the DaemonSet rollback</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/manage-daemon/rollback-daemon-set/#understanding-daemonset-revisions"><strong>Understanding DaemonSet Revisions</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/manage-daemon/rollback-daemon-set/#troubleshooting"><strong>Troubleshooting</strong></a></li></ul><h4>Before you begin</h4><ul><li>The DaemonSet rollout history and DaemonSet rollback features are only supported in <strong>kubectl</strong>in Kubernetes version 1.7 or later.</li><li>Make sure you know how to <a href="https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/">perform a rolling update on a DaemonSet</a>.</li></ul><h4>Performing a Rollback on a DaemonSet</h4><h5><strong>Step 1: Find the DaemonSet revision you want to roll back to</strong></h5><p>You can skip this step if you just want to roll back to the last revision.</p><p>List all revisions of a DaemonSet:</p><p><strong>kubectl rollout history daemonset <code>&lt;daemonset-name&gt;</code></strong></p><p>This returns a list of DaemonSet revisions:</p><p><strong>daemonsets &quot;<code>&lt;daemonset-name&gt;</code>&quot;</strong></p><p><strong>REVISION CHANGE-CAUSE</strong></p><p><strong>1 <!-- -->.<!-- -->..</strong></p><p><strong>2 <!-- -->.<!-- -->..</strong></p><p><strong>.<!-- -->..</strong></p><ul><li>Change cause is copied from DaemonSet annotation <strong>kubernetes.io/change-cause</strong> to its revisions upon creation. You may specify <strong>--record=true</strong> in <strong>kubectl</strong> to record the command executed in the change cause annotation.</li></ul><p>To see the details of a specific revision:</p><p><strong>kubectl rollout history daemonset <code>&lt;daemonset-name&gt;</code> --revision=1</strong></p><p>This returns the details of that revision:</p><p><strong>daemonsets &quot;<code>&lt;daemonset-name&gt;</code>&quot; with revision <em>#1</em></strong></p><p><strong>Pod Template:</strong></p><p><strong>Labels: foo=bar</strong></p><p><strong>Containers:</strong></p><p><strong>app:</strong></p><p><strong>Image: <!-- -->.<!-- -->..</strong></p><p><strong>Port: <!-- -->.<!-- -->..</strong></p><p><strong>Environment: <!-- -->.<!-- -->..</strong></p><p><strong>Mounts: <!-- -->.<!-- -->..</strong></p><p><strong>Volumes: <!-- -->.<!-- -->..</strong></p><h5><strong>Step 2: Roll back to a specific revision</strong></h5><p><strong><em># Specify the revision number you get from Step 1 in --to-revision</em></strong></p><p><strong>kubectl rollout undo daemonset <code>&lt;daemonset-name&gt; --to-revision=&lt;revision&gt;</code></strong></p><p>If it succeeds, the command returns:</p><p><strong>daemonset &quot;<code>&lt;daemonset-name&gt;</code>&quot; rolled back</strong></p><p>If <strong>--to-revision</strong> flag is not specified, the last revision will be picked.</p><h5><strong>Step 3: Watch the progress of the DaemonSet rollback</strong></h5><p><strong>kubectl rollout undo daemonset</strong> tells the server to start rolling back the DaemonSet. The real rollback is done asynchronously on the server side.</p><p>To watch the progress of the rollback:</p><p><strong>kubectl rollout status ds/<code>&lt;daemonset-name&gt;</code></strong></p><p>When the rollback is complete, the output is similar to this:</p><p><strong>daemonset &quot;<code>&lt;daemonset-name&gt;</code>&quot; successfully rolled out</strong></p><h4>Understanding DaemonSet Revisions</h4><p>In the previous <strong>kubectl rollout history</strong> step, you got a list of DaemonSet revisions. Each revision is stored in a resource named <strong>ControllerRevision</strong>. <strong>ControllerRevision</strong> is a resource only available in Kubernetes release 1.7 or later.</p><p>To see what is stored in each revision, find the DaemonSet revision raw resources:</p><p><strong>kubectl get controllerrevision -l <code>&lt;daemonset-selector-key&gt;=&lt;daemonset-selector-value&gt;</code></strong></p><p>This returns a list of <strong>ControllerRevisions</strong>:</p><p><strong>NAME CONTROLLER REVISION AGE</strong></p><p><strong><code>&lt;daemonset-name&gt;-&lt;revision-hash&gt; DaemonSet/&lt;daemonset-name&gt;</code> 1 1h</strong></p><p><strong><code>&lt;daemonset-name&gt;-&lt;revision-hash&gt; DaemonSet/&lt;daemonset-name&gt;</code> 2 1h</strong></p><p>Each <strong>ControllerRevision</strong> stores the annotations and template of a DaemonSet revision.</p><p><strong>kubectl rollout undo</strong> takes a specific <strong>ControllerRevision</strong> and replaces DaemonSet template with the template stored in the <strong>ControllerRevision</strong>. <strong>kubectl rollout undo</strong> is equivalent to updating DaemonSet template to a previous revision through other commands, such as <strong>kubectl edit</strong> or <strong>kubectl apply</strong>.</p><p>Note that DaemonSet revisions only roll forward. That is to say, after a rollback is complete, the revision number (<strong>.revision</strong> field) of the <strong>ControllerRevision</strong> being rolled back to will advance. For example, if you have revision 1 and 2 in the system, and roll back from revision 2 to revision 1, the <strong>ControllerRevision</strong> with <strong>.revision: 1</strong> will become <strong>.revision: 3</strong>.</p><h4>Troubleshooting</h4><ul><li>See <a href="https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/#troubleshooting">troubleshooting DaemonSet rolling update</a>.</li></ul><h2>Schedule GPUs</h2><p>Kubernetes includes <strong>experimental</strong> support for managing NVIDIA GPUs spread across nodes. The support for NVIDIA GPUs was added in v1.6 and has gone through multiple backwards incompatible iterations. This page describes how users can consume GPUs across different Kubernetes versions and the current limitations.</p><h3>v1.8 onwards</h3><p><strong>From 1.8 onwards, the recommended way to consume GPUs is to use </strong><a href="https://kubernetes.io/docs/concepts/cluster-administration/device-plugins"><strong>device plugins</strong></a><strong>.</strong></p><p>To enable GPU support through device plugins before 1.10, the <strong>DevicePlugins</strong> feature gate has to be explicitly set to true across the system: <strong>--feature-gates=&quot;DevicePlugins=true&quot;</strong>. This is no longer required starting from 1.10.</p><p>Then you have to install NVIDIA drivers on the nodes and run an NVIDIA GPU device plugin (<a href="https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/#deploying-nvidia-gpu-device-plugin">see below</a>).</p><p>When the above conditions are true, Kubernetes will expose <strong>nvidia.com/gpu</strong> as a schedulable resource.</p><p>You can consume these GPUs from your containers by requesting <strong>nvidia.com/gpu</strong> just like you request <strong>cpu</strong> or <strong>memory</strong>. However, there are some limitations in how you specify the resource requirements when using GPUs:</p><ul><li>GPUs are only supposed to be specified in the <strong>limits</strong> section, which means:<ul><li>You can specify GPU <strong>limits</strong> without specifying <strong>requests</strong> because Kubernetes will use the limit as the request value by default.</li><li>You can specify GPU in both <strong>limits</strong> and <strong>requests</strong> but these two values must be equal.</li><li>You cannot specify GPU <strong>requests</strong> without specifying <strong>limits</strong>.</li></ul></li><li>Containers (and pods) do not share GPUs. There&#x27;s no overcommitting of GPUs.</li><li>Each container can request one or more GPUs. It is not possible to request a fraction of a GPU.</li></ul><p>Here&#x27;s an example:</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: cuda-vector-add</strong></p><p><strong>spec:</strong></p><p><strong>restartPolicy: OnFailure</strong></p><p><strong>containers:</strong></p><p><strong>- name: cuda-vector-add</strong></p><p><strong><em># <a href="https://github.com/kubernetes/kubernetes/blob/v1.7.11/test/images/nvidia-cuda/Dockerfile">https://github.com/kubernetes/kubernetes/blob/v1.7.11/test/images/nvidia-cuda/Dockerfile</a></em></strong></p><p><strong>image: &quot;k8s.gcr.io/cuda-vector-add:v0.1&quot;</strong></p><p><strong>resources:</strong></p><p><strong>limits:</strong></p><p><strong>nvidia.com/gpu: 1 <em># requesting 1 GPU</em></strong></p><h4>Deploying NVIDIA GPU device plugin</h4><p>There are currently two device plugin implementations for NVIDIA GPUs:</p><h5><strong>Official NVIDIA GPU device plugin</strong></h5><p>The <a href="https://github.com/NVIDIA/k8s-device-plugin">official NVIDIA GPU device plugin</a> has the following requirements:</p><ul><li>Kubernetes nodes have to be pre-installed with NVIDIA drivers.</li><li>Kubernetes nodes have to be pre-installed with <a href="https://github.com/NVIDIA/nvidia-docker">nvidia-docker 2.0</a></li><li>nvidia-container-runtime must be configured as the <a href="https://github.com/NVIDIA/k8s-device-plugin#preparing-your-gpu-nodes">default runtime</a> for docker instead of runc.</li><li>NVIDIA drivers <!-- -->~<!-- -->= 361.93</li></ul><p>To deploy the NVIDIA device plugin once your cluster is running and the above requirements are satisfied:</p><p><strong># For Kubernetes v1.8</strong></p><p><strong>kubectl create -f <a href="https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v1.8/nvidia-device-plugin.yml">https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v1.8/nvidia-device-plugin.yml</a></strong></p><p><strong># For Kubernetes v1.9</strong></p><p><strong>kubectl create -f <a href="https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v1.9/nvidia-device-plugin.yml">https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v1.9/nvidia-device-plugin.yml</a></strong></p><p>Report issues with this device plugin to <a href="https://github.com/NVIDIA/k8s-device-plugin">NVIDIA/k8s-device-plugin</a>.</p><h5><strong>NVIDIA GPU device plugin used by GKE/GCE</strong></h5><p>The <a href="https://github.com/GoogleCloudPlatform/container-engine-accelerators/tree/master/cmd/nvidia_gpu">NVIDIA GPU device plugin used by GKE/GCE</a> doesn&#x27;t require using nvidia-docker and should work with any container runtime that is compatible with the Kubernetes Container Runtime Interface (CRI). It&#x27;s tested on <a href="https://cloud.google.com/container-optimized-os/">Container-Optimized OS</a> and has experimental code for Ubuntu from 1.9 onwards.</p><p>On your 1.9 cluster, you can use the following commands to install the NVIDIA drivers and device plugin:</p><p><strong># Install NVIDIA drivers on Container-Optimized OS:</strong></p><p><strong>kubectl create -f <a href="https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/k8s-1.9/daemonset.yaml">https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/k8s-1.9/daemonset.yaml</a></strong></p><p><strong># Install NVIDIA drivers on Ubuntu (experimental):</strong></p><p><strong>kubectl create -f <a href="https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/k8s-1.9/nvidia-driver-installer/ubuntu/daemonset.yaml">https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/k8s-1.9/nvidia-driver-installer/ubuntu/daemonset.yaml</a></strong></p><p><strong># Install the device plugin:</strong></p><p><strong>kubectl create -f <a href="https://raw.githubusercontent.com/kubernetes/kubernetes/release-1.9/cluster/addons/device-plugins/nvidia-gpu/daemonset.yaml">https://raw.githubusercontent.com/kubernetes/kubernetes/release-1.9/cluster/addons/device-plugins/nvidia-gpu/daemonset.yaml</a></strong></p><p>Report issues with this device plugin and installation method to <a href="https://github.com/GoogleCloudPlatform/container-engine-accelerators">GoogleCloudPlatform/container-engine-accelerators</a>.</p><h3>Clusters containing different types of NVIDIA GPUs</h3><p>If different nodes in your cluster have different types of NVIDIA GPUs, then you can use <a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/">Node Labels and Node Selectors</a> to schedule pods to appropriate nodes.</p><p>For example:</p><p><strong><em># Label your nodes with the accelerator type they have.</em></strong></p><p><strong>kubectl label nodes <code>&lt;node-with-k80&gt;</code> accelerator=nvidia-tesla-k80</strong></p><p><strong>kubectl label nodes <code>&lt;node-with-p100&gt;</code> accelerator=nvidia-tesla-p100</strong></p><p>Specify the GPU type in the pod spec:</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: cuda-vector-add</strong></p><p><strong>spec:</strong></p><p><strong>restartPolicy: OnFailure</strong></p><p><strong>containers:</strong></p><p><strong>- name: cuda-vector-add</strong></p><p><strong><em># <a href="https://github.com/kubernetes/kubernetes/blob/v1.7.11/test/images/nvidia-cuda/Dockerfile">https://github.com/kubernetes/kubernetes/blob/v1.7.11/test/images/nvidia-cuda/Dockerfile</a></em></strong></p><p><strong>image: &quot;k8s.gcr.io/cuda-vector-add:v0.1&quot;</strong></p><p><strong>resources:</strong></p><p><strong>limits:</strong></p><p><strong>nvidia.com/gpu: 1</strong></p><p><strong>nodeSelector:</strong></p><p><strong>accelerator: nvidia-tesla-p100 <em># or nvidia-tesla-k80 etc.</em></strong></p><p>This will ensure that the pod will be scheduled to a node that has the GPU type you specified.</p><h3>v1.6 and v1.7</h3><p>To enable GPU support in 1.6 and 1.7, a special <strong>alpha</strong> feature gate <strong>Accelerators</strong> has to be set to true across the system: <strong>--feature-gates=&quot;Accelerators=true&quot;</strong>. It also requires using the Docker Engine as the container runtime.</p><p>Further, the Kubernetes nodes have to be pre-installed with NVIDIA drivers. Kubelet will not detect NVIDIA GPUs otherwise.</p><p>When you start Kubernetes components after all the above conditions are true, Kubernetes will expose <strong>alpha.kubernetes.io/nvidia-gpu</strong> as a schedulable resource.</p><p>You can consume these GPUs from your containers by requesting <strong>alpha.kubernetes.io/nvidia-gpu</strong> just like you request <strong>cpu</strong> or <strong>memory</strong>. However, there are some limitations in how you specify the resource requirements when using GPUs:</p><ul><li>GPUs are only supposed to be specified in the <strong>limits</strong> section, which means:<ul><li>You can specify GPU <strong>limits</strong> without specifying <strong>requests</strong> because Kubernetes will use the limit as the request value by default.</li><li>You can specify GPU in both <strong>limits</strong> and <strong>requests</strong> but these two values must be equal.</li><li>You cannot specify GPU <strong>requests</strong> without specifying <strong>limits</strong>.</li></ul></li><li>Containers (and pods) do not share GPUs. There&#x27;s no overcommitting of GPUs.</li><li>Each container can request one or more GPUs. It is not possible to request a fraction of a GPU.</li></ul><p>When using <strong>alpha.kubernetes.io/nvidia-gpu</strong> as the resource, you also have to mount host directories containing NVIDIA libraries (libcuda.so, libnvidia.so etc.) to the container.</p><p>Here&#x27;s an example:</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>name: cuda-vector-add</strong></p><p><strong>spec:</strong></p><p><strong>restartPolicy: OnFailure</strong></p><p><strong>containers:</strong></p><p><strong>- name: cuda-vector-add</strong></p><p><strong><em># <a href="https://github.com/kubernetes/kubernetes/blob/v1.7.11/test/images/nvidia-cuda/Dockerfile">https://github.com/kubernetes/kubernetes/blob/v1.7.11/test/images/nvidia-cuda/Dockerfile</a></em></strong></p><p><strong>image: &quot;k8s.gcr.io/cuda-vector-add:v0.1&quot;</strong></p><p><strong>resources:</strong></p><p><strong>limits:</strong></p><p><strong>alpha.kubernetes.io/nvidia-gpu: 1 <em># requesting 1 GPU</em></strong></p><p><strong>volumeMounts:</strong></p><p><strong>- name: &quot;nvidia-libraries&quot;</strong></p><p><strong>mountPath: &quot;/usr/local/nvidia/lib64&quot;</strong></p><p><strong>volumes:</strong></p><p><strong>- name: &quot;nvidia-libraries&quot;</strong></p><p><strong>hostPath:</strong></p><p><strong>path: &quot;/usr/lib/nvidia-375&quot;</strong></p><p>The <strong>Accelerators</strong> feature gate and <strong>alpha.kubernetes.io/nvidia-gpu</strong> resource works on 1.8 and 1.9 as well. It will be deprecated in 1.10 and removed in 1.11.</p><h3>Future</h3><ul><li>Support for hardware accelerators in Kubernetes is still in alpha.</li><li>Better APIs will be introduced to provision and consume accelerators in a scalable manner.</li><li>Kubernetes will automatically ensure that applications consuming GPUs get the best possible performance.</li></ul><h2>Manage HugePages</h2><p><strong>FEATURE STATE:</strong> <strong>Kubernetes v1.10</strong> <a href="https://kubernetes.io/docs/tasks/manage-hugepages/scheduling-hugepages/">beta</a></p><p>Kubernetes supports the allocation and consumption of pre-allocated huge pages by applications in a Pod as a <strong>beta</strong> feature. This page describes how users can consume huge pages and the current limitations.</p><ul><li><a href="https://kubernetes.io/docs/tasks/manage-hugepages/scheduling-hugepages/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/manage-hugepages/scheduling-hugepages/#api"><strong>API</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/manage-hugepages/scheduling-hugepages/#future"><strong>Future</strong></a></li></ul><h3>Before you begin</h3><ol><li>Kubernetes nodes must pre-allocate huge pages in order for the node to report its huge page capacity. A node may only pre-allocate huge pages for a single size.</li></ol><p>The nodes will automatically discover and report all huge page resources as a schedulable resource.</p><h3>API</h3><p>Huge pages can be consumed via container level resource requirements using the resource name <strong>hugepages-<code>&lt;size&gt;</code></strong>, where size is the most compact binary notation using integer values supported on a particular node. For example, if a node supports 2048KiB page sizes, it will expose a schedulable resource <strong>hugepages-2Mi</strong>. Unlike CPU or memory, huge pages do not support overcommit.</p><p><strong>apiVersion: v1</strong></p><p><strong>kind: Pod</strong></p><p><strong>metadata:</strong></p><p><strong>generateName: hugepages-volume-</strong></p><p><strong>spec:</strong></p><p><strong>containers:</strong></p><p><strong>- image: fedora:latest</strong></p><p><strong>command:</strong></p><p><strong>- sleep</strong></p><p><strong>- inf</strong></p><p><strong>name: example</strong></p><p><strong>volumeMounts:</strong></p><p><strong>- mountPath: /hugepages</strong></p><p><strong>name: hugepage</strong></p><p><strong>resources:</strong></p><p><strong>limits:</strong></p><p><strong>hugepages-2Mi: 100Mi</strong></p><p><strong>volumes:</strong></p><p><strong>- name: hugepage</strong></p><p><strong>emptyDir:</strong></p><p><strong>medium: HugePages</strong></p><ul><li>Huge page requests must equal the limits. This is the default if limits are specified, but requests are not.</li><li>Huge pages are isolated at a pod scope, container isolation is planned in a future iteration.</li><li>EmptyDir volumes backed by huge pages may not consume more huge page memory than the pod request.</li><li>Applications that consume huge pages via <strong>shmget()</strong> with <strong>SHM_HUGETLB</strong> must run with a supplemental group that matches <strong>proc/sys/vm/hugetlb_shm_group</strong>.</li><li>Huge page usage in a namespace is controllable via ResourceQuota similar to other compute resources like <strong>cpu</strong> or <strong>memory</strong> using the <strong>hugepages-<code>&lt;size&gt;</code></strong> token.</li></ul><h3>Future</h3><ul><li>Support container isolation of huge pages in addition to pod isolation.</li><li>NUMA locality guarantees as a feature of quality of service.</li><li>LimitRange support.</li></ul><h2>Extend kubectl with plugins</h2><p><strong>FEATURE STATE:</strong> <strong>Kubernetes v1.10</strong> <a href="https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/">alpha</a></p><p>This guide shows you how to install and write extensions for <a href="https://kubernetes.io/docs/user-guide/kubectl/">kubectl</a>. Usually called plugins or binary extensions, this feature allows you to extend the default set of commands available in <strong>kubectl</strong> by adding new subcommands to perform new tasks and extend the set of features available in the main distribution of <strong>kubectl</strong>.</p><ul><li><a href="https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/#before-you-begin"><strong>Before you begin</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/#installing-kubectl-plugins"><strong>Installing kubectl plugins</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/#plugin-loader"><strong>Plugin loader</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/#search-order"><strong>Search order</strong></a></li></ul></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/#writing-kubectl-plugins"><strong>Writing kubectl plugins</strong></a><ul><li><a href="https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/#the-pluginyaml-descriptor"><strong>The plugin.yaml descriptor</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/#recommended-directory-structure"><strong>Recommended directory structure</strong></a></li><li><a href="https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/#accessing-runtime-attributes"><strong>Accessing runtime attributes</strong></a></li></ul></li><li><a href="https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/#whats-next"><strong>What&#x27;s next</strong></a></li></ul><h3>Before you begin</h3><p>You need to have a working <strong>kubectl</strong> binary installed. Note that plugins were officially introduced as an alpha feature in the v1.8.0 release. So, while some parts of the plugins feature were already available in previous versions, a <strong>kubectl</strong> version of 1.8.0 or later is recommended.</p><p>Until a GA version is released, plugins will only be available under the <strong>kubectl plugin</strong>subcommand.</p><h3>Installing kubectl plugins</h3><p>A plugin is nothing more than a set of files: at least a <strong>plugin.yaml</strong> descriptor, and likely one or more binary, script, or assets files. To install a plugin, copy those files to one of the locations in the filesystem where <strong>kubectl</strong> searches for plugins.</p><p>Note that Kubernetes does not provide a package manager or something similar to install or update plugins, so it&#x27;s your responsibility to place the plugin files in the correct location. We recommend that each plugin is located on its own directory, so installing a plugin that is distributed as a compressed file is as simple as extracting it to one of the locations specified in the <a href="https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/#plugin-loader">Plugin loader</a> section.</p><h4>Plugin loader</h4><p>The plugin loader is responsible for searching plugin files in the filesystem locations specified below, and checking if the plugin provides the minimum amount of information required for it to run. Files placed in the right location that don&#x27;t provide the minimum amount of information, for example an incomplete plugin.yaml descriptor, are ignored.</p><h5><strong>Search order</strong></h5><p>The plugin loader uses the following search order:</p><ol><li><strong>${KUBECTL_PLUGINS_PATH}</strong> If specified, the search stops here.</li><li><strong>${XDG_DATA_DIRS}/kubectl/plugins</strong></li><li><strong>~<!-- -->/.kube/plugins</strong></li></ol><p>If the <strong>KUBECTL_PLUGINS_PATH</strong> environment variable is present, the loader uses it as the only location to look for plugins. The <strong>KUBECTL_PLUGINS_PATH</strong> environment variable is a list of directories. In Linux and Mac, the list is colon-delimited. In Windows, the list is semicolon-delimited.</p><p>If <strong>KUBECTL_PLUGINS_PATH</strong> is not present, the loader searches these additional locations:</p><p>First, one or more directories specified according to the <a href="https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html">XDG System Directory Structure</a>specification. Specifically, the loader locates the directories specified by the <strong>XDG_DATA_DIRS</strong>environment variable, and then searches <strong>kubectl/plugins</strong> directory inside of those. If <strong>XDG_DATA_DIRS</strong> is not specified, it defaults to <strong>/usr/local/share:/usr/share</strong>.</p><p>Second, the <strong>plugins</strong> directory under the user&#x27;s kubeconfig dir. In most cases, this is <strong>~<!-- -->/.kube/plugins</strong>.</p><p><strong><em># Loads plugins from both /path/to/dir1 and /path/to/dir2</em></strong></p><p><strong>KUBECTL_PLUGINS_PATH=/path/to/dir1:/path/to/dir2 kubectl plugin -h</strong></p><h3>Writing kubectl plugins</h3><p>You can write a plugin in any programming language or script that allows you to write command-line commands. A plugin does not necessarily need to have a binary component. It could rely entirely on operating system utilities like <strong>echo</strong>, <strong>sed</strong>, or <strong>grep</strong>. Or it could rely on the <strong>kubectl</strong> binary.</p><p>The only strong requirement for a <strong>kubectl</strong> plugin is the <strong>plugin.yaml</strong> descriptor file. This file is responsible for declaring at least the minimum attributes required to register a plugin and must be located under one of the locations specified in the <a href="https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/#search-order">Search order</a> section.</p><h4>The plugin.yaml descriptor</h4><p>The descriptor file supports the following attributes:</p><p><strong>name: &quot;targaryen&quot; # REQUIRED: the plugin command name, to be invoked under \&#x27;kubectl\&#x27;</strong></p><p><strong>shortDesc: &quot;Dragonized plugin&quot; # REQUIRED: the command short description, for help</strong></p><p><strong>longDesc: &quot;&quot; # the command long description, for help</strong></p><p><strong>example: &quot;&quot; # command example(s), for help</strong></p><p><strong>command: &quot;./dracarys&quot; # REQUIRED: the command, binary, or script to invoke when running the plugin</strong></p><p><strong>flags: # flags supported by the plugin</strong></p><p><strong>- name: &quot;heat&quot; # REQUIRED for each flag: flag name</strong></p><p><strong>shorthand: &quot;h&quot; # short version of the flag name</strong></p><p><strong>desc: &quot;Fire heat&quot; # REQUIRED for each flag: flag description</strong></p><p><strong>defValue: &quot;extreme&quot; # default value of the flag</strong></p><p><strong>tree: # allows the declaration of subcommands</strong></p><p><strong>- <!-- -->.<!-- -->.. # subcommands support the same set of attributes</strong></p><p>The preceding descriptor declares the <strong>kubectl plugin targaryen</strong> plugin, which has one flag named <strong>-h | --heat</strong>. When the plugin is invoked, it calls the <strong>dracarys</strong> binary or script, which is located in the same directory as the descriptor file. The <a href="https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/#accessing-runtime-attributes">Accessing runtime attributes</a> section describes how the <strong>dracarys</strong> command accesses the flag value and other runtime context.</p><h4>Recommended directory structure</h4><p>It is recommended that each plugin has its own subdirectory in the filesystem, preferably with the same name as the plugin command. The directory must contain the <strong>plugin.yaml</strong> descriptor and any binary, script, asset, or other dependency it might require.</p><p>For example, the directory structure for the <strong>targaryen</strong> plugin could look like this:</p><p><strong>~<!-- -->/.kube/plugins/</strong></p><p><strong>└── targaryen</strong></p><p><strong>├── plugin.yaml</strong></p><p><strong>└── dracarys</strong></p><h4>Accessing runtime attributes</h4><p>In most use cases, the binary or script file you write to support the plugin must have access to some contextual information provided by the plugin framework. For example, if you declared flags in the descriptor file, your plugin must have access to the user-provided flag values at runtime. The same is true for global flags. The plugin framework is responsible for doing that, so plugin writers don&#x27;t need to worry about parsing arguments. This also ensures the best level of consistency between plugins and regular <strong>kubectl</strong> commands.</p><p>Plugins have access to runtime context attributes through environment variables. So to access the value provided through a flag, for example, just look for the value of the proper environment variable using the appropriate function call for your binary or script.</p><p>The supported environment variables are:</p><ul><li><strong>KUBECTL_PLUGINS_CALLER</strong>: The full path to the <strong>kubectl</strong> binary that was used in the current command invocation. As a plugin writer, you don&#x27;t have to implement logic to authenticate and access the Kubernetes API. Instead, you can invoke <strong>kubectl</strong> to obtain the information you need, through something like <strong>kubectl get --raw=/apis</strong>.</li><li><strong>KUBECTL_PLUGINS_CURRENT_NAMESPACE</strong>: The current namespace that is the context for this call. This is the actual namespace to be used, meaning it was already processed in terms of the precedence between what was provided through the kubeconfig, the <strong>--namespace</strong> global flag, environment variables, and so on.</li><li><strong>KUBECTL_PLUGINS_DESCRIPTOR<!-- -->_<!-- -->*</strong>: One environment variable for every attribute declared in the <strong>plugin.yaml</strong> descriptor. For example, <strong>KUBECTL_PLUGINS_DESCRIPTOR_NAME</strong>, <strong>KUBECTL_PLUGINS_DESCRIPTOR_COMMAND</strong>.</li><li><strong>KUBECTL_PLUGINS_GLOBAL_FLAG<!-- -->_<!-- -->*</strong>: One environment variable for every global flag supported by <strong>kubectl</strong>. For example, <strong>KUBECTL_PLUGINS_GLOBAL_FLAG_NAMESPACE</strong>, <strong>KUBECTL_PLUGINS_GLOBAL_FLAG_V</strong>.</li><li><strong>KUBECTL_PLUGINS_LOCAL_FLAG<!-- -->_<!-- -->*</strong>: One environment variable for every local flag declared in the <strong>plugin.yaml</strong> descriptor. For example, <strong>KUBECTL_PLUGINS_LOCAL_FLAG_HEAT</strong> in the preceding <strong>targaryen</strong> example.</li></ul><h3>What&#x27;s next</h3><ul><li>Check the repository for <a href="https://github.com/kubernetes/kubernetes/tree/master/pkg/kubectl/plugins/examples">some more examples</a> of plugins.</li><li>In case of any questions, feel free to reach out to the <a href="https://github.com/kubernetes/community/tree/master/sig-cli">CLI SIG team</a>.</li><li>Binary plugins is still an alpha feature, so this is the time to contribute ideas and improvements to the codebase. We&#x27;re also excited to hear about what you&#x27;re planning to implement with plugins, so <a href="https://github.com/kubernetes/community/tree/master/sig-cli">let us know</a>!</li></ul><h1>kubectl Cheat Sheet</h1><h2>Kubectl Autocomplete</h2><p>$ source &lt;(kubectl completion bash) <em># setup autocomplete in bash, bash-completion package should be installed first.</em></p><p>$ source &lt;(kubectl completion zsh) <em># setup autocomplete in zsh</em></p><h2>Kubectl Context and Configuration</h2><p>Set which Kubernetes cluster <strong>kubectl</strong> communicates with and modifies configuration information. See <a href="https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/">Authenticating Across Clusters with kubeconfig</a> documentation for detailed config file information.</p><p>$ kubectl config view <em># Show Merged kubeconfig settings.</em></p><h1>use multiple kubeconfig files at the same time and view merged config</h1><p>$ KUBECONFIG=<!-- -->~<!-- -->/.kube/config:<!-- -->~<!-- -->/.kube/kubconfig2 kubectl config view</p><h1>Get the password for the e2e user</h1><p>$ kubectl config view -o jsonpath=\&#x27;{.users<!-- -->[?(@.name == &quot;e2e&quot;)]<!-- -->.user.password}\&#x27;</p><p>$ kubectl config current-context <em># Display the current-context</em></p><p>$ kubectl config use-context my-cluster-name <em># set the default context to my-cluster-name</em></p><h1>add a new cluster to your kubeconf that supports basic auth</h1><p>$ kubectl config set-credentials kubeuser/foo.kubernetes.com --username=kubeuser --password=kubepassword</p><h1>set a context utilizing a specific username and namespace.</h1><p>$ kubectl config set-context gce --user=cluster-admin --namespace=foo <!-- -->\</p><p>&amp;&amp; kubectl config use-context gce</p><h2>Creating Objects</h2><p>Kubernetes manifests can be defined in json or yaml. The file extension <strong>.yaml</strong>, <strong>.yml</strong>, and <strong>.json</strong>can be used.</p><p>$ kubectl create -f ./my-manifest.yaml <em># create resource(s)</em></p><p>$ kubectl create -f ./my1.yaml -f ./my2.yaml <em># create from multiple files</em></p><p>$ kubectl create -f ./dir # create resource(s) in all manifest files in dir</p><p>$ kubectl create -f <a href="https://git.io/vPieo">https://git.io/vPieo</a> # create resource(s) from url</p><p>$ kubectl run nginx --image=nginx # start a single instance of nginx</p><p>$ kubectl explain pods,svc <em># get the documentation for pod and svc manifests</em></p><h1>Create multiple YAML objects from stdin</h1><p>$ cat &lt;&lt;EOF | kubectl create -f -</p><p>apiVersion: v1</p><p>kind: Pod</p><p>metadata:</p><p>name: busybox-sleep</p><p>spec:</p><p>containers:</p><ul><li>name: busybox</li></ul><p>image: busybox</p><p>args:</p><ul><li><p>sleep</p></li><li><p>&quot;1000000&quot;</p></li></ul><hr/><p>apiVersion: v1</p><p>kind: Pod</p><p>metadata:</p><p>name: busybox-sleep-less</p><p>spec:</p><p>containers:</p><ul><li>name: busybox</li></ul><p>image: busybox</p><p>args:</p><ul><li><p>sleep</p></li><li><p>&quot;1000&quot;</p></li></ul><p>EOF</p><h1>Create a secret with several keys</h1><p>$ cat &lt;&lt;EOF | kubectl create -f -</p><p>apiVersion: v1</p><p>kind: Secret</p><p>metadata:</p><p>name: mysecret</p><p>type: Opaque</p><p>data:</p><p>password: $(echo -n &quot;s33msi4&quot; | base64)</p><p>username: $(echo -n &quot;jane&quot; | base64)</p><p>EOF</p><h2>Viewing, Finding Resources</h2><h1>Get commands with basic output</h1><p>$ kubectl get services <em># List all services in the namespace</em></p><p>$ kubectl get pods --all-namespaces <em># List all pods in all namespaces</em></p><p>$ kubectl get pods -o wide <em># List all pods in the namespace, with more details</em></p><p>$ kubectl get deployment my-dep <em># List a particular deployment</em></p><p>$ kubectl get pods --include-uninitialized <em># List all pods in the namespace, including uninitialized ones</em></p><h1>Describe commands with verbose output</h1><p>$ kubectl describe nodes my-node</p><p>$ kubectl describe pods my-pod</p><p>$ kubectl get services --sort-by=.metadata.name <em># List Services Sorted by Name</em></p><h1>List pods Sorted by Restart Count</h1><p><strong>$ kubectl get pods --sort-by=\&#x27;.status.containerStatuses<!-- -->[0]<!-- -->.restartCount\&#x27;</strong></p><p><strong># Get the version label of all pods with label app=cassandra</strong></p><p><strong>$ kubectl get pods --selector=app=cassandra rc -o <!-- -->\</strong></p><p><strong>jsonpath=\&#x27;{.items<!-- -->[*]<!-- -->.metadata.labels.version}\&#x27;</strong></p><p><strong># Get all running pods in the namespace</strong></p><p><strong>$ kubectl get pods --field-selector=status.phase=Running</strong></p><p><strong># Get ExternalIPs of all nodes</strong></p><p><strong>$ kubectl get nodes -o jsonpath=\&#x27;{.items<!-- -->[*]<!-- -->.status.addresses<!-- -->[?(@.type==&quot;ExternalIP&quot;)]<!-- -->.address}\&#x27;</strong></p><p><strong># List Names of Pods that belong to Particular RC</strong></p><p><strong># &quot;jq&quot; command useful for transformations that are too complex for jsonpath, it can be found at <a href="https://stedolan.github.io/jq/">https://stedolan.github.io/jq/</a></strong></p><p><strong>$ sel=${$(kubectl get rc my-rc --output=json | jq -j \&#x27;.spec.selector | to_entries | .[] | &quot;<!-- -->\<!-- -->(.key)=<!-- -->\<!-- -->(.value),&quot;\&#x27;)%?}</strong></p><p><strong>$ echo $(kubectl get pods --selector=$sel --output=jsonpath={.items..metadata.name})</strong></p><p><strong># Check which nodes are ready</strong></p><p><strong>$ JSONPATH=\&#x27;{range .items<!-- -->[*]<!-- -->}{@.metadata.name}:{range @.status.conditions<!-- -->[*]<!-- -->}{@.type}={@.status};{end}{end}\&#x27; <!-- -->\</strong></p><p><strong>&amp;&amp; kubectl get nodes -o jsonpath=&quot;$JSONPATH&quot; | grep &quot;Ready=True&quot;</strong></p><p><strong># List all Secrets currently in use by a pod</strong></p><p><strong>$ kubectl get pods -o json | jq \&#x27;.items[].spec.containers[].env[]?.valueFrom.secretKeyRef.name\&#x27; | grep -v null | sort | uniq</strong></p><p><strong># List Events sorted by timestamp</strong></p><p><strong>$ kubectl get events --sort-by=.metadata.creationTimestamp</strong></p><h2>Updating Resources</h2><p><strong>$ kubectl rolling-update frontend-v1 -f frontend-v2.json <em># Rolling update pods of frontend-v1</em></strong></p><p><strong>$ kubectl rolling-update frontend-v1 frontend-v2 --image=image:v2 <em># Change the name of the resource and update the image</em></strong></p><p><strong>$ kubectl rolling-update frontend --image=image:v2 <em># Update the pods image of frontend</em></strong></p><p><strong>$ kubectl rolling-update frontend-v1 frontend-v2 --rollback <em># Abort existing rollout in progress</em></strong></p><p><strong>$ cat pod.json | kubectl replace -f - <em># Replace a pod based on the JSON passed into stdin</em></strong></p><p><strong># Force replace, delete and then re-create the resource. Will cause a service outage.</strong></p><p><strong>$ kubectl replace --force -f ./pod.json</strong></p><p><strong># Create a service for a replicated nginx, which serves on port 80 and connects to the containers on port 8000</strong></p><p><strong>$ kubectl expose rc nginx --port=80 --target-port=8000</strong></p><p><strong># Update a single-container pod\&#x27;s image version (tag) to v4</strong></p><p><strong>$ kubectl get pod mypod -o yaml | sed \&#x27;s/<!-- -->\<!-- -->(image: myimage<!-- -->\<!-- -->):.<!-- -->*<!-- -->$/<!-- -->\<!-- -->1:v4/\&#x27; | kubectl replace -f -</strong></p><p><strong>$ kubectl label pods my-pod new-label=awesome <em># Add a Label</em></strong></p><p><strong>$ kubectl annotate pods my-pod icon-url=<a href="http://goo.gl/XXBTWq">http://goo.gl/XXBTWq</a> <em># Add an annotation</em></strong></p><p><strong>$ kubectl autoscale deployment foo --min=2 --max=10 <em># Auto scale a deployment &quot;foo&quot;</em></strong></p><h2>Patching Resources</h2><p>$ kubectl patch node k8s-node-1 -p \&#x27;{&quot;spec&quot;:{&quot;unschedulable&quot;:true}}\&#x27; <em># Partially update a node</em></p><h1>Update a container\&#x27;s image; spec.containers<!-- -->[*]<!-- -->.name is required because it\&#x27;s a merge key</h1><p>$ kubectl patch pod valid-pod -p \&#x27;{&quot;spec&quot;:{&quot;containers&quot;:<!-- -->[{&quot;name&quot;:&quot;kubernetes-serve-hostname&quot;,&quot;image&quot;:&quot;new image&quot;}]<!-- -->}}\&#x27;</p><h1>Update a container\&#x27;s image using a json patch with positional arrays</h1><p>$ kubectl patch pod valid-pod --type=\&#x27;json\&#x27; -p=\&#x27;<!-- -->[{&quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/spec/containers/0/image&quot;, &quot;value&quot;:&quot;new image&quot;}]<!-- -->\&#x27;</p><h1>Disable a deployment livenessProbe using a json patch with positional arrays</h1><p>$ kubectl patch deployment valid-deployment --type json -p=\&#x27;<!-- -->[{&quot;op&quot;: &quot;remove&quot;, &quot;path&quot;: &quot;/spec/template/spec/containers/0/livenessProbe&quot;}]<!-- -->\&#x27;</p><h1>Add a new element to a positional array</h1><p>$ kubectl patch sa default --type=\&#x27;json\&#x27; -p=\&#x27;<!-- -->[{&quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/secrets/1&quot;, &quot;value&quot;: {&quot;name&quot;: &quot;whatever&quot; } }]<!-- -->\&#x27;</p><h2>Editing Resources</h2><p>The edit any API resource in an editor.</p><p>$ kubectl edit svc/docker-registry <em># Edit the service named docker-registry</em></p><p>$ KUBE_EDITOR=&quot;nano&quot; kubectl edit svc/docker-registry <em># Use an alternative editor</em></p><h2>Scaling Resources</h2><p><strong>$ kubectl scale --replicas=3 rs/foo <em># Scale a replicaset named \&#x27;foo\&#x27; to 3</em></strong></p><p><strong>$ kubectl scale --replicas=3 -f foo.yaml <em># Scale a resource specified in &quot;foo.yaml&quot; to 3</em></strong></p><p><strong>$ kubectl scale --current-replicas=2 --replicas=3 deployment/mysql <em># If the deployment named mysql\&#x27;s current size is 2, scale mysql to 3</em></strong></p><p><strong>$ kubectl scale --replicas=5 rc/foo rc/bar rc/baz <em># Scale multiple replication controllers</em></strong></p><h2>Deleting Resources</h2><p>$ kubectl delete -f ./pod.json <em># Delete a pod using the type and name specified in pod.json</em></p><p>$ kubectl delete pod,service baz foo <em># Delete pods and services with same names &quot;baz&quot; and &quot;foo&quot;</em></p><p>$ kubectl delete pods,services -l name=myLabel <em># Delete pods and services with label name=myLabel</em></p><p>$ kubectl delete pods,services -l name=myLabel --include-uninitialized <em># Delete pods and services, including uninitialized ones, with label name=myLabel</em></p><p>$ kubectl -n my-ns delete po,svc --all <em># Delete all pods and services, including uninitialized ones, in namespace my-ns,</em></p><h2>Interacting with running Pods</h2><p>$ kubectl logs my-pod <em># dump pod logs (stdout)</em></p><p>$ kubectl logs my-pod -c my-container <em># dump pod container logs (stdout, multi-container case)</em></p><p>$ kubectl logs -f my-pod <em># stream pod logs (stdout)</em></p><p>$ kubectl logs -f my-pod -c my-container <em># stream pod container logs (stdout, multi-container case)</em></p><p>$ kubectl run -i --tty busybox --image=busybox -- sh <em># Run pod as interactive shell</em></p><p>$ kubectl attach my-pod -i <em># Attach to Running Container</em></p><p>$ kubectl port-forward my-pod 5000:6000 <em># Listen on port 5000 on the local machine and forward to port 6000 on my-pod</em></p><p>$ kubectl exec my-pod -- ls / <em># Run command in existing pod (1 container case)</em></p><p>$ kubectl exec my-pod -c my-container -- ls / <em># Run command in existing pod (multi-container case)</em></p><p>$ kubectl top pod POD_NAME --containers <em># Show metrics for a given pod and its containers</em></p><h2>Interacting with Nodes and Cluster</h2><p>$ kubectl cordon my-node <em># Mark my-node as unschedulable</em></p><p>$ kubectl drain my-node <em># Drain my-node in preparation for maintenance</em></p><p>$ kubectl uncordon my-node <em># Mark my-node as schedulable</em></p><p>$ kubectl top node my-node <em># Show metrics for a given node</em></p><p>$ kubectl cluster-info <em># Display addresses of the master and services</em></p><p>$ kubectl cluster-info dump <em># Dump current cluster state to stdout</em></p><p>$ kubectl cluster-info dump --output-directory=/path/to/cluster-state <em># Dump current cluster state to /path/to/cluster-state</em></p><h1>If a taint with that key and effect already exists, its value is replaced as specified.</h1><p>$ kubectl taint nodes foo dedicated=special-user:NoSchedule</p><h2>Resource types</h2><p>The following table includes a list of all the supported resource types and their abbreviated aliases:</p><hr/><p>  Resource type                Abbreviated alias
all                           
certificatesigningrequests   <strong>csr</strong>
clusterrolebindings           
clusterroles                  
componentstatuses            <strong>cs</strong>
configmaps                   <strong>cm</strong>
controllerrevisions           
cronjobs                      
customresourcedefinition     <strong>crd</strong>, <strong>crds</strong>
daemonsets                   <strong>ds</strong>
deployments                  <strong>deploy</strong>
endpoints                    <strong>ep</strong>
events                       <strong>ev</strong>
horizontalpodautoscalers     <strong>hpa</strong>
ingresses                    <strong>ing</strong>
jobs                          
limitranges                  <strong>limits</strong>
namespaces                   <strong>ns</strong>
networkpolicies              <strong>netpol</strong>
nodes                        <strong>no</strong>
persistentvolumeclaims       <strong>pvc</strong>
persistentvolumes            <strong>pv</strong>
poddisruptionbudgets         <strong>pdb</strong>
podpreset                     
pods                         <strong>po</strong>
podsecuritypolicies          <strong>psp</strong>
podtemplates                  
replicasets                  <strong>rs</strong>
replicationcontrollers       <strong>rc</strong>
resourcequotas               <strong>quota</strong>
rolebindings                  
roles                         
secrets                       
serviceaccount               <strong>sa</strong>
services                     <strong>svc</strong>
statefulsets                 <strong>sts</strong>
storageclasses               <strong>sc</strong></p><hr/><h3>Formatting output</h3><p>To output details to your terminal window in a specific format, you can add either the <strong>-o</strong> or <strong>-output</strong> flags to a supported <strong>kubectl</strong> command.</p><hr/><p>  Output format                         Description
<code style="background-color:lightgray">-o=custom-columns=&lt;spec&gt;</code>            Print a table using a comma separated list of custom columns
<code style="background-color:lightgray">-o=custom-columns-file=&lt;filename&gt;</code>   Print a table using the custom columns template in the <code style="background-color:lightgray">&lt;filename&gt;</code> file
<code style="background-color:lightgray">-o=json</code>                               Output a JSON formatted API object
<code style="background-color:lightgray">-o=jsonpath=&lt;template&gt;</code>              Print the fields defined in a <a href="https://kubernetes.io/docs/reference/kubectl/jsonpath">jsonpath</a> expression
<code style="background-color:lightgray">-o=jsonpath-file=&lt;filename&gt;</code>         Print the fields defined by the <a href="https://kubernetes.io/docs/reference/kubectl/jsonpath">jsonpath</a> expression in the <code style="background-color:lightgray">&lt;filename&gt;</code> file
<code style="background-color:lightgray">-o=name</code>                               Print only the resource name and nothing else
<code style="background-color:lightgray">-o=wide</code>                               Output in the plain-text format with any additional information, and for pods, the node name is included
<code style="background-color:lightgray">-o=yaml</code>                               Output a YAML formatted API object</p><hr/><h3>Kubectl output verbosity and debugging</h3><p>Kubectl verbosity is controlled with the <strong>-v</strong> or <strong>--v</strong> flags followed by an integer representing the log level. General Kubernetes logging conventions and the associated log levels are described <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/logging.md">here</a>.</p><hr/><p>  Verbosity   Description
--v=0      Generally useful for this to ALWAYS be visible to an operator.
--v=1      A reasonable default log level if you don&#x27;t want verbosity.
--v=2      Useful steady state information about the service and important log messages that may correlate to significant changes in the system. This is the recommended default log level for most systems.
--v=3      Extended information about changes.
--v=4      Debug level verbosity.
--v=6      Display requested resources.
--v=7      Display HTTP request headers.
--v=8      Display HTTP request contents.
--v=9      Display HTTP request contents without truncation of contents.</p><hr/></div></div></div><div id="gatsby-announcer" style="position:absolute;top:0;width:1px;height:1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border:0" aria-live="assertive" aria-atomic="true"></div></div><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/kubernetes/";/*]]>*/</script><script id="gatsby-chunk-mapping">/*<![CDATA[*/window.___chunkMapping={"polyfill":["/polyfill-e4ff66ed5ec0b4bd2598.js"],"app":["/app-eeb4193423903187a951.js"],"component---src-pages-404-tsx":["/component---src-pages-404-tsx-5f7f07bdd531e2b5021f.js"],"component---src-pages-index-tsx":["/component---src-pages-index-tsx-e4ae0f7d9ba5aaf1fdc1.js"],"component---src-templates-blog-post-template-tsx":["/component---src-templates-blog-post-template-tsx-a3788de39add0692b44b.js"]};/*]]>*/</script><script src="/personal-blog/polyfill-e4ff66ed5ec0b4bd2598.js" nomodule=""></script><script src="/personal-blog/component---src-templates-blog-post-template-tsx-a3788de39add0692b44b.js" async=""></script><script src="/personal-blog/ce5c1ec6b0c5670e22550a7ef5fd5c2de8a4bdeb-f3cc52c60397f69658d6.js" async=""></script><script src="/personal-blog/app-eeb4193423903187a951.js" async=""></script><script src="/personal-blog/framework-2601ed29d039b1458055.js" async=""></script><script src="/personal-blog/webpack-runtime-500034a3e408444d0cae.js" async=""></script></body></html>